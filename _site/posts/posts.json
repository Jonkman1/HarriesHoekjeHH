[
  {
    "path": "posts/2022-05-31-van-distill-naar-quarto/",
    "title": "Van distill naar quarto?",
    "description": "Een blog maken met quarto.",
    "author": [
      {
        "name": "Danielle Navarro bewerking HarrieJonkman",
        "url": {}
      }
    ],
    "date": "2022-05-31",
    "categories": [],
    "contents": "\r\n\r\nEen jaar geleden besloot Danielle Navarro om weer te gaan bloggen en\r\ndeze blog zette ze op: blog van\r\nDanielle Navarro. Ze koos voor distill, haar keuze.\r\nDestijds heeft zij een bewuste keuze gemaakt om distill te gebruiken als\r\nmijn blogging platform in plaats van een statische site generator zoals\r\nhugo of jekyll en ze heeft geen spijt van die\r\nkeuze. Gaandeweg vond zij echter een paar dingen die haar dwars zaten\r\nbij het gebruik van distill. Het is echter nooit de moeite waard geweest\r\nom te overwegen over te stappen op iets nieuws omdat distill zoveel\r\ndingen heeft die zij waardeert. Tot nu toe dan.\r\nNu komt quarto binnen. Ook ik werk\r\nal enige tijd met distill. De ervaringen van Danielle\r\nherken ik sterk en daarom schrijf ik vanaf nu vanuit Danielle, maar kun\r\nje ook mijzelf lezen.\r\nQuarto, volop in de belangstelling nu, biedt de belofte van een\r\ncross-platform, overall format, open source publicatietool gebaseerd op\r\npandoc. Geïntrigeerd besloot ik er een tijdje mee te spelen, en\r\nuiteindelijk nam ik de beslissing om mijn blog over te zetten van\r\ndistill naar quarto. Deze post schetst mijn proces.\r\n(Ik ben een beetje nerveus: een blog overzetten betekent vaak dingen\r\nopnieuw programmeren. Zal het werken? Zal alles reproduceerbaar blijken\r\nte zijn? Ik hoop het…)\r\nAan de slag\r\nHet allereerste wat ik doe is Alison Hill’s prachtige We\r\ndon’t talk about quarto lezen. Als je een R markdown gebruiker bent\r\ndie overweegt de sprong naar quarto te maken en haar samenvatting nog\r\nniet gelezen hebt, zul je er geen spijt van krijgen dat nu wel te doen.\r\nHet is een mooi overzicht op hoog niveau. Ik raad ook Nick\r\nTierney’s notities aan over het maken van de overstap, die zijn ook\r\nerg behulpzaam. (Zelf dus ook Danielle Navarro’s blog goed gelezen en\r\nbewerkt en de presentatie van Mine\r\nCetinkaya-Rundel gaf interessant.\r\nNa het eigen maken van deze achtergrondinformatie, ga ik naar de get started pagina op de\r\nquarto website om het installatiebestand te downloaden.\r\nNu ik quarto geïnstalleerd heb, ben ik in staat om het te gebruiken\r\nom een blog te maken. Mijn oude distill blog bestaat in een project map\r\ndie ik Harrie's Hoekje heb genoemd, dus ik besluit de\r\nquarto versie te maken en de map HHquarto te maken.\r\nEr is een pagina op de quarto website die je door het proces leidt\r\nvoor creating a\r\nblog blog, die ik plichtsgetrouw volg. Vanaf de terminal\r\n(Power-shell voor Windows) gebruik ik het\r\nquarto create-project commando, en er worden verschillende\r\nbestanden aangemaakt:\r\nquarto create-project quarto-blog --type website:blog\r\nCreating project at /home/danielle/GitHub/sites/quarto-blog:\r\n  - Created _quarto.yml\r\n  - Created index.qmd\r\n  - Created posts/welcome/index.qmd\r\n  - Created posts/post-with-code/index.qmd\r\n  - Created about.qmd\r\n  - Created styles.css\r\n  - Created posts/_metadata.yml\r\nKomende van een R markdown achtergrond, is dit erg vertrouwd:\r\nDe bestanden met een .qmd extensie zijn de quarto\r\nmarkdown documenten. Deze bevatten broncode voor de blog posts (de twee\r\nbestanden in de posts map), de home page (het\r\nindex.qmd bestand in de project root map) en een standalone\r\n“over mij” pagina voor de blog (het about.qmd\r\nbestand).\r\nDe bestanden met een .yml extensie zijn de YAML\r\nbestanden die gebruikt worden om het blog te configureren. Dit valt in\r\neerste instantie niet op, maar het feit dat het er twee zijn is wel\r\nbelangrijk. Het _quarto.yml bestand wordt gebruikt voor\r\ninstellingen die voor de hele site gelden, maar je zult vaak\r\ninstellingen willen configureren die alleen voor je blog posts gelden.\r\nDeze kunnen worden ingesteld door het posts/_metadata.yml\r\nbestand te bewerken.\r\nHet styles.css bestand kan gebruikt worden om CSS\r\nregels op te geven die voor de hele site gelden. Ik zal later meer\r\nvertellen over stijlen.\r\nBlog posts renderen\r\nEr zijn verschillende manieren om met quarto te werken. Bijvoorbeeld,\r\nlater in de post zal ik het hebben over de\r\nquarto commando-regel interface die je toestaat om met quarto te werken\r\nzonder door R of RStudio te gaan. Echter, als ik begin probeer ik de\r\ndingen eenvoudig te houden en ga ik voor de optie die mij het meest\r\nvertrouwd is: Ik gebruik RStudio.\r\nOm dit te doen, is het handig om een RStudio project te hebben voor\r\nmijn blog. Met behulp van het RStudio bestandsmenu, maak ik een nieuw\r\nproject vanuit een bestaande directory (d.w.z. mijn\r\nHHquarto folder), die het HHquarto.Rproj\r\nbestand en andere infrastructuur levert die nodig is om met mijn nieuwe\r\nquarto blog te werken als een RStudio project. Als dat eenmaal gedaan\r\nis, kan ik een quarto bestand openen in de RStudio editor en zie ik een\r\nvertrouwd ogende interface:\r\n\r\n\r\n\r\nFigure 1: Een blog post geschreven in quarto markdown open\r\nin de RStudio editor. Merk op dat op de plaats waar je normaal de ‘Knit’\r\nknop zou verwachten voor een R markdown document, er een ‘Render’ knop\r\nis. Die heeft dezelfde functie en is toegewezen aan dezelfde sneltoetsen\r\nals de knop ’Knit\r\n\r\n\r\n\r\nVan hieruit kan ik op de “Render” knop klikken om een enkele pagina\r\nte renderen, of ik kan naar het RStudio menu gaan en de “Render Project”\r\noptie selecteren om de hele site te bouwen. Standaard wordt de blog\r\ngebouwd in de _site map.\r\nAan het spelen\r\nVooruitgang! Ik maak vooruitgang. Maar voordat ik andere praktische\r\ndingen ga doen, heb ik eerst nog wat belangrijke zaken te regelen: wat\r\nrondspelen. Doelloos de functionaliteit van een nieuw gereedschap\r\nverkennen is altijd leuk en ik vind het een goede manier om mezelf met\r\niets vertrouwd te maken. Ik ben al redelijk vertrouwd met R markdown en\r\nik veronderstel dat de meeste lezers van deze post dat ook zullen zijn,\r\ndus voor het grootste deel zijn er geen verrassingen. Toch is het de\r\nmoeite waard om mezelf de gebruikelijke vragen te stellen:\r\nKan ik voetnoten toevoegen? 1\r\nKunnen ze genest worden? 2\r\nKan ik commentaar in de kantlijn toevoegen?\r\n\r\nA comment in the margin\r\n\r\nAls je kijkt naar de quarto\r\narticle layout documentation, ontdek ik enkele aardige kenmerken. Je\r\nkunt de :::{.class} notatie om een deel van de ‘CSS class’\r\ntoe te passen op de output, zoals hier:\r\n:::{.column-margin}\r\nA comment in the margin\r\n:::\r\nDe .column-margin code voor tekst in de kantlijn, maar\r\ner zijn verschillende andere commando’s die handig zijn als je plaatjes\r\nwilt afbeelden in de blog posts:\r\n.column-body overspant de gebruikelijke breedte van de\r\npost\r\n.column-body-outset strekt zich iets uit buiten de\r\ngebruikelijke breedte\r\n.column-page overspant de hele pagina (inclusief beide\r\nkantlijnen)\r\n.column-screen overspant de breedte van het scherm\r\n.column-screen-inset code stopt net voor de volledige\r\nschermbreedte\r\nJe kunt deze instellen binnen een chunk-optie. Bijvoorbeeld, als je\r\ncolumn: margin als chunk-optie instelt, krijgt de uitvoer\r\neen .column-margin code en de resulterende figuur\r\nverschijnt in de marge in plaats van onder de code. Op dezelfde manier\r\nzal het instellen van column: screen als chunk optie de\r\nuitvoer een .column-screen klasse geven en de uitvoer zal\r\nde volledige breedte beslaan. Hier is een eenvoudig voorbeeld, gebaseerd\r\nop het voorbeeld in de quarto documentatie:\r\n\r\n\r\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"https://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"https://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addMarkers\",\"args\":[-33.85943,151.22251,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},null,null,null,null,\"Mrs Macquarie's Chair\",{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addProviderTiles\",\"args\":[\"CartoDB.Positron\",null,null,{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]}],\"limits\":{\"lat\":[-33.85943,-33.85943],\"lng\":[151.22251,151.22251]}},\"evals\":[],\"jsHooks\":[]}\r\n\r\nIk moet toegeven, ik ben al een beetje verliefd.\r\nOpmerkingen over de YAML\r\nkoppen\r\nDe YAML koppen die gebruikt worden voor blogposts zijn een beetje\r\nanders in quarto dan hun equivalenten in\r\ndistill waren, en het kost me even om uit te zoeken hoe ik\r\nde YAML headers van mijn oude R markdown posts moet aanpassen voor de\r\nnieuwe quarto blog. Hier is een kort overzicht. Ten eerste, sommige\r\nvelden vereisen bijna geen veranderingen:\r\nHet title veld is ongewijzigd. Dat was een\r\nmakkelijke!\r\nHet date veld is in essentie ongewijzigd, behalve het\r\nfeit dat er een kleine bug lijkt te zijn in datum parsing voor blogs\r\nwaarvan ik zeker weet dat die snel zal verdwijnen. Als je ISO-8601\r\ndatumformaten gebruikt zoals date: \"2022-04-20\" zal het\r\ngoed gaan. 3\r\nHet categorieën veld neemt een lijst van waarden, die\r\n(denk ik?) niet anders is dan hoe het er eerder uitzag. Om eerlijk te\r\nzijn weet ik het niet meer omdat mijn oude blog ze niet gebruikte. Ik\r\nben er nu mee begonnen.\r\nAndere veranderingen zijn kunstmatig: - Het description\r\nveld dat ik gebruikte op mijn oude blog doet nog steeds wat het deed:\r\nhet geeft een preview tekst op de listing pagina en een samenvatting\r\nbovenaan het bestand. Er is echter ook een subtitle veld\r\ndat je voor dit doel kunt gebruiken, en de uitvoer heeft hetzelfde\r\nuiterlijk en veld als mijn oude beschrijvingen, dus ik heb besloten om\r\nal mijn oude beschrijvingsvelden om te zetten naar subtitle\r\nvermeldingen. - Om een voorbeeldafbeelding te specificeren die bij een\r\nblog post hoort, gebruik je het image veld (bijv. iets als\r\nimage: thumbnail.jpg) in plaats van het\r\npreview veld uit distill. - Er is een nieuw\r\nlicence veld dat het creative_commons veld uit\r\ndistill vervangt. Onderaan deze post zie je een “Reuse” appendix die\r\nlinkt naar een licentie bestand. Om dit te genereren, heb ik een\r\nlicense: \"CC BY\" regel opgenomen in de YAML.\r\nAndere veranderingen gaan dieper:\r\nIn distill is het mogelijk om het author\r\nveld in detail te specificeren, wat de academische conventie\r\nweerspiegelt om een auteurs affiliatie te vermelden naast hun werkgever,\r\ndigitale identificeerder (‘orcid record’) en contactdetails. Quarto\r\nondersteunt dit ook, hoewel de tags iets veranderd zijn:\r\norcid_id is nu orcid, bijvoorbeeld. Een\r\nvoorbeeld hiervan wordt verderop in dit artikel getoond.\r\nHet specificeren van de inhoudsopgave is iets anders. Net als in\r\ndistill, kun je de inhoudsopgave aanzetten door\r\ntoc: true als regel in de YAML header op te nemen, en het\r\ntoc-depth veld in te stellen om te bepalen hoe\r\ngedetailleerd de inhoudsopgave moet zijn. Maar er zijn nieuwe opties. U\r\nkunt de tekst aanpassen die boven de inhoudsopgave verschijnt en de\r\nplaats waar deze verschijnt. Ik besluit om saai te zijn en met een\r\naantal standaard opties te gaan: toc-title: Inhoudsopgave\r\nen toc-location: left.\r\nEen functie in distill die ik leuk vind is dat het een\r\ncitaat genereert voor elke post. Je kunt dat ook doen in\r\nquarto, en je zult onderaan deze post zien dat ik die\r\nfunctie hier heb gebruikt. Maar quarto beheert dit op een\r\nandere manier dan distill, en gebruikt een YAML versie van citation style language (CSL)\r\nformattering om de citatie te definiëren. Om te zien hoe het werkt, kun\r\nje de quarto\r\npages on citations en creating\r\ncitable articles doorlezen. Het is iets uitgebreider dan de\r\ndistill versie, maar veel flexibeler. Voor deze blog is het\r\nzo simpel als citation: true in de YAML, maar het kan\r\nuitgebreider en geschikt voor elk academisch citatiepatroon dat je maar\r\nwilt.\r\nEen nieuwe blog maken\r\nOké. Tijd om aan de slag te gaan en de standaard-blog omvormen tot\r\neen quarto-versie van mijn distill-blog. Mijn\r\neerste stap is het verwijderen van de twee posts die bij de\r\nstandaard-blog zaten, en dan deze aanmaken.\r\nEen map met een index.qmd bestand is het absolute\r\nminimum dat ik nodig heb om aan de slag te gaan met een nieuwe post. Ik\r\nveronderstel dat er andere manieren zijn om dit te doen, maar wat ik\r\neigenlijk doe is het aanmaken van de map en een leeg bestand vanaf de\r\nterminal (om redenen die alleen God kent)\r\nmkdir posts/2022-04-20_porting-to-quarto\r\ntouch posts/2022-04-20_porting-to-quarto/index.qmd\r\nOm eerlijk te zijn, het gebruik van de terminal was overkill. Wat ik\r\nin plaats daarvan had kunnen doen, als ik RStudio had bekeken in plaats\r\nvan de terminal, is de optie “New File” gebruiken in het bestandsmenu en\r\ndan de optie “Quarto Doc” kiezen. Dat maakt een nieuw titelloos quarto\r\ndocument dat je kunt opslaan op de juiste locatie.\r\nYAML instellingen overnemen\r\nEen handige functie in quarto websites is dat YAML velden worden\r\novergeërfd. Bijvoorbeeld, deze post heeft zijn eigen YAML header die de\r\nvolgende – en alleen de volgende – velden bevat:\r\ntitle: \"Een distill blog overzetten naar quarto\"\r\nsubtitle: | \r\n  Net als Danielle Navarro recent heb ik mijn blog van distill naar quarto overgezet. Ik heb haar notities gevolgd en her en der aangepast. Ook voor mij lijkt dit op een veelbelovende zet en in ieder geval is het een interessanten test op het gebied van reproduceerbaar. \r\n  to be an interesting reproducibility test\r\nauthor: Danielle Navarro. bewerking Harrie Jonkman   \r\ndate: \"2022-05-31\"\r\ncategories: [Quarto, Blogging, Reproducibility]\r\nimage: \"img/preview.jpg\"\r\nDat is een beetje eigenaardig, want veel van de metadata die nodig\r\nzijn om deze post te specificeren ontbreken. De reden dat het ontbreekt\r\nis dat ik een aantal velden in het posts/_metadata.yml\r\nbestand heb geplaatst. Deze velden worden geërfd door elke blog post.\r\nDit is de volledige inhoud van mijn post metadata bestand:\r\n# Bevries computer outputs\r\nfreeze: true\r\n\r\n# Schakel banner stijl titelblokken in\r\ntitle-block-banner: true\r\n\r\n# Activeer bijlage CC-licentie\r\nlicense: \"CC BY\"\r\n\r\n# Default voor inhoudsopgave\r\ntoc: true\r\ntoc-title: Table of contents\r\ntoc-location: left\r\n\r\n# Default knitr opties\r\nexecute:\r\n  echo: true\r\n  message: true\r\n  warning: true\r\n\r\n# Default author\r\nauthor:\r\n  - name: Danielle Navarro\r\n    url: https://djnavarro.net\r\n    affiliation: Voltron Data\r\n    affiliation-url: https://voltrondata.com\r\n    orcid: 0000-0001-7648-6578\r\n\r\n# Default voor velden citeren\r\ncitation: true\r\n\r\nDe bevries\r\noptie is bijzonder makkelijk in de context van bloggen. Ik adviseer\r\ndeze documentatiepagina hierover te lezen!\r\n\r\nDat verklaart een hoop, maar als je goed kijkt zul je je realiseren\r\ndat er niets in deze velden staat dat het uitvoerformaat specificeert!\r\nIn Rmarkdown zou ik hiervoor een output veld\r\nhebben opgenomen, maar in quarto heet het relevante veld\r\nformat. Omdat de output voor de hele site geldt, staat dat\r\ndeel van de YAML header in het _quarto.yml bestand. De\r\nrelevante regels van dat bestand zijn:\r\nformat:\r\n  html:\r\n    theme: ember.scss\r\n    css: styles.css\r\nIk kom hier later op terug. Voor nu is het genoeg om te erkennen dat\r\ndit aangeeft dat alle pagina’s op deze site moeten worden gerenderd naar\r\nHTML documenten, en met behulp van de ember.scss en\r\nstyles.css bestanden de blog stijl te specificeren.\r\nConverteren van mijn oude\r\nposts\r\nDe tijd is aangebroken voor een beetje handwerk. Hoewel\r\nquarto compatibel is met de meeste bestaande R markdown en\r\nik er waarschijnlijk mee weg kan komen om ze ongemoeid te laten,\r\nverwacht ik dat ik op de langere termijn naar andere talen zal\r\noverstappen, dus het spreekt me aan om nu van de gelegenheid gebruik te\r\nmaken om alles over te zetten naar quarto. Het hernoemen van alle\r\nindex.Rmd bestanden naar index.qmd bestanden\r\nis eenvoudig genoeg en kan programmatisch worden gedaan. Maar de meeste\r\nvan mijn bewerkingen vereisen een kleine hoeveelheid handmatig\r\nknutselwerk bij elke post. Niet veel, want het is vooral een kwestie van\r\nhet hernoemen van een paar YAML velden. Gezien het feit dat er maar een\r\nstuk of 20 posts overgezet moeten worden, besluit ik dat het\r\ngemakkelijker is om het handmatig te doen dan om te proberen een script\r\nte schrijven om de taak te automatiseren. Ik heb het in een middag\r\ngedaan (Nou, Danielle dan ben je sneller dan mij. Mij kostte het wel\r\nenkele dagen).\r\nStyleren van de nieuwe blog\r\nTot nu toe heeft het gebruik van quarto erg\r\n“distill-achtig” gevoeld. De structuur van de blog voelt vertrouwd aan,\r\nde YAML koppen zijn vergelijkbaar in de geest (hoewel verschillend in de\r\ndetails), enzovoort. Als het aankomt op het aanpassen van het uiterlijk\r\nvan de blog, lijkt het helemaal niet op distill, en voelt\r\nhet meer als eenvoudige R markdown-sites.\r\nQuarto websites zijn bootstrap gebaseerd, en zoals\r\nbesproken op de quarto\r\ntheming page, komen ze met dezelfde thema’s die je misschien kent\r\nvan R markdown. Als je bijvoorbeeld beslist, zoals ik deed, dat je een\r\nheel eenvoudig wit thema wil, dan zou je het “litera” thema kunnen\r\nkiezen. Om dit op je blog toe te passen, hoef je er alleen maar voor te\r\nzorgen dat je _quarto.yml bestand de volgende regels\r\nbevat:\r\nformat:\r\n  html:\r\n    theme: litera\r\n    css: styles.css\r\nDit zorgt ervoor dat de uitvoer zal worden weergegeven als HTML\r\nobjecten, gebruikmakend van het litera bootswatch thema en het toepassen\r\nvan aangepaste CSS regels die je toevoegt in het styles.css\r\nbestand.\r\nEen erg leuke eigenschap van quarto, als je SASS kunt\r\ngebruiken om stijlen te definiëren en iets weet over hoe de bootstrap\r\nSASS bestanden zijn georganiseerd,4 is dat het je toestaat\r\nje eigen .scss bestand te schrijven om je blog thema\r\npreciezer te definiëren, waarbij je toegang hebt tot bootstrap\r\nparameters enzovoort. Ik zou je sterk aanraden om eerst meer te lezen\r\nover het quarto\r\ntheming system voordat je zelf met dit aspect aan de slag gaat, maar\r\nals je meer kennis (of meer domheid) hebt dan ik, dan lees je hier hoe\r\nik mijn blog heb opgezet. Ten eerste, in plaats van te verwijzen naar\r\nhet litera thema, verwijst de YAML in mijn _quarto.yml\r\nbestand naar mijn eigen aangepaste .scss bestand:\r\nformat:\r\n  html:\r\n    theme: ember.scss\r\n    css: styles.css\r\nDe inhoud van de ember.scss file ziet er (bij Danielle)\r\nals volgt uit:\r\n/*-- scss:defaults --*/\r\n\r\n// use litera as the base\r\n$theme: \"litera\" !default;\r\n\r\n// import google fonts\r\n@import 'https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap';\r\n@import 'https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap';\r\n\r\n// use Atkinson Hyperlegible font if available\r\n$font-family-sans-serif:  \"Atkinson Hyperlegible\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\" !default;\r\n\r\n/*-- scss:rules --*/\r\n\r\n// litera is serif by default: revert to san-serif\r\np {\r\n  font-family: $font-family-sans-serif;\r\n}\r\nZoals je kunt zien, doe ik op dit moment niet veel anders dan wat\r\nkleine aanpassingen aan het litera thema, maar er is potentieel zo veel\r\nmeer mee te doen dan ik heb gedaan bij het opzetten van deze blog. Ik\r\nben van plan om hier later meer aan te sleutelen!\r\nEen RSS feed toevoegen\r\nMijn oude distill blog had een RSS feed, en hoewel ik\r\nerken dat het steeds meer een esoterische functie is die de meeste\r\nmensen niet gebruiken, heb ik een voorliefde voor RSS.\r\nQuarto ondersteunt dit, maar het is niet standaard\r\ningeschakeld. Wat ik moet doen is de YAML aanpassen in het\r\nindex.qmd bestand dat correspondeert met de homepage, want\r\ndat is waar ik mijn primaire lijst van berichten heb. Daarin zie ik een\r\nlisting veld. Alles wat ik hoef te doen is\r\nfeed: true eronder te zetten en er is nu een RSS feed voor\r\nde site:\r\ntitle: \"Notes from a data witch\"\r\nsubtitle: A data science blog by Danielle Navarro\r\nlisting:\r\n  feed: true\r\n  contents: posts\r\nDe quarto\r\nsectie over feeds geeft meer informatie hierover.\r\nUitzetten van de site\r\nHet voorbereiden van de site om deze uit te zetten is relatief\r\npijnloos. Ik vond het nuttig om de quarto\r\nwebsite optie pagina te lezen voordat ik dit deed, omdat het een\r\nheleboel instellingen noemt om aan te sleutelen, meestal in het\r\n_quarto.yml bestand. Ik kies er bijvoorbeeld voor om de\r\nnavigatiebalk aan te passen, de voorbeeldafbeeldingen van de sociale\r\nmedia, enzovoort. Uiteindelijk bereik ik het punt waar ik tevreden ben\r\nen ga ik verder met de implementatie.\r\nGelukkig valt er over het uitzetproces zelf niet veel te zeggen. De\r\nquarto\r\ndeployment pagina bespreekt verschillende opties voor hoe je dit\r\nkunt doen. De meeste van mijn websites worden uitgerold via GitHub Pages\r\nof via Netlify. Dit is een Netlify site, dus ik volg de instructies daar\r\nen alles gaat soepel. Dit brengt me echter wel bij een ander\r\nonderwerp…\r\nNetlify herleidt\r\nIk heb mijn blog op een bepaalde manier gestructureerd. Net als de\r\nstandaard quarto blog, staan alle posts in de\r\nposts map, en ze hebben een systematische naam: ze hebben\r\neerst een ISO-8601 geformatteerde datum, en dan een semantische slug.\r\nDus de volledige URL voor deze blog post is:\r\nblog.djnavarro.net/posts/2022-04-20_porting-to-quarto\r\nDat is handig voor archiveringsdoeleinden en om alles netjes geordend\r\nte houden in mijn projectmap, maar het is ook een beetje onhandig voor\r\nhet delen van links. In de praktijk is het “posts” gedeelte een beetje\r\noverbodig, en ik ga nooit twee keer dezelfde slug gebruiken, dus is het\r\nhandig om het zo in te stellen dat er ook een kortere URL is voor de\r\npost,\r\nblog.djnavarro.net/porting-to-quarto\r\nen dat deze korte URL automatisch naar de langere herleidt.\r\nAangezien ik van plan ben om deze blog uit te rollen naar Netlify,\r\nmoet ik ervoor zorgen dat wanneer de site gebouwd wordt, er een\r\n_redirects bestand wordt aangemaakt in de site\r\nmap. Dit bestand moet één regel per redirect bevatten, met als eerste\r\nhet “redirect from” pad, gevolgd door het “redirect to” pad. Hier is hoe\r\ndie regel eruit ziet voor deze post:\r\n/porting-to-quarto /posts/2022-04-20_porting-to-quarto\r\nIk ben niet van plan om deze regels handmatig toe te voegen, dus wat\r\nik in plaats daarvan doe is een R chunk toevoegen aan het\r\nindex.qmd bestand dat correspondeert met de startpagina van\r\nde blog, met de volgende code:\r\n# lijst namen van de post folders\r\nposts <- list.dirs(\r\n  path = here::here(\"posts\"),\r\n  full.names = FALSE,\r\n  recursive = FALSE\r\n)\r\n\r\n# extraheer de slugs\r\nslugs <- gsub(\"^.*_\", \"\", posts)\r\n\r\n# regels om een netlify _redirect file toe te voegen\r\nredirects <- paste0(\"/\", slugs, \" \", \"/posts/\", posts)\r\n\r\n# Schrijf de _redirect file\r\nwriteLines(redirects, here::here(\"_site\", \"_redirects\"))\r\nElke keer als deze site herbouwd wordt – wat meestal inhoudt dat de\r\nhome page herbouwd wordt omdat die de lijst met berichten bevat – wordt\r\nhet _redirects bestand vernieuwd. Er is misschien een\r\nschonere manier, maar dit werkt.\r\nThe quarto CLI\r\nIets wat ik eerder vergat te melden. Ongeveer halverwege het proces\r\nvan het aanpassen van mijn oude posts om ze geschikt te maken voor de\r\nquarto-blog, heb ik besloten om RStudio niet langer te\r\ngebruiken voor de rendering, en heb ik wat tijd besteed om mezelf\r\nvertrouwd te maken met de quarto-command line interface. Ik\r\nheb nog geen specifieke beslissingen genomen over hoe mijn lange termijn\r\nworkflow met quarto eruit gaat zien, maar ik vond het wel nuttig om een\r\ngevoel te krijgen voor het concept van quarto als een\r\nstandalone installatie. Ik ga hier niet in detail treden, maar even\r\nkort: aan de terminal kan ik zien dat ik een aantal help opties heb,\r\n\r\n\r\n\r\n  Usage:   quarto \r\n  Version: 0.9.282\r\n                  \r\n\r\n  Description:\r\n\r\n    Quarto CLI\r\n\r\n  Options:\r\n\r\n    -h, --help     - Show this help.                            \r\n    -V, --version  - Show the version number for this program.  \r\n\r\n  Commands:\r\n\r\n    render          [input] [args...]  - Render input file(s) to various document types.                                                \r\n    serve           [input]            - Serve an interactive document.                                                                 \r\n    create-project  [dir]              - Create a project for rendering multiple documents                                              \r\n    preview         [file] [args...]   - Render and preview a Quarto document or website project. Automatically reloads the browser when\r\n    convert         [input]            - Convert documents to alternate representations.                                                \r\n    capabilities                       - Query for current capabilities (output formats, engines, kernels etc.)                         \r\n    check           [target]           - Check and verify correct functioning of Quarto installation and computation engines.           \r\n    inspect         [path]             - Inspect a Quarto project or input path. Inspecting a project returns its config and engines.   \r\n    tools           [command] [tool]   - Manage the installation, update, and uninstallation of useful tools.                           \r\n    help            [command]          - Show this help or the help of a sub-command.\r\n    \r\nVan daaruit kan ik de help documentatie voor het\r\nquarto render commando bekijken door het volgende in te\r\ntypen,\r\n\r\n\r\n\r\nenzovoort. Het doorbladeren van deze documentatie naast alle\r\nuitstekende inhoud op de quarto-website is een handige\r\nmanier om extra opties te vinden. Als ik de huidige post zou willen\r\nrenderen, en mijn terminal bevond zich momenteel in de hoofdmap van het\r\nproject (d.w.z. mijn quarto-blog map), dan kan ik het als\r\nvolgt renderen:\r\n\r\n\r\n\r\nDe mogelijkheid om dit netjes vanaf de terminal te doen lijkt een\r\nhandige eigenschap van quarto, hoewel ik moet toegeven dat ik nog niet\r\nzeker weet hoe ik het zal gebruiken.\r\nEpiloog\r\nToen ik aan dit proces begon was ik er niet helemaal zeker van of ik\r\ndoor zou zetten en de blog daadwerkelijk zou overschakelen naar\r\nquarto. De distill-blog heeft me het afgelopen\r\ntijd goed gediend en ik hou er niet van om dingen te repareren als ze\r\nniet kapot zijn. Hoe langer ik echter met quarto speelde,\r\nhoe meer het me beviel, en het proces was veel minder pijnlijk dan ik\r\nvreesde dat het zou zijn. Ik heb het gevoel dat het de dingen heeft\r\nbehouden die ik leuk vind aan distill, maar deze netjes\r\nheeft geïntegreerd met andere functies (bijv. de bootstrap grid!) die ik\r\necht miste in distill. Zo nu en dan kom ik wat kleine\r\neigenaardigheden tegen waar sommige ruwe kantjes van quarto\r\nnog zichtbaar zijn – het is nog steeds een nieuwe tool – maar ik geniet\r\ner erg van.\r\nMaar hier is Harrie zelf weer. Ik twijfel nog. Vooralsnog blijf ik\r\nmet distill mijn blog schrijven, maar zal wel nog wat meer\r\nmeer quarto oefenen.\r\n\r\nJa↩︎\r\nNee, maar ze kunnen recursief\r\nzijn\\[\\^3\\]↩︎\r\nVoor het geval je geïnteresseerd\r\nbent: de “Welkom op mijn blog” post in de start blog vermeldt de datum\r\nals date: \"20/04/2022\", die wordt verwerkt als een\r\nletterlijke string wanneer de post wordt gebouwd (d.w.z., de postdatum\r\nwordt weergegeven als “20/04/2022”), logisch genoeg. Echter, wanneer je\r\nde hele site bouwt, wordt deze weergegeven als “4 mei 2023”.↩︎\r\nWat ik eerlijk gezegd niet doe, maar\r\nik ben ook dom en probeer dingen toch↩︎\r\n",
    "preview": "posts/2022-05-31-van-distill-naar-quarto/welcome_post.png",
    "last_modified": "2022-05-31T20:44:25+02:00",
    "input_file": "van-distill-naar-quarto.knit.md"
  },
  {
    "path": "posts/2022-05-05-missende-waarden/",
    "title": "Missende waarden",
    "description": "Over het pakket `mice` en missende waarden",
    "author": [
      {
        "name": "Michy Allice, bewerking HarrieJonkman",
        "url": {}
      }
    ],
    "date": "2022-05-05",
    "categories": [],
    "contents": "\r\nData imputeren met R; het\r\nMICE pakket\r\nInleiding\r\nOnlangs gaf Stef van Buuren een verhelderende introductie op het\r\npakket mice dat hij de laatste jaren ontwikkeld heeft en\r\ndat een standaardpakket geworden is om missende data om te gaan. Zijn\r\npresentatie is hier te vinden slides.\r\nHij verwees tijdens de presentatie naar het artikel dat hij hierover\r\neerder schreef (Van Buuren, and Groothuis-Oudshoorn, 2011)[hier](mice: Multivariate\r\nImputation by Chained Equations in R by Stef van Buuren, het boek\r\ndat hij hierover maakte hier en ook verwees hij naar\r\neen korte Nederlandse talige introductie hier.\r\nNa zijn presentatie las ik via Rbloggers de korte post van Michy Allice\r\nhier.Voor\r\ndeze blog heb ik dat artile van Allice bewerkt. Tot slot las ik ook nog\r\nhet boek van Heymans en Eekhout over dit onderwerp hier dat ook goed\r\nis.\r\nHet mice-pakket\r\nOntbrekende gegevens zijn niet zo’n triviaal probleem bij de analyse\r\nvan een dataset. Het is meestal ook niet zo eenvoudig om er rekening mee\r\nte houden.\r\nAls de hoeveelheid ontbrekende gegevens zeer klein is in verhouding\r\ntot de grootte van de dataset, dan kan het weglaten van de weinige data\r\nmet ontbrekende kenmerken de beste strategie zijn om de analyse niet te\r\nvertekenen,. Met hety weglaten van beschikbare datapunten verdwijnt een\r\nbepaalde hoeveelheid informatie. Afhankelijk van de situatie waarmee u\r\nte maken hebt, kunt u op zoek gaan naar andere oplossingen voordat u\r\npotentieel nuttige datapunten uit uw dataset verwijdert.\r\nHoewel sommige snelle oplossingen zoals het vervangen door het\r\ngemiddelde in sommige gevallen goed kunnen zijn, heb je met zulke\r\neenvoudige benaderingen gewoonlijk bias in de data. Het toepassen van\r\ngemiddelde-substitutie laat het gemiddelde onveranderd (wat wenselijk\r\nis), maar vermindert de variantie, wat onwenselijk kan zijn.\r\n\r\nHet micepakket in R helpt bij het imputeren van\r\nontbrekende waarden met plausibele gegevenswaarden. Deze plausibele\r\nwaarden worden getrokken uit een distributie die speciaal ontworpen is\r\nvoor elk ontbrekend datapunt.\r\n\r\nHieronder gaan we ontbrekende waarden imputeren met behulp van de\r\ndataset airquality (standaard beschikbaar in R). Voor dit\r\nblog verwijder ik een aantal datapunten uit de dataset.\r\n\r\n\r\nlibrary(mice)\r\ndata <- airquality\r\ndata[4:10,3] <- rep(NA,7)\r\ndata[1:5,4] <- NA\r\nsummary(data)\r\n\r\n\r\n     Ozone           Solar.R           Wind             Temp      \r\n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  \r\n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:73.00  \r\n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \r\n Mean   : 42.13   Mean   :185.9   Mean   : 9.806   Mean   :78.28  \r\n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \r\n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \r\n NA's   :37       NA's   :7       NA's   :7        NA's   :5      \r\n     Month            Day      \r\n Min.   :5.000   Min.   : 1.0  \r\n 1st Qu.:6.000   1st Qu.: 8.0  \r\n Median :7.000   Median :16.0  \r\n Mean   :6.993   Mean   :15.8  \r\n 3rd Qu.:8.000   3rd Qu.:23.0  \r\n Max.   :9.000   Max.   :31.0  \r\n                               \r\n\r\nWat categorische variabelen betreft, is het vervangen van\r\ncategorische variabelen gewoonlijk niet aan te bevelen. Het is wel\r\ngebruikelijk om ontbrekende categorische variabelen te vervangen door de\r\nmodus van de waargenomen variabelen, maar het is de vraag of dat een\r\ngoede keuze is. Ook al ontbreken er in dit geval geen datapunten van de\r\ncategorische variabelen (Month, Day), we verwijderen ze uit onze dataset\r\n(we kunnen ze later weer toevoegen als dat nodig is) en bekijken de\r\ngegevens met summary().\r\n\r\n\r\ndata <- data[-c(5,6)]\r\nsummary(data)\r\n\r\n\r\n     Ozone           Solar.R           Wind             Temp      \r\n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  \r\n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:73.00  \r\n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \r\n Mean   : 42.13   Mean   :185.9   Mean   : 9.806   Mean   :78.28  \r\n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \r\n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \r\n NA's   :37       NA's   :7       NA's   :7        NA's   :5      \r\n\r\nOzon is blijkbaar de variabele met de meeste ontbrekende datapunten.\r\nHieronder gaan we dieper in op de ontbrekende datapatronen.\r\nSnelle\r\nclassificatie van ontbrekende gegevens\r\nEr zijn twee soorten ontbrekende gegevens:\r\nMCAR: volledig willekeurig ontbrekend. Dit is het wenselijke\r\nscenario in geval van ontbrekende data.\r\nMNAR: missing not at random. Niet-willekeurig ontbrekende gegevens\r\nzijn een ernstiger probleem en in dit geval kan het verstandig zijn het\r\nproces van gegevensverzameling verder te controleren en te proberen te\r\nbegrijpen waarom de informatie ontbreekt. Als bijvoorbeeld de meeste\r\nmensen in een enquête een bepaalde vraag niet hebben beantwoord, waarom\r\nhebben zij dat dan gedaan? Was de vraag onduidelijk?\r\nErvan uitgaande dat de gegevens MCAR zijn, kan een teveel aan\r\nontbrekende gegevens ook een probleem zijn. Gewoonlijk is een veilige\r\nmaximumdrempel 5% van het totaal voor grote datasets. Als de ontbrekende\r\ngegevens voor een bepaald kenmerk of een bepaalde steekproef meer dan 5%\r\nbedragen, moet u dat kenmerk of die steekproef waarschijnlijk weglaten.\r\nDaarom controleren we op kenmerken (kolommen) en steekproeven (rijen)\r\nwaar meer dan 5% van de data ontbreekt met een eenvoudige functie\r\n\r\n\r\npMiss <- function(x){sum(is.na(x))/length(x)*100}\r\napply(data,2,pMiss)\r\n\r\n\r\n    Ozone   Solar.R      Wind      Temp \r\n24.183007  4.575163  4.575163  3.267974 \r\n\r\napply(data,1,pMiss)\r\n\r\n\r\n  [1]  25  25  25  50 100  50  25  25  25  50  25   0   0   0   0   0\r\n [17]   0   0   0   0   0   0   0   0  25  25  50   0   0   0   0  25\r\n [33]  25  25  25  25  25   0  25   0   0  25  25   0  25  25   0   0\r\n [49]   0   0   0  25  25  25  25  25  25  25  25  25  25   0   0   0\r\n [65]  25   0   0   0   0   0   0  25   0   0  25   0   0   0   0   0\r\n [81]   0   0  25  25   0   0   0   0   0   0   0   0   0   0   0  25\r\n [97]  25  25   0   0   0  25  25   0   0   0  25   0   0   0   0   0\r\n[113]   0   0  25   0   0   0  25   0   0   0   0   0   0   0   0   0\r\n[129]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\r\n[145]   0   0   0   0   0  25   0   0   0\r\n\r\nWe zien dat Ozon bijna 25% van de datapunten mist, dus we kunnen\r\noverwegen om het uit de analyse te laten of meer metingen te verzamelen.\r\nDe andere variabelen blijven onder de drempel van 5%, zodat we ze kunnen\r\nbehouden. Wat de data betreft, leidt het ontbreken van slechts één\r\nkenmerk tot 25% ontbrekende gegevens per dataset.\r\nGebruik\r\n`mice voor het bekijken van ontbrekende data patronen\r\nHet mice pakket biedt een mooie functie\r\nmd.pattern() om een beter inzicht te krijgen in het patroon\r\nvan ontbrekende gegevens.\r\n\r\n\r\nmd.pattern(data)\r\n\r\n\r\n\r\n    Temp Solar.R Wind Ozone   \r\n104    1       1    1     1  0\r\n34     1       1    1     0  1\r\n3      1       1    0     1  1\r\n1      1       1    0     0  2\r\n4      1       0    1     1  1\r\n1      1       0    1     0  2\r\n1      1       0    0     1  2\r\n3      0       1    1     1  1\r\n1      0       1    0     1  2\r\n1      0       0    0     0  4\r\n       5       7    7    37 56\r\n\r\nDe output vertelt ons dat 104 gegevens compleet zijn, 34 data missen\r\nalleen de Ozonmeting, 3 data missen alleen de Solar.R waarde\r\nenzovoort.\r\nEen wellicht meer behulpzame visuele weergave kan worden verkregen\r\nmet behulp van het VIM pakket als volgt\r\n\r\n\r\nlibrary(VIM)\r\naggr_plot <- aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c(\"Histogram van missende data\",\"Patroon\"))\r\n\r\n\r\n\r\n\r\n Variables sorted by number of missings: \r\n Variable      Count\r\n    Ozone 0.24183007\r\n  Solar.R 0.04575163\r\n     Wind 0.04575163\r\n     Temp 0.03267974\r\n\r\nUit de grafiek kunnen we opmaken dat bij bijna 70% van de data geen\r\ninformatie ontbreekt, bij 22% ontbreekt de ozonwaarde en bij de overige\r\nvariabelen ontbreekt zo’n 5%. Door deze aanpak ziet de situatie er naar\r\nmijn mening een stuk duidelijker uit.\r\nEen andere (hopelijk) behulpzame visuele benadering is een speciale\r\nboxplot\r\n\r\n\r\nmarginplot(data[c(1,2)])\r\n\r\n\r\n\r\n\r\nUiteraard zijn we hier beperkt tot het plotten van slechts 2\r\nvariabelen tegelijk, maar desalniettemin kunnen we hier een aantal\r\ninteressante inzichten uit verkrijgen. De rode boxplot links toont de\r\nverdeling van Solar.R waarbij Ozone ontbreekt, terwijl de blauwe boxplot\r\nde verdeling van de resterende datapunten toont. Hetzelfde geldt voor de\r\nboxplots van Ozone onderaan de grafiek. Als onze aanname van\r\nMCAR-gegevens juist is, dan verwachten we dat de rode en blauwe boxplots\r\nsterk op elkaar lijken.\r\nImputeren van missende data\r\nDe mice() functie zorgt voor het imputatieprocess\r\n\r\n\r\ntempData <- mice(data,m=5,maxit=50,meth='pmm',seed=500)\r\n\r\n\r\n\r\n iter imp variable\r\n  1   1  Ozone  Solar.R  Wind  Temp\r\n  1   2  Ozone  Solar.R  Wind  Temp\r\n  1   3  Ozone  Solar.R  Wind  Temp\r\n  1   4  Ozone  Solar.R  Wind  Temp\r\n  1   5  Ozone  Solar.R  Wind  Temp\r\n  2   1  Ozone  Solar.R  Wind  Temp\r\n  2   2  Ozone  Solar.R  Wind  Temp\r\n  2   3  Ozone  Solar.R  Wind  Temp\r\n  2   4  Ozone  Solar.R  Wind  Temp\r\n  2   5  Ozone  Solar.R  Wind  Temp\r\n  3   1  Ozone  Solar.R  Wind  Temp\r\n  3   2  Ozone  Solar.R  Wind  Temp\r\n  3   3  Ozone  Solar.R  Wind  Temp\r\n  3   4  Ozone  Solar.R  Wind  Temp\r\n  3   5  Ozone  Solar.R  Wind  Temp\r\n  4   1  Ozone  Solar.R  Wind  Temp\r\n  4   2  Ozone  Solar.R  Wind  Temp\r\n  4   3  Ozone  Solar.R  Wind  Temp\r\n  4   4  Ozone  Solar.R  Wind  Temp\r\n  4   5  Ozone  Solar.R  Wind  Temp\r\n  5   1  Ozone  Solar.R  Wind  Temp\r\n  5   2  Ozone  Solar.R  Wind  Temp\r\n  5   3  Ozone  Solar.R  Wind  Temp\r\n  5   4  Ozone  Solar.R  Wind  Temp\r\n  5   5  Ozone  Solar.R  Wind  Temp\r\n  6   1  Ozone  Solar.R  Wind  Temp\r\n  6   2  Ozone  Solar.R  Wind  Temp\r\n  6   3  Ozone  Solar.R  Wind  Temp\r\n  6   4  Ozone  Solar.R  Wind  Temp\r\n  6   5  Ozone  Solar.R  Wind  Temp\r\n  7   1  Ozone  Solar.R  Wind  Temp\r\n  7   2  Ozone  Solar.R  Wind  Temp\r\n  7   3  Ozone  Solar.R  Wind  Temp\r\n  7   4  Ozone  Solar.R  Wind  Temp\r\n  7   5  Ozone  Solar.R  Wind  Temp\r\n  8   1  Ozone  Solar.R  Wind  Temp\r\n  8   2  Ozone  Solar.R  Wind  Temp\r\n  8   3  Ozone  Solar.R  Wind  Temp\r\n  8   4  Ozone  Solar.R  Wind  Temp\r\n  8   5  Ozone  Solar.R  Wind  Temp\r\n  9   1  Ozone  Solar.R  Wind  Temp\r\n  9   2  Ozone  Solar.R  Wind  Temp\r\n  9   3  Ozone  Solar.R  Wind  Temp\r\n  9   4  Ozone  Solar.R  Wind  Temp\r\n  9   5  Ozone  Solar.R  Wind  Temp\r\n  10   1  Ozone  Solar.R  Wind  Temp\r\n  10   2  Ozone  Solar.R  Wind  Temp\r\n  10   3  Ozone  Solar.R  Wind  Temp\r\n  10   4  Ozone  Solar.R  Wind  Temp\r\n  10   5  Ozone  Solar.R  Wind  Temp\r\n  11   1  Ozone  Solar.R  Wind  Temp\r\n  11   2  Ozone  Solar.R  Wind  Temp\r\n  11   3  Ozone  Solar.R  Wind  Temp\r\n  11   4  Ozone  Solar.R  Wind  Temp\r\n  11   5  Ozone  Solar.R  Wind  Temp\r\n  12   1  Ozone  Solar.R  Wind  Temp\r\n  12   2  Ozone  Solar.R  Wind  Temp\r\n  12   3  Ozone  Solar.R  Wind  Temp\r\n  12   4  Ozone  Solar.R  Wind  Temp\r\n  12   5  Ozone  Solar.R  Wind  Temp\r\n  13   1  Ozone  Solar.R  Wind  Temp\r\n  13   2  Ozone  Solar.R  Wind  Temp\r\n  13   3  Ozone  Solar.R  Wind  Temp\r\n  13   4  Ozone  Solar.R  Wind  Temp\r\n  13   5  Ozone  Solar.R  Wind  Temp\r\n  14   1  Ozone  Solar.R  Wind  Temp\r\n  14   2  Ozone  Solar.R  Wind  Temp\r\n  14   3  Ozone  Solar.R  Wind  Temp\r\n  14   4  Ozone  Solar.R  Wind  Temp\r\n  14   5  Ozone  Solar.R  Wind  Temp\r\n  15   1  Ozone  Solar.R  Wind  Temp\r\n  15   2  Ozone  Solar.R  Wind  Temp\r\n  15   3  Ozone  Solar.R  Wind  Temp\r\n  15   4  Ozone  Solar.R  Wind  Temp\r\n  15   5  Ozone  Solar.R  Wind  Temp\r\n  16   1  Ozone  Solar.R  Wind  Temp\r\n  16   2  Ozone  Solar.R  Wind  Temp\r\n  16   3  Ozone  Solar.R  Wind  Temp\r\n  16   4  Ozone  Solar.R  Wind  Temp\r\n  16   5  Ozone  Solar.R  Wind  Temp\r\n  17   1  Ozone  Solar.R  Wind  Temp\r\n  17   2  Ozone  Solar.R  Wind  Temp\r\n  17   3  Ozone  Solar.R  Wind  Temp\r\n  17   4  Ozone  Solar.R  Wind  Temp\r\n  17   5  Ozone  Solar.R  Wind  Temp\r\n  18   1  Ozone  Solar.R  Wind  Temp\r\n  18   2  Ozone  Solar.R  Wind  Temp\r\n  18   3  Ozone  Solar.R  Wind  Temp\r\n  18   4  Ozone  Solar.R  Wind  Temp\r\n  18   5  Ozone  Solar.R  Wind  Temp\r\n  19   1  Ozone  Solar.R  Wind  Temp\r\n  19   2  Ozone  Solar.R  Wind  Temp\r\n  19   3  Ozone  Solar.R  Wind  Temp\r\n  19   4  Ozone  Solar.R  Wind  Temp\r\n  19   5  Ozone  Solar.R  Wind  Temp\r\n  20   1  Ozone  Solar.R  Wind  Temp\r\n  20   2  Ozone  Solar.R  Wind  Temp\r\n  20   3  Ozone  Solar.R  Wind  Temp\r\n  20   4  Ozone  Solar.R  Wind  Temp\r\n  20   5  Ozone  Solar.R  Wind  Temp\r\n  21   1  Ozone  Solar.R  Wind  Temp\r\n  21   2  Ozone  Solar.R  Wind  Temp\r\n  21   3  Ozone  Solar.R  Wind  Temp\r\n  21   4  Ozone  Solar.R  Wind  Temp\r\n  21   5  Ozone  Solar.R  Wind  Temp\r\n  22   1  Ozone  Solar.R  Wind  Temp\r\n  22   2  Ozone  Solar.R  Wind  Temp\r\n  22   3  Ozone  Solar.R  Wind  Temp\r\n  22   4  Ozone  Solar.R  Wind  Temp\r\n  22   5  Ozone  Solar.R  Wind  Temp\r\n  23   1  Ozone  Solar.R  Wind  Temp\r\n  23   2  Ozone  Solar.R  Wind  Temp\r\n  23   3  Ozone  Solar.R  Wind  Temp\r\n  23   4  Ozone  Solar.R  Wind  Temp\r\n  23   5  Ozone  Solar.R  Wind  Temp\r\n  24   1  Ozone  Solar.R  Wind  Temp\r\n  24   2  Ozone  Solar.R  Wind  Temp\r\n  24   3  Ozone  Solar.R  Wind  Temp\r\n  24   4  Ozone  Solar.R  Wind  Temp\r\n  24   5  Ozone  Solar.R  Wind  Temp\r\n  25   1  Ozone  Solar.R  Wind  Temp\r\n  25   2  Ozone  Solar.R  Wind  Temp\r\n  25   3  Ozone  Solar.R  Wind  Temp\r\n  25   4  Ozone  Solar.R  Wind  Temp\r\n  25   5  Ozone  Solar.R  Wind  Temp\r\n  26   1  Ozone  Solar.R  Wind  Temp\r\n  26   2  Ozone  Solar.R  Wind  Temp\r\n  26   3  Ozone  Solar.R  Wind  Temp\r\n  26   4  Ozone  Solar.R  Wind  Temp\r\n  26   5  Ozone  Solar.R  Wind  Temp\r\n  27   1  Ozone  Solar.R  Wind  Temp\r\n  27   2  Ozone  Solar.R  Wind  Temp\r\n  27   3  Ozone  Solar.R  Wind  Temp\r\n  27   4  Ozone  Solar.R  Wind  Temp\r\n  27   5  Ozone  Solar.R  Wind  Temp\r\n  28   1  Ozone  Solar.R  Wind  Temp\r\n  28   2  Ozone  Solar.R  Wind  Temp\r\n  28   3  Ozone  Solar.R  Wind  Temp\r\n  28   4  Ozone  Solar.R  Wind  Temp\r\n  28   5  Ozone  Solar.R  Wind  Temp\r\n  29   1  Ozone  Solar.R  Wind  Temp\r\n  29   2  Ozone  Solar.R  Wind  Temp\r\n  29   3  Ozone  Solar.R  Wind  Temp\r\n  29   4  Ozone  Solar.R  Wind  Temp\r\n  29   5  Ozone  Solar.R  Wind  Temp\r\n  30   1  Ozone  Solar.R  Wind  Temp\r\n  30   2  Ozone  Solar.R  Wind  Temp\r\n  30   3  Ozone  Solar.R  Wind  Temp\r\n  30   4  Ozone  Solar.R  Wind  Temp\r\n  30   5  Ozone  Solar.R  Wind  Temp\r\n  31   1  Ozone  Solar.R  Wind  Temp\r\n  31   2  Ozone  Solar.R  Wind  Temp\r\n  31   3  Ozone  Solar.R  Wind  Temp\r\n  31   4  Ozone  Solar.R  Wind  Temp\r\n  31   5  Ozone  Solar.R  Wind  Temp\r\n  32   1  Ozone  Solar.R  Wind  Temp\r\n  32   2  Ozone  Solar.R  Wind  Temp\r\n  32   3  Ozone  Solar.R  Wind  Temp\r\n  32   4  Ozone  Solar.R  Wind  Temp\r\n  32   5  Ozone  Solar.R  Wind  Temp\r\n  33   1  Ozone  Solar.R  Wind  Temp\r\n  33   2  Ozone  Solar.R  Wind  Temp\r\n  33   3  Ozone  Solar.R  Wind  Temp\r\n  33   4  Ozone  Solar.R  Wind  Temp\r\n  33   5  Ozone  Solar.R  Wind  Temp\r\n  34   1  Ozone  Solar.R  Wind  Temp\r\n  34   2  Ozone  Solar.R  Wind  Temp\r\n  34   3  Ozone  Solar.R  Wind  Temp\r\n  34   4  Ozone  Solar.R  Wind  Temp\r\n  34   5  Ozone  Solar.R  Wind  Temp\r\n  35   1  Ozone  Solar.R  Wind  Temp\r\n  35   2  Ozone  Solar.R  Wind  Temp\r\n  35   3  Ozone  Solar.R  Wind  Temp\r\n  35   4  Ozone  Solar.R  Wind  Temp\r\n  35   5  Ozone  Solar.R  Wind  Temp\r\n  36   1  Ozone  Solar.R  Wind  Temp\r\n  36   2  Ozone  Solar.R  Wind  Temp\r\n  36   3  Ozone  Solar.R  Wind  Temp\r\n  36   4  Ozone  Solar.R  Wind  Temp\r\n  36   5  Ozone  Solar.R  Wind  Temp\r\n  37   1  Ozone  Solar.R  Wind  Temp\r\n  37   2  Ozone  Solar.R  Wind  Temp\r\n  37   3  Ozone  Solar.R  Wind  Temp\r\n  37   4  Ozone  Solar.R  Wind  Temp\r\n  37   5  Ozone  Solar.R  Wind  Temp\r\n  38   1  Ozone  Solar.R  Wind  Temp\r\n  38   2  Ozone  Solar.R  Wind  Temp\r\n  38   3  Ozone  Solar.R  Wind  Temp\r\n  38   4  Ozone  Solar.R  Wind  Temp\r\n  38   5  Ozone  Solar.R  Wind  Temp\r\n  39   1  Ozone  Solar.R  Wind  Temp\r\n  39   2  Ozone  Solar.R  Wind  Temp\r\n  39   3  Ozone  Solar.R  Wind  Temp\r\n  39   4  Ozone  Solar.R  Wind  Temp\r\n  39   5  Ozone  Solar.R  Wind  Temp\r\n  40   1  Ozone  Solar.R  Wind  Temp\r\n  40   2  Ozone  Solar.R  Wind  Temp\r\n  40   3  Ozone  Solar.R  Wind  Temp\r\n  40   4  Ozone  Solar.R  Wind  Temp\r\n  40   5  Ozone  Solar.R  Wind  Temp\r\n  41   1  Ozone  Solar.R  Wind  Temp\r\n  41   2  Ozone  Solar.R  Wind  Temp\r\n  41   3  Ozone  Solar.R  Wind  Temp\r\n  41   4  Ozone  Solar.R  Wind  Temp\r\n  41   5  Ozone  Solar.R  Wind  Temp\r\n  42   1  Ozone  Solar.R  Wind  Temp\r\n  42   2  Ozone  Solar.R  Wind  Temp\r\n  42   3  Ozone  Solar.R  Wind  Temp\r\n  42   4  Ozone  Solar.R  Wind  Temp\r\n  42   5  Ozone  Solar.R  Wind  Temp\r\n  43   1  Ozone  Solar.R  Wind  Temp\r\n  43   2  Ozone  Solar.R  Wind  Temp\r\n  43   3  Ozone  Solar.R  Wind  Temp\r\n  43   4  Ozone  Solar.R  Wind  Temp\r\n  43   5  Ozone  Solar.R  Wind  Temp\r\n  44   1  Ozone  Solar.R  Wind  Temp\r\n  44   2  Ozone  Solar.R  Wind  Temp\r\n  44   3  Ozone  Solar.R  Wind  Temp\r\n  44   4  Ozone  Solar.R  Wind  Temp\r\n  44   5  Ozone  Solar.R  Wind  Temp\r\n  45   1  Ozone  Solar.R  Wind  Temp\r\n  45   2  Ozone  Solar.R  Wind  Temp\r\n  45   3  Ozone  Solar.R  Wind  Temp\r\n  45   4  Ozone  Solar.R  Wind  Temp\r\n  45   5  Ozone  Solar.R  Wind  Temp\r\n  46   1  Ozone  Solar.R  Wind  Temp\r\n  46   2  Ozone  Solar.R  Wind  Temp\r\n  46   3  Ozone  Solar.R  Wind  Temp\r\n  46   4  Ozone  Solar.R  Wind  Temp\r\n  46   5  Ozone  Solar.R  Wind  Temp\r\n  47   1  Ozone  Solar.R  Wind  Temp\r\n  47   2  Ozone  Solar.R  Wind  Temp\r\n  47   3  Ozone  Solar.R  Wind  Temp\r\n  47   4  Ozone  Solar.R  Wind  Temp\r\n  47   5  Ozone  Solar.R  Wind  Temp\r\n  48   1  Ozone  Solar.R  Wind  Temp\r\n  48   2  Ozone  Solar.R  Wind  Temp\r\n  48   3  Ozone  Solar.R  Wind  Temp\r\n  48   4  Ozone  Solar.R  Wind  Temp\r\n  48   5  Ozone  Solar.R  Wind  Temp\r\n  49   1  Ozone  Solar.R  Wind  Temp\r\n  49   2  Ozone  Solar.R  Wind  Temp\r\n  49   3  Ozone  Solar.R  Wind  Temp\r\n  49   4  Ozone  Solar.R  Wind  Temp\r\n  49   5  Ozone  Solar.R  Wind  Temp\r\n  50   1  Ozone  Solar.R  Wind  Temp\r\n  50   2  Ozone  Solar.R  Wind  Temp\r\n  50   3  Ozone  Solar.R  Wind  Temp\r\n  50   4  Ozone  Solar.R  Wind  Temp\r\n  50   5  Ozone  Solar.R  Wind  Temp\r\n\r\nsummary(tempData)\r\n\r\n\r\nClass: mids\r\nNumber of multiple imputations:  5 \r\nImputation methods:\r\n  Ozone Solar.R    Wind    Temp \r\n  \"pmm\"   \"pmm\"   \"pmm\"   \"pmm\" \r\nPredictorMatrix:\r\n        Ozone Solar.R Wind Temp\r\nOzone       0       1    1    1\r\nSolar.R     1       0    1    1\r\nWind        1       1    0    1\r\nTemp        1       1    1    0\r\n\r\nEen paar opmerkingen over de parameters:\r\nm=5 verwijst naar het aantal geïmputeerde datasets.\r\nVijf is de standaard waarde.\r\nmeth='pmm' verwijst naar de imputatie methode. In dit\r\ngeval gebruiken we pmm (predictive mean matching) als imputatiemethode.\r\nEr kunnen ook andere imputatiemethoden worden gebruikt, type\r\nmethods(mice) voor een lijst van de beschikbare\r\nimputatiemethoden.\r\nAls u de geïmputeerde gegevens wilt controleren, bijvoorbeeld voor de\r\nvariabele Ozon, moet u de volgende regel code invoeren\r\n\r\n\r\ntempData$imp$Ozone\r\n\r\n\r\n     1   2   3   4   5\r\n5   13  19  12 115  63\r\n10  30  12  13  21   7\r\n25   8  28   6  18  28\r\n26   9  32   4  18  37\r\n27  37  21   4  32  32\r\n32  40  39  35  32  47\r\n33  44  28  36  52  20\r\n34  20  23  37  37  19\r\n35  32  28  16  32  35\r\n36  89  80  48  49 115\r\n37  18   7  16  30  22\r\n39  96  77 135  76  85\r\n42  50 168  64  50  41\r\n43  96  78  96  96  78\r\n45  63  20  18  24  31\r\n46  71  37  20  20  28\r\n52  20  35  37  63  63\r\n53  16  78  73  48 115\r\n54  59  35  46  44  23\r\n55  16  39  28  40  49\r\n56  24  36  52  21  44\r\n57  36  20  20  18  23\r\n58  11  11  24   7  23\r\n59  44  13  23  23  27\r\n60  23   4  19   4  32\r\n61  44  16  46  37  35\r\n65  30  23  65  30  30\r\n72  45  37  63  63  44\r\n75  39  46  32  39  28\r\n83  37  40  59  32  35\r\n84  40  59  28  28  35\r\n102 61  85  96  79  78\r\n103 31  59  20  31  36\r\n107 32  24  11  21  21\r\n115 52  16  11  14  13\r\n119 78  96 168  76  50\r\n150 14  12  13  23  11\r\n\r\nDe uitvoer toont de geïmputeerde data voor elke observatie (eerste\r\nkolom links) binnen elke geïmputeerde dataset (eerste rij bovenaan). Als\r\nu de gebruikte imputatiemethode voor elke variabele wilt controleren,\r\nkunt u dat met mice heel eenvoudig doen\r\n\r\n\r\ntempData$meth\r\n\r\n\r\n  Ozone Solar.R    Wind    Temp \r\n  \"pmm\"   \"pmm\"   \"pmm\"   \"pmm\" \r\n\r\nNu kunnen we de voltooide dataset terugkrijgen met de\r\ncomplete()functie. Het is bijna gewoon Engels:\r\n\r\n\r\ncompletedData <- complete(tempData,1)\r\n\r\n\r\n\r\nDe ontbrekende waarden zijn vervangen door de geïmputeerde waarden in\r\nde eerste van de vijf datasets. Als je een andere wilt gebruiken,\r\nverander dan de tweede parameter in de complete()\r\nfunctie.\r\nDe\r\nverdeling van de originele en geïmputeerde data bekijken\r\nLaten we de verdelingen van de originele en geïmputeerde data\r\nvergelijken met behulp van een aantal handige plots. Allereerst kunnen\r\nwe een scatterplot gebruiken en Ozon uitzetten tegen alle andere\r\nvariabelen.\r\nHier is dat het geval:\r\n\r\n\r\nxyplot(tempData,Ozone ~ Wind+Temp+Solar.R,pch=18,cex=1)\r\n\r\n\r\n\r\n\r\nWat wij willen zien is dat de vorm van de magenta punten\r\n(geïmputeerd) overeenkomt met de vorm van de blauwe punten\r\n(waargenomen). De overeenkomstige vorm zegt ons dat de geïmputeerde\r\nwaarden inderdaad “plausibele waarden” zijn. Een andere nuttige grafiek\r\nis de densitygrafiek:\r\n\r\n\r\ndensityplot(tempData)\r\n\r\n\r\n\r\n\r\nDe dichtheid van de geïmputeerde gegevens voor elke geïmputeerde\r\ndataset wordt getoond in magenta, terwijl de dichtheid van de\r\nwaargenomen gegevens in blauw wordt getoond. Nogmaals, onder onze\r\neerdere aannames verwachten we dat de verdelingen vergelijkbaar\r\nzijn.\r\nEen andere nuttige visuele kijk op de verdelingen kan worden\r\nverkregen met de stripplot() functie die de verdelingen van\r\nde variabelen als afzonderlijke punten toont\r\n\r\n\r\nstripplot(tempData, pch = 20, cex = 1.2)\r\n\r\n\r\n\r\n\r\nPooling\r\nVeronderstel dat de volgende stap in onze analyse erin bestaat een\r\nlineair model op de gegevens toe te passen. Dan kun je je afvragen welke\r\ngeïmputeerde dataset je moet kiezen. Het mice pakket maakt\r\nhet weer heel gemakkelijk om een model te passen op elk van de\r\ngeïmputeerde datasets en dan deze resultaten samen te voegen\r\n\r\n\r\nmodelFit1 <- with(tempData,lm(Temp~ Ozone+Solar.R+Wind))\r\nsummary(pool(modelFit1))\r\n\r\n\r\n         term    estimate   std.error statistic        df\r\n1 (Intercept) 72.70719792 2.761360433 26.330209 117.27936\r\n2       Ozone  0.15924872 0.025914423  6.145177  49.30693\r\n3     Solar.R  0.01252384 0.008678358  1.443112  18.19046\r\n4        Wind -0.34547006 0.207866970 -1.661977 123.65905\r\n       p.value\r\n1 0.000000e+00\r\n2 1.367056e-07\r\n3 1.659893e-01\r\n4 9.905045e-02\r\n\r\nDe variabele modelFit1 bevat de resultaten van de\r\naanpassing die is uitgevoerd over de geïmputeerde datasets, terwijl de\r\npool() functie ze allemaal samenvoegt. Blijkbaar is alleen\r\nde Ozon variabele statistisch significant.\r\nVergeet niet dat we de mice-functie hebben\r\ngeïnitialiseerd met een specifieke ‘seed’-instelling. Daarom zijn de\r\nresultaten enigszins afhankelijk van onze initiële keuze. Om dit effect\r\nte verminderen, kunnen we een hoger aantal datasets toerekenen, door de\r\nstandaard m=5 parameter in de mice() functie\r\nals volgt te wijzigen\r\n\r\n\r\ntempData2 <- mice(data,m=50,seed=245435)\r\n\r\n\r\n\r\n iter imp variable\r\n  1   1  Ozone  Solar.R  Wind  Temp\r\n  1   2  Ozone  Solar.R  Wind  Temp\r\n  1   3  Ozone  Solar.R  Wind  Temp\r\n  1   4  Ozone  Solar.R  Wind  Temp\r\n  1   5  Ozone  Solar.R  Wind  Temp\r\n  1   6  Ozone  Solar.R  Wind  Temp\r\n  1   7  Ozone  Solar.R  Wind  Temp\r\n  1   8  Ozone  Solar.R  Wind  Temp\r\n  1   9  Ozone  Solar.R  Wind  Temp\r\n  1   10  Ozone  Solar.R  Wind  Temp\r\n  1   11  Ozone  Solar.R  Wind  Temp\r\n  1   12  Ozone  Solar.R  Wind  Temp\r\n  1   13  Ozone  Solar.R  Wind  Temp\r\n  1   14  Ozone  Solar.R  Wind  Temp\r\n  1   15  Ozone  Solar.R  Wind  Temp\r\n  1   16  Ozone  Solar.R  Wind  Temp\r\n  1   17  Ozone  Solar.R  Wind  Temp\r\n  1   18  Ozone  Solar.R  Wind  Temp\r\n  1   19  Ozone  Solar.R  Wind  Temp\r\n  1   20  Ozone  Solar.R  Wind  Temp\r\n  1   21  Ozone  Solar.R  Wind  Temp\r\n  1   22  Ozone  Solar.R  Wind  Temp\r\n  1   23  Ozone  Solar.R  Wind  Temp\r\n  1   24  Ozone  Solar.R  Wind  Temp\r\n  1   25  Ozone  Solar.R  Wind  Temp\r\n  1   26  Ozone  Solar.R  Wind  Temp\r\n  1   27  Ozone  Solar.R  Wind  Temp\r\n  1   28  Ozone  Solar.R  Wind  Temp\r\n  1   29  Ozone  Solar.R  Wind  Temp\r\n  1   30  Ozone  Solar.R  Wind  Temp\r\n  1   31  Ozone  Solar.R  Wind  Temp\r\n  1   32  Ozone  Solar.R  Wind  Temp\r\n  1   33  Ozone  Solar.R  Wind  Temp\r\n  1   34  Ozone  Solar.R  Wind  Temp\r\n  1   35  Ozone  Solar.R  Wind  Temp\r\n  1   36  Ozone  Solar.R  Wind  Temp\r\n  1   37  Ozone  Solar.R  Wind  Temp\r\n  1   38  Ozone  Solar.R  Wind  Temp\r\n  1   39  Ozone  Solar.R  Wind  Temp\r\n  1   40  Ozone  Solar.R  Wind  Temp\r\n  1   41  Ozone  Solar.R  Wind  Temp\r\n  1   42  Ozone  Solar.R  Wind  Temp\r\n  1   43  Ozone  Solar.R  Wind  Temp\r\n  1   44  Ozone  Solar.R  Wind  Temp\r\n  1   45  Ozone  Solar.R  Wind  Temp\r\n  1   46  Ozone  Solar.R  Wind  Temp\r\n  1   47  Ozone  Solar.R  Wind  Temp\r\n  1   48  Ozone  Solar.R  Wind  Temp\r\n  1   49  Ozone  Solar.R  Wind  Temp\r\n  1   50  Ozone  Solar.R  Wind  Temp\r\n  2   1  Ozone  Solar.R  Wind  Temp\r\n  2   2  Ozone  Solar.R  Wind  Temp\r\n  2   3  Ozone  Solar.R  Wind  Temp\r\n  2   4  Ozone  Solar.R  Wind  Temp\r\n  2   5  Ozone  Solar.R  Wind  Temp\r\n  2   6  Ozone  Solar.R  Wind  Temp\r\n  2   7  Ozone  Solar.R  Wind  Temp\r\n  2   8  Ozone  Solar.R  Wind  Temp\r\n  2   9  Ozone  Solar.R  Wind  Temp\r\n  2   10  Ozone  Solar.R  Wind  Temp\r\n  2   11  Ozone  Solar.R  Wind  Temp\r\n  2   12  Ozone  Solar.R  Wind  Temp\r\n  2   13  Ozone  Solar.R  Wind  Temp\r\n  2   14  Ozone  Solar.R  Wind  Temp\r\n  2   15  Ozone  Solar.R  Wind  Temp\r\n  2   16  Ozone  Solar.R  Wind  Temp\r\n  2   17  Ozone  Solar.R  Wind  Temp\r\n  2   18  Ozone  Solar.R  Wind  Temp\r\n  2   19  Ozone  Solar.R  Wind  Temp\r\n  2   20  Ozone  Solar.R  Wind  Temp\r\n  2   21  Ozone  Solar.R  Wind  Temp\r\n  2   22  Ozone  Solar.R  Wind  Temp\r\n  2   23  Ozone  Solar.R  Wind  Temp\r\n  2   24  Ozone  Solar.R  Wind  Temp\r\n  2   25  Ozone  Solar.R  Wind  Temp\r\n  2   26  Ozone  Solar.R  Wind  Temp\r\n  2   27  Ozone  Solar.R  Wind  Temp\r\n  2   28  Ozone  Solar.R  Wind  Temp\r\n  2   29  Ozone  Solar.R  Wind  Temp\r\n  2   30  Ozone  Solar.R  Wind  Temp\r\n  2   31  Ozone  Solar.R  Wind  Temp\r\n  2   32  Ozone  Solar.R  Wind  Temp\r\n  2   33  Ozone  Solar.R  Wind  Temp\r\n  2   34  Ozone  Solar.R  Wind  Temp\r\n  2   35  Ozone  Solar.R  Wind  Temp\r\n  2   36  Ozone  Solar.R  Wind  Temp\r\n  2   37  Ozone  Solar.R  Wind  Temp\r\n  2   38  Ozone  Solar.R  Wind  Temp\r\n  2   39  Ozone  Solar.R  Wind  Temp\r\n  2   40  Ozone  Solar.R  Wind  Temp\r\n  2   41  Ozone  Solar.R  Wind  Temp\r\n  2   42  Ozone  Solar.R  Wind  Temp\r\n  2   43  Ozone  Solar.R  Wind  Temp\r\n  2   44  Ozone  Solar.R  Wind  Temp\r\n  2   45  Ozone  Solar.R  Wind  Temp\r\n  2   46  Ozone  Solar.R  Wind  Temp\r\n  2   47  Ozone  Solar.R  Wind  Temp\r\n  2   48  Ozone  Solar.R  Wind  Temp\r\n  2   49  Ozone  Solar.R  Wind  Temp\r\n  2   50  Ozone  Solar.R  Wind  Temp\r\n  3   1  Ozone  Solar.R  Wind  Temp\r\n  3   2  Ozone  Solar.R  Wind  Temp\r\n  3   3  Ozone  Solar.R  Wind  Temp\r\n  3   4  Ozone  Solar.R  Wind  Temp\r\n  3   5  Ozone  Solar.R  Wind  Temp\r\n  3   6  Ozone  Solar.R  Wind  Temp\r\n  3   7  Ozone  Solar.R  Wind  Temp\r\n  3   8  Ozone  Solar.R  Wind  Temp\r\n  3   9  Ozone  Solar.R  Wind  Temp\r\n  3   10  Ozone  Solar.R  Wind  Temp\r\n  3   11  Ozone  Solar.R  Wind  Temp\r\n  3   12  Ozone  Solar.R  Wind  Temp\r\n  3   13  Ozone  Solar.R  Wind  Temp\r\n  3   14  Ozone  Solar.R  Wind  Temp\r\n  3   15  Ozone  Solar.R  Wind  Temp\r\n  3   16  Ozone  Solar.R  Wind  Temp\r\n  3   17  Ozone  Solar.R  Wind  Temp\r\n  3   18  Ozone  Solar.R  Wind  Temp\r\n  3   19  Ozone  Solar.R  Wind  Temp\r\n  3   20  Ozone  Solar.R  Wind  Temp\r\n  3   21  Ozone  Solar.R  Wind  Temp\r\n  3   22  Ozone  Solar.R  Wind  Temp\r\n  3   23  Ozone  Solar.R  Wind  Temp\r\n  3   24  Ozone  Solar.R  Wind  Temp\r\n  3   25  Ozone  Solar.R  Wind  Temp\r\n  3   26  Ozone  Solar.R  Wind  Temp\r\n  3   27  Ozone  Solar.R  Wind  Temp\r\n  3   28  Ozone  Solar.R  Wind  Temp\r\n  3   29  Ozone  Solar.R  Wind  Temp\r\n  3   30  Ozone  Solar.R  Wind  Temp\r\n  3   31  Ozone  Solar.R  Wind  Temp\r\n  3   32  Ozone  Solar.R  Wind  Temp\r\n  3   33  Ozone  Solar.R  Wind  Temp\r\n  3   34  Ozone  Solar.R  Wind  Temp\r\n  3   35  Ozone  Solar.R  Wind  Temp\r\n  3   36  Ozone  Solar.R  Wind  Temp\r\n  3   37  Ozone  Solar.R  Wind  Temp\r\n  3   38  Ozone  Solar.R  Wind  Temp\r\n  3   39  Ozone  Solar.R  Wind  Temp\r\n  3   40  Ozone  Solar.R  Wind  Temp\r\n  3   41  Ozone  Solar.R  Wind  Temp\r\n  3   42  Ozone  Solar.R  Wind  Temp\r\n  3   43  Ozone  Solar.R  Wind  Temp\r\n  3   44  Ozone  Solar.R  Wind  Temp\r\n  3   45  Ozone  Solar.R  Wind  Temp\r\n  3   46  Ozone  Solar.R  Wind  Temp\r\n  3   47  Ozone  Solar.R  Wind  Temp\r\n  3   48  Ozone  Solar.R  Wind  Temp\r\n  3   49  Ozone  Solar.R  Wind  Temp\r\n  3   50  Ozone  Solar.R  Wind  Temp\r\n  4   1  Ozone  Solar.R  Wind  Temp\r\n  4   2  Ozone  Solar.R  Wind  Temp\r\n  4   3  Ozone  Solar.R  Wind  Temp\r\n  4   4  Ozone  Solar.R  Wind  Temp\r\n  4   5  Ozone  Solar.R  Wind  Temp\r\n  4   6  Ozone  Solar.R  Wind  Temp\r\n  4   7  Ozone  Solar.R  Wind  Temp\r\n  4   8  Ozone  Solar.R  Wind  Temp\r\n  4   9  Ozone  Solar.R  Wind  Temp\r\n  4   10  Ozone  Solar.R  Wind  Temp\r\n  4   11  Ozone  Solar.R  Wind  Temp\r\n  4   12  Ozone  Solar.R  Wind  Temp\r\n  4   13  Ozone  Solar.R  Wind  Temp\r\n  4   14  Ozone  Solar.R  Wind  Temp\r\n  4   15  Ozone  Solar.R  Wind  Temp\r\n  4   16  Ozone  Solar.R  Wind  Temp\r\n  4   17  Ozone  Solar.R  Wind  Temp\r\n  4   18  Ozone  Solar.R  Wind  Temp\r\n  4   19  Ozone  Solar.R  Wind  Temp\r\n  4   20  Ozone  Solar.R  Wind  Temp\r\n  4   21  Ozone  Solar.R  Wind  Temp\r\n  4   22  Ozone  Solar.R  Wind  Temp\r\n  4   23  Ozone  Solar.R  Wind  Temp\r\n  4   24  Ozone  Solar.R  Wind  Temp\r\n  4   25  Ozone  Solar.R  Wind  Temp\r\n  4   26  Ozone  Solar.R  Wind  Temp\r\n  4   27  Ozone  Solar.R  Wind  Temp\r\n  4   28  Ozone  Solar.R  Wind  Temp\r\n  4   29  Ozone  Solar.R  Wind  Temp\r\n  4   30  Ozone  Solar.R  Wind  Temp\r\n  4   31  Ozone  Solar.R  Wind  Temp\r\n  4   32  Ozone  Solar.R  Wind  Temp\r\n  4   33  Ozone  Solar.R  Wind  Temp\r\n  4   34  Ozone  Solar.R  Wind  Temp\r\n  4   35  Ozone  Solar.R  Wind  Temp\r\n  4   36  Ozone  Solar.R  Wind  Temp\r\n  4   37  Ozone  Solar.R  Wind  Temp\r\n  4   38  Ozone  Solar.R  Wind  Temp\r\n  4   39  Ozone  Solar.R  Wind  Temp\r\n  4   40  Ozone  Solar.R  Wind  Temp\r\n  4   41  Ozone  Solar.R  Wind  Temp\r\n  4   42  Ozone  Solar.R  Wind  Temp\r\n  4   43  Ozone  Solar.R  Wind  Temp\r\n  4   44  Ozone  Solar.R  Wind  Temp\r\n  4   45  Ozone  Solar.R  Wind  Temp\r\n  4   46  Ozone  Solar.R  Wind  Temp\r\n  4   47  Ozone  Solar.R  Wind  Temp\r\n  4   48  Ozone  Solar.R  Wind  Temp\r\n  4   49  Ozone  Solar.R  Wind  Temp\r\n  4   50  Ozone  Solar.R  Wind  Temp\r\n  5   1  Ozone  Solar.R  Wind  Temp\r\n  5   2  Ozone  Solar.R  Wind  Temp\r\n  5   3  Ozone  Solar.R  Wind  Temp\r\n  5   4  Ozone  Solar.R  Wind  Temp\r\n  5   5  Ozone  Solar.R  Wind  Temp\r\n  5   6  Ozone  Solar.R  Wind  Temp\r\n  5   7  Ozone  Solar.R  Wind  Temp\r\n  5   8  Ozone  Solar.R  Wind  Temp\r\n  5   9  Ozone  Solar.R  Wind  Temp\r\n  5   10  Ozone  Solar.R  Wind  Temp\r\n  5   11  Ozone  Solar.R  Wind  Temp\r\n  5   12  Ozone  Solar.R  Wind  Temp\r\n  5   13  Ozone  Solar.R  Wind  Temp\r\n  5   14  Ozone  Solar.R  Wind  Temp\r\n  5   15  Ozone  Solar.R  Wind  Temp\r\n  5   16  Ozone  Solar.R  Wind  Temp\r\n  5   17  Ozone  Solar.R  Wind  Temp\r\n  5   18  Ozone  Solar.R  Wind  Temp\r\n  5   19  Ozone  Solar.R  Wind  Temp\r\n  5   20  Ozone  Solar.R  Wind  Temp\r\n  5   21  Ozone  Solar.R  Wind  Temp\r\n  5   22  Ozone  Solar.R  Wind  Temp\r\n  5   23  Ozone  Solar.R  Wind  Temp\r\n  5   24  Ozone  Solar.R  Wind  Temp\r\n  5   25  Ozone  Solar.R  Wind  Temp\r\n  5   26  Ozone  Solar.R  Wind  Temp\r\n  5   27  Ozone  Solar.R  Wind  Temp\r\n  5   28  Ozone  Solar.R  Wind  Temp\r\n  5   29  Ozone  Solar.R  Wind  Temp\r\n  5   30  Ozone  Solar.R  Wind  Temp\r\n  5   31  Ozone  Solar.R  Wind  Temp\r\n  5   32  Ozone  Solar.R  Wind  Temp\r\n  5   33  Ozone  Solar.R  Wind  Temp\r\n  5   34  Ozone  Solar.R  Wind  Temp\r\n  5   35  Ozone  Solar.R  Wind  Temp\r\n  5   36  Ozone  Solar.R  Wind  Temp\r\n  5   37  Ozone  Solar.R  Wind  Temp\r\n  5   38  Ozone  Solar.R  Wind  Temp\r\n  5   39  Ozone  Solar.R  Wind  Temp\r\n  5   40  Ozone  Solar.R  Wind  Temp\r\n  5   41  Ozone  Solar.R  Wind  Temp\r\n  5   42  Ozone  Solar.R  Wind  Temp\r\n  5   43  Ozone  Solar.R  Wind  Temp\r\n  5   44  Ozone  Solar.R  Wind  Temp\r\n  5   45  Ozone  Solar.R  Wind  Temp\r\n  5   46  Ozone  Solar.R  Wind  Temp\r\n  5   47  Ozone  Solar.R  Wind  Temp\r\n  5   48  Ozone  Solar.R  Wind  Temp\r\n  5   49  Ozone  Solar.R  Wind  Temp\r\n  5   50  Ozone  Solar.R  Wind  Temp\r\n\r\nmodelFit2 <- with(tempData2,lm(Temp~ Ozone+Solar.R+Wind))\r\nsummary(pool(modelFit2))\r\n\r\n\r\n         term    estimate   std.error statistic        df\r\n1 (Intercept) 72.60178955 2.915916315 24.898448 105.59368\r\n2       Ozone  0.16345639 0.026054628  6.273603  99.86352\r\n3     Solar.R  0.01193645 0.007134344  1.673097 120.41496\r\n4        Wind -0.33592048 0.222350762 -1.510768 107.41496\r\n       p.value\r\n1 0.000000e+00\r\n2 9.122568e-09\r\n3 9.690433e-02\r\n4 1.337838e-01\r\n\r\nNa aanpassing, krijgen we (in dit geval) min of meer dezelfde\r\nresultaten als voorheen, waarbij alleen Ozone statistische significantie\r\nvertoont.\r\nDe code kun je hier\r\nvinden.\r\nLiteratuur\r\nAlice, M. (2015). Imputing missing data with R;\r\nmicepackage. R-bloggers, 4-10-2015. https://www.r-bloggers.com/2015/10/imputing-missing-data-with-r-mice-package/\r\nHeymans, M. en Eekhout, I. (2019). Applied Missing Data Analysis\r\nWith SPSS and (R)Studio. Amsterdam. https://bookdown.org/mwheymans/bookmi/\r\nVan Buuren, S. and Groothuis-Oudshoorn, C.G.M.\r\n(2011).mice: Multivariate Imputation by Chained Equations\r\nin R. Journal of Statistical Software, 45(3), 1–67.\r\nVan Buuren (2012). Multiple imputatie in vogelvlucht. https://stefvanbuuren.name/publications/2012%20Vogelvlucht%20-%20STAtOR.pdf\r\nVan Buuren, S. (2018). Flexible Imputation of Missing Data.\r\nSecond Edition. Chapman & Hall/CRC, Boca Raton, FL\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-05-missende-waarden/missende-waarden_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-31T20:29:28+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-05-kaarten-maken-met-r/",
    "title": "Kaarten maken met R",
    "description": "Hieronder een korte introductie op hoe je kaarten maakt met R, met name met het pakket `sf`.",
    "author": [
      {
        "name": "Euginio Petrovich, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-04-20",
    "categories": [],
    "contents": "\r\nInleiding\r\nHieronder een korte handleidingen die je leert om met R kaarten te\r\nmaken. Euginio Petrovich schreef: Drawing\r\nmaps with R. A basic tutorial in 2020. Hij laat je zien hoe je een\r\nkaart van Europa maakt met vooral de pakketten sfen\r\n`ggplot, hoe je geografische data combineert met een eenvoudige dataset\r\nen hoe je vervolgens deze kaart met deze gegevens verfijnt.Inderdaad een\r\nbasishandleiding. Dank je Euginio.\r\nMet R kaarten\r\ntekenen. Een basishandleiding\r\nMet kaarten kun je op een krachtige wijze informatie visualiseren.\r\nHet plotten van gegevens op een kaart kan trends en patronen aan het\r\nlicht brengen die moeilijk te zien zijn door alleen een spreadsheet te\r\nonderzoeken. Kaarten zijn ook zeer nuttig om informatie op een\r\naantrekkelijke en makkelijkere manier over te brengen aan het\r\npubliek.\r\nIn deze korte handleiding leren we hoe we eenvoudige geografische\r\nkaarten kunnen genereren met R. In het bijzonder zullen we leren hoe we\r\nde volgende kaart van de DR2 leden in Europa kunnen maken:\r\nAan de slag\r\nR is een gratis en open-source software die vele oplossingen biedt\r\nvoor het berekenen van gegevens en het produceren van visualisaties. Een\r\ngroot voordeel van R is dat de basisfunctionaliteiten kunnen worden\r\nuitgebreid met andere pakketten die vrij beschikbaar zijn op CRAN, het\r\nComprehensive R Archive Network. Bovendien is er een actieve\r\nR-gemeenschap over de hele wereld die de meeste codeervragen beantwoordt\r\ndie je kunt hebben.\r\nDe pakketten die nodig zijn voor deze handleiding kunnen worden\r\ngeïnstalleerd met:\r\n\r\n\r\n\r\nDe eerste vijf pakketten zijn specifiek ontwikkeld voor kaarten:\r\nsf wordt gebruikt om ruimtelijke gegevens te beheren,\r\nrnaturalearth en rnaturalearthdata bevatten\r\ninformatie over alle landen van de wereld, alsook informatie die nodig\r\nis om die landen op een kaart uit te zetten, en ggspatial\r\nverbetert de visualisatie van ruimtelijke gegevens. ggrepel\r\nhelpt ons bij het beheer van de labels op de kaart, terwijl\r\ntidyverse een set R-bibliotheken omvat die de standaard\r\nzijn geworden voor gegevensmanipulatie en -visualisatie.\r\nNa de installatie van de pakketten, moeten we ze laden:\r\n\r\n\r\n\r\nAlvorens de kaarten te maken, moeten we de geografische gegevens in R\r\nimporteren. Wij hebben ze opgeslagen in DR2_data, het volgende\r\ndataframe.\r\nZoals je kunt zien, zijn steden de basiseenheid van dit dataframe.\r\nVoor elk van hen hebben we het land opgegeven, het aantal leden\r\n(Members), het label dat we op de kaart zullen weergeven (het bestaat\r\nuit de naam van de stad plus het aantal leden tussen haakjes), en de\r\nbreedtegraad en lengtegraad.\r\nWe importeren het dataframe, dat is opgeslagen in een CSV bestand, in\r\nR met de functie read.csv. Aangezien we een header met de namen van de\r\nkolommen hebben gebruikt, zetten we het argument header op TRUE. We\r\nmoeten ook specificeren dat het scheidingsteken tussen de kolommen de\r\npuntkomma is en dat het decimaal scheidingsteken de komma is (en niet de\r\npunt, omdat er een Italiaanse versie van Excel is gebruikt om het\r\nbestand te produceren).\r\n\r\n\r\n\r\nJe kunt de eerste records van het dataframe controleren met het\r\ncommando head(DR2_data).\r\n\r\n  ï..ID      City     Country Members         Label      Lat      Lng\r\n1     1     Turin       Italy      14    Turin (14) 45.07049  7.68682\r\n2     2     Siena       Italy       2     Siena (2) 43.31822 11.33064\r\n3     3      Pisa       Italy       1      Pisa (1) 43.70853 10.40360\r\n4     4  Florence       Italy       1  Florence (1) 43.77925 11.24626\r\n5     5 Barcelona       Spain       1 Barcelona (1) 41.38879  2.15899\r\n6     6 Amsterdam Netherlands       1 Amsterdam (1) 52.37403  4.88969\r\n\r\nWe zijn nu klaar om onze kaart te maken.\r\nDe wereldkaart maken\r\nDe eerste stap van onze kaartoefening is het creëren van een\r\nwereldkaart. Om dit te doen, gebruiken we de functie ne_countries om\r\nlandengegevens op te halen uit rnaturalearth. We\r\nspecificeren medium als scale en sf als\r\nreturnclass van het dataframe, zodat de gegevens al in het\r\njuiste formaat zijn voor geografische kaarten maken.\r\n\r\n\r\n\r\nWij plotten deze gegevens met ggplot2, het\r\ntidyversepakket voor visualisatie, en sf:\r\n\r\n\r\n\r\nWij zullen de wereldkaart gebruiken als basiskaart waarop wij de\r\nlanden waar DR2-leden gevestigd zijn, zullen markeren.\r\nOm de DR2-landen op de kaart te markeren, moeten wij nu onze\r\nDR2-gegevens “toevoegen” aan het dataframe van de wereld. Wij doen dit\r\nmet de functie left.join.\r\n\r\n\r\n\r\nDeze functie vertelt R dat het de DR2-data moet\r\nverbinden met de worlddata door te zoeken naar een overeenkomst op de\r\nnaam van het land (we specificeren de overeenkomstige sleutel tussen de\r\ntwee gegevenssets in het by argument). Wanneer een overeenkomst wordt\r\ngevonden, worden de records uit de twee tabellen gecombineerd. Wanneer\r\ngeen overeenkomst wordt gevonden, zoals in het geval van Brazilië, wordt\r\nde waarde van de DR2-kolommen (bv. “Leden”) van de niet-overeenkomende\r\nrecords op NA gezet, de standaardcode die door\r\nR wordt gebruikt voor ontbrekende waarden. Zo zal de record\r\nBrazilië NA krijgen als waarde van de kolom “Members”. Het\r\nis belangrijk om alle landen in de wereld te behouden en niet alleen die\r\nmet DR2-leden. Anders zullen wij, wanneer wij onze gegevens op de kaart\r\nuitzetten, alle landen zonder DR2 verliezen! Daarom hebben we de\r\nleft.join gebruikt in plaats van de simple join: we willen dat R alle\r\nrecords in de “linker” dataset behoudt (d.w.z. degene die het eerste\r\nargument in de functie bevat).\r\nWij willen nu de landen met DR2 leden op de wereldkaart markeren. Om\r\ndit te doen gebruiken we een if...else in het\r\nfill argument. Indien de waarde van de kolom “Members” null\r\nis (d.w.z. gelijk aan NA), stellen wij de kleur van het land in op\r\ngrijs. Indien de waarde niet nul is, d.w.z. indien er DR2 leden zijn in\r\ndat land, stellen we de kleur in op rood. Merk op dat we in het eerste\r\ngeval de kleurnaam hebben gebruikt, terwijl we in het tweede geval de\r\nhexadecimale kleurcode hebben gebruikt die overeenkomt met de kleur van\r\nhet DR2-logo. Het argument kleur specificeert de kleur van de grenzen\r\nvan de landen.\r\n\r\n\r\n\r\nDe Europese landen waar DR2-leden gevestigd zijn, zijn te klein om op\r\neen wereldkaart op te vallen. De wereldschaal is dus niet erg effectief\r\nom de geografische spreiding van DR2 weer te geven. We moeten inzoomen\r\nop het niveau van Europa. Een zeer nuttig kenmerk van het\r\nsf-pakket is dat dit zeer gemakkelijk kan worden gedaan\r\ndoor een reeks coördinaten op te geven van het gebied waarin wij\r\ngeïnteresseerd zijn:\r\n\r\n\r\n\r\nSteden markeren (puntdata)\r\nWij weten dat DR2-leden niet alleen in bepaalde landen gevestigd\r\nzijn, maar ook in specifieke steden binnen die landen. In ons\r\nDR2-dataset hadden we de DR2-steden samen met hun geografische\r\ncoördinaten. We willen deze steden nu als punten op onze kaart\r\nplaatsen.\r\nWe moeten eerst ons dataframe converteren naar een sf object:\r\n\r\n\r\n\r\nMerk op dat we de kolommen moesten aanduiden waarin de geografische\r\ncoördinaten van onze steden zijn opgeslagen, evenals andere parameters\r\nzoals de gebruikte geografische projectie (hier WGS84, wat de CRS-code\r\n#4326 is).\r\nWe kunnen nu de punten van de steden op de kaart plotten:\r\n\r\n\r\n\r\nOm ze duidelijk te zien, laten we inzoomen op Europa, zoals we eerder\r\nhebben geleerd:\r\n\r\n\r\n\r\nLabels toevoegen\r\nOm de interpretatie van onze kaart te vergemakkelijken, is het zeer\r\nnuttig om enkele labels toe te voegen. Wij willen bijvoorbeeld weten\r\nhoeveel DR2-leden gevestigd zijn in de steden die wij eerder hebben\r\naangegeven. We hebben de tekst van de labels al in de kolom “Label” van\r\nhet DR2-datasetje. Nu moeten we deze tekst op de kaart visualiseren. We\r\ndoen dit door gebruik te maken van de functie\r\ngeom_label_repel. Deze functie, die is opgenomen in het\r\npakket ggrepel, verbetert de positionering van labels op\r\neen plot: ze stoot labels van elkaar af, weg van datapunten, en weg van\r\nde randen van het plotgebied.\r\nIn de parameter esthetica van de functie specificeren we dat we de\r\netiketten op de kaart willen plaatsen op basis van de breedte- en\r\nlengtegraad van de steden, en dat hun tekst wordt aangegeven in de kolom\r\n“Label”. De andere parameters specificeren de kleur van de labels, de\r\ngrootte van de tekst, en de hoeveelheid “afstotingskracht” van het\r\npositioneringsalgoritme.\r\n\r\n\r\n\r\nEr is echter een probleem. Als we inzoomen op Europa, vinden we een\r\n“indringer”: het label “Montreal (1)” zou niet mogen verschijnen op de\r\nEuropese kaart!\r\nOm dit probleempje op te lossen, moeten we de steden in landen buiten\r\nEuropa uitfilteren. We maken dus een deelverzameling van het\r\nDR2-datasetje en specificeren dat we alle records willen behouden\r\nwaarvan het land niet ( != ) Canada is:\r\n\r\n\r\n\r\nAls we de nieuwe dataset op de Europese kaart uitzetten, ontdekken we\r\ndat de indringer is verwijderd:\r\n\r\n\r\n\r\nHet is duidelijk dat er redenen kunnen zijn om het label Montreal te\r\nbehouden: bijvoorbeeld om aan te tonen dat DR2 ook overzeese leden\r\nheeft.\r\nVerbetering van de kaart\r\nIn de tot nu toe gegenereerde versies van de kaart wordt de\r\ninformatie over het aantal leden van DR2 in de labels weergegeven, als\r\neen getal tussen haakjes. Is het mogelijk om dit als een visueel kenmerk\r\nweer te geven, zodat het meteen in het oog springt? Een eerste idee zou\r\nkunnen zijn om de grootte van de labels evenredig met het aantal leden\r\nte veranderen:\r\n\r\n\r\n\r\nHet resultaat is echter vrij slecht, vanwege het grote verschil in\r\ngrootte tussen Turijn en de andere steden. Aangezien de meeste steden\r\nslechts één lid hebben, zijn hun labels te klein om leesbaar te zijn.\r\nMerk op dat R automatisch een legende toevoegt om de\r\ngrootte van de labels te interpreteren.\r\nEen betere oplossing is om de grootte van de stadspunten evenredig te\r\nlaten zijn met het aantal leden:\r\n\r\n\r\n\r\nMerk op dat R automatisch een legende creëert op basis van de grootte\r\nvan de punten:\r\nOp dezelfde manier kunnen we ook de kleur van de punten gebruiken om\r\nhet aantal leden weer te geven. We passen de kleurenschaal aan door het\r\nuiterste in te stellen op blauw en groen, zodat de grote steden in blauw\r\nen de kleine steden in groen worden gekleurd:\r\n\r\n\r\n\r\nMerk op dat R een tweede legende toevoegt om de kleur\r\nvan de punten te interpreteren:\r\nDe laatste kaart lijkt me echter “overbelast”. Dezelfde informatie\r\n(de DR2 leden) wordt op drie verschillende manieren gevisualiseerd: met\r\neen getal in het label, met de grootte van de punten, en met de kleur\r\nvan de punten. Persoonlijk vind ik deze oplossing overbodig. Ik denk dat\r\nde tweede kaart het meest evenwichtig (en esthetisch het meest\r\naangenaam) is.\r\nLaatste accenten\r\nEen groot voordeel van ggplot2 is dat het toelaat om\r\nbijna alle grafische aspecten van de visualisaties te controleren. Door\r\nde parameters in de thema-functie te wijzigen, kunnen we onze kaart\r\nverfijnen tot ze aan onze smaak beantwoordt. Om de uiteindelijke versie\r\nvan de kaart te realiseren, veranderen we de kleur van de achtergrond\r\nvan de kaart (dat is de oceaan) in lichtblauw, we verwijderen de titels\r\nvan de assen, de teksten en de vinkjes en de legenda. Tenslotte voegen\r\nwe een titel toe aan onze kaart.\r\n\r\n\r\n\r\nDe laatste stap is het opslaan van de kaart in een geschikt formaat.\r\nWe slaan zowel een PDF-versie van de kaart op, die de hoogste kwaliteit\r\nbehoudt, als een lichtere PNG-versie:\r\n\r\n\r\n\r\n\r\nZo ziet het eruit\r\nVerder lezen\r\nDeze korte handleiding is grotendeels geïnspireerd door de tutorial\r\ndie Euginio gebruikte om de basis van mapping met R te leren. Hij legt\r\nheel duidelijk verschillende andere onderwerpen uit die met maps te\r\nmaken hebben en hij raadt deze zeker aan. Een andere nuttige tutorial is\r\nvoor hem deze,\r\ndie uitlegt hoe je een ander R-pakket voor kaarten, ggmap, gebruikt en\r\neen aantal veelvoorkomende data wrangling operaties.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-05-kaarten-maken-met-r/kaarten-maken-met-r_files/figure-html5/wereldkaart-maken-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-hierarchische-logistische-regressie-met-bayes/",
    "title": "Hierarchische logistische regressie met Bayes",
    "description": "Dit is een blog over hoe hiërarchische logistische regressie werkt met gebruik van Bayesiaanse technieken.",
    "author": [
      {
        "name": "Johnson e.a. en Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-03-22",
    "categories": [],
    "contents": "\r\nInleiding\r\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\r\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\r\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\r\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van een deel van het achttiende hoofdstuk van het vierde deel (Non-Normal Hierarchical Regression & Classification).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling\r\nHierarchische logistische regressie\r\nEerst maar een enkele pakketten laden:\r\n\r\n\r\n\r\nBergbeklimmers proberen grote hoogten te beklimmen in de majestueuze Nepalese Himalaya. Dit doen ze vanwege de sensatie van ijle lucht, de uitdaging of het buitenleven. Succes is niet gegarandeerd; slecht weer, defecte uitrusting, verwondingen of gewoon pech zorgen ervoor dat niet alle klimmers hun bestemming bereiken. Dit roept enkele vragen op. Hoe groot is de kans dat een bergbeklimmer de top haalt? Welke factoren kunnen bijdragen aan een hoger succespercentage? Naast het vage gevoel dat een gemiddelde klimmer 50% kans op succes heeft, wegen we dit zwak informatief inzicht af tegen data van klimmers die in het bayesrules pakket zitten. Dit deel van de data is beschikbaar gesteld door ‘The Himalayan Database’ (2020) en verspreid via het #tidytuesday project (R for Data Science 2020b):\r\n\r\n\r\n\r\nDeze dataset bevat de resultaten van 2076 klimmers vanaf 1978. Slechts 38,87% van hen slaagde erin de top te bereiken:\r\n\r\n[1] 2076\r\n success    n   percent\r\n   FALSE 1269 0.6112717\r\n    TRUE  807 0.3887283\r\n\r\nOmdat member_id in essentie een rij van klimmersid is en we maar één observatie per klimmer hebben, is dit geen groepsvariabele. Verder, hoewel het seizon (seison), rol bij de expeditie (expedition_role) en het gebruik van zuurstof (oxygen_used) categorische variabelen zijn meerdere malen geobserveerd, zijn dit potentiële voorspellers van van succes (succes), ook geen groepsvariabele. Dan blijft expeditie_id (expedition_id) over - dit is wel een groepsvariabele. De dataset beslaat 200 verschillende expedities:\r\n\r\n[1] 200\r\n\r\nElke expeditie bestaat uit meerdere klimmers. Zo vertrokken onze eerste drie expedities met respectievelijk 5, 6 en 12 klimmers:\r\n\r\n# A tibble: 3 x 2\r\n  expedition_id count\r\n  <chr>         <int>\r\n1 AMAD03107         5\r\n2 AMAD03327         6\r\n3 AMAD05338        12\r\n\r\nHet zou fout zijn om deze groepsstructuur te negeren en er anders van uit te gaan dat de individuele klimmers onafhankelijke resultaten boeken. Aangezien elke expeditie als een team werkt, hangt het succes of falen van de ene klimmer in díe expeditie gedeeltelijk af van het succes of falen van anderen in de groep. Bovendien vertrekken alle leden van een expeditie met dezelfde bestemming, met dezelfde leiders en onder dezelfde weersomstandigheden, en zijn dus onderhevig aan dezelfde externe succesfactoren. Het is dus niet alleen juist om rekening te houden met de groepering van de gegevens, maar het kan ook duidelijk maken in welke mate deze factoren variabiliteit veroorzaken in de succespercentages tussen expedities. Meer dan 75 van onze 200 expedities hadden een 0% succesratio - m.a.w. geen enkele klimmer in deze expedities slaagde erin de top te bereiken. Daarentegen hadden bijna 20 expedities een 100% succespercentage. Tussen deze extremen in, is er heel wat variatie in het succespercentage van de expedities.\r\n\r\n\r\n\r\n\r\n\r\n\r\nModel bouw en simulatie\r\nOm de ‘gegroepeerde’ aard van onze gegevens te weerspiegelen, laat \\(Y_ij\\) aangeven of klimmer \\(i\\) in expeditie\\(j\\) succesvol de top van hun piek bereikt:\r\n\\[\\\r\nY_ij = \\begin{cases}\r\n1 Ja \\\\\r\n0 Nee\\\\\r\n\\end{cases}\r\n\\]\\] Er zijn verschillende potentiële voorspellers voor het succes van klimmers in onze dataset. We kijken hier naar slechts twee voorspellers: de leeftijd van de klimmer en of hij extra zuurstof heeft gekregen om gemakkelijker te kunnen ademen op grote hoogte. Als zodanig, definiëren we: \\[ X_ij1=leeftijd van klimmer *i* in expeditie *j*\\]\r\n\\[X_ij2=of de klimmer in *i* in expeditie *j* zuurstof (oxygen) heeft gekregen\\] Door het aandeel van succes te berekenen bij elke combinatie van leeftijd en zuurstofgebruik, krijgen we een idee van hoe deze factoren gerelateerd zijn aan het klimmerssucces (zij het een wankel idee gezien de kleine steekproefgroottes van sommige combinaties). Kort samengevat lijkt het erop dat het succes van klimmers afneemt met de leeftijd en sterk toeneemt met het gebruik van zuurstof:\r\n\r\n\r\n\r\nOm een Bayesiaans model van deze relatie op te stellen, erkennen we eerst dat het Bernoulli model redelijk is voor onze binaire responsvariabele \\(Y_ij\\). Stel \\(\\pi_ij\\) de waarschijnlijkheid is dat klimmer\\(i\\) in expeditie\\(j\\) zijn piek succesvol beklimt, d.w.z. dat \\(Y_ij=1\\),\r\n\\[Y_ij|\\pi_ij \\sim \\Bern{\\pi_ij}\\] Dit is een complete pooling benadering waarbij een simpel model wordt omgezet in een logistisch regressie model van \\(Y\\) met enkele voorspellers \\(X\\)\r\n\\[Y_ij|\\beta_0,\\beta_1,beta_2 \\sim^{ind} \\Bernoulli(\\pi_ij) with log(\\frac{\\pi_ij}{1-\\pi_ij})=\\beta_0+\\beta_1X_ij1+\\beta_2X_ij2) \\\\\r\n \\beta_0c \\sim N(m_0,s_0^2) \\\\\r\n \\beta_1 \\sim N(m_1, s_1^2) \\\\\r\n \\beta_2 \\sim N(m_2, s_1^2)\\]\r\nDit is een goed begin, MAAR het houdt geen rekening met de groepsstructuur van onze data. Overweeg in plaats daarvan het volgende hiërarchische alternatief met onafhankelijke, zwak informatieve priors hieronder afgestemd via stan_glmer() en met een prior model voor \\(beta_0\\) uitgedrukt via het gecentreerde intercept \\(beta_0c\\). Het is immers zinvoller om na te denken over de baseline succesratio bij de typische/gemiddelde klimmer, \\(\\beta_0c\\), dan bij 0-jarige klimmers die geen zuurstof gebruiken, \\(\\beta_0\\)$. Daarom begonnen we onze analyse met de zwakke veronderstelling dat de typische klimmer een kans op succes heeft van 0,5, of met log(kans op succes)=0.\r\nNet zo goed kunnen we dit logistische regressiemodel met willekeurige intercepts omvormen door de expeditiespecifieke intercepties uit te drukken als aanpassingen op het algemene intercept,\r\n\\[log(\\frac{\\pi_ij}{1-\\pi_ij})=(\\beta_0+b_0j) +\\beta_1X_ij1 + \\beta_2X_ij2\\] met \\(\\beta_0j|\\sigma_0 \\sim^{ind} N(0,\\sigma_0^2)\\) Laten we eens naar de betekenis van en de veronderstellingen achter de modelparameters kijken:\r\nDe expeditie-specifieke intercepten \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\r\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een gemiddeld intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\r\nBeta_1$ beschrijft het gemiddelde verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(beta_2\\) de gemiddelde relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\r\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar delen de gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\). Anders gezegd, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\r\nOm de posterior van het model te simuleren, combineert de stan_glmer() code hieronder het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat (à la Hoofdstuk 13) en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groeperingsstructuur (à la Hoofdstuk 17): Consider the meaning of, and assumptions behind, the model parameters:\r\nDe expeditie-specifieke intercepts \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\r\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een globaal intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\r\nBeta_1$ beschrijft het globale verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(beta_2\\) de globale relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\r\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\) delen. In gewone taal, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\r\nOm de posterior van het model te simuleren, combineert de stan_glmer() code, zie hieronder, het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groepstructuur:\r\n\r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 83.034 seconds (Warm-up)\r\nChain 1:                91.697 seconds (Sampling)\r\nChain 1:                174.731 seconds (Total)\r\nChain 1: \r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\r\nChain 2: \r\nChain 2: Gradient evaluation took 0.001 seconds\r\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\r\nChain 2: Adjust your expectations accordingly!\r\nChain 2: \r\nChain 2: \r\nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 2: \r\nChain 2:  Elapsed Time: 88.463 seconds (Warm-up)\r\nChain 2:                77.247 seconds (Sampling)\r\nChain 2:                165.71 seconds (Total)\r\nChain 2: \r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\r\nChain 3: \r\nChain 3: Gradient evaluation took 0 seconds\r\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 3: Adjust your expectations accordingly!\r\nChain 3: \r\nChain 3: \r\nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 3: \r\nChain 3:  Elapsed Time: 84.377 seconds (Warm-up)\r\nChain 3:                77.284 seconds (Sampling)\r\nChain 3:                161.661 seconds (Total)\r\nChain 3: \r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\r\nChain 4: \r\nChain 4: Gradient evaluation took 0 seconds\r\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 4: Adjust your expectations accordingly!\r\nChain 4: \r\nChain 4: \r\nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 4: \r\nChain 4:  Elapsed Time: 83.91 seconds (Warm-up)\r\nChain 4:                71.386 seconds (Sampling)\r\nChain 4:                155.296 seconds (Total)\r\nChain 4: \r\n\r\nJe wordt aangemoedigd deze simulatie te volgen met de uitvoering van de code hierboven en te kijken naar enkele MCMC-diagnoses die hieronder staan. De r:\r\n\r\n\r\n\r\nTerwijl deze diagnostiek bevestigt dat onze MCMC simulatie op het juiste spoor zit, geeft een posterior predictive check hieronder aan dat ons model op het juiste spoor zit. Van elk van de 100 posterior gesimuleerde datasets, stellen we de proportie klimmers vast die succesvol waren met de success_rate() functie. Deze succespercentages variëren van ruwweg 37% tot 41%, in een klein venster rond het werkelijk waargenomen succespercentage van 38.9% in de klimmers data.\r\n\r\n\r\n\r\nPosterior analyse\r\nIn onze posterior analyse van het succes van bergbeklimmers, concentreren we ons op het geheel. Behalve dat we gerustgesteld zijn door het feit dat we correct rekening houden met de groepsstructuur van onze gegevens, zijn we niet geïnteresseerd in een specifieke expeditie. Hieronder volgen enkele posterior samenvattingen voor onze regressieparameters \\(\\beta_0\\), \\(\\beta_1\\) en \\(\\beta_2\\).\r\n\r\n# A tibble: 3 x 5\r\n  term            estimate std.error conf.low conf.high\r\n  <chr>              <dbl>     <dbl>    <dbl>     <dbl>\r\n1 (Intercept)      -1.41     0.476    -2.04     -0.802 \r\n2 age              -0.0475   0.00940  -0.0596   -0.0358\r\n3 oxygen_usedTRUE   5.80     0.485     5.21      6.46  \r\n\r\nOm te beginnen zien we dat het 80% posterior ‘çredible’ (geloofwaardigheids) interval (CI) voor de age coëfficiënt \\(\\beta_1\\) ruim onder 0 ligt. We hebben dus significant posterior bewijs dat, wanneer we controleren of een klimmer al dan niet zuurstof gebruikt, de kans op succes afneemt met de leeftijd. Meer specifiek, als we de informatie in \\(\\beta_1\\) vertalen van de log(kansen) naar de kans schaal, is er 80% kans dat de kans op een succesvolle beklimming daalt tussen 3,5% en 5,8% voor elk jaar extra leeftijd: \\(e^{-0,0594}, e^{-0,0358}=(0,942, 0,965)\\).\r\nOp dezelfde manier levert het 80% posterior geloofwaardig interval voor de oxygen_usedTRUE coëfficiënt \\(beta_2\\) significant posterior bewijs dat, wanneer gecontroleerd wordt voor leeftijd, het gebruik van zuurstof de kans op het beklimmen van de top drastisch verhoogt. Er is een kans van 80% dat het gebruik van zuurstof kan overeenkomen met een 182- tot 617-voudige toename van de kans op succes: \\(e^{5.2}}, e^{6.43}=(182,617)\\), Zuurstof alstublieft!\r\nDoor onze waarnemingen voor \\(\\beta_1\\) en \\(\\beta_2\\) te combineren, wordt het posterior mediaan model voor de relatie tussen de log(kans op succes) van de klimmers en hun leeftijd (\\(X_1\\))en zuurstofgebruik (\\(X_2\\)) \\[log(\\frac{\\pi}{1-\\pi})=-1.42-0.0474X_1+5.79X_2\\]\r\nOf, op de schaal van waarschijnlijkheid: \\[\\pi=\\frac{e^{-1.42-0.0474X_1+5.79X_2}}{1+e^{-1.42-0.0474X_1+5.79X_2}}\\] Dit posterior mediaan model vertegenwoordigt slechts het midden van een bereik van posterior plausibele relaties tussen succes, leeftijd en zuurstofgebruik. Om een idee te krijgen van dit bereik, toont figuur hieronder 100 posterior plausibele alternatieve modellen. Zowel met als zonder zuurstof neemt de kans op succes af met de leeftijd. Bovendien, op elke leeftijd, is de kans op succes dramatisch hoger wanneer klimmers zuurstof gebruiken. Echter, onze zekerheid over deze trends varieert nogal per leeftijd. We hebben veel minder zekerheid over de slaagkans voor oudere klimmers met zuurstof dan voor jongere klimmers met zuurstof, voor wie de slaagkans over het geheel hoog is. Op dezelfde manier, maar minder drastisch, hebben we minder zekerheid over de slaagkans voor jongere klimmers die geen zuurstof gebruiken dan voor oudere klimmers die geen zuurstof gebruiken, voor wie de slaagkans uniform laag is.\r\n\r\n\r\n\r\nPosterior classificatie\r\nStel dat vier klimmers op een nieuwe expeditie gaan. Twee van hen zijn 20 jaar oud en twee zijn 60 jaar. Van beide leeftijdsgroepen is één klimmer van plan zuurstof te gebruiken en de andere niet:\r\n\r\n  age oxygen_used expedition_id\r\n1  20       FALSE           new\r\n2  20        TRUE           new\r\n3  60       FALSE           new\r\n4  60        TRUE           new\r\n\r\nNatuurlijk willen ze allemaal weten hoe groot de kans is dat ze de top zullen bereiken. Om dit vast te stellen werken we hier met de posterior_predict() snelkoppelingsfunctie om 20.000 posterior voorspellingen (0 of 1) te simuleren voor elk van onze 4 nieuwe klimmers:\r\n\r\n     1 2 3 4\r\n[1,] 0 1 0 0\r\n[2,] 1 1 0 1\r\n[3,] 1 1 0 1\r\n\r\nVoor elke klimmer wordt de kans op succes benaderd door het geobserveerde aandeel van succes onder hun 20.000 posterieure voorspellingen. Aangezien deze kansen de onzekerheid in het basissuccespercentage van de nieuwe expeditie omvatten, zijn ze gematigder dan de algemene trends die we eerder zichtbaar maakten.\r\n\r\n      1       2       3       4 \r\n0.27815 0.80110 0.14630 0.64710 \r\n\r\nDeze voorspellingen geven meer inzicht in de verbanden tussen leeftijd, zuurstof, en succes. Bijvoorbeeld, onze posterior voorspelling is dat klimmer 1, die 20 jaar oud is en niet van plan is om zuurstof te gebruiken, 27.88% kans heeft om de top te halen. Deze kans is natuurlijk lager dan voor klimmer 2, die ook 20 is maar wel van plan is om zuurstof te gebruiken. Het is hoger dan de posterior voorspelling van succes voor klimmer 3, die ook niet van plan is zuurstof te gebruiken maar wel 60 jaar oud is. Over het algemeen is de voorspelling van succes het hoogst voor klimmer 2, die jonger is en van plan is zuurstof te gebruiken, en het laagst voor klimmer 3, die ouder is en niet van plan is zuurstof te gebruiken.\r\nPosterior kans voorspellingen kunnen omgezet worden in posterior classificaties van binaire uitkomsten: ja of nee, verwachtingen of de klimmer zal slagen of niet? Als we een eenvoudige cut-off van 0,5 zouden gebruiken om dit te bepalen, dan zouden we klimmers 1 en 3 aanraden niet aan de expeditie deel te nemen (tenminste, niet zonder zuurstof) en klimmers 2 en 4 het groene licht geven. Maar in deze specifieke context moeten we het waarschijnlijk aan de individuele klimmers overlaten om hun eigen resultaten te interpreteren en hun eigen ja-of-nee beslissingen te nemen over het al dan niet voortzetten van hun expeditie. Zo kan een kans op succes van 65,16% voor sommigen de moeite en het risico waard zijn, maar voor anderen niet.\r\nModel evaluatie\r\nOm onze klimanalyse af te ronden, vragen we ons af: Is ons hiërarchisch-logistisch model een goed model? Lang verhaal kort, het antwoord is ja. - Ten eerste, ons model is eerlijk. De gegevens die we hebben gebruikt zijn openbaar en we verwachten niet dat onze analyse een negatief effect zal hebben op individuen of de samenleving. (Nogmaals, saaie antwoorden op de vraag naar eerlijkheid zijn de beste soort.)\r\n- Ten tweede Posterior Predictive Checque controle toonde aan dat ons model niet al te verkeerd lijkt - onze posterior gesimuleerde succespercentages schommelen rond de waargenomen succespercentages in onze gegevens.\r\n- Tenslotte, voor de vraag naar posterior classificatie nauwkeurigheid, kunnen we onze posterior classificaties van succes vergelijken met de werkelijke uitkomsten voor de 2076 klimmers in onze dataset. Standaard beginnen we met een kans cut-off van 0.5 - als de kans op succes van een klimmer groter is dan 0.5, voorspellen we dat hij zal slagen. We implementeren en evalueren deze classificatieregel met classification_summary() hieronder.\r\n\r\n$confusion_matrix\r\n     y    0   1\r\n FALSE 1172  97\r\n  TRUE   77 730\r\n\r\n$accuracy_rates\r\n                          \r\nsensitivity      0.9045849\r\nspecificity      0.9235619\r\noverall_accuracy 0.9161850\r\n\r\nIn het algemeen voorspelt ons model met deze classificatieregel de resultaten goed voor 91,61% van onze klimmers. Dit ziet er behoorlijk fantastisch uit gezien het feit dat we enkel informatie gebruiken over de leeftijd en het zuurstofverbruik van de klimmers (terwijl er nog andere voorspellers te bedenken zijn (bv. bestemming, seizoen, enz.). Maar gezien de gevolgen van een foute classificatie in deze specifieke context (bv. risico op verwondingen), moeten we voorrang geven aan specificiteit, ons vermogen om te anticiperen wanneer een klimmer niet zou slagen. Om dit te bereiken voorspelde ons model slechts 92.51% van de mislukte beklimmingen correct. Om dit percentage te verhogen, kunnen we de waarschijnlijkheidsgrens in onze classificatieregel aanpassen.\r\nIn het algemeen kunnen we, om de specificiteit te verhogen, de waarschijnlijkheidsdrempel verhogen, waardoor het moeilijker wordt om “succes” te voorspellen. Na wat trial and error lijkt het erop dat cut-offs van ruwweg 0.65 of hoger een gewenst specificiteitsniveau van 95% zullen bereiken. Deze overschakeling naar 0.65 verlaagt natuurlijk de gevoeligheid van onze posterior classificaties, van 90.46% naar 81.54%, en dus ons vermogen om te detecteren wanneer een klimmer succesvol zal zijn. Wij denken dat de extra voorzichtigheid hier van belang is.\r\n\r\n$confusion_matrix\r\n     y    0   1\r\n FALSE 1213  56\r\n  TRUE  149 658\r\n\r\n$accuracy_rates\r\n                          \r\nsensitivity      0.8153656\r\nspecificity      0.9558708\r\noverall_accuracy 0.9012524\r\n\r\nLiteratuur\r\nFast, Shannon, and Thomas Hegland. 2011. “Book Challenges: A Statistical Examination.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College.\r\nLegler, Julie, and Paul Roback. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R. Chapman; Hall/CRC. https://bookdown.org/roback/bookdown-BeyondMLR/. ———. 2020b. “Himalayan Climbing Expeditions.” TidyTuesday Github Repostitory. https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-22.\r\nThe Himalayan Database. 2020. https://www.himalayandatabase.com/. Trinh, Ly, and Pony Ameri. 2016. “AirBnB Price Determinants: A Multilevel Modeling Approach.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-naeve-bayesiaanse-classificatie/",
    "title": "Naïve Bayesiaanse classificatie",
    "description": "In deze blog wordt getoond hoe Naïeve Bayesiaanse classificatieanalyse werkt.",
    "author": [
      {
        "name": "Johnson e.a. en Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-03-19",
    "categories": [],
    "contents": "\r\nBayes Rules!Inleiding\r\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\r\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en laat het enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook in op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\r\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\r\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van het veertiende hoofdstuk van het derde deel (Naïve Bayes Classification).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling\r\nNaïeve Bayesiaanse classificatie\r\nOp Antartica zijn er verschillende penguinsoorten te vinden waaronder de *Adelie**, Chinstrap en Gentoo-soorten.\r\n\\[\\\r\nY = \\begin{cases}\r\nA=Adelie \\\\\r\nC=Chinstrap \\\\\r\nG=Gentoo\\\\\r\n\\end{cases}\r\n\\]\\]\r\nWe zullen deze drie soorten classificeren op basis van het gewicht\r\n\\[\\\r\nX_1 = \\begin{cases}\r\n1=bovenhetgemiddeldegewicht \\\\\r\n0=onderhetgemiddeldegewicht\\\\\r\n\\end{cases}\r\n\\]\\]\r\n(\\(x_1=1\\) als het boven het gemiddelde gewicht van 4200 g ligt en een \\(X_1=0\\) als dat gemiddelde lager is dan 4300 gram).\r\nVerder is er \\[x_2=snavellengte (in mm)\\]\\[x_3=flipperlengte\\]\r\nDe penguins_bayes data, oorspronkelijk ter beschikking gesteld door Gorman, Williams, en Fraser (2014) en vervolgens verspreid door Horst, Hill, en Gorman (2020), bevat de bovenstaande soort- en kenmerkinformatie voor een steekproef van 344 Antarctische pinguïns:\r\n\r\n\r\n\r\nOnder deze 344 pinguïns zijn er 152 Adelies, 68 Chinstrap/Kinband pinguïns, en 124 Gentoos. We gaan er steeds van uit dat de proportionele verdeling van deze soorten in onze dataset de verdeling van de soorten in het wild weerspiegelt. Dat wil zeggen dat we bij elke nieuwe pinguïn aannemen dat het hoogst waarschijnlijk een Adélie is (44,2%) en het minst waarschijnlijk een Chinstrap(19,8%). Zo ziet de geschatte verdeling er dan uit:\r\n\r\n   species   n   percent\r\n    Adelie 152 0.4418605\r\n Chinstrap  68 0.1976744\r\n    Gentoo 124 0.3604651\r\n\r\nEr zijn drie mogelijkheden of categorieën voor de \\(Y\\). Gelukkig zijn er allerlei hulpmiddel om te schatten over welke soort we het hebben. Logistische regressie werkt hier niet zo goed voor, naïeve Bayes classificatie wel. Ten opzichte van Bayesiaanse logistische regressie heeft naïeve Bayes classificatie een paar voordelen:\r\nhet kan categorische respons variabelen classificeren, dus Y met twee of meer categorieën;\r\ner is niet veel theorie nodig buiten de regel van Bayes;\r\nen zij is rekenkundig efficiënt, d.w.z. dat er geen MCMC-simulatie voor nodig is.\r\nClassificeren van één pinquin\r\nLaten we met het Bayesiaans classificeren van één penguin beginnen. Stel dat we een penguin vinden die minder dan 4200 gram weegt, die een snavel heeft van 50 mm en een flipper van 195mm. We willen iets ontwikkelen dat ons helpt om vast te stellen met welk soort we hier te maken hebben.Laten we de drie soorten en hun gewicht eens afbeelden.\r\n\r\n\r\n\r\nFigure 1: Fig 1. Proporties van elk soort met bovengemiddeld gewicht\r\n\r\n\r\n\r\nChinstraps is relatief het lichtste soort. Maar we moeten tegelijkertijd in ons achterhoofd houden dat dit de minst voorkomende soort is. Dat wil zeggen dat we moeten denken als Bayesianen door de informatie uit onze gegevens over het te combineren met onze informatie vooraf (prior) over de proportionele verdeling van het soort om een posterior model te construeren voor de soort van onze pinguïn. De naïeve Bayes classificatie benadering van deze taak is niets meer dan een direct beroep op de beproefde Bayes’ Regel. In het algemeen, om de posterior waarschijnlijkheid te berekenen dat onze pinguïn van de soort\r\n\\[f(y|x_1) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normaliserende constante}}=\\frac{f(y)L(y|X_1)}{f(x_1)}\\] waarvoor geldt de wet van de totale waarschijnlijkheid\r\n\\[f(x_1)=\\sum_{ally^'}\\]\r\nEen tabel waarin de bovengemiddelde gewichtsstatus per soort (above_average_weight) wordt uitgesplitst, verschaft de nodige informatie om deze Bayesiaanse berekening te voltooien:\r\n\r\n   species   0   1 Total\r\n    Adelie 126  25   151\r\n Chinstrap  61   7    68\r\n    Gentoo   6 117   123\r\n     Total 193 149   342\r\n\r\nIn feite kunnen we het posterior model van de soort van onze pinguïn rechtstreeks uit deze tabel berekenen. Bijvoorbeeld, merk op dat van de 193 pinguïns die onder het gemiddelde gewicht zitten, 126 Adelies zijn. Er is dus ongeveer 65% posterior kans dat deze pinguïn een Adelie is:\r\n\\[f(y=A|x_1=0)=\\frac{126}{93}\\approx{0.6528}\\] Laten we dit resultaat bevestigen door de informatie uit onze tabel hierboven in de regel van Bayes in te voeren. Deze vervelende stap is niet om te ergeren, maar om te oefenen voor generalisaties die we zullen moeten maken in meer ingewikkelde omgevingen. Ten eerste, onze informatie over het soort geeft aan dat Adelies het meest voorkomen en Chinstraps het minst:\r\n\\[f(y=A)=\\frac{151}{342}, f(y=C)=\\frac{68}{342}, f(y=G)=\\frac{123}{342}\\]\r\nVerder tonen de waarschijnlijkheden aan dat een lager dan gemiddeld gewicht het meest voorkomt bij Chinstrapspinguïns. Bijvoorbeeld, 89.71% van de Chinstrapspinguïns maar slechts 4.88% van de Gentoos hebben een lager dan gemiddeld gewicht:\r\n\\[L=(y=A|x_1=0)=\\frac{126}{151}\\approx{0.8344} \\\\\r\n  L=(y=A|x_1=0)=\\frac{61}{68}\\approx{0.8971} \\\\\r\n  L=(y=A|x_1=0)=\\frac{6}{123}\\approx{0.0488}\\]\r\nDoor deze priors en waarschijnlijkheden te gebruiken, wordt de totale waarschijnlijkheid van een pinguïn met een lager dan gemiddeld gewicht voor alle soorten\r\n\\[f(x_1=0)=\\frac{151}{342}\\times\\frac{126}{151}+\\frac{68}{342}\\times\\frac{61}{68}+\\frac{123}{342}\\times\\frac{6}{123}=\\frac{193}{342}\\] Tenslotte, door de regel van Bayes kunnen we bevestigen dat er een 65% posterior kans is dat deze pinguïn een Adelie is:\r\n\\[f(y=A|x_1=0)=\\frac{f(y=A)L(y=A|x_1=0)}{f(x_1=0)} \\\\\r\n              =\\frac{151/342.(126/151)}{193/342}\\approx{0.6528}\\]\r\nTegelijk zien we dat\r\n\\[f(y=C|x_1=0)\\approx{0.3161}\\] en \\[f(y=G|x_1=0)\\approx{0.0311}\\] Alles bij elkaar is de posterior waarschijnlijkheid dat deze pinguïn een Adelie is meer dan dubbel zo groot als die van de andere twee soorten. Dus, onze naïeve Bayes classificatie, gebaseerd op onze voorinformatie en het onder-gemiddelde gewicht van de pinguïn alleen, is dat deze pinguïn een Adelie is. Hoewel een lager dan gemiddeld gewicht relatief minder voorkomt bij Adélie’s dan bij Kinbandpinguïns, werd de uiteindelijke classificatie over de rand geduwd door het feit dat Adélie’s veel algemener zijn.\r\nÉén kwantitatieve voorspeller\r\nLaten we het gewicht van de pinguïn even buiten beschouwing en het soort indelen aan de hand van het feit dat hij een snavel van 50 mm heeft.\r\n\r\n\r\n\r\n\r\n# A tibble: 3 x 3\r\n  species    mean    sd\r\n  <fct>     <dbl> <dbl>\r\n1 Adelie     38.8  2.66\r\n2 Chinstrap  48.8  3.34\r\n3 Gentoo     47.5  3.08\r\n\r\nHet uitzetten van de afgestemde normale modellen voor elke soort bevestigt dat deze naïeve Bayes-aanname niet perfect is - het is iets idealistischer dan de dichtheidsplots van de ruwe gegevens van het figuur hierboven. Maar het is goed genoeg om verder te gaan.\r\n\r\n\r\n\r\nHerinner u dat deze Normaliteitsveronderstelling het mechanisme verschaft dat we nodig hebben om de waarschijnlijkheid van het waarnemen van een 50 mm lange snavel bij elk van de drie soorten te evalueren, \\(L(y|x_2=50)\\) Terugkomend op figuur 14.3, komen deze waarschijnlijkheden overeen met de hoogte van de normale densiteitskromme bij een snavellengte van 50 mm. Dus, een 50 mm lange bek is iets waarschijnlijker bij Kinband- dan bij Gentoo pinguïns, en hoogst onwaarschijnlijk bij Adelie pinguïns. Meer specifiek kunnen we de waarschijnlijkheden berekenen met dnorm()\r\n\r\n[1] 2.119955e-05\r\n[1] 0.1119782\r\n[1] 0.09317395\r\n\r\nTwee voorspellers\r\nWe hebben nu twee naïeve Bayes classificaties gemaakt van de soort van onze pinguïn: de ene enkel gebaseerd op het feit dat onze pinguïn onder het gemiddelde gewicht zit en de andere enkel gebaseerd op zijn 50mm lange snavel (naast onze eerdere informatie). En deze classificaties zijn niet identiek: wij classificeerden de pinguïn als Adelie in de eerste analyse en Gentoo in de tweede. Deze discrepantie toont aan dat er ruimte is voor verbetering in onze naïeve Bayes classificatie methode. In het bijzonder, in plaats van enkel te vertrouwen op één enkele voorspeller, kunnen we meerdere voorspellers in ons classificatieproces opnemen.\r\nBeschouw de informatie dat onze pinguïn een snavellengte heeft van\\(X_2=50mm\\) en een flipperlengte van \\(X_3=195mm\\). Elk van deze metingen alleen kan leiden tot een verkeerde classificatie. Net zoals het moeilijk is om een onderscheid te maken tussen de Chinstrap- en de Gentoopinguïn op basis van hun snavellengte, is het moeilijk om een onderscheid te maken tussen de Chinstrap en de Adéliepinguïn op basis van hun flipperlengte alleen (zie hieronder).\r\n\r\n\r\n\r\nMAAR de soorten zijn redelijk te onderscheiden wanneer we de informatie over snavel- en vleugellengte combineren. Onze pinguïn met een 50 mm lange bek en 195 mm lange vleugels, voorgesteld op het snijpunt van de stippellijnen in de figuur hieronder, ligt nu precies tussen de Chinstrap waarnemingen:\r\n\r\n\r\n\r\nLaten we naïeve Bayes classificatie gebruiken om deze gegevens in evenwicht te brengen met onze voorafgaande informatie over het soort lidmaatschap. Om de posterior waarschijnlijkheid te berekenen dat de pinguïn van de soort \\(Y=y\\) kunnen we de Bayes aanpassen aan onze twee voorspellers, \\(X_2=x_2\\) en \\(X_3=x_3\\).\r\nDit geeft weer een nieuwe wending: Hoe kunnen we de likelihood functie berekenen die twee variabelen omvat,\\(L(y|x_2, x_3)\\)? Dit is waar weer een andere “naïeve” veronderstelling binnensluipt. Naïeve Bayes classificatie gaat ervan uit dat voorspellers voorwaardelijk onafhankelijk zijn, dus \\[L(y|x_2, x_3)=f(x_2, x_3|y)=f(x_2|y)f(x_3|y)\\] Met andere woorden, binnen elke soort nemen we aan dat de lengte van de snavel van een pinguïn geen verband houdt met de lengte van zijn flipper. Wiskundig en computationeel gezien, maakt deze veronderstelling het naïeve Bayes algoritme efficiënt en beheersbaar. Maar het kan het ook verkeerd maken. Kijk nog eens naar de figuur hiervoven. Binnen elke soort lijken de vleugellengte en de snavellengte positief gecorreleerd te zijn, niet onafhankelijk. Toch gaan we naïef om met de veronderstelling van onvolmaakte onafhankelijkheid, en dus met de mogelijkheid dat onze classificatienauwkeurigheid zou kunnen worden afgezwakt.\r\nGecombineerd gaat het multivariabele naïeve Bayes-model ervan uit dat onze twee voorspellers Normaal en voorwaardelijk onafhankelijk zijn. We hebben dit normale model al afgestemd op de snalvellengte \\(x_2\\). Op dezelfde manier kunnen we de soortspecifieke normale modellen voor de lengte van de vleugels afstemmen op de overeenkomstige steekproefgemiddelden en standaardafwijkingen:\r\n\r\n# A tibble: 3 x 3\r\n  species    mean    sd\r\n  <fct>     <dbl> <dbl>\r\n1 Adelie     190.  6.54\r\n2 Chinstrap  196.  7.13\r\n3 Gentoo     217.  6.48\r\n\r\nZo kunnen wij voor elk van de drie soorten nagaan hoe groot de kans is dat een vleugelengte van 195 mm wordt waargenomen, \\(L(y|x_3=195)=f(x_3=195|y)\\)\r\n\r\n[1] 0.04554175\r\n[1] 0.05540502\r\n[1] 0.0001933746\r\n\r\nVoor elke soort hebben we nu de waarschijnlijkheid dat we een snavellengte waarnemen van \\(x_2=50mm\\), de waarschijnlijkheid van het waarnemen van een vleugellengte van \\(x_3=195\\) en de voorafgaande waarschijnlijkheid (prior). Gecombineerd is de kans op het waarnemen van een 50 mm lange snavel en een 195 mm lange vleugel voor elk soort \\(Y=y\\), gewogen met de prior van elk van de soorten als volgt:\r\n\\[f(y^{'}=A)L(y^{'}=A|x_2=50, x3=195)=\\frac{151}{342}\\times0.0000212\\times0.04554  \\\\\r\n  f(y^{'}=C)L(y^{'}=C|x_2=50, x3=195)=\\frac{68}{342}\\times0.0112\\times0.05541      \\\\\r\n  f(y^{'}=G)L(y^{'}=G|x_2=50, x3=195)=\\frac{123}{342}\\times0.09317\\times0.0001934  \\] met een som van \\[\\sum_{all y^{'}}f(y^{'})L(y^{'}|x_2=50, x_3=195)\\approx0.001241\\]\r\nAls we dat in de Bayes’regel stoppen dan wordt de waarschijnlijkheid dat de penguin een Adeliesoort is:\r\n\\[f(y=A|x_2=50,x_3=195)=\\frac{\\frac{151}{342}\\times0.0000212\\times0.04555}{0.001241}\\approx0.0003\\] Zo kunnen we de kans op de andere twee soorten ook berekeken: \\[f(y=C|x_2=50,x_3=195)\\approx0.9944 \\\\\r\n  f(y=G|x_2=50,x_3=195)\\approx0.0052\\] Conclusie, onze penguin is vrijwel zeker een Chinstrap. Hoewel we niet tot deze conclusie zijn gekomen op basis van een fysiek kenmerk alleen, schetsen deze twee samen een vrij duidelijk beeld.\r\nImplementeren en evalueren van naive Bayesisaanse classificatie\r\nDat was aardig, maar we hoeven niet al dit werk met de hand te doen. Om naïeve Bayes classificatie in R te implementeren, gebruiken we de naïeveBayes() functie in het e1071 pakket (Meyer et al. 2021). Net zoals bij stan_glm(), voeden we naiveBayes() met de data en een formule die aangeeft welke variabelen in de analyse moeten worden gebruikt. Maar aangezien naive Bayes de prioriteitswaarschijnlijkheden rechtstreeks uit de gegevens berekent en de implementatie geen MCMC-simulatie vereist, hoeven we ons geen zorgen te maken over het verstrekken van informatie m.b.t. de priormodellen of Markovketens. Hieronder bouwen we twee naïeve Bayes classificatie-algoritmen, een die alleen bill_length_mm gebruikt en een die ook flipper_length_mm bevat:\r\n\r\n\r\n\r\nLaten we deze beide toepassen om onze_penguin te classificeren die we de hele tijd hebben bestudeerd:\r\n\r\n\r\n\r\nWe beginnen met naive_model_1. De predict() functie geeft de posterior waarschijnlijkheden van elke soort terug, samen met een uiteindelijke classificatie. Deze classificatie volgt een eenvoudige regel: classificeer de pinguïn als de soort met de hoogste posterior waarschijnlijkheid. De resultaten van dit proces zijn gelijkaardig aan deze die we hierboven “met de hand” verkregen met een kleine afwijking door een afrondingsfout. In feite, gebaseerd op de snavellengte alleen, is onze beste gok dat deze pinguïn een Gentoo is:\r\n\r\n           Adelie Chinstrap    Gentoo\r\n[1,] 0.0001690279 0.3978306 0.6020004\r\n[1] Gentoo\r\nLevels: Adelie Chinstrap Gentoo\r\n\r\n\r\n           Adelie Chinstrap      Gentoo\r\n[1,] 0.0003445688 0.9948681 0.004787365\r\n\r\nEn net zoals we hierboven concludeerden, als we rekening houden met zowel de snavellengte als de lengte van de vleugels, is onze beste gok dat deze pinguïn een Chinstrap is.\r\nWe kunnen op dezelfde manier onze naïeve Bayes modellen toepassen om een willekeurig aantal pinguïns te classificeren. Zoals met logistische regressie, zullen we twee gebruikelijke benaderingen volgen om de nauwkeurigheid van deze classificaties te evalueren:\r\nconfusion matrixen construeren die de waargenomen soorten van onze steekproef pinguïns vergelijken met hun naïeve Bayes soortclassificaties;\r\nom een beter idee te krijgen van hoe goed onze naïeve Bayes modellen nieuwe pinguïns classificeren, berekenen we kruisgevalideerde schattingen van de nauwkeurigheid van de classificatie.\r\nOm met de eerste benadering te beginnen, classificeren we elk van de pinguïns met zowel naive_model_1 als naive_model_2 en slaan deze op in penguins als class_1 en class_2:\r\n\r\n\r\n\r\nDe classificatieresultaten worden hieronder getoond voor vier willekeurig gekozen pinguïns, afgezet tegen de werkelijke species van deze pinguïns. Voor de laatste twee pinguïns, geven de twee modellen dezelfde classificaties (Adelie) en deze classificaties zijn correct. Voor de eerste twee pinguïns, leiden de twee modellen tot verschillende classificaties. In beide gevallen is naive_model_2 correct.\r\n\r\n# A tibble: 4 x 5\r\n   bill flipper species   class_1 class_2  \r\n  <dbl>   <int> <fct>     <fct>   <fct>    \r\n1  47.5     199 Chinstrap Gentoo  Chinstrap\r\n2  40.9     214 Gentoo    Adelie  Gentoo   \r\n3  41.3     194 Adelie    Adelie  Adelie   \r\n4  38.5     190 Adelie    Adelie  Adelie   \r\n\r\nHet valt natuurlijk nog te bezien of naive_model_2 beter presteert dan naive_model_1 in het algemeen. Daartoe geven de onderstaande confusion matrixen een overzicht van de classificatienauwkeurigheid van de modellen over alle pinguïns in onze steekproef\r\n\r\n   species       Adelie Chinstrap       Gentoo\r\n    Adelie 95.39% (145) 0.00% (0)  4.61%   (7)\r\n Chinstrap  5.88%   (4) 8.82% (6) 85.29%  (58)\r\n    Gentoo  6.45%   (8) 4.84% (6) 88.71% (110)\r\n   species       Adelie   Chinstrap       Gentoo\r\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\r\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\r\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\r\n\r\nLaten we, met deze aanwijzingen in gedachten, de twee confusion matrixen onderzoeken. Een snelle blik leert dat naive_model_2 het over de hele linie beter doet. Niet alleen zijn de classificatiepercentages voor elk van de Adelie, Chinstrap en Gentoo soorten hoger dan in naive_model_1, maar ook de totale nauwkeurigheid is hoger.\r\nHet naive_model_2 classificeert 327 (146+59+122) van de 344 pinguïns correct (95%). Terwijl het naive_model_1 slechts 261 pinguïns correct classificeert (76%). Waar naive_model_2 de grootste verbetering vertoont t.o.v. naive_model_1 is in de classificatie van de Chinstrappinguïns. In naive_model_1 wordt slechts 9% van de Chinstrapspinguïns juist geklasseerd, met maar liefst 85% die verkeerd geklasseerd wordt als Gentoo. Met 87% is de classificatienauwkeurigheid voor Chinstraps veel hoger in naive_model_2.\r\nTenslotte kunnen we, voor de nodige zorgvuldigheid, 10-voudige kruisvalidatie gebruiken om te evalueren en te vergelijken hoe goed onze naïeve Bayes classificatiemodellen nieuwe pinguïns classificeren, niet enkel deze uit onze steekproef. We doen dit met behulp van de naive_classification_summary_cv() functie in het bayesrules pakket:\r\n\r\n\r\n\r\nHet cv_model_2$folds object bevat de classificatienauwkeurigheid voor elk van de 10 vouwen (k=10) terwijl cv_model_2$cv het gemiddelde neemt van de resultaten over alle 10 vouwen:\r\n\r\n   species       Adelie   Chinstrap       Gentoo\r\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\r\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\r\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\r\n\r\nDe nauwkeurigheidspercentages in deze kruisgevalideerde confusion matrixen zijn vergelijkbaar met die in de niet-kruisgevalideerde verwarringsmatrix hierboven. Dit impliceert dat ons naïef Bayes model bijna even goed lijkt te presteren op nieuwe pinguïns als op het originele pinguïn monster dat we gebruikten om dit model te bouwen.\r\nNaëve Bayes vs logistische regressie\r\nGezien de drie pinguïnsoorten, vereiste onze classificatie-analyse hierboven een naïeve Bayes classificatie - logistische regressie toepassen was zelfs geen optie. Echter, in scenario’s met een binaire categorische respons variabele \\(Y\\) zijn zowel logistische regressie als naïeve Bayes haalbare classificatiebenaderingen. Zowel naïeve Bayes als logistische regressie hebben hun voor- en nadelen. Hoewel naïeve Bayes zeker rekenkundig efficiënt is, maakt het ook een aantal zeer starre en vaak ongeschikte veronderstellingen over datastructuren. Je hebt misschien ook opgemerkt dat we wat nuance verliezen met naïeve Bayes. In tegenstelling tot het logistische regressiemodel met\r\n\\[log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta_1X_1+...\\beta_kX_p,\\] naïeve Bayes mist regressiecoëfficiënten \\(\\beta_1\\). Dus, hoewel naïeve Bayes informatie over voorspellers \\(X\\) kan omzetten in classificaties van \\(Y\\) maar doet dit zonder veel opheldering over de relaties tussen deze variabelen.\r\nOf naïeve Bayes of logistische regressie het juiste instrument is voor een binaire classificatie hangt af van de situatie. In het algemeen, als de starre naïeve Bayes-aannamen ongepast zijn of als u belang hecht aan de specifieke verbanden tussen \\(Y\\) en \\(X\\) (d.w.z. je wilt niet gewoon een reeks classificaties), dan moet je logistische regressie gebruiken. Anders is naïeve Bayes misschien precies wat je nodig hebt. Beter nog, kies niet! Probeer beide tools uit en leer ervan.\r\nSamengevat\r\nNaive Bayes classificatie is een handig hulpmiddel voor het classificeren van categorische responsvariabelen\\(Y\\) met twee of meer categorieën. Stel (\\(X_1, X_2, ...,X_p)\\) zijn een set van \\(p\\) mogelijke voorspellers van \\(Y\\), naïef Bayes berekent de posterior waarschijnlijkheid van elke categorie via de Bayes regel:\r\n\\[f(y|x_1, x_2,...,x_p)=\\frac{f(y)L(y|x_1, x_2, ...,x_p)}{\\sum_{ally^{'}f(y^{'}L(y^{'}|x_1, x_2, ...,x_p)}}\\]\r\nDaarbij worden enkele zeer naïeve veronderstellingen gemaakt over het gegevensmodel op basis waarvan wij de waarschijnlijkheid \\(L(y|x_1,x_2, ...,x_p\\) bepalen. De voorspellers \\(X_1\\) zijn voorwaardelijk onafhankelijk en de waarden van de kwantitatieve voorspellers \\(X_i\\)X variëren normaal binnen elke categorie \\(Y=y\\). Deze vereenvoudigende veronderstellingen maken het naïeve Bayes-model rekenkundig efficiënt en eenvoudig toe te passen. Maar als deze vereenvoudigende veronderstellingen niet worden nageleefd (wat vaak voorkomt), kan het naïeve Bayes-model misleidende classificaties opleveren.\r\nLiteratuur\r\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLoS ONE 9(3) (e90081). hier.\r\nJohnson, A.A., Ott, M.Q. & Dogucu, M. (2022). *Bayes Rules! An introduction to applied Bayesian Modeling. CRC Press. hier\r\nHorst, Allison, Alison Hill, and Kristen Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. hier.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-22-naeve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-wat-kun-je-met-bayes/",
    "title": "Wat kun je met Bayes?",
    "description": "In deze blog wordt getoond wat je met Bayes wetenschappelijk kunt: schatten, testen en voorspellen.",
    "author": [
      {
        "name": "Johnson e.a. en Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-03-14",
    "categories": [],
    "contents": "\r\nBayes RulesInleiding\r\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\r\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en laat het enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook in op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\r\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\r\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van het achtste hoofdstuk van het tweede deel (Posterior Inference & Prediction).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling\r\nPosterior inferentie en voorspelling\r\nStel je voor dat je in het Museum of Modern Art (MoMA) in New York City staat, gefascineerd door het kunstwerk voor je. Hoewel je begrijpt dat “moderne” kunst niet noodzakelijk “nieuwe” kunst betekent, komt er toch een vraag bij je op: wat is de kans dat deze moderne kunstenaar Gen X is of zelfs jonger, d.w.z. geboren in 1965 of later? Hier voeren we een Bayesiaanse analyse uit met als doel deze vraag te beantwoorden. Laat daartoe\\(\\pi\\) het aandeel zijn van de kunstenaars vertegenwoordigd in de grote Amerikaanse musea voor moderne kunst die Gen X of jonger zijn. Het Beta(4,6) prior model voor \\(\\pi\\) dat onze eigen zeer vage aanname weerspiegelt dat grote moderne kunstmusea onevenredig veel kunstenaars tonen die geboren zijn voor 1965, d.w.z, \\(\\pi\\) valt hoogstwaarschijnlijk onder 0,5. Moderne kunst” dateert immers van de jaren 1880 en het kan een tijdje duren voor men zo’n hoge erkenning in de kunstwereld bereikt.\r\nOm meer te weten te komen over \\(\\pi\\), zullen we \\(n\\)=100 kunstenaars uit de collectie van het MoMA nemen. Deze moma_sample dataset in het bayesrules pakket is een subset van gegevens die door het MoMA zelf beschikbaar zijn gesteld (zie MuseumofModernArt 2020-“MoMA – Collection.” GitHub Repository).\r\n\r\n\r\n\r\nOnder deze groep artiesten zitten 14 Gen X of jongere artiesten (\\(Y=14\\)).\r\n\r\n# A tibble: 2 x 2\r\n  genx      n\r\n  <lgl> <int>\r\n1 FALSE    86\r\n2 TRUE     14\r\n\r\n\\(Y|\\pi\\) kun je het beste opvatten en dat betekent dat onze analyse het bèta-binomiale kader volgt. On aangepast posterior model van \\(\\p\\) in het licht van de waargenomen kunstgegevens ziet er als volgt uit:\r\n\\[Y|\\pi \\sim Bin(100,\\pi) \\\\\r\n  \\pi\\sim Beta(4,6)\\] wordt \\[\\pi|Y=14) \\sim Beta(18,92)\\]\r\nmet de corresponderende posterior pdf \\[f(\\pi|y=14)=\\frac{\\Gamma(18+92)}{\\Gamma(18)\\Gamma(92)}\\pi^{18-1}(1-\\pi)^{92-1} for \\pi \\epsilon[0,1].\\]\r\nDe evolutie in ons begrip van \\(\\pi\\) is hieronder te zien. Terwijl we begonnen met een vaag begrip dat minder dan de helft van de tentoongestelde kunstenaars Gen X zijn, hebben de gegevens ons met enige zekerheid doen stellen dat dit cijfer waarschijnlijk onder 25% ligt.\r\n\r\n\r\n\r\nNadat we succesvol de posterior hebben geconstrueerd, besefen we ons dat er nog veel werk voor ons ligt. We moeten deze posterior kunnen gebruiken om een rigoureuze posterior analyse uit te voeren. Er zijn drie algemene taken in posterior analyse: schatting, hypothesetest, en voorspelling. Bijvoorbeeld, wat is onze preciese schatting van \\(\\pi\\)? Ondersteunt ons model de bewering dat minder dan 20% van de museumkunstenaars Gen X of jonger zijn? Als we 20 extra museumkunstenaars zouden nemen, hoeveel voorspellen we dan dat Gen X of jonger zullen zijn? Laat je gevoel eens spreken, wat denk je:\r\na. Ongeveer 16% van de museum kunstenaars zijn Gen X of jonger.\r\nb. Het is zeer waarschijnlijk dat ongeveer 16% van de museumkunstenaars Gen X of jonger is, maar dat cijfer zou ook tussen 9% en 26% kunnen liggen.\r\nAls je antwoordde met antwoord b, is je denkwijze Bayesiaans van geest.\r\n[Figuur: Ons Beta(18, 92) posterior model voor \\(π\\) (links) naast een alternatief Beta(4, 16) posterior model (rechts). De gekleurde gebieden geven de overeenkomstige 95% posterior geloofwaardigheids intervallen weer voor ]\r\nDe clou hier is dat posterieure schattingen zowel de centrale tendens als de variabiliteit in \\(\\pi\\). Het posterior gemiddelde en de modus van \\(\\pi\\) geven een snel overzicht van enkel de centrale tendens. Deze kenmerken voor onze Beta(18, 92) posterior volgen uit de algemene Beta-eigenschappen komen overeen met onze bovenstaande observatie dat de Gen X vertegenwoordiging hoogstwaarschijnlijk rond 16% ligt:\r\n\\[E(\\pi|Y=14)=\\frac{18}{18+92} \\approx 0.164 \\\\\r\nMode(\\pi|Y=14)=\\frac{18-1}{18+92-2} \\approx 0.157\\]\r\nPosterior schatting\r\nLaten we eens opnieuw kijken naar het Beta(18, 92) posterior model voor \\(\\pi\\), het percentage moderne kunst museumkunstenaars dat Gen X of jonger is (zie figuur hierboven). In een Bayesiaanse analyse kunnen we dit hele posterior model zien als een schatting van \\(\\pi\\). Immers, dit model van posterior plausibele waarden geeft een compleet beeld van de centrale tendens en onzekerheid in \\(\\pi\\). Maar bij het specificeren en communiceren van ons posterior begrip is het ook nuttig om eenvoudige posterior samenvattingen te berekenen van \\(\\pi\\).\r\nBeter is het nog, om zowel de centrale tendens als de variabiliteit in \\(\\pi\\) te kunnen rapporteren als een reeks van aannemelijke posterior aannemelijke \\(\\pi\\) waarden. Dit bereik wordt een posterior geloofwaardigheids interval (CI, Credible Interval) genoemd voor \\(\\pi\\). We hebben bijvoorbeeld eerder opgemerkt dat het aandeel museumkunstenaars dat Gen X of jonger is, hoogstwaarschijnlijk tussen 10% en 24% ligt. Dit bereik vangt de meer plausibele waarden van \\(\\pi\\) terwijl de meer extreme en onwaarschijnlijke scenario’s worden geëlimineerd. In feite zijn 0,1 en 0,24 de 2,5e en 97,5e posterior percentielen (d.w.z. 0,025ste en 0,975ste posterior kwantielen). Dit zijn is het middelste deel, 95% CI van de posterior geloofwaardige \\(\\pi\\)waarden. We kunnen deze Beta(18,92) posterior quantiel berekeningen bevestigen met qbeta():\r\n\r\n[1] 0.1009084 0.2379286\r\n\r\nHet resulterende 95% geloofwaardigheidsinterval voor \\(\\pi\\), (0,1, 0,24), wordt weergegeven door het gekleurde gebied in de figuur hierboven (links). Terwijl het gebied onder de gehele posterior pdf 1 is, is het gebied van dit gekleurde gebied 0,95, dus de fractie van \\(\\pi\\)waarden die in dit gebied vallen. Dit onthult een intuïtieve interpretatie van de CI. Er is een 95% posterior waarschijnlijkheid dat ergens tussen 10% en 24% van de museumkunstenaars Gen X of jonger zijn:\r\n\\[P(\\pi\\epsilon(0.1,0.24)|Y=14)=\\int_{0.1}^{0.24}f(\\pi|y=14)d\\pi=0.95\\] Sta hier alstublieft even stil. Voelt deze interpretatie natuurlijk en intuïtief aan? Dus, een beetje anticlimactisch? Als dat zo is, zijn we blij dat je er zo over denkt - het betekent dat je denkt als een Bayesiaan.\r\nBij de constructie van de CI hierboven hebben we een “middelste 95%” benadering gebruikt. Dit is niet onze enige optie. De eerste aanpassing die we kunnen doen is het geloofwaardige niveau van 95% (zie figuur hieronder). Bijvoorbeeld, een middelste 50% CI, van het 25ste tot het 75ste percentiel, zou onze aandacht vestigen op een kleiner bereik van enkele van de meer plausibele \\(\\pi\\) waarden. Er is een 50% posterior waarschijnlijkheid dat ergens ligt tussen 14% en 19% van de museumkunstenaars Gen X of jonger zijn:\r\n\r\n[1] 0.1388414 0.1862197\r\n\r\nIn de andere richting zou een bredere 99%-controlegrens van 0,5 tot 99,5 percentiel lopen. In dat geval wordt alleen de extreme 1% uitgeslopten. Als zodanig zou een 99% CI ons een vollediger beeld geven van plausibele en in sommige gevallen zeer onwaarschijnlijke \\(\\pi\\)waarden:\r\n\r\n[1] 0.08530422 0.26468037\r\n\r\nHoewel een 95%-niveau een gebruikelijke keuze is, is het enigszins arbitrair en gewoon ingebakken door decennia van traditie. Er is niet één “juist” geloofwaardigheidsniveau. Je kunt net zo makkelijk 50%, 80% of 95% niveaus gebruiken, afhankelijk van de context van de analyse. Elk geeft een ander posterior begrip.\r\nPosterior hypothese testing\r\nEenzijdige tests\r\nHet testen van hypothesen is een andere veel voorkomende taak bij posterior analyse. Bijvoorbeeld, stel dat we een artikel lezen waarin wordt beweerd dat minder dan 20% van de museumkunstenaars Gen X of jonger zijn. Twee aanwijzingen die we hebben waargenomen uit ons posterior model van \\(\\pi\\) wijzen erop dat deze bewering op zijn minst voor een deel plausibel is:\r\nHet grootste deel van de posterior pdf in figuur hieronder valt onder 0.2.\r\nHet 95% CI voor \\(\\pi\\) (0.1, 0.24) zit vooral onder 0.2.\r\nDeze waarnemingen zijn een goed begin. Maar we kunnen nog preciezer zijn. Om precies te evalueren hoe aannemelijk het is dat \\(\\pi<0.2\\), kunnen we de posterior waarschijnlijkheid van dit scenario berekenen, $P(<0.2|Y=14). Deze posterior waarschijnlijkheid wordt weergegeven door het gearceerde gebied onder de posterior pdf in xxxxx. Het wordt wiskundig berekend door de posterior pdf te integreren in het gebied van 0 tot 0,2:\r\n\\[P(\\pi<0.2|Y=14)=\\int_{0}^{0.2}f(\\pi|y=14)d\\pi\\] We zullen de integratie omzeilen en deze Beta(18,92) posterior waarschijnlijkheid verkrijgen met pbeta() hieronder. Het resultaat toont een sterk bewijs ten gunste van onze bewering: er is ongeveer 84,9% posterior kans dat Gen X-ers minder dan 20% van de moderne kunst museumkunstenaars uitmaken.\r\n\r\n[1] 0.8489856\r\n\r\nDe analyse van onze bewering is verfrissend eenvoudig. Wij hebben eenvoudigweg de posterior waarschijnlijkheid van het scenario van belang berekend. Hoewel dit niet altijd nodig is, formaliseren mensen uit de praktijk deze procedure vaak in een raamwerk voor het testen van hypothesen. Wij kunnen onze analyse bijvoorbeeld omkaderen met twee concurrerende hypothesen: de nulhypothese \\(H_0\\) stelt dat minstens 20% van de museumkunstenaars Gen X of jonger zijn (de status quo hier), terwijl de alternatieve hypothese \\(H_{\\alpha}\\) (onze bewering) stelt dat dit cijfer lager is dan 20%. In wiskundige notatie:\r\n\\[H_0:\\pi\\geq 0.2 \\\\\r\n  H_{\\alpha}:\\pi< 0.2\\] Merk op dat \\(H_{\\alpha}\\) beweert dat \\(\\pi\\) aan één kant van 0,2 ligt (\\(\\pi<0.2\\)) in tegenstelling tot gewoon verschillend zijn dan 0.2 (\\(\\pi\\ne0.2\\)). We noemen dit dus een eenzijdige hypothesetoets. We hebben de posterior waarschijnlijkheid van de alternatieve hypothese al berekend als \\(P(H_{\\alpha}|Y=14=0.849\\). De posterieure waarschijnlijkheid van de nulhypothese is dus \\(P(H_0|Y=14)=0.151\\). Samengenomen is de posterior odds dat \\(\\pi<0.2\\) ruwweg 5,62 zijn. Dat wil zeggen, onze posterieure beoordeling is dat \\(\\pi\\) bijna 6 keer meer kans heeft om onder 0,2 te liggen dan om boven 0,2 te liggen:\r\n\\[posterior odds=\\frac{P(H_{alpha}|Y=14)}{P(H_0|Y=14)}\\approx5.62\\]\r\n\r\n[1] 5.621883\r\n\r\n\r\n[1] 0.08564173\r\n\r\n\r\n[1] 0.09366321\r\n\r\n\r\n[1] 60.02232\r\n\r\nTweezijdige tests\r\nPosterior voorspelling\r\nPosterior analyse met MCMC\r\nHet is goed te weten dat er enige theorie achter Bayesiaanse posterior analyse zit. En wanneer we werken met modellen die zo eenvoudig zijn als de Beta-Binomiaal, kunnen we deze theorie direct implementeren - dat wil zeggen, we kunnen exacte posterior geloofwaardige intervallen, waarschijnlijkheden, en voorspellende modellen berekenen. Maar het is duidelijk dat dit mooie terrein ook betreden kan worden bij scenario’s waarin we geen posterior modellen kunnen specificeren, laat staan exacte samenvattingen van hun eigenschappen kunnen berekenen. In deze scenario’s kunnen we posteriores benaderen met behulp van MCMC methoden. Laten we nu eens onderzoeken hoe we dit soort Markov chain steekproefwaarden ook kunnen worden gebruikt om specifieke posterior kenmerken te benaderen. Laten we eens zien hoe we dit soort methodes kunnen gebruiken om posterior analyses uit te voeren.\r\nHieronder worden vier parallelle Markovketens uitgevoerd van \\(\\pi\\) voor 10.000 iteraties elk. Na het weggooien van de eerste 5.000 iteraties van elke keten, houden we nog vier afzonderlijke Markovketens van 5.000 over, {\\({\\pi^{1}, \\pi^{(2)}, ..., \\pi^{(5000)}}\\)}, of een gecombineerde Markov-keten steekproefgrootte van 20.000.\r\nPosterior simulatie\r\n\r\n\r\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 0.035 seconds (Warm-up)\r\nChain 1:                0.037 seconds (Sampling)\r\nChain 1:                0.072 seconds (Total)\r\nChain 1: \r\n\r\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 2).\r\nChain 2: \r\nChain 2: Gradient evaluation took 0 seconds\r\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 2: Adjust your expectations accordingly!\r\nChain 2: \r\nChain 2: \r\nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 2: \r\nChain 2:  Elapsed Time: 0.036 seconds (Warm-up)\r\nChain 2:                0.039 seconds (Sampling)\r\nChain 2:                0.075 seconds (Total)\r\nChain 2: \r\n\r\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 3).\r\nChain 3: \r\nChain 3: Gradient evaluation took 0 seconds\r\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 3: Adjust your expectations accordingly!\r\nChain 3: \r\nChain 3: \r\nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 3: \r\nChain 3:  Elapsed Time: 0.035 seconds (Warm-up)\r\nChain 3:                0.037 seconds (Sampling)\r\nChain 3:                0.072 seconds (Total)\r\nChain 3: \r\n\r\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 4).\r\nChain 4: \r\nChain 4: Gradient evaluation took 0 seconds\r\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 4: Adjust your expectations accordingly!\r\nChain 4: \r\nChain 4: \r\nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\r\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\r\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\r\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\r\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\r\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\r\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\r\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\r\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\r\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\r\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\r\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\r\nChain 4: \r\nChain 4:  Elapsed Time: 0.036 seconds (Warm-up)\r\nChain 4:                0.035 seconds (Sampling)\r\nChain 4:                0.071 seconds (Total)\r\nChain 4: \r\n\r\nBekijk de numerieke en visuele diagnostiek in de figuur hieronder eens. Ten eerste duiden de willekeurigheid in de sporenplots (links), de overeenstemming in de dichtheidplots van de vier parallelle ketens (midden) en een Rhat-waarde van effectief 1 erop dat onze simulatie uiterst stabiel is. Verder gedragen onze afhankelijke ketens zich elk “genoeg” als een onafhankelijk sample. De autocorrelatie, rechts weergegeven voor slechts één keten, neemt snel af en de verhouding van de effectieve steekproefgrootte is bevredigend hoog - onze 20.000 Markov-ketenwaarden zijn even effectief als 7600 onafhankelijke steekproeven (0,38 ⋅ 20000).\r\n\r\n\r\n\r\n\r\n[1] 1.000508\r\n[1] 0.4032414\r\n\r\nPosterior schatting en hypothese testen\r\nWe kunnen nu de gecombineerde 20.000 Markovketenwaarden gebruiken, met vertrouwen, om het Beta(18, 92) posterior model te benaderen van \\(\\pi\\). Het figuur hieronder bevestigt inderdaad dat de volledige MCMC-benadering (rechts) de werkelijke posterior (links) dicht benadert.\r\n\r\n\r\n\r\nAls zodanig kunnen wij elk kenmerk van het Beta(18, 92) posterior model benaderen door het overeenkomstige kenmerk van de Markov-keten. Wij kunnen bijvoorbeeld het posterieure gemiddelde benaderen door het gemiddelde van de MCMC-steekproefwaarden, of het 2,5-percentiel posterior benaderen door het 2,5-percentiel van de MCMC-steekproefwaarden. Hiertoe levert de tidy() functie in het broom.mixed pakket (Bolker en Robinson 2021. Broom.mixed: Tidying Methods for Mixed Models)[https://github.com/bbolker/broom.mixed] een aantal handige statistieken voor de gecombineerde 20.000 Markov chain waarden die zijn opgeslagen in art_sim:\r\n\r\n# A tibble: 1 x 5\r\n  term  estimate std.error conf.low conf.high\r\n  <chr>    <dbl>     <dbl>    <dbl>     <dbl>\r\n1 pi       0.161    0.0352    0.100     0.239\r\n\r\nEn de mcmc_areas()-functie in het bayesplot-pakket biedt een visuele aanvulling (zie de figuur hieronder)\r\n\r\n\r\n\r\nIn de tidy() samenvatting geven conf.low en conf.high de 2,5e en 97,5e percentielen van de Markov-ketenwaarden aan, respectievelijk 0,101 en 0,239. Deze vormen bij benadering het midden 95%- geloofwaardigheidsinterval voor \\(\\pi\\) dat wordt weergegeven door het gearceerde gebied in de mcmc_areas()-plot. Verder meldt de schatting dat de mediaan van onze 20.000 Markov chain-waarden, en dus onze benadering van de werkelijke posterieure mediaan, 0,162 is. Deze mediaan wordt weergegeven door de verticale lijn in de mcmc_areas()-plot. Net als het gemiddelde en de modus geeft de mediaan een andere maat voor een “typische” posterior \\(\\pi\\) waarde. Hij komt overeen met het 50ste posterior percentiel - 50% van posterior \\(\\pi\\) waarden liggen boven de mediaan en 50% liggen eronder. Maar in tegenstelling tot het gemiddelde en de modus, bestaat er geen eenduidige formule voor een Beta(\\(\\alpha, \\beta\\))-mediaan. Dit legt nog meer moois bloot van MCMC simulatie: zelfs als een formule ongrijpbaar is, kunnen we een posterior eenheid schatten door de overeenkomstige eigenschap van onze waargenomen Markov chain steekproefwaarden.\r\nHoewel het een mooie eerste stop is, geeft de tidy() functie niet altijd elke samenvattende statistiek die van belang is. Hij rapporteert bijvoorbeeld niet het gemiddelde of de modus van onze Markov chain steekproefwaarden. Geen probleem. We kunnen samenvattende statistieken rechtstreeks uit de Markov chain waarden berekenen. De eerste stap is het omzetten van een matrix van de vier parallelle ketens in een enkel dataframe van de gecombineerde ketens:\r\n\r\n[1] 20000     1\r\n\r\nMet de ketens in dataframe-vorm kunnen wij op de gebruikelijke manier te werk gaan en onze dplyr gereedschappen gebruiken om een en ander te transformeren en samen te vatten. Wij kunnen bijvoorbeeld direct het steekproefgemiddelde, de mediaan, de modus en de kwantielen van de gecombineerde Markov-ketenwaarden berekenen. De mediaan- en kwantielwaarden zijn precies die welke door tidy() hierboven worden gerapporteerd, en elimineren dus elk mysterie over die functie!\r\n\r\n  post_mean post_median post_mode  lower_95  upper_95\r\n1 0.1635785   0.1614223 0.1549176 0.1004316 0.2389237\r\n\r\nWij kunnen de ruwe kettingwaarden ook gebruiken om de volgende taak in onze posterior analyse aan te pakken - het testen van de bewering dat minder dan 20% van de grote museumkunstenaars Gen X zijn. Daartoe kunnen wij de posterior waarschijnlijkheid van dit scenario benaderen, \\(P(\\pi<0.20|Y=14)\\), door het aandeel Markovketen \\(\\pi\\)πwaarden die onder 0,20 vallen. Volgens deze benadering is er een kans van 84,6% dat de vertegenwoordiging van Gen X-artiesten onder 0,20 ligt:\r\n\r\n exceeds     n percent\r\n   FALSE  3043 0.15215\r\n    TRUE 16957 0.84785\r\n\r\nLaat het op je inwerken en onthoud het punt. We hebben onze MCMC simulatie gebruikt om het posterior model van \\(\\pi\\) te benaderen samen met de eigenschappen van belang. Ter vergelijking, de tabel hieronder toont de Beta(18,92) posterior kenmerken die we eerder hebben berekend naast hun overeenkomstige MCMC-benaderingen. De clou is dit: MCMC werkte. De benaderingen zijn vrij nauwkeurig. Laat dit u geruststellen - ook als modellen te ingewikkeld zijn om te specificeren, kunnen we vertrouwen hebben in onze MCMC-benaderingen van deze modellen (zolang de diagnostiek maar klopt!).\r\n\r\ngemiddelde\r\nmodus\r\n2.5\r\n97.5\r\nposterior\r\n0.16\r\n0.16\r\n0.1\r\n0.24\r\nMCMC\r\n0.1642\r\n0.1598\r\n0.1011\r\n0.2388\r\nPosterior voorspelling\r\nTenslotte kunnen wij onze Markov-ketenwaarden gebruiken om het posterior voorspellingsmodel van \\(Y^{'}\\) het aantal van de volgende 20 artiesten uit de steekproef dat Gen X of jonger zal zijn. Bonus: het simuleren van dit model helpt ons ook intuïtie op te bouwen voor de theorie die ten grondslag ligt aan de posterior voorspelling. Herinner je dat het posterior voorspellingsmodel twee bronnen van variabiliteit weerspiegelt:\r\nSteekproefvariabiliteit in de gegevens\\(Y^{'}\\) kan een willekeurig aantal kunstenaars in {0,1,…,20} en hangt af van het onderliggende aandeel van kunstenaars die Gen X zijn, \\(\\pi:Y^{'}|\\pi\\sim Bin(20,\\pi)\\).\r\nPosterieure variabiliteit in \\(\\pi\\)π De verzameling van 20.000 Markovketens \\(\\pi\\) waarden geeft bij benadering een idee van de variabiliteit en het bereik in plausibele \\(\\pi\\)waarden.\r\nOm beide bronnen van variabiliteit in posterieure voorspellingen te vatten \\(Y^{'}\\) kunnen we rbinom() gebruiken om één \\(Bin(20,\\pi\\) uitkomst \\(Y^{'}\\) van elk van de 20.000\\(\\pi\\) ketenwaarden. De eerste drie resultaten weerspiegelen een algemene trend: kleinere waarden van \\(\\pi\\) zullen meestal kleinere waarden van \\(Y^{'}\\) geven. Dit is logisch. Hoe lager de onderliggende vertegenwoordiging van Gen X-kunstenaars in het museum, hoe minder Gen X-kunstenaars we mogen verwachten in onze volgende steekproef van 20 kunstwerken.\r\n\r\n         pi y_predict\r\n1 0.1300704         2\r\n2 0.1755149         3\r\n3 0.2214144         5\r\n\r\n\r\n\r\n\r\n\r\n   mean lower_80 upper_80\r\n1 3.274        1        6\r\n\r\nBayesiaanse voordelen\r\nZoals je waarschijnlijk merkte, is het moeilijkste deel van een Bayesiaanse analyse vaak het bouwen of simuleren van het posterior model. Zodra je dat stuk op zijn plaats hebt, is het vrij eenvoudig om deze posterior te gebruiken voor schatting, hypothese toetsing en voorspelling. Daarentegen is het opbouwen van de formules om de analoge frequentistische berekeningen uit te voeren vaak minder intuïtief.\r\nWe kunnen ons ook koesteren in het gemak waarmee Bayesiaanse resultaten kunnen worden geïnterpreteerd. In het algemeen beoordeelt een Bayesiaanse analyse de onzekerheid over een onbekende parameter \\(\\pi\\) in het licht van de waargenomen gegevens \\(Y\\). Neem bijvoorbeeld de studie van de kunstenaars zoals hierboven gepresenteerd. In het licht van de waarneming dat \\(Y=14\\) van de 100 kunstenaars in de steekproef Gen X of jonger waren, stelden we vast dat er een 84,9% posterior kans was dat Gen X vertegenwoordigd was in het hele museum,\\(π\\) lager is dan 0,20:\r\n\\[P(\\pi<0.20|Y=14)=0.849\\] Deze berekening heeft geen zin in een frequentistische analyse. Omgekeerd beoordeelt een frequentistische analyse de onzekerheid van de waargenomen gegevens \\(Y\\) in het licht van veronderstelde waarden van \\(π\\). De frequentistische tegenhanger van de Bayesiaanse posterior waarschijnlijkheid hierboven is bijvoorbeeld de p-waarde, waarvan we de formule hier niet zullen behandelen:\r\n\\[PY(\\leq14|\\pi=0.20)=0.08\\] De omgekeerde volgorde van de conditionering in deze waarschijnlijkheid, \\(Y\\) gegeven \\(π\\) (\\(Y|\\pi\\)) in plaats van \\(π\\) gegeven \\(Y\\) (\\(\\pi|Y)\\)) leidt tot een andere berekening en interpretatie dan de Bayesiaanse waarschijnlijkheid: als \\(π\\) slechts 0,20 zou zijn, dan is er slechts een kans van 8% dat we een steekproef zouden hebben waargenomen waarin ten hoogste \\(Y=14\\) van de 100 kunstenaars Gen X waren. Het is niet onze manier van schrijven die onhandig is, het is de p-waarde. Hoewel het ons interessante informatie verschaft, is de vraag die het beantwoordt een beetje minder natuurlijk voor het menselijk brein: aangezien we de gegevens eigenlijk hebben geobserveerd, maar \\(\\pi\\) niet weten, kan het een breinbreker zijn om een berekening te interpreteren die het tegendeel veronderstelt. Voornamelijk bij het testen van hypothesen is het natuurlijker om te vragen “hoe waarschijnlijk is mijn hypothese?” (wat de Bayesiaanse waarschijnlijkheid antwoordt) dan “hoe waarschijnlijk zijn mijn data als mijn hypothese niet waar zou zijn?” (wat de frequentistische waarschijnlijkheid antwoordt). Aangezien p-waarden zo vaak verkeerd worden geïnterpreteerd, en dus verkeerd worden gebruikt, worden zij in het hele frequentistische en Bayesiaanse spectrum steeds minder benadrukt.\r\nDeze blog leert je hoe je een posterior model in antwoorden kunt omzetten. Dat wil zeggen dat je gebruik maakte van posterior modellen, exact of bij benadering, om drie posterior analyse taken uit te voeren voor een onbekende parameter \\(\\pi\\):\r\nPosterior schatting\r\nEen posterior geloofwaardigheidsinterval (CI) geeft een reeks van posterior aannemelijke waarden van \\(\\pi\\) en dus een idee van zowel de posterior typische waarden als de onzekerheid in \\(\\pi\\).\r\nPosterior toetsing van hypothesen Posterior waarschijnlijkheden geven inzicht in overeenkomstige hypothesen betreffende \\(\\pi\\).\r\nPosterior voorspelling\r\nHet posterior voorspellingsmodel voor een nieuw gegevenspunt \\(Y\\) houdt rekening met zowel de steekproefvariabiliteit in \\(Y\\) en de posterior variabiliteit in \\(\\pi\\).\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-22-wat-kun-je-met-bayes/wat-kun-je-met-bayes_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-bayesiaanse-principes/",
    "title": "Bayes' principes",
    "description": "Een blog over de principes van de Bayesiaanse theorie",
    "author": [
      {
        "name": "Johnson e.a. en Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-03-07",
    "categories": [],
    "contents": "\r\nBayes Rules!Inleiding\r\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q.\r\nOt en Mine Dogucu onder de titel Bayes Rules! An Introduction to\r\nApplied Bayesian Modeling en het verscheen bij CRC Press (2022).\r\nEerdere versies kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer\r\ndat ik het zag, was ik hier heel enthousiast over. Het boek heb ik\r\ndirect besteld en vorige week kon ik het ophalen.\r\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in\r\nop de fundamenten van het Bayesiaanse perspectief. Het leert je denken\r\nals een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel\r\n\\(posterior=\\frac{prior.likelihood}{normaliserende\r\nconstante}\\). Aan de hand van enkele voorbeelden gaan Johnson\r\ne.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe\r\nkennis en data op elkaar inwerken en het laat enkele basisanalyses zien\r\nen hoe dat in deze vorm van statistiek werkt (normaal, binair en\r\npoisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook\r\nonder de moterkap van deze techniek kijken. Het gaat ook op de\r\nwetenschappelijke principes van de benadering, waar je hier op moet\r\nletten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een\r\nnulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de\r\nandere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee\r\nvolgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat\r\nin op regressieanalyses voor continue variabelen en\r\nclassificatieanalyses voor binaire variabelen. Het vierde deel, ten\r\nslotte, gaat in op geclusterde datasets en hoe je hierarchische\r\nBayesiaanse regressie en classificatieanalyses uitvoert.\r\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te\r\nkrijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan\r\nwerken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke\r\nverschenen de afgelopen tien/vijftien jaar en leren je dit. Maar\r\nBayes Rules! vind ik op dit moment als introductieboek mogelijk\r\nwel het beste.\r\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer\r\neen korte recensie over schrijven. Voor nu heb ik uit elk deel een\r\nhoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een\r\nbewerking van het het vierde hoofdstuk van het eerste deel (Balance\r\nand Sequentiallity in Bayesian Analysis).Deze hoofdstukken zo\r\noverzetten is voor mij niet alleen een goede manier om het mij eigen te\r\nmaken, maar ook een manier om het boek anderen aan te raden. Dus lezen\r\nen gebruiken deze Bayes Rules! An Introduction to Applied Bayesian\r\nModeling\r\nBalans en\r\nopeenvolging in Bayesiaanse Analyses\r\nIn Alison Bechdels stripverhaal The Rule uit 1985 zegt een\r\npersonage dat ze alleen naar een film gaan als die aan de volgende drie\r\nregels voldoet (Bechdel 1986):\r\ner moeten minstens twee vrouwen in de film voorkomen;\r\ndeze twee vrouwen praten met elkaar; en\r\nze praten over iets anders dan een man.\r\nDeze criteria vormen de Bechdel-test voor de\r\nvertegenwoordiging van vrouwen in films. Als je aan films denkt die je\r\nhebt gezien, welk percentage van alle recente films slaagt dan volgens\r\njou voor de Bechdel-test? Ligt dat dichter bij 10%, 50%, 80%, of\r\n100%?\r\nLaat \\(\\pi\\), een willekeurige\r\nwaarde tussen 0 en 1, het onbekende aandeel van recente films zijn die\r\nde Bechdel test doorstaan. Drie vrienden - de feminist, de onwetende, en\r\nde optimist - hebben enkele vooroordelen over \\(\\pi\\). Nadenkend over films die zij in het\r\nverleden heeft gezien, begrijpt de feminist dat het in de\r\nmeeste films ontbreekt aan sterke vrouwelijke personages. De\r\nonwetende herinnert zich niet echt de films die hij gezien\r\nheeft, en weet dus niet zeker of het halen van de Bechdel test\r\ngebruikelijk of ongewoon is. De optimist ten slotte denkt dat\r\nde Bechdel-test een erg lage lat is voor de vertegenwoordiging van\r\nvrouwen in films, en gaat er dus van uit dat bijna alle films de test\r\ndoorstaan. Dit alles om te zeggen dat drie vrienden drie verschillende\r\nvoorafgaande modellen hebben van \\(\\pi\\). Geen probleem! Een Beta kan\r\nvoorafgaand aan het opstellen van een model worden afgestemd op iemands\r\nvoorkennis (zie de figuur hieronder).\r\nDoor de grootste aannemelijkheid vooraf te geven aan de waarden van\r\n\\(\\pi\\) die lager zijn dan 0,5,\r\nweerspiegelt de Beta(5,11) prior het inzicht van de feminist: de\r\nmeerderheid van de films doorstaat de Bechdel test niet. De Beta(14,1)\r\ndaarentegen plaatst een grotere aannemelijkheid vooraf op waarden van\r\n\\(\\pi\\) in de buurt van 1, en komt dus\r\novereen met de vooronderstelling van de optimist. Dan blijft de\r\nBeta(1,1) of Unif(0,1) over die, door een gelijke aannemelijkheid te\r\ngeven aan alle waarden van \\(\\pi\\)\r\ntussen 0 en 1, dat overeenkomt met het figuurlijke schouderophalen van\r\nde onwetende - het enige dat zij weet is dat \\(\\pi\\) een proportie is, en dus ergens\r\ntussen 0 en 1 ligt.\r\nDe drie analisten komen overeen een steekproef te nemen van \\(n\\) recente films en noteren \\(Y\\), het aantal dat de Bechdel test\r\ndoorstaat. Herkenning van \\(Y\\) als het\r\naantal “successen” in een vast aantal onafhankelijke proeven,\r\nspecificeren zij de afhankelijkheid van \\(Y\\) van \\(\\pi\\) met behulp van een binomiaal model.\r\nElke analist heeft dus een uniek Beta-Binomiaal model van \\(\\pi\\) met verschillende voorafgaande\r\nhyperparameters\\(\\alpha\\) en \\(\\beta\\):\r\n\\[Y|\\pi \\sim (Bin(n, \\pi) \\\\\r\n  \\pi \\sim Beta(\\alpha, \\beta)\\]\r\nWe weten dat elke analist een uniek posterior model heeft\r\nvan \\(\\pi\\) dat afhangt van zijn of\r\nhaar unieke prior (via \\(\\alpha\\) en \\(\\beta\\)) en de gemeenschappelijke\r\nwaargenomen gegevens (via \\(Y\\) en\r\n\\(n\\))\r\n\\[\\pi|(Y=y) \\sim Beta (\\alpha + y, \\beta\r\n+n - Y)\\] Als je denkt “Kan iedereen zijn eigen\r\nvoorkeuren hebben?! Zal dit altijd zo subjectief zijn?!” dan stel je de\r\njuiste vragen! En de vragen houden daar niet op. In hoeverre zouden hun\r\nverschillende priors van de analisten kunnen leiden tot drie\r\nverschillende posterior conclusies over de Bechdel test? Hoe zou dit\r\nkunnen afhangen van de steekproefgrootte en de uitkomsten van de\r\nfilmgegevens die ze verzamelen? In hoeverre zullen de posterior\r\ninzichten van de analisten evolueren naarmate ze meer en meer gegevens\r\nverzamelen? Zullen ze het ooit eens worden over de vertegenwoordiging\r\nvan vrouwen in film? We zullen deze fundamentele vragen hier onderzoeken\r\nen wij kijken naar ons vermogen om het Bayesiaanse denken verder te\r\nontwikkelen.\r\nDus:\r\n- We onderzoek de evenwichtige invloed van de prior en data op\r\nde posterior.\r\n- We voeren verschillende achtereenvolgende Bayesiaanse analyses\r\nuit.\r\n\r\n\r\n\r\nVerschillende\r\npriors, verschillende posteriors\r\nDe voorafgaande modellen van \\(\\pi\\)\r\n(het percentage recente films dat de Bechdel test doorstaat) zijn\r\nafgestemd op de onwetende, de feministe, en de optimist. Niet alleen\r\nweerspiegelen de verschillende prior gemiddelden onenigheid over de\r\nvraag of \\(\\pi\\) dichter bij 0 of 1\r\nligt, maar ook weerspiegelen hun verschillende niveaus van variabiliteit\r\nhet feit dat de drie analisten niet allemaal even zeker zijn van hun\r\nkennis en informatie. Hoe zekerder ze daarover zijn, hoe kleiner de\r\nvoorafgaande variabiliteit. Maar ook, hoe vager de prior-informatie, hoe\r\ngroter de prior-variabiliteit. De priors van de optimist en de onwetende\r\nvertegenwoordigen hier twee uitersten. Een Beta(14,1) prior vertoont de\r\nkleinste variabiliteit en daarmee is de optimist het meest zeker in zijn\r\nprioriteitsbegrip van \\(\\pi\\) (of om\r\nduidelijker te zijn, dat bijna alle films de Bechdel test zullen\r\ndoorstaan). Dergelijke priors noemen we informatief.\r\n\r\nEen informatieve prior reflecteert specifieke\r\ninformatie over de onbekende variabele met grote zekerheid, bv. lage\r\nvariabiliteit.\r\n\r\nMet de grootste prioriteitsvariabiliteit is de onwetende het minst\r\nzeker over \\(\\pi\\). In feite kent deze\r\nBeta(1,1) prior evenveel prior plausibiliteit toe aan elke waarde van\r\n\\(\\pi\\) tussen 0 en 1. Dit type prior\r\nmodel van “schouderophalen” (van “ik weet het echt niet”) heeft een\r\nofficiële naam: het is een vage prior.\r\n\r\nEen vage of diffuse prior\r\nreflecteert weinig specifieke informatie over de onbekende variabele.\r\nEen vlakke prior die gelijke plausibiliteit toekent aan\r\nalle mogelijke waarden van de variabele, is een speciaal geval.\r\n\r\nDe volgende voor de hand liggende vraag is dan: hoe zullen hun\r\nverschillende priors de posterior conclusies van de feminist, de\r\nonwetende en de optimist beïnvloeden? Om deze vraag te beantwoorden,\r\nhebben we gegevens nodig. Onze analisten besluiten een willekeurige\r\nsteekproef te nemen van \\(n=20\\)\r\nrecente films te bekijken, gebruikmakend van gegevens die verzameld zijn\r\nvoor het FiveThirtyEight-artikel over de Bechdel-test Het\r\nbayesrules-pakket bevat een gedeeltelijke versie van deze\r\ndataset, genaamd bechdel. Een volledige versie wordt\r\ngeleverd door het fivethirtyeight R-pakket (Kim, Ismay, and\r\nChunn 2020). Samen met de titel en het jaar\r\nvan elke film in deze dataset, geeft de binaire variabele\r\naan of de film slaagde of niet voor de Bechdel test:\r\n\r\n# A tibble: 3 x 3\r\n   year title      binary\r\n  <dbl> <chr>      <chr> \r\n1  2005 King Kong  FAIL  \r\n2  1983 Flashdance PASS  \r\n3  2013 The Purge  FAIL  \r\n\r\nVan de 20 films in deze steekproef slaagden er slechts 9 (45%) voor\r\nde test:\r\n\r\n binary  n percent\r\n   FAIL 11    0.55\r\n   PASS  9    0.45\r\n  Total 20    1.00\r\n\r\nDe posterior modellen van de drie analisten voor\\(\\pi\\) die volgen uit de formule \\(\\pi|(Y=y) \\sim Beta(\\alpha+y, \\beta+n-y)\\)\r\nop hun unieke voorgaande modellen en gemeenschappelijke filmgegevens,\r\nzijn samengevat in tabel hieronder en de figuur hierboven. Bijvoorbeeld,\r\nde posterior parameters van de feminist worden berekend door \\(\\alpha+y=5+9=14\\) en \\(\\beta+n-y=11+20-9=22\\)\r\nAnalyst\r\nPrior\r\nPosterior\r\nfeminist\r\nBeta(5,11)\r\nBeta(14,22)\r\nonwetende\r\nBeta(1,1)\r\nBeta(10,12)\r\noptimist\r\nBeta(14,1)\r\nBeta(23,12)\r\nHad je instinct gelijk? Herinner je dat de optimist begon met het\r\nvasthoudend optimistisme vooraf over \\(\\pi\\) - zijn prior model had een hoog\r\ngemiddelde met lage variabiliteit. Het is dan ook niet verwonderlijk dat\r\nzijn posterior model niet zo synchroon loopt met de gegevens als de\r\nposteriors van de andere analisten. De trieste gegevens waarbij slechts\r\n45% van de 20 films de test doorstonden was niet genoeg om hem ervan te\r\novertuigen dat er een probleem is in Hollywood - hij denkt nog steeds\r\ndat de waarden van \\(\\pi\\) boven 0,5\r\nhet meest plausibel is. Aan het andere uiterste staat de onwetende die\r\nbegon met een vlak, vaag model van \\(\\pi\\). Zonder enige voorinformatie\r\nweerspiegelt zijn posterior model direct de inzichten die zijn verkregen\r\nuit de waargenomen filmgegevens. In feite is zijn posterior niet te\r\nonderscheiden van de geschaalde likelihood functie.\r\nPosterior modelsVerschillende data,\r\nverschillende posteriors\r\nAls jij je zorgen maakt over het feit dat onze drie analisten\r\nachteraf verschillende opvattingen hebben over \\(\\pi\\), de proportie van recente films die\r\nde Bechdel halen, wanhoop dan nog niet. Vergeet niet dat data ook een\r\nrol spelen in een Bayesiaanse analyse. Om deze dynamiek te onderzoeken,\r\nbeschouwen we drie nieuwe analisten - Morteza, Nadide, en Ursula - die\r\nallen de optimistische Beta(14,1) prior voor \\(\\pi\\) delen maar elk toegang hebben tot\r\nandere gegevens. Morteza beoordeelt \\(n=13\\) films uit het jaar 1991, waarvan\r\n\\(Y=6\\) (ongeveer 46%) door de Bechdel\r\nkomen:\r\n\r\n binary  n   percent\r\n   FAIL  7 0.5384615\r\n   PASS  6 0.4615385\r\n  Total 13 1.0000000\r\n\r\nNadide beoordeelt \\(n=63\\) films uit\r\n2000, waaronder \\(Y=29\\) (ongeveer 46%)\r\ndoor de Bechdel komen:\r\n\r\n binary  n   percent\r\n   FAIL 34 0.5396825\r\n   PASS 29 0.4603175\r\n  Total 63 1.0000000\r\n\r\nTot slot, beoordeelt Ursula \\(n=99\\)\r\nfilms uit 2013, waarvan er \\(Y=46\\)\r\n(ongeveer 46%) door de Bechdel komen:\r\n\r\n binary  n   percent\r\n   FAIL 53 0.5353535\r\n   PASS 46 0.4646465\r\n  Total 99 1.0000000\r\n\r\nWat een toeval! Hoewel Morteza, Nadide, en Ursula verschillende\r\ngegevens hebben verzameld, constateren ze elk een Bechdel\r\nslaagpercentage van ongeveer 46%. Toch is hun steekproefomvang \\(n\\) verschillend - Morteza bekeek slechts\r\n13 films terwijl Ursula er 99 bekeek.\r\nDe posterior modellen van de drie analisten voor \\(\\pi\\) die volgen uit de toepassing van hun\r\ngegevens op hun gemeenschappelijke Beta(14,1) voormodel en unieke\r\nfilmgegevens, zijn samengevat in de figuur en tabel hieronder. Merk op\r\ndat hoe groter de steekproefgrootte \\(n\\) hoe “indringender” de\r\nlikelihoodfunctie. Bijvoorbeeld, de waarschijnlijkheidsfunctie die het\r\nslagingspercentage van 46% in Morteza’s kleine steekproef van 13 films\r\nweergeeft is vrij breed - deze gegevens zijn relatief plausibel voor\r\nelke \\(\\pi\\) tussen 15% en 75%.\r\nUrsula’s waarschijnlijkheidsfunctie daarentegen, met het\r\nslagingspercentage van 46% in een veel grotere steekproef van 99 films\r\nweergeeft, is smal - haar gegevens zijn onwaarschijnlijk voor \\(\\pi\\) waarden buiten het bereik van 35% tot\r\n55%. We zien dat hoe groter de waarschijnlijkheid, hoe meer invloed de\r\ngegevens hebben op de posterior. Morteza blijft het minst overtuigd door\r\nhet lage Bechdel-slaagpercentage dat in zijn kleine steekproef wordt\r\nwaargenomen, terwijl Ursula het meest overtuigd is. Haar vroege\r\noptimisme evolueerde naar een posterior begrip dat \\(\\p\\) waarschijnlijk tussen 40% en 55%\r\nligt.\r\nPosterior modelsTabel: De prior en posterior modellen voor \\(\\pi\\) zijn geconstrueerd in het licht van\r\neen gemeenschappelijke Beta(14,1)-prior en verschillende gegevens.\r\nAnalyst\r\nData\r\nPosterior\r\nMorteza\r\n\\(Y=6;n=13\\)\r\nBeta(20,8)\r\nNadide\r\n\\(Y=26;n=63\\)\r\nBeta(43,35)\r\nUrsula\r\n\\(Y=46;n=99\\)\r\nBeta(60,54)\r\nHet vinden\r\nvan de balans tussen de prior en de data\r\nObservaties aan concepten\r\nverbinden\r\nWe hebben gekeken naar de invloed die verschillende priors en\r\nverschillende gegevens kunnen hebben op ons posterior begrip van een\r\nonbekende variabele. De posterior is echter een meer genuanceerd\r\ntouwtrekken tussen deze twee kanten. De figuren hieronder illustreren\r\nhet evenwicht dat het posterior model vindt tussen de prior en de\r\ngegevens. Elke rij komt overeen met een uniek prior model en elke kolom\r\nmet een unieke reeks gegevens.\r\nVan links naar rechts neemt de steekproefgrootte toe van \\(n=13\\) tot \\(n=99\\) films met behoud van het aandeel\r\nfilms dat de Bechdel-test doorstaat: \\(Y/n\\sim0.46\\) . De waarschijnlijkheid en,\r\ndienovereenkomstig, de invloed van de gegevens op de posterior nemen toe\r\nmet de steekproefgrootte \\(n\\). Dit\r\nbetekent ook dat de invloed van onze voorkennis afneemt naarmate we\r\nnieuwe gegevens vergaren. Verder hangt de snelheid waarmee de posterior\r\nbalans doorslaat in het voordeel van de gegevens af van de prior. Van\r\nboven naar beneden over het rooster gaan de priors van informatief\r\n(Beta(14,1)) naar vaag (Beta(1,1)). Het spreekt vanzelf dat hoe\r\ninformatiever de prior is, hoe groter zijn invloed op de posterior\r\nis.\r\nDoor deze waarnemingen te combineren levert de laatste kolom in het\r\nrooster een zeer belangrijke Bayesiaanse clou op: ongeacht de sterkte\r\nvan en discrepanties tussen hun prioriteitsbegrip van \\(\\pi\\) zullen drie analisten tot een\r\ngemeenschappelijke posterior interpretatie komen in het licht van sterke\r\ngegevens. Deze vaststelling is een opluchting. Als Bayesiaanse modellen\r\nniets zouden betekenen in het licht van steeds meer gegevens, zouden we\r\neen probleem hebben.\r\nMet dit soort gegevens kun je spelen en kijken naar de rol die de\r\nprior en de data spelen in een posterior analyse, gebruik je de\r\nplot_beta_binomial() en\r\nsummarize_beta_binomial() functies in het\r\nbayesrules pakket om het Beta-Binomial posterior model van\r\n\\(\\pi\\) onder verschillende combinaties\r\nvan voorafgaande Beta(\\(\\alpha,\r\n\\beta\\)) modellen en waargenomen gegevens, \\(Y\\) successen in \\(n\\) proeven:\r\n# Plot the Beta-Binomial model\r\nplot_beta_binomial(alpha = ___, beta = ___, y = ___, n = ___)\r\n\r\n# Obtain numerical summaries of the Beta-Binomial model\r\nsummarize_beta_binomial(alpha = ___, beta = ___, y = ___, n = ___)\r\nConcepten met theorie\r\nverbinden\r\nDe patronen die we hebben waargenomen in het posterior evenwicht\r\ntussen de prior en de data zijn intuïtief. Ze worden ook ondersteund\r\ndoor een elegant wiskundig resultaat. Daar moet je het boek maar voor\r\nlezen.\r\nSequentiële analyse:\r\nEvolueren met dat\r\nWe hebben de toenemende invloed van de gegevens en de afnemende\r\ninvloed van de prior op de posterior onderzocht naarmate er meer en meer\r\ngegevens binnenkomen. Overweeg de nuances van dit concept. De\r\nuitdrukking “naarmate er meer gegevens binnenkomen” roept het idee op\r\ndat het verzamelen van gegevens, en daarmee de evolutie in ons begrip\r\nvan de posterior, stapsgewijs gebeurt. Zo is het inzicht van\r\nwetenschappers in de klimaatverandering in de loop van tientallen jaren\r\ngeëvolueerd naarmate zij nieuwe informatie kregen. Het inzicht van\r\npresidentskandidaten in hun kansen om een verkiezing te winnen evolueert\r\nin de loop van maanden naarmate nieuwe opiniepeilingen beschikbaar\r\nkomen. Het bieden van een formeel kader voor deze evolutie is een van de\r\nmeest krachtige eigenschappen van Bayesiaanse statistiek!\r\nIn een eerder hoofdstuk gingen ze in op Milgram’s gedragsstudie over\r\ngehoorzaamheid. Daar gaat \\(\\pi\\) om\r\nhet aandeel van de mensen die het gezag zullen gehoorzamen zelfs als dit\r\nbetekent dat ze anderen schade toebrengen. In de studie van Milgram\r\nbetekende gehoorzamen aan het gezag het toedienen van een zware\r\nelektrische schok aan een andere deelnemer (wat in feite een list was).\r\nVoorafgaand aan de experimenten van Milgram verwachtte een fictieve\r\npsycholoog dat weinig mensen gezag zouden gehoorzamen als ze een ander\r\nzouden schaden:\\(\\pi\\sim Beta(1,10)\\). In het\r\nonderzoek stelden we vast dat 26 van de 40 deelnemers aan het onderzoek\r\neen schok toebrachten die zij opvatten als een zware schok.\r\nStel nu dat de psycholoog deze gegevens stapsgewijs, dag na dag, over\r\neen periode van drie dagen verzamelde. Elke dag, evalueerde ze \\(n\\) proefpersonen en registreerde ze \\(Y\\), het aantal dat de zwaarste schok kreeg\r\n(dus \\(Y|\\pi \\sim Bin(n,\\pi)\\)Y. Van de\r\n\\(n=10\\) dag-één deelnemers, alleen\r\n\\(Y=1\\) de zwaarste schok. Dus aan het\r\neind van de eerste dag is het begrip van de psycholoog van \\(\\pi\\) al geëvolueerd en blijkt \\[\\pi|Y=1) \\sim Beta(2,19)\\]\r\nDag twee was veel drukker en de resultaten grimmiger: onder \\(n=20\\) deelnemers, gaf \\(Y=17\\) de zwaarste schok. Aan het eind van\r\ndag twee was het begrip van de psycholoog van \\(\\pi\\) opnieuw geëvolueerd - \\(\\pi\\) was waarschijnlijk groter dan zij\r\nhadden verwacht. Dus, het posterior model van \\(\\pi\\) aan het eind van dag twee is \\(Beta(19,22)\\). Op dag drie heeft \\(Y=8\\) van \\(n=10\\) deelnemers de zwaarste schok\r\ntoegediend, en dus is het model geëvolueerd van een \\(Beta(19,22)\\) prior naar een \\(Beta(27,24)\\) posterior. De volledige\r\nevolutie van de oorspronkelijke \\(Beta(1,10)\\) prior van de psycholoog naar\r\neen \\(Beta(27,24)\\) posterior aan het\r\neinde van de driedaagse studie is samengevat in de tabel hieronder.\r\nDag\r\nData\r\nModel\r\n0\r\nNA\r\nBeta(1,10)\r\n1\r\n\\(Y=1;n=10\\)\r\nBeta(2,19)\r\n2\r\n\\(Y=17;n=20\\)\r\nBeta(19,22)\r\n3\r\n\\(Y=8;n=10\\)\r\nBeta(27,24)\r\nHet proces dat we zojuist hebben doorlopen, het incrementeel\r\nbijwerken van het posterior model van de psycholoog wordt meer algemeen\r\neen sequentiële Bayesiaanse analyse of\r\nBayesiaans leren genoemd.\r\nDe mogelijkheid om te evolueren naarmate nieuwe gegevens binnenkomen,\r\nis een van de krachtigste kenmerken van het Bayesiaanse raamwerk. Dit\r\nsoort sequentiële analyses heeft ook twee fundamentele en\r\ngemeenschappelijke sensitieve eigenschappen. Ten eerste is het\r\nuiteindelijke posterior model invariant t.a.v. data\r\nvolgorde: het wordt niet beïnvloed door de volgorde waarin we\r\nde data observeren. Bijvoorbeeld, stel dat de psycholoog de\r\nstudiegegevens van Milgram in omgekeerde volgorde had geobserveerd:\r\n\\(Y=8\\) van\\(n=10\\) op dag één, \\(Y=17\\) van \\(n=20\\) op dag twee, en \\(Y=1\\) van \\(n=10\\) op dag drie. De resulterende\r\nevolutie in hun begrip van \\(\\pi\\) is\r\nsamengevat in de tabel hieronder. Het evoluerende begrip van de\r\npsycholoog van \\(\\pi\\) verloopt een\r\nander pad. Het eindigt echter nog steeds op dezelfde plaats - de\r\nBeta(27,24) posterior.\r\nDag\r\nData\r\nModel\r\n0\r\nNA\r\nBeta(1,10)\r\n1\r\n\\(Y=8;n=10\\)\r\nBeta(9,12)\r\n2\r\n\\(Y=17;n=20\\)\r\nBeta(26,15)\r\n3\r\n\\(Y=1;n=10\\)\r\nBeta(27,24)\r\nHet tweede fundamentele kenmerk van een sequentiële analyse is dat de\r\nuiteindelijke posterior alleen afhangt van de cumulatieve\r\ngegevens. Bijvoorbeeld, in de gecombineerde drie dagen van\r\nMilgram’s experiment, waren er \\(n=10+20+20=40\\) deelnemers die \\(Y=1+17+8=26\\) de zwaarste schok opleverde.\r\nIn paragraaf 3.6 evalueerden wij deze gegevens in één keer, niet\r\nstapsgewijs. Daarbij sprongen wij rechtstreeks van het oorspronkelijke\r\nBeta(1,10) model van de psycholoog naar het Beta(27,24) posterior model\r\nvan \\(\\pi\\). Dat wil zeggen, of we de\r\ngegevens nu incrementeel of in één keer evalueren, we komen op dezelfde\r\nplaats uit.\r\nBewijs van\r\ninvariantie van gegevensvolgorde\r\nIn de vorige sectie zag je het bewijs van datavolgorde invariantie in\r\nactie. Hier zullen we bewijzen dat alle Bayesiaanse modellen van deze\r\neigenschap genieten. Dit deel is leuk, maar hoef je niet echt te weten\r\nvoor je toekomstige werk.Hier gaan we daar niet verder op in (ook voor\r\ndit meer technische deel moet je het boek lezen).\r\nEen opmerking over\r\nsubjectiviteit\r\nWe zinspeelden eerder op een veelgehoorde kritiek op Bayesiaanse\r\nstatistiek - het is te subjectief. In het bijzonder zijn sommigen\r\nbezorgd dat het “subjectief” afstemmen van een prior model een\r\nBayesiaanse analist in staat stelt om tot elke conclusie te komen die\r\nhij wil. In het licht van wat we hier hebben geleerd, kunnen we deze\r\nkritiek meer rigoureus bestrijden. Voordat we dat doen, willen we eerst\r\neen aantal concepten nog eens nader bekijken en uitbreiden.\r\nBevestigd is dat een Bayesiaan inderdaad een prior kan bouwen op\r\nbasis van “subjectieve” ervaring. Heel zelden is dit een slechte zaak,\r\nen heel vaak is het een goede zaak! In het beste geval kan een\r\nsubjectieve prioriteit een schat aan ervaringen uit het verleden\r\nweerspiegelen die in onze analyse moeten worden opgenomen - het zou\r\njammer zijn dat niet te doen. Zelfs als een subjectieve prior ingaat\r\ntegen het feitelijk waargenomen bewijs, verdwijnt zijn invloed op de\r\nposterior naarmate dit bewijs zich opstapelt. We hebben één uitzondering\r\ngezien in het ergste geval. En het was te voorkomen. Als een subjectieve\r\nprioriteit koppig genoeg is om de waarschijnlijkheid van 0 toe te kennen\r\naan een mogelijke parameterwaarde, zal geen hoeveelheid tegenbewijs\r\ngenoeg zijn om het te veranderen.\r\nTenslotte, hoewel we je aanmoedigen om kritisch te zijn in je\r\ntoepassing van Bayesiaanse methoden, maak je alsjeblieft geen zorgen dat\r\nze subjectiever zijn dan frequentistische methoden. Geen mens is in\r\nstaat om alle subjectiviteit uit een analyse te halen. De\r\nlevenservaringen en kennis die we met ons meedragen bepalen alles, van\r\nde onderzoeksvragen die we stellen tot de data die we verzamelen. Het is\r\nbelangrijk om zowel bij Bayesiaanse als bij frequentistische analyses\r\nrekening te houden met de mogelijke implicaties van deze\r\nsubjectiviteit.\r\nSamenvatting\r\nWij hebben het evenwicht onderzocht dat een posterior model aanbrengt\r\ntussen een prior model en de gegevens. In het algemeen zagen wij de\r\nvolgende tendensen:\r\nInvloed van de prior\r\nHoe minder vaag en hoe informatiever de prior, d.w.z. hoe groter onze\r\npriorzekerheid, hoe meer invloed de prior op de posterior\r\nheeft.\r\nInvloed van de gegevens\r\nHoe meer gegevens we hebben, hoe meer invloed de gegevens hebben op de\r\nposterior. Dus, als twee onderzoekers voldoende gegevens hebben, zullen\r\nze met verschillende priors vergelijkbare posteriors hebben.\r\nVerder hebben we gezien dat we in een sequentiële Bayesiaanse\r\nanalyse ons posterior model incrementeel bijwerken naarmate er\r\nmeer en meer gegevens binnenkomen. De uiteindelijke bestemming van deze\r\nposterior wordt niet beïnvloed door de volgorde waarin we deze gegevens\r\nobserveren (d.w.z. de posterior is datavolgorde\r\ninvariant) of door de vraag of we de gegevens in één keer of\r\nopbouwend observeren.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-31T20:30:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-machine-learning-workflow/",
    "title": "Machine Learning Workflow",
    "description": "Deze blog is een inleiding op de workflow van Machine Learning.",
    "author": [
      {
        "name": "Harrie Jonkman en Mr.X",
        "url": {}
      }
    ],
    "date": "2022-03-03",
    "categories": [],
    "contents": "\r\nINTRODUCTION\r\nTidymodels is the relatively new package for machine learning with R. It is the successor to the caret package which is used during the Introduction to Data Science-course of the Harvard University (Kuhn & Johnson, 2013; Irizarry, 2020). Tidymodels is a collection of modeling packages that, like the tidyverse, has consistent API and are designed to work together specifically to support predictive analytics and machine learning. I followed and looked at different books (Kuhn & Silge, 2021; Kuhn en Johnson, 2019) blogs (Lendway, 2020; Roamiar (2021); Ruiz (2019), Barter (2019; Seyedia (2021) and couses/video’s (Lewis, 2020; Silge, 2021; Silge 2020). I tried to learn this new system and wrote different blogs in Dutch on thishere.\r\nTidymodels is a grammar for modeling that makes things a lot easier and provides a unified modeling and analysis interface to seamlessly access several model varities in R. tidymodels is a meta-package that installs and load the core packages listed below that you need for modeling and machine learning;\r\n- recipes is tidy interface for to data pre-processing tools for feature/variables engineering;\r\n- rsample provides infrastucture for efficient data splitting and resampling;\r\n- parsnip is a tidy, unified interface to models that can be used to try a range of models without getting bagged down in the syntactical minutae of the underlying packages;\r\n- tune helps you optimize the hyperparameters of your model and chose pre-processing steps;\r\n- yardstick measures the effectiveness of models during performance metrics;\r\n- workflow bundles your pre-processing modeling and post-processing together;\r\n- dials creates and manages tuning parameters and parameters grids;\r\n- brooms convert the information in common statistical R objects into user-friendly predictable formats.\r\nThis year (2021) I learned working with tidymodels. I tried to finish the Capstone course with the use of this metapackage. I will show you the different steps in the working proces.\r\nLet us first open the packages used in this article (tidymodels, but also tidyverse, finalfit, caret, rpart and randomforest):\r\nPROBLEM DEFINITION\r\nData mining approaches are used in this article to predict human wine taste preferences that are based on easily available analytical tests at certification steps. A data set on red wine from Portugal is used here to research quality of the wine and different predictors for the quality (Cortez et al., 2009) Supervised machine learning supports us in this. In this world two kind of algorithms are often used. One is called regression (see also Attalides, 2020) and the other is called classification (not used here).\r\nIn this study we use regression for predicting quality of wine based on several predictors.The wine data used here contains the following eleven independent variables (predictors, I1-I11) and one dependent variable (outcome, D1)\r\nIndependent variables: (symbol I) - I1 Fixed acidity (g(tartaric acid/dm3) - I2 Volatile acidity (g(acetic acid)/dm3) - I3 Citric acid (g/dm3) - I4 Residual sugar (g/dm3) - I5 Chlorides (g(sodium chloride)/dm3) - I6 Free sulfar dioxide (mg/dm3) - I7 Total sulfar dioxide (mg/dm3) - I8 Density (g/cm3) - I9 pH - I10 Sulphates (g(potassium sulphate)/dm3) - I11 Alcohol (vol%)\r\nDependent variable: (symbol D) - D1 Quality\r\nDATA LOADING, PREPROCESSING, EXPLORING\r\nLet us load the data set (wine.rds) first.\r\nThen, we look at the column names.\r\nAnd defined them on a consistent way.\r\nLet us make an overview and summary now of this data frame.\r\n\r\nRows: 1,599\r\nColumns: 12\r\n$ fixed_acidity        <dbl> 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3~\r\n$ volatile_acidity     <dbl> 0.700, NA, 0.760, 0.280, 0.700, NA, 0.6~\r\n$ citric_acid          <dbl> 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.0~\r\n$ residual_sugar       <dbl> 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2,~\r\n$ chlorides            <dbl> 0.076, 0.098, 0.092, 0.075, 0.076, 0.07~\r\n$ free_sulfur_dioxide  <dbl> 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, ~\r\n$ total_sulfur_dioxide <dbl> 34, 67, 54, 60, 34, 40, 59, 21, 18, 102~\r\n$ density              <dbl> 0.9978, 0.9968, 0.9970, 0.9980, 0.9978,~\r\n$ pH                   <dbl> 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.3~\r\n$ sulphates            <dbl> 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.4~\r\n$ alcohol              <dbl> 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, NA, ~\r\n$ quality              <dbl> 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, ~\r\n fixed_acidity   volatile_acidity  citric_acid    residual_sugar  \r\n Min.   : 4.60   Min.   :0.1200   Min.   :0.000   Min.   : 0.900  \r\n 1st Qu.: 7.10   1st Qu.:0.3900   1st Qu.:0.090   1st Qu.: 1.900  \r\n Median : 7.90   Median :0.5200   Median :0.260   Median : 2.200  \r\n Mean   : 8.32   Mean   :0.5275   Mean   :0.271   Mean   : 2.539  \r\n 3rd Qu.: 9.20   3rd Qu.:0.6400   3rd Qu.:0.420   3rd Qu.: 2.600  \r\n Max.   :15.90   Max.   :1.5800   Max.   :1.000   Max.   :15.500  \r\n                 NA's   :3                                        \r\n   chlorides       free_sulfur_dioxide total_sulfur_dioxide\r\n Min.   :0.01200   Min.   : 1.00       Min.   :  6.00      \r\n 1st Qu.:0.07000   1st Qu.: 7.00       1st Qu.: 22.00      \r\n Median :0.07900   Median :14.00       Median : 38.00      \r\n Mean   :0.08747   Mean   :15.87       Mean   : 46.47      \r\n 3rd Qu.:0.09000   3rd Qu.:21.00       3rd Qu.: 62.00      \r\n Max.   :0.61100   Max.   :72.00       Max.   :289.00      \r\n                                                           \r\n    density             pH          sulphates         alcohol     \r\n Min.   :0.9901   Min.   :2.740   Min.   :0.3300   Min.   : 8.40  \r\n 1st Qu.:0.9956   1st Qu.:3.210   1st Qu.:0.5500   1st Qu.: 9.50  \r\n Median :0.9968   Median :3.310   Median :0.6200   Median :10.20  \r\n Mean   :0.9967   Mean   :3.311   Mean   :0.6581   Mean   :10.43  \r\n 3rd Qu.:0.9978   3rd Qu.:3.400   3rd Qu.:0.7300   3rd Qu.:11.10  \r\n Max.   :1.0037   Max.   :4.010   Max.   :2.0000   Max.   :14.90  \r\n                                                   NA's   :5      \r\n    quality     \r\n Min.   :3.000  \r\n 1st Qu.:5.000  \r\n Median :6.000  \r\n Mean   :5.636  \r\n 3rd Qu.:6.000  \r\n Max.   :8.000  \r\n                \r\n\r\nWe have twelve variables inside this data set which are all continuous variables.\r\nAt this moment we want to know also something about the missings?\r\nWe have eight missings (three on volatile.acidity and five on alcohol). We remove any missing values and kept 1591 cases. Let us show it here.\r\n\r\n                                    label var_type    n missing_n\r\nfixed_acidity               fixed_acidity    <dbl> 1591         0\r\nvolatile_acidity         volatile_acidity    <dbl> 1591         0\r\ncitric_acid                   citric_acid    <dbl> 1591         0\r\nresidual_sugar             residual_sugar    <dbl> 1591         0\r\nchlorides                       chlorides    <dbl> 1591         0\r\nfree_sulfur_dioxide   free_sulfur_dioxide    <dbl> 1591         0\r\ntotal_sulfur_dioxide total_sulfur_dioxide    <dbl> 1591         0\r\ndensity                           density    <dbl> 1591         0\r\npH                                     pH    <dbl> 1591         0\r\nsulphates                       sulphates    <dbl> 1591         0\r\nalcohol                           alcohol    <dbl> 1591         0\r\nquality                           quality    <dbl> 1591         0\r\n                     missing_percent\r\nfixed_acidity                    0.0\r\nvolatile_acidity                 0.0\r\ncitric_acid                      0.0\r\nresidual_sugar                   0.0\r\nchlorides                        0.0\r\nfree_sulfur_dioxide              0.0\r\ntotal_sulfur_dioxide             0.0\r\ndensity                          0.0\r\npH                               0.0\r\nsulphates                        0.0\r\nalcohol                          0.0\r\nquality                          0.0\r\n\r\nNow we have wrangeled and preprocessed the data, we can explore them. Let us first vizualise correlations within the data-set. For this you need the package corrplot.\r\n\r\n\r\n\r\nSPLITTING THE DATA\r\nNow we understand the data we have to split the data into: a) Train set, b) Test set. Here we work on the last pre-model analysis. All functions below come from the rsample package, which is part of tidymodels. First we set the seed to fix the randomization and to make reproducabiltiy possible. We use 80% of the dataset for the trainingset. For a big dataset as this wine-data set with 1591 observations, 80:20-splitting works well. We split it and than make a training- and test-dataset.\r\nMODELING AND DATA ANALYSIS\r\nNow we will compare different models with each other and want to know which one works the best for this data set with this dependent and these independent variables. This part of machine learning is called supervised learning of which the basic goal is to find a function that accurately describes how different measured explanatory variables can be combined to make a prediction about the target variable. We start with linear modelling. Regression models can help us quantify the magnitude and direction of relationships among variables.\r\n1. Linear modelling\r\nFor the outcome or target variable quality, we first research some different linear regression models and choose the best one based on indices. For these tasks, we store each formula in a different R object.\r\nWe have to define the data: - The target variable. quality is the target variable and it is numeric - The features of the model (predictors) are the other (independent) variables here and they are numeric variables also.\r\nFuthermore, we design a simple formula to predict the target variable. In this formula (f1) all the available 11 predictors are used.\r\n\r\n\r\n\r\nLet us fit a linear regression model to the data. What we do: - First, we created an object that will store the model fit.\r\n- Then, we specify the model.\r\n- Then, We specify also that we work with regression because of the continue target variable (quality). - Then, we specify also the lm package to train the model. - And we finish in this chunck by adding the formula and the training data to fit the model.\r\nLet us see how this workflow works.\r\n\r\n\r\n\r\nWe present the results on different ways\r\nBut this is probably the best and clearest way to show the results.\r\n\r\n\r\nCall:\r\nstats::lm(formula = quality ~ fixed_acidity + volatile_acidity + \r\n    citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + \r\n    total_sulfur_dioxide + density + pH + sulphates + alcohol, \r\n    data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.68924 -0.37784 -0.04982  0.46247  1.89438 \r\n\r\nCoefficients:\r\n                       Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)           2.125e+01  2.458e+01   0.864   0.3877    \r\nfixed_acidity         1.515e-02  3.015e-02   0.503   0.6154    \r\nvolatile_acidity     -9.626e-01  1.419e-01  -6.783 1.80e-11 ***\r\ncitric_acid          -1.749e-02  1.682e-01  -0.104   0.9172    \r\nresidual_sugar        1.879e-02  1.750e-02   1.074   0.2831    \r\nchlorides            -2.160e+00  4.601e-01  -4.696 2.95e-06 ***\r\nfree_sulfur_dioxide   5.057e-03  2.477e-03   2.042   0.0414 *  \r\ntotal_sulfur_dioxide -4.053e-03  8.358e-04  -4.850 1.39e-06 ***\r\ndensity              -1.685e+01  2.511e+01  -0.671   0.5022    \r\npH                   -4.781e-01  2.216e-01  -2.157   0.0312 *  \r\nsulphates             9.522e-01  1.256e-01   7.581 6.63e-14 ***\r\nalcohol               2.675e-01  3.070e-02   8.712  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6545 on 1260 degrees of freedom\r\nMultiple R-squared:  0.3583,    Adjusted R-squared:  0.3527 \r\nF-statistic: 63.96 on 11 and 1260 DF,  p-value: < 2.2e-16\r\n\r\nWe can also visualize the fit summary by using the broom package which is inside tidymodels.\r\n\r\n# A tibble: 12 x 5\r\n   term                 estimate std.error statistic p.value\r\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\r\n 1 (Intercept)            21.2      24.6       0.864   0.388\r\n 2 fixed_acidity           0.015     0.03      0.503   0.615\r\n 3 volatile_acidity       -0.963     0.142    -6.78    0    \r\n 4 citric_acid            -0.017     0.168    -0.104   0.917\r\n 5 residual_sugar          0.019     0.018     1.07    0.283\r\n 6 chlorides              -2.16      0.46     -4.70    0    \r\n 7 free_sulfur_dioxide     0.005     0.002     2.04    0.041\r\n 8 total_sulfur_dioxide   -0.004     0.001    -4.85    0    \r\n 9 density               -16.9      25.1      -0.671   0.502\r\n10 pH                     -0.478     0.222    -2.16    0.031\r\n11 sulphates               0.952     0.126     7.58    0    \r\n12 alcohol                 0.267     0.031     8.71    0    \r\n\r\n2. Decision tree\r\nAfter we worked with linear regression, it is possible to work with other models which maybe give us better results for predicting the outcome. Let us first look at decision tree modeling. A decision tree is tree-like flowchart that assigns labels to individual observations. It splits it into homogeneous subsets, which share the same class labels. For this you need decision tree package and for this you have to install and open the library of rpart. We see similar steps here in the machine learning workflow. Once again, the workflow: - define an object dt_fit;\r\n- tell that we work with decision tree;\r\n- set the mode on regression;\r\n- set the engine on rpart;\r\n- fit the formula on the training data-set.\r\nPrint the results\r\n\r\nn= 1272 \r\n\r\nnode), split, n, deviance, yval\r\n      * denotes terminal node\r\n\r\n 1) root 1272 841.093600 5.645440  \r\n   2) alcohol< 10.45 735 314.223100 5.345578  \r\n     4) sulphates< 0.585 323  99.275540 5.133127 *\r\n     5) sulphates>=0.585 412 188.939300 5.512136  \r\n      10) total_sulfur_dioxide>=59.5 121  33.239670 5.198347 *\r\n      11) total_sulfur_dioxide< 59.5 291 138.831600 5.642612  \r\n        22) volatile_acidity>=0.405 220  94.836360 5.527273 *\r\n        23) volatile_acidity< 0.405 71  32.000000 6.000000 *\r\n   3) alcohol>=10.45 537 370.324000 6.055866  \r\n     6) sulphates< 0.645 240 156.733300 5.716667  \r\n      12) volatile_acidity>=0.995 7   6.857143 4.142857 *\r\n      13) volatile_acidity< 0.995 233 132.017200 5.763948  \r\n        26) pH>=3.365 114  62.280700 5.543860  \r\n          52) free_sulfur_dioxide< 8.5 34  19.529410 5.117647 *\r\n          53) free_sulfur_dioxide>=8.5 80  33.950000 5.725000 *\r\n        27) pH< 3.365 119  58.924370 5.974790 *\r\n     7) sulphates>=0.645 297 163.663300 6.329966  \r\n      14) alcohol< 11.45 180  95.777780 6.111111  \r\n        28) volatile_acidity>=0.405 94  35.276600 5.829787 *\r\n        29) volatile_acidity< 0.405 86  44.930230 6.418605  \r\n          58) total_sulfur_dioxide>=49.5 20   8.550000 5.850000 *\r\n          59) total_sulfur_dioxide< 49.5 66  27.954550 6.590909 *\r\n      15) alcohol>=11.45 117  46.000000 6.666667 *\r\n\r\nAs a sidestep, we can visualize this, but than we have to install and open the visNetwork and sparkline packages. Then we see this.\r\n\r\n\r\n{\"x\":{\"nodes\":{\"id\":[1,2,4,5,10,11,22,23,3,6,12,13,26,52,53,27,7,14,28,29,58,59,15],\"label\":[\"alcohol\",\"sulphates\",\"5.133\",\"total_sulfur_dioxide\",\"5.198\",\"volatile_acidity\",\"5.527\",\"6\",\"sulphates\",\"volatile_acidity\",\"4.143\",\"pH\",\"free_sulfur_dioxide\",\"5.118\",\"5.725\",\"5.975\",\"alcohol\",\"volatile_acidity\",\"5.83\",\"total_sulfur_dioxide\",\"5.85\",\"6.591\",\"6.667\"],\"level\":[1,2,3,3,4,4,5,5,2,3,4,4,5,6,6,5,3,4,5,5,6,6,4],\"color\":[\"#F1B8C2\",\"#E1C1A1\",\"#C189DC\",\"#BECC9B\",\"#BF83DA\",\"#96D4B5\",\"#B267D1\",\"#A13EC3\",\"#E1C1A1\",\"#96D4B5\",\"#E6E0F8\",\"#8DD2D8\",\"#B4C7ED\",\"#C28BDC\",\"#AB56CB\",\"#A240C4\",\"#F1B8C2\",\"#96D4B5\",\"#A74CC8\",\"#BECC9B\",\"#A74BC7\",\"#8B0AB3\",\"#8904B1\"],\"value\":[1272,735,323,412,121,291,220,71,537,240,7,233,114,34,80,119,297,180,94,86,20,66,117],\"shape\":[\"dot\",\"dot\",\"square\",\"dot\",\"square\",\"dot\",\"square\",\"square\",\"dot\",\"dot\",\"square\",\"dot\",\"dot\",\"square\",\"square\",\"square\",\"dot\",\"dot\",\"square\",\"dot\",\"square\",\"square\",\"square\"],\"title\":[\"<div style=\\\"text-align:center;\\\">N : <b>100%<\\/b> (1272)<br>Complexity : <b>0.186<\\/b><br>Mean : <b>5.645<\\/b><br>Variance : <b>0.662<\\/b><\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>57.8%<\\/b> (735)<br>Complexity : <b>0.031<\\/b><br>Mean : <b>5.346<\\/b><br>Variance : <b>0.428<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>25.4%<\\/b> (323)<br>Complexity : <b>0.005<\\/b><br>Mean : <b>5.133<\\/b><br>Variance : <b>0.308<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<br><b> sulphates <\\/b> < 0.585<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>32.4%<\\/b> (412)<br>Complexity : <b>0.02<\\/b><br>Mean : <b>5.512<\\/b><br>Variance : <b>0.46<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<br><b> sulphates <\\/b> >= 0.585<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>9.5%<\\/b> (121)<br>Complexity : <b>0.005<\\/b><br>Mean : <b>5.198<\\/b><br>Variance : <b>0.277<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<br><b> sulphates <\\/b> >= 0.585<br><b> total_sulfur_dioxide <\\/b> >= 59.5<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>22.9%<\\/b> (291)<br>Complexity : <b>0.014<\\/b><br>Mean : <b>5.643<\\/b><br>Variance : <b>0.479<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<br><b> sulphates <\\/b> >= 0.585<br><b> total_sulfur_dioxide <\\/b> < 59.5<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>17.3%<\\/b> (220)<br>Complexity : <b>0.007<\\/b><br>Mean : <b>5.527<\\/b><br>Variance : <b>0.433<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<br><b> sulphates <\\/b> >= 0.585<br><b> total_sulfur_dioxide <\\/b> < 59.5<br><b> volatile_acidity <\\/b> >= 0.405<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>5.6%<\\/b> (71)<br>Complexity : <b>0.005<\\/b><br>Mean : <b>6<\\/b><br>Variance : <b>0.457<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> < 10.45<br><b> sulphates <\\/b> >= 0.585<br><b> total_sulfur_dioxide <\\/b> < 59.5<br><b> volatile_acidity <\\/b> < 0.405<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>42.2%<\\/b> (537)<br>Complexity : <b>0.059<\\/b><br>Mean : <b>6.056<\\/b><br>Variance : <b>0.691<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>18.9%<\\/b> (240)<br>Complexity : <b>0.021<\\/b><br>Mean : <b>5.717<\\/b><br>Variance : <b>0.656<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>0.6%<\\/b> (7)<br>Complexity : <b>0.01<\\/b><br>Mean : <b>4.143<\\/b><br>Variance : <b>1.143<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<br><b> volatile_acidity <\\/b> >= 0.995<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>18.3%<\\/b> (233)<br>Complexity : <b>0.013<\\/b><br>Mean : <b>5.764<\\/b><br>Variance : <b>0.569<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<br><b> volatile_acidity <\\/b> < 0.995<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>9%<\\/b> (114)<br>Complexity : <b>0.01<\\/b><br>Mean : <b>5.544<\\/b><br>Variance : <b>0.551<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<br><b> volatile_acidity <\\/b> < 0.995<br><b> pH <\\/b> >= 3.365<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>2.7%<\\/b> (34)<br>Complexity : <b>0.003<\\/b><br>Mean : <b>5.118<\\/b><br>Variance : <b>0.592<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<br><b> volatile_acidity <\\/b> < 0.995<br><b> pH <\\/b> >= 3.365<br><b> free_sulfur_dioxide <\\/b> < 8.5<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>6.3%<\\/b> (80)<br>Complexity : <b>0.005<\\/b><br>Mean : <b>5.725<\\/b><br>Variance : <b>0.43<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<br><b> volatile_acidity <\\/b> < 0.995<br><b> pH <\\/b> >= 3.365<br><b> free_sulfur_dioxide <\\/b> >= 8.5<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>9.4%<\\/b> (119)<br>Complexity : <b>0.01<\\/b><br>Mean : <b>5.975<\\/b><br>Variance : <b>0.499<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> < 0.645<br><b> volatile_acidity <\\/b> < 0.995<br><b> pH <\\/b> < 3.365<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>23.3%<\\/b> (297)<br>Complexity : <b>0.026<\\/b><br>Mean : <b>6.33<\\/b><br>Variance : <b>0.553<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 10.45<br><b> sulphates <\\/b> >= 0.645<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>14.2%<\\/b> (180)<br>Complexity : <b>0.019<\\/b><br>Mean : <b>6.111<\\/b><br>Variance : <b>0.535<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b>  <\\/b> 10.45 <= <b>alcohol<\\/b> < 11.45<br><b> sulphates <\\/b> >= 0.645<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>7.4%<\\/b> (94)<br>Complexity : <b>0.006<\\/b><br>Mean : <b>5.83<\\/b><br>Variance : <b>0.379<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b>  <\\/b> 10.45 <= <b>alcohol<\\/b> < 11.45<br><b> sulphates <\\/b> >= 0.645<br><b> volatile_acidity <\\/b> >= 0.405<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>6.8%<\\/b> (86)<br>Complexity : <b>0.01<\\/b><br>Mean : <b>6.419<\\/b><br>Variance : <b>0.529<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b>  <\\/b> 10.45 <= <b>alcohol<\\/b> < 11.45<br><b> sulphates <\\/b> >= 0.645<br><b> volatile_acidity <\\/b> < 0.405<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>1.6%<\\/b> (20)<br>Complexity : <b>0.004<\\/b><br>Mean : <b>5.85<\\/b><br>Variance : <b>0.45<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b>  <\\/b> 10.45 <= <b>alcohol<\\/b> < 11.45<br><b> sulphates <\\/b> >= 0.645<br><b> volatile_acidity <\\/b> < 0.405<br><b> total_sulfur_dioxide <\\/b> >= 49.5<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>5.2%<\\/b> (66)<br>Complexity : <b>0.002<\\/b><br>Mean : <b>6.591<\\/b><br>Variance : <b>0.43<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b>  <\\/b> 10.45 <= <b>alcohol<\\/b> < 11.45<br><b> sulphates <\\/b> >= 0.645<br><b> volatile_acidity <\\/b> < 0.405<br><b> total_sulfur_dioxide <\\/b> < 49.5<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\",\"<div style=\\\"text-align:center;\\\">N : <b>9.2%<\\/b> (117)<br>Complexity : <b>0.008<\\/b><br>Mean : <b>6.667<\\/b><br>Variance : <b>0.397<\\/b><hr class = \\\"rPartvisNetwork\\\">\\n<div class =\\\"showOnMe2\\\"><div style=\\\"text-align:center;\\\"><U style=\\\"color:blue;\\\"  onmouseover=\\\"this.style.cursor='pointer';\\\" onmouseout=\\\"this.style.cursor='default';\\\">Rules<\\/U><\\/div>\\n<div class=\\\"showMeRpartTTp2\\\" style=\\\"display:none;\\\">\\n<b> alcohol <\\/b> >= 11.45<br><b> sulphates <\\/b> >= 0.645<\\/script><script type=\\\"text/javascript\\\">$(document).ready(function(){\\n$(\\\".showOnMe2\\\").click(function(){\\n$(\\\".showMeRpartTTp2\\\").toggle();\\n$.sparkline_display_visible();\\n});\\n  });<\\/script><\\/div><\\/div>\\n\\n<\\/div>\"],\"fixed\":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],\"colorClust\":[\"#AE5DCD\",\"#B977D6\",\"#C189DC\",\"#B368D1\",\"#BF83DA\",\"#AE5DCD\",\"#B267D1\",\"#A13EC3\",\"#9F39C2\",\"#AC56CB\",\"#E6E0F8\",\"#AA52CA\",\"#B265D0\",\"#C28BDC\",\"#AB56CB\",\"#A240C4\",\"#9521BA\",\"#9D34C0\",\"#A74CC8\",\"#9219B7\",\"#A74BC7\",\"#8B0AB3\",\"#8904B1\"],\"labelClust\":[5.645,5.346,5.133,5.512,5.198,5.643,5.527,6,6.056,5.717,4.143,5.764,5.544,5.118,5.725,5.975,6.33,6.111,5.83,6.419,5.85,6.591,6.667],\"Leaf\":[0,0,1,0,1,0,1,1,0,0,1,0,0,1,1,1,0,0,1,0,1,1,1],\"font.size\":[16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16],\"scaling.min\":[22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5],\"scaling.max\":[22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5]},\"edges\":{\"id\":[\"edge1\",\"edge2\",\"edge3\",\"edge4\",\"edge5\",\"edge6\",\"edge7\",\"edge8\",\"edge9\",\"edge10\",\"edge11\",\"edge12\",\"edge13\",\"edge14\",\"edge15\",\"edge16\",\"edge17\",\"edge18\",\"edge19\",\"edge20\",\"edge21\",\"edge22\"],\"from\":[1,2,2,5,5,11,11,1,3,6,6,13,26,26,13,3,7,14,14,29,29,7],\"to\":[2,4,5,10,11,22,23,3,6,12,13,26,52,53,27,7,14,28,29,58,59,15],\"label\":[\"< 10.45\",\"< 0.585\",\">= 0.585\",\">= 59.5\",\"< 59.5\",\">= 0.405\",\"< 0.405\",\">= 10.45\",\"< 0.645\",\">= 0.995\",\"< 0.995\",\">= 3.365\",\"< 8.5\",\">= 8.5\",\"< 3.365\",\">= 0.645\",\"< 11.45\",\">= 0.405\",\"< 0.405\",\">= 49.5\",\"< 49.5\",\">= 11.45\"],\"value\":[735,323,412,121,291,220,71,537,240,7,233,114,34,80,119,297,180,94,86,20,66,117],\"title\":[\"<div style=\\\"text-align:center;\\\"><b>alcohol<\\/b><\\/div><div style=\\\"text-align:center;\\\"><10.45<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>sulphates<\\/b><\\/div><div style=\\\"text-align:center;\\\"><0.585<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>sulphates<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=0.585<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>total_sulfur_dioxide<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=59.5<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>total_sulfur_dioxide<\\/b><\\/div><div style=\\\"text-align:center;\\\"><59.5<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>volatile_acidity<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=0.405<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>volatile_acidity<\\/b><\\/div><div style=\\\"text-align:center;\\\"><0.405<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>alcohol<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=10.45<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>sulphates<\\/b><\\/div><div style=\\\"text-align:center;\\\"><0.645<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>volatile_acidity<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=0.995<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>volatile_acidity<\\/b><\\/div><div style=\\\"text-align:center;\\\"><0.995<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>pH<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=3.365<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>free_sulfur_dioxide<\\/b><\\/div><div style=\\\"text-align:center;\\\"><8.5<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>free_sulfur_dioxide<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=8.5<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>pH<\\/b><\\/div><div style=\\\"text-align:center;\\\"><3.365<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>sulphates<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=0.645<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>alcohol<\\/b><\\/div><div style=\\\"text-align:center;\\\"><11.45<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>volatile_acidity<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=0.405<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>volatile_acidity<\\/b><\\/div><div style=\\\"text-align:center;\\\"><0.405<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>total_sulfur_dioxide<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=49.5<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>total_sulfur_dioxide<\\/b><\\/div><div style=\\\"text-align:center;\\\"><49.5<\\/div>\",\"<div style=\\\"text-align:center;\\\"><b>alcohol<\\/b><\\/div><div style=\\\"text-align:center;\\\">>=11.45<\\/div>\"],\"color\":[\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\",\"#8181F7\"],\"font.size\":[14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14],\"font.align\":[\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\",\"horizontal\"],\"smooth.enabled\":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],\"smooth.type\":[\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\",\"cubicBezier\"],\"smooth.roundness\":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\"},\"manipulation\":{\"enabled\":false},\"layout\":{\"hierarchical\":{\"enabled\":true,\"direction\":\"UD\"}},\"interaction\":{\"dragNodes\":false,\"selectConnectedEdges\":false,\"tooltipDelay\":500,\"zoomSpeed\":1},\"edges\":{\"scaling\":{\"label\":{\"enabled\":false}}}},\"groups\":null,\"width\":\"100%\",\"height\":\"600px\",\"idselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"useLabels\":true,\"main\":\"Select by id\"},\"byselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"multiple\":false,\"hideColor\":\"rgba(200,200,200,0.5)\",\"highlight\":false},\"main\":{\"text\":\"\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":{\"text\":\"\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-size:12px;text-align:center;\"},\"footer\":{\"text\":\"\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-size:12px;text-align:center;\"},\"background\":\"rgba(0, 0, 0, 0)\",\"highlight\":{\"enabled\":true,\"hoverNearest\":false,\"degree\":{\"from\":50000,\"to\":0},\"algorithm\":\"hierarchical\",\"hideColor\":\"rgba(200,200,200,0.5)\",\"labelOnly\":true},\"collapse\":{\"enabled\":true,\"fit\":true,\"resetHighlight\":true,\"clusterOptions\":{\"fixed\":true,\"physics\":false},\"keepCoord\":true,\"labelSuffix\":\"(cluster)\"},\"tooltipStay\":300,\"tooltipStyle\":\"position: fixed;visibility:hidden;padding: 5px;\\n                      white-space: nowrap;\\n                      font-family: cursive;font-size:12px;font-color:purple;background-color: #E6E6E6;\\n                      border-radius: 15px;\",\"OnceEvents\":{\"stabilized\":\"function() { \\n        this.setOptions({layout:{hierarchical:false}, physics:{solver:'barnesHut', enabled:true, stabilization : false}, nodes : {physics : false, fixed : true}});\\n    }\"},\"legend\":{\"width\":0.1,\"useGroups\":false,\"position\":\"left\",\"ncol\":1,\"stepX\":100,\"stepY\":100,\"zoom\":true,\"nodes\":{\"label\":[\"alcohol\",\"free_sulfur_dioxide\",\"pH\",\"sulphates\",\"total_sulfur_dioxide\",\"volatile_acidity\"],\"color\":[\"#F1B8C2\",\"#B4C7ED\",\"#8DD2D8\",\"#E1C1A1\",\"#BECC9B\",\"#96D4B5\"],\"shape\":[\"dot\",\"dot\",\"dot\",\"dot\",\"dot\",\"dot\"],\"size\":[22,22,22,22,22,22],\"Leaf\":[0,0,0,0,0,0],\"font.size\":[16,16,16,16,16,16],\"id\":[10000,10001,10002,10003,10004,10005]},\"nodesToDataframe\":true},\"tree\":{\"updateShape\":true,\"shapeVar\":\"dot\",\"shapeY\":\"square\",\"colorVar\":{\"variable\":[\"alcohol\",\"sulphates\",\"total_sulfur_dioxide\",\"volatile_acidity\",\"pH\",\"free_sulfur_dioxide\"],\"color\":[\"#F1B8C2\",\"#E1C1A1\",\"#BECC9B\",\"#96D4B5\",\"#8DD2D8\",\"#B4C7ED\"]},\"colorY\":{\"colRamp\":[\"function (x) \",\"roundcolor(cbind(palette[[1L]](x), palette[[2L]](x), palette[[3L]](x), \",\"    if (alpha) palette[[4L]](x))) * 255\"],\"colorTerm\":[\"#AE5DCD\",\"#B977D6\",\"#C189DC\",\"#B368D1\",\"#BF83DA\",\"#AE5DCD\",\"#B267D1\",\"#A13EC3\",\"#9F39C2\",\"#AC56CB\",\"#E6E0F8\",\"#AA52CA\",\"#B265D0\",\"#C28BDC\",\"#AB56CB\",\"#A240C4\",\"#9521BA\",\"#9D34C0\",\"#A74CC8\",\"#9219B7\",\"#A74BC7\",\"#8B0AB3\",\"#8904B1\"],\"colorY\":[\"#E6E0F8\",\"#8904B1\"],\"vardecidedClust\":[5.645,5.346,5.133,5.512,5.198,5.643,5.527,6,6.056,5.717,4.143,5.764,5.544,5.118,5.725,5.975,6.33,6.111,5.83,6.419,5.85,6.591,6.667]}},\"export\":{\"type\":\"png\",\"css\":\"float:right;-webkit-border-radius: 10;\\n                  -moz-border-radius: 10;\\n                  border-radius: 10px;\\n                  font-family: Arial;\\n                  color: #ffffff;\\n                  font-size: 12px;\\n                  background: #090a0a;\\n                  padding: 4px 8px 4px 4px;\\n                  text-decoration: none;\",\"background\":\"#fff\",\"name\":\"network.png\",\"label\":\"Export as png\"}},\"evals\":[\"OnceEvents.stabilized\"],\"jsHooks\":[]}\r\n3. Random forest\r\nA third model we use here is RandomForest. RandomForest is a natural extension of DecisionTree. A RandomForest is a collection of Deciontrees that are aggregated by majority rule, and is in essence a collection ‘bootstrapped’ decision trees. You need to install randomForest package and open the library randomForest. And also here, once again the same steps: - define object rf_fit;\r\n- tell we want to use randomforest;\r\n- set the mode again on regression;\r\n- set the engine here on randomForest;\r\n- fit the model on the training_set.\r\nPrint these results (not shown here).\r\n\r\n\r\nCall:\r\n randomForest(x = maybe_data_frame(x), y = y) \r\n               Type of random forest: regression\r\n                     Number of trees: 500\r\nNo. of variables tried at each split: 3\r\n\r\n          Mean of squared residuals: 0.3259127\r\n                    % Var explained: 50.71\r\n\r\nEVALUATION AND PREDICTION\r\nNow we have three objects of the three models we ran and which we have to compare and evaluate. We do this on the test-set. We compare the three models (lm_fit, dt_fit and rf_fit) on the Men Square Score (MSE) score. We need to find a model algorithm that produces predictors for the outcome (quality) that minimizes the MSE-score. So, the lower the mse-score of the model, the better.\r\n1. Accuracy of the lm-model\r\nLet us first look at the accuracy of the linear-model.\r\nNow we see a new column, .pred, with a predicted scores for each row.\r\nIt gives here the following mse-score for linear modeling, which we show here.\r\n\r\n# A tibble: 1 x 2\r\n  type    MSE\r\n  <chr> <dbl>\r\n1 lm    0.467\r\n\r\n2. Accuracy of the Decision Tree Model\r\nThen we look at the accuracy of the DecisionTree Model.\r\nThe decision model gives the following mse-score.\r\n\r\n# A tibble: 1 x 2\r\n  type    MSE\r\n  <chr> <dbl>\r\n1 dt    0.596\r\n\r\n3. Accuracy of the Random Forest Model\r\nAnd then ofcourse we also have to look at the accuracy of the RandomForest-model.\r\nThe Random Forest Model gives us the following mse-score:\r\n\r\n# A tibble: 1 x 2\r\n  type    MSE\r\n  <chr> <dbl>\r\n1 rf    0.408\r\n\r\nAll results together\r\nLet us put all the results together and compare them with each other.\r\nLet us show these results together.\r\n\r\n# A tibble: 3 x 2\r\n  type    MSE\r\n  <chr> <dbl>\r\n1 lm    0.467\r\n2 dt    0.596\r\n3 rf    0.408\r\n\r\nAltogether the prediction scores don’t look very well, but we know that RandomForest is the best model for prediction.\r\n\r\n# A tibble: 3 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard       0.638\r\n2 rsq     standard       0.378\r\n3 mae     standard       0.351\r\n\r\nNow we choosed the random forest model, we can look at the importance of the ten independent variables and compare them with each other. We see that alcohol is the most import predictor for quality followed by sulphates ad volatile_acidity. Residul-sugar, pH and fixed_acidity are the lowest important predictors for quality of wine.\r\n\r\n\r\n\r\nLet us look at which percentage of the test sample are wrongly predicted.\r\n\\(105/307*100= 34,2%\\) is not correctly predicted. So \\(65,8%\\) is predicted correctly with this model. We choose the random_forest model as the best opportunity here. Let us look at it once again.\r\n\r\n# A tibble: 6 x 13\r\n  fixed_acidity volatile_acidity citric_acid residual_sugar chlorides\r\n          <dbl>            <dbl>       <dbl>          <dbl>     <dbl>\r\n1          11.2             0.28        0.56            1.9     0.075\r\n2           7.5             0.5         0.36            6.1     0.071\r\n3           8.9             0.62        0.18            3.8     0.176\r\n4           8.9             0.22        0.48            1.8     0.077\r\n5           7.9             0.43        0.21            1.6     0.106\r\n6           7.1             0.71        0               1.9     0.08 \r\n# ... with 8 more variables: free_sulfur_dioxide <dbl>,\r\n#   total_sulfur_dioxide <dbl>, density <dbl>, pH <dbl>,\r\n#   sulphates <dbl>, alcohol <dbl>, quality <dbl>, pred <dbl>\r\n\r\nCONCLUSION\r\nIn this simple scenario, we were interested in seeing how the model performs on the testing data that were left out. The code fitted the model to the training data and apply it to the testing data. There are other ways we could have done this, but the way we do it here will be useful when we start using more complex models where we need to tune model parameters. Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data. RMSE is a good measure to use if we want to estimate the standard deviation of a typical observed value from our model’s prediction, R-squared is a statistical measure that represents the goodness of fit of a regression model. The ideal value for r-square is 1. The closer the value of r-square to 1, the better is the model fitted. In Machine Learning, MAE is a model evaluation metric often used with regression models. After the model is fitted and applied, we collected the performance metrics and display them and show the predictions from the testing data. 34,2% is predicted wrong, which is at the end maybe a bit disappointing after all the work. But we know what the best model is for this data-set.\r\nThis work has some strengths We found a uniform and consistent way to compare models with each and to choose the best one out of them. One of the big advantages of the random forest model (which is choosen here) is the versality and flexibility. It can be used for both regression and classification problems. But this work has also some limitations. Random forest is good for predictions and regression, so this could be used by the researcher for interpretation here. But for of modelling is relatively new for this researcher (instead of linear regression for example) so he found himself restricted here at the end. A limitation of random forest is also that this algorithm is fast to train, but quit slow to create predictions once they are trained: a more accurate prediction needs more trees, which results in a slower model. And a last limitation which we have to mention here is, that we used only three models and maybe other models were better for these data.\r\nLesson learned is that we found a consitent workflow for analyzing data as presented here on quality of wine. It is a very good starting point for further research. The next step would be now to work on increasing the predictive power of the model and start with tuning on the hyperparameters.\r\nReferences\r\nAttalides, N. (2020). Introduction to machine learning. Barcelona. Presentation\r\nBarter, R.(2020). Tidymodels: tidy machine learning in R\r\nBaumer, B. Kaplan, D.T., Horton, N.J. (2017). Modern data science with R. CRCPress: Boca Raton.\r\nBoehmke, B. & Greenwell, B. (2020). Hands on machine learning with R. Bookdown version\r\nCortez, P., Cerdeira, A., Almeida, F., Matos, T. & Reis, J. (2009). Modeling wine prefernces by data mining from physicochemical properties. Decision Support Systems, 47, 547-533.\r\nHartie, T., Tibskirani, R. & Friedmann, J. (2009). The elements of statistical learning. Data mining, inference and prediction. 2nd edition. Springer: New York.\r\nIrizarry, R.A. (2020). Introduction to data science. Data analysis and prediction algorithms with R. CRC Press: Boca Raton.\r\nJames, S., Witten, D., Hastie, T.. & Tibskirani, R. (2013). An introduction to statistical learning with application in R.\r\nJonkman, H. (2019-2021). Harrie’s hoekje, his website with different blogs on machine learning in Dutch\r\nKuhn, M. 7 Johnson, K. (2013). Applied predictive modeling. Springer: New York.\r\nKuhn, M. & Johnson, K. (2019). Feature engineering and selection: A practical approach for predictive models\r\nKuhn, M. & Silge, J. (2021). Tydy modeling with R. Bookdown version\r\nLendway, L. (2020). 2020_north-tidymodels.Introduction on github\r\nLewis, J.E. (2020). Coding machine learning models\r\nRaoniar, R. (2021). Modeling binary logistic regression using tidymodels library in R (Part 1). Towards data science\r\nRuiz, E. (2019). A gentle introduction to tidymodels\r\nSeyedian, A. (2021). Medical cost personal datasets. Insurance forcast by using linear regression\r\nSilge, J. (2020). Get started with tidymodels and #TidyTuesdag Palmer penguins\r\nSilge, J. (2021). Supervised machine learning case studies in R\r\nTidymodels. Tidymodels website\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-22-machine-learning-workflow/machine-learning-workflow_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-11-regressie-en-zo/",
    "title": "Regressie en nog zo iets",
    "description": "Dit is een blog naar aanleiding van Gelman/Hill/Vehtari nieuwe boek Regresion and other stories",
    "author": [
      {
        "name": "Harrie Jonkman en Mr.X",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nRegression and other stories\r\nVijftien jaar geleden schreven Gelman en Hill Data analysis using regression and multilevel/hierarchical models, een klassieker over moderne data-analyse. Ze gebruikte R en WinBugs voor lineaire en logistische, hierarchische regressieanalyse en causale inferentie. Ze lieten zien hoe je dat op de frequentistische en Bayesiaanse manier kunt doen. Het boek werd voor mij een naslagwerk dat ik steeds maar weer uit de kast trok. Vorig jaar dacht ik, laat ik eens zien of Gelman al weer iets nieuws heeft geschreven en toen zag ik dat Regression and other stories hiernet uit was is. Dat heeft Andrew Gelman weer met Jennifer Hill geschreven maar nu ook met de Fin Aki Vehtari. Ik was er nog niet aan toe gekomen om het te lezen. Dat heb ik deze maand gedaan. Ook dit boek zal ik vaker uit de kast trekken. Dit boek gaat over allerlei aspecten van regressie. Het is een theoretisch én praktisch boek. Je leert, wat ze noemen, voorspellende modellen beter begrijpen, toepassen in verschillende praktische problemen en je leert het simuleren. Je leert het opbouwen vanaf de basis en daarna kun je het in verschillende situaties toepassen. Het wil kritisch zijn, zonder nihilistisch te worden en vooral laten zien dat je van statistische analyse kunt leren. Ook dit boek staat op twee benen: frequentisch en Bayesiaans en laat zien hoe informatie wordt gebruikt in het schattingsproces, de assumpties die eraan ten grondslag liggen en hoe schattingen en voorspellingen kunnen worden geïnterpreteerd in beide raamwerken. Beide kunnen worden gebruikt, maar het is ook duidelijk dat de voorkeur bij Bayesiaanse benadering ligt. Dan kun je ook andere informatie gebruiken om te schatten of te voorspellen. En omdat je simuleert (het model duizenden keren draait) kunt je met de Bayesiaanse techniek meer zeggen over onzekerheid. Dat maakt deze techniek zeer geschikt voor regressieanalyses zoals in dit boek gepresenteerd. Wat ik zelf van dit boek heb geleerd zijn de mogelijkheden om op basis van gegevens te voorspellen. Vooral hoofdstuk 9 (Voorspellen en Bayesiaanse inferentie) vond ik interessant. Maar het boek zit vol informatie en kennis en laat zich amper samenvatten. Het lijkt erop dat het een eerste deel is en ik verwacht dat er later nog een tweede deel komt dat de nadruk legt op multilevel analyse. We zullen zien Bij het boek zit ook nog een website met data en scripts om zelf uit te proberen, prachtig onderwijsmateriaal opgesteld door Aki Vehtari hier.\r\nInterssante blog\r\nToen ik het boek uit had kwam ik een een blog tegen op R-bloggers. Het verscheen op 1 september 2021 hier, maar ik kon niet zien van wie het is (Mister X, sorry. Hij of zij schreef het nadat deze persoon Regression and other stories had gelezen. Het vat heel goed samen hoe moderne regressieanalyse werkt en daarom heb ik het voor hier vertaald.\r\n\r\n\r\n\r\nBayesiaanse regressieanalyse met Rstanarm\r\nIn deze post zullen we een eenvoudig voorbeeld van Bayesiaanse regressieanalyse doornemen met het rstanarm pakket in R. Ik heb Gelman, Hill en Vehtari’s recente boek Regression and Other Stories” gelezen, en deze blog post is mijn poging om enkele van de dingen die ik heb geleerd toe te passen. Ik heb de afgelopen jaren stukjes en beetjes van de Bayesiaanse benadering opgevangen, en ik vind het een heel interessante manier om over gegevensanalyse na te denken en ze uit te voeren. Ik heb met veel plezier het nieuwe boek van Gelman en collega’s doorgewerkt en geëxperimenteerd met deze technieken, en ik ben blij dat ik hier iets kan delen van wat ik heb geleerd.\r\nJe kunt de gegevens en alle code van deze blogpost hier op Github vinden.\r\nDe data\r\nDe gegevens die we in deze blog zullen onderzoeken bestaan uit de dagelijkse totale stappentellingen van verschillende fitnesstrackers die ik de afgelopen 6 jaar heb gehad. De eerste waarneming werd geregistreerd op 2015-03-04 en de laatste op 2021-03-15. Gedurende deze periode bevat de dataset de dagelijkse totale stappentellingen voor 2.181 dagen.\r\nNaast de dagelijkse totale stappentelling bevat de dataset informatie over de dag van de week (bijv. maandag, dinsdag, etc.), het apparaat dat is gebruikt om de stappentelling vast te leggen (door de jaren heen heb ik er 3 gehad - Accupedo, Fitbit en Mi-Band), en het weer voor elke datum (de gemiddelde dagelijkse temperatuur in graden Celsius en de totale dagelijkse neerslag in millimeters, verkregen via het GSODR pakket in R).\r\nDe dataset (genaamd steps_weather) ziet er als volgt uit:\r\n\r\n# A tibble: 6 x 7\r\n  date       daily_total dow   week_weekend device    temp  prcp\r\n  <date>           <dbl> <chr> <chr>        <chr>    <dbl> <dbl>\r\n1 2015-03-04       14136 Wed   Weekday      Accupedo   4.3   1.3\r\n2 2015-03-05       11248 Thu   Weekday      Accupedo   4.7   0  \r\n3 2015-03-06       12803 Fri   Weekday      Accupedo   5.4   0  \r\n4 2015-03-07       15011 Sat   Weekend      Accupedo   7.9   0  \r\n5 2015-03-08        9222 Sun   Weekend      Accupedo  10.2   0  \r\n6 2015-03-09       21452 Mon   Weekday      Accupedo   8.8   0  \r\n\r\nHieronder zie je de eerste en laatste data.\r\n\r\n[1] \"2015-03-04\"\r\n[1] \"2021-03-15\"\r\n\r\nHieronder zie je histogrammen van twee variabelen, namelijk dagelijks totale aantal stappen en de gemiddelde temperatuur over deze periode.\r\n\r\n\r\n\r\nRegressie Analyse\r\nHet doel van deze blogbijdrage is Bayesiaanse regressiemodellering te verkennen met behulp van het rstanarm pakket. Daarom zullen we de gegevens gebruiken om een zeer eenvoudig model te maken en ons te concentreren op het begrijpen van de modelfit en verschillende regressiediagnoses.\r\nOns model hier is een lineair regressiemodel dat de gemiddelde temperatuur in graden Celsius gebruikt om het totale dagelijkse aantal stappen te voorspellen. We gebruiken het stan_glm commando om de regressie-analyse uit te voeren. We kunnen het model uitvoeren en een samenvatting van de resultaten zien die de volgende tabel oplevert.\r\nHet draait 4.000 iteraties en daarna worden de resultaten gepresenteerd.\r\n\r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 0.21 seconds (Warm-up)\r\nChain 1:                0.175 seconds (Sampling)\r\nChain 1:                0.385 seconds (Total)\r\nChain 1: \r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\r\nChain 2: \r\nChain 2: Gradient evaluation took 0 seconds\r\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 2: Adjust your expectations accordingly!\r\nChain 2: \r\nChain 2: \r\nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 2: \r\nChain 2:  Elapsed Time: 0.498 seconds (Warm-up)\r\nChain 2:                0.17 seconds (Sampling)\r\nChain 2:                0.668 seconds (Total)\r\nChain 2: \r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\r\nChain 3: \r\nChain 3: Gradient evaluation took 0 seconds\r\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 3: Adjust your expectations accordingly!\r\nChain 3: \r\nChain 3: \r\nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 3: \r\nChain 3:  Elapsed Time: 0.426 seconds (Warm-up)\r\nChain 3:                0.146 seconds (Sampling)\r\nChain 3:                0.572 seconds (Total)\r\nChain 3: \r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\r\nChain 4: \r\nChain 4: Gradient evaluation took 0 seconds\r\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 4: Adjust your expectations accordingly!\r\nChain 4: \r\nChain 4: \r\nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 4: \r\nChain 4:  Elapsed Time: 0.271 seconds (Warm-up)\r\nChain 4:                0.145 seconds (Sampling)\r\nChain 4:                0.416 seconds (Total)\r\nChain 4: \r\n\r\nModel Info:\r\n function:     stan_glm\r\n family:       gaussian [identity]\r\n formula:      daily_total ~ temp\r\n algorithm:    sampling\r\n sample:       4000 (posterior sample size)\r\n priors:       see help('prior_summary')\r\n observations: 2181\r\n predictors:   2\r\n\r\nEstimates:\r\n              mean    sd      10%     50%     90%  \r\n(Intercept) 16214.4   276.7 15855.8 16220.4 16565.0\r\ntemp           26.8    21.7    -0.8    26.5    54.6\r\nsigma        6198.5    96.3  6073.5  6194.9  6325.3\r\n\r\nFit Diagnostics:\r\n           mean    sd      10%     50%     90%  \r\nmean_PPD 16517.8   184.2 16281.9 16517.8 16756.5\r\n\r\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\r\n\r\nMCMC diagnostics\r\n              mcse Rhat n_eff\r\n(Intercept)   4.4  1.0  4042 \r\ntemp          0.3  1.0  3948 \r\nsigma         1.5  1.0  4023 \r\nmean_PPD      3.0  1.0  3896 \r\nlog-posterior 0.0  1.0  1471 \r\n\r\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\r\n\r\nDeze tabel bevat de volgende variabelen:\r\nIntercept: Dit cijfer geeft het verwachte dagelijkse aantal stappen weer wanneer de gemiddelde dagtemperatuur 0 is. Met andere woorden, het model voorspelt dat, wanneer de gemiddelde dagtemperatuur 0 graden Celsius is, ik op die dag 16211,7 stappen zal lopen.\r\ntemp: Dit is de geschatte toename van het dagelijkse aantal stappen per 1 eenheid stijging van de gemiddelde dagelijkse temperatuur in graden Celsius. Met andere woorden, het model voorspelt dat voor elke 1 graad stijging van de gemiddelde dagtemperatuur, ik die dag 26,8 extra stappen zal zetten.\r\nsigma: Dit is de geschatte standaardafwijking van de residuen van het regressiemodel. (Het residu is het verschil tussen de modelvoorspelling en de waargenomen waarde voor het dagelijkse totale aantal stappen). De verdeling van de residuele waarden heeft een standaardafwijking van 6195,1.\r\ngeman_PPD: De mean_ppd is de gemiddelde posterior predictive distributie van de door het model geïmpliceerde uitkomstvariabele (we zullen dit verder bespreken in het gedeelte over posterior predictive checques hieronder).\r\nDe uitvoer in de samenvattende tabel hierboven lijkt vrij veel op de uitvoer van een standaard gewone kleinste kwadratenregressie. In de Bayesiaanse benadering van regressiemodellering krijgen we echter niet gewoon puntschattingen van coëfficiënten, maar eerder volledige verdelingen van simulaties die mogelijke waarden van de coëfficiënten vertegenwoordigen, gegeven het model. Met andere woorden, de getallen in de bovenstaande tabel zijn gewoon samenvattingen van verdelingen van coëfficiënten die de relatie tussen de voorspellers en de uitkomstvariabele beschrijven.\r\nStandaard geven de rstanarm regressiemodellen 4.000 simulaties van de posterior verdeling voor elke modelparameter. We kunnen de simulaties uit het modelobject halen en ze als volgt bekijken:\r\n\r\n\r\n\r\nEn dat geeft 4000 posterior simulaties van de parameters intercept, temp en sd. Deze simulaties drukken de onzekerheid uit van de modeloutput die je hierboven vindt\r\n\r\n          parameters\r\niterations (Intercept)      temp    sigma\r\n      [1,]    16390.01  6.863927 6212.239\r\n      [2,]    16212.82 33.740601 6117.295\r\n      [3,]    16184.85 28.457402 6425.134\r\n      [4,]    16110.01 41.626205 6234.812\r\n      [5,]    16320.90 17.457927 6098.088\r\n      [6,]    16455.84 21.699611 6043.128\r\n\r\nDe gemiddelde waarden van deze verdelingen van simulaties worden weergegeven in de hierboven afgebeelde tabel met regressiesamenvattingsuitvoer.\r\n\r\n[1] 16214.44\r\n[1] 276.6901\r\n[1] 26.81344\r\n[1] 26.48466\r\n[1] 21.70327\r\n[1] 6198.463\r\n[1] 96.33321\r\n\r\nVisualiseren van de posterior distributies met bayesplot\r\nHet uitstekende bayesplot-pakket bevat een aantal handige functies om de posterior distributies van onze coëfficiënten te visualiseren. Laten we de mcmc_areas-functie gebruiken om de 90% geloofwaardige intervallen voor de modelcoëfficiënten weer te geven. die de volgende plot oplevert.\r\n\r\n\r\n\r\nDeze grafiek is zeer interessant en toont ons de posterior distributie van de simulaties van het model dat we hierboven hebben getoond. De plot geeft ons een idee van de variatie van alle parameters, maar de coëfficiënten liggen op zulke verschillende schalen dat de details verloren gaan door ze allemaal samen weer te geven.\r\nLaten we ons concentreren op de temperatuurcoëfficiënt en een gebiedsplot maken met alleen deze parameter:\r\n\r\n\r\n\r\nDeze grafiek toont de mediaan van de verdeling (26,69, die zeer dicht bij ons gemiddelde van 26,83 ligt). We kunnen de grenzen van het hierboven getoonde gearceerde gebied bepalen met de posterior_interval functie, of rechtstreeks uit de simulaties zelf:\r\n\r\n            5%      95%\r\ntemp -8.750841 62.49293\r\n       5%       95% \r\n-8.750841 62.492926 \r\n\r\nBeide methoden geven hetzelfde resultaat.\r\nVisualiseren van slopes van de posterior distributie\r\nEen andere interessante manier om de verschillende coëfficiënten uit de posterior verdeling te visualiseren is door de regressielijnen van vele simulaties uit de posterior distributie gelijktijdig uit te zetten tegen de ruwe data. Deze visualisatietechniek wordt veel gebruikt in zowel Richard McElreath’s Statistical Rethinking als in Gelmans Regression and Other Stories. Beide boeken maken dit soort tekeningen met behulp van plotfuncties in basis-R. Ik was erg blij deze blog post te vinden met een voorbeeld van hoe je deze plots kan maken met behulp van ggplot2! Ik heb de code lichtjes aangepast om de onderstaande figuur te maken.\r\nDe eerste stap is het extraheren van de basisinformatie om elke regressielijn te tekenen. We doen dit met de volgende code, waarbij we in essentie ons model-object doorgeven aan een dataframe, en dan enkel het intercept en de temperatuur-hellingen voor elk van onze 4.000 simulaties uit de posterior distributie houden.\r\nDit geeft het volgende dataframe terug:\r\n\r\n# A tibble: 6 x 2\r\n  intercept  temp\r\n      <dbl> <dbl>\r\n1    16390.  6.86\r\n2    16213. 33.7 \r\n3    16185. 28.5 \r\n4    16110. 41.6 \r\n5    16321. 17.5 \r\n6    16456. 21.7 \r\n\r\nDit dataframe heeft 4.000 rijen, één voor elke simulatie uit de posterior verdeling in onze originele sims matrix.\r\nWe stellen dan enkele “esthetische regelaars” in, die specificeren hoeveel lijnen van de posterior verdeling we willen plotten, hun transparantie (de alpha parameter), en de kleuren voor de individuele posterior lijnen en het algemene gemiddelde van de posterior schattingen. De ggplot2 code stelt dan de assen in met het originele data frame (steps_weather), plot een steekproef van regressielijnen uit de posterior verdeling in licht grijs, en plot dan de gemiddelde helling van alle posterior simulaties in blauw.\r\nDat levert de volgende plot op:\r\n\r\n\r\n\r\nDe gemiddelde helling (weergegeven in de grafiek en ook terug te vinden in de modelsamenvatting hierboven) van de temperatuur is 26,8. Maar het plotten van samples uit de posterior verdeling maakt duidelijk dat er nogal wat onzekerheid is over de grootte van deze relatie! Sommige van de hellingen uit de verdeling zijn negatief - zoals we zagen in onze berekening van de onzekerheidsintervallen hierboven. In essentie is er een “gemiddelde” coëfficiëntschatting, maar wat het Bayesiaanse raamwerk heel goed doet (via de posterior verdelingen) is extra informatie verschaffen over de onzekerheid van onze schattingen.\r\nPosterior voorspellingscontroles\r\nEen laatste manier om grafieken te gebruiken om ons model te begrijpen is door gebruik te maken van posterior predictive checks. Ik hou van deze intuïtieve manier om de logica achter deze reeks technieken uit te leggen: “Het idee achter posterior predictive checking is simpel: als een model een goede fit is, moeten we het kunnen gebruiken om gegevens te genereren die veel lijken op de gegevens die we hebben waargenomen. De gegenereerde gegevens worden de posterior predictive distributie genoemd, dat is de verdeling van de uitkomstvariabele (dagelijks totaal aantal stappen in ons geval) die wordt geïmpliceerd door een model (het regressiemodel dat hierboven is gedefinieerd). Het gemiddelde van deze verdeling wordt weergegeven in de bovenstaande uitvoer van het regressieoverzicht, met de naam mean_PPD.\r\nEr zijn vele soorten visualisaties die men kan maken om posterieure voorspellende controles uit te voeren. Wij zullen één zo’n analyse uitvoeren (voor meer informatie over dit onderwerp), die het gemiddelde van onze uitkomstvariabele (dagelijks totaal aantal stappen) in onze oorspronkelijke dataset en de posterior predictive distributie van ons regressie model.\r\nDe code is rechttoe rechtaan.\r\n\r\n\r\n\r\nWe kunnen zien dat het gemiddelde van onze dagelijkse stappen variabele in de originele dataset in principe in het midden van de posterior voorspellende distributie valt. Volgens deze analyse “genereert ons regressiemodel gegevens die veel lijken op de gegevens die we hebben geobserveerd!”\r\nHet model gebruiken om voorspellingen te doen met nieuwe gegevens\r\nTenslotte zullen we het model gebruiken om voorspellingen te doen over het aantal stappen per dag, gebaseerd op een specifieke waarde van de gemiddelde dagtemperatuur. In Regression and Other Stories bespreken de auteurs in hoofdstuk 9 hoe een Bayesiaans regressiemodel kan worden gebruikt om voorspellingen te doen op een aantal verschillende manieren, waarbij telkens verschillende niveaus van onzekerheid in de voorspellingen worden opgenomen. Wij zullen elk van deze methoden achtereenvolgens toepassen.\r\nVoor elk van de onderstaande methoden zullen we het gemiddelde aantal stappen per dag voorspellen wanneer de temperatuur 10 graden Celsius bedraagt. We kunnen een nieuw dataframe opzetten dat we zullen gebruiken om de modelvoorspellingen te verkrijgen:\r\n\r\n\r\n\r\nPuntvoorspellingen met behulp van de samenvattingen van de afzonderlijke waardecoëfficiënten van de posterieure verdelingen\r\nDe eerste benadering komt overeen met die welke we zouden gebruiken bij een klassieke regressieanalyse. We gebruiken gewoon de puntschattingen uit de modelsamenvatting, voegen de nieuwe temperatuur in waarvoor we een voorspelling willen, en produceren onze voorspelling in de vorm van een enkel getal. We kunnen dit doen met de predict functie in R, of door de coëfficiënten uit onze modelsamenvatting te vermenigvuldigen. Beide methoden leveren dezelfde voorspelling op:\r\n\r\n\r\n\r\nBeide leveren een puntvoorspelling van 16483.71.\r\n\r\n       1 \r\n16482.57 \r\n      temp\r\n1 16482.57\r\n\r\nLineaire voorspellingen met onzekerheid (in de interceptie + temperatuurcoëfficiënten)\r\nWe kunnen echter genuanceerder zijn in onze voorspelling van het dagelijkse totale aantal stappen. Het hierboven berekende regressiemodel geeft 4.000 simulaties voor drie parameters - de intercept, de temperatuurcoëfficiënt, en sigma (de standaardafwijking van de residuen).\r\nDe volgende methode is geïmplementeerd in rstanarm met de posterior_linpred functie, en we kunnen deze gebruiken om de voorspellingen direct te berekenen. We kunnen hetzelfde resultaat ook “met de hand” berekenen met behulp van de matrix van simulaties uit de posterior verdeling van onze coëfficiëntschattingen. Bij deze aanpak wordt gewoon de temperatuur ingevoerd waarvoor wij voorspellingen willen (10 graden Celsius) en wordt voor elk van de simulaties het intercept opgeteld bij de temperatuurcoëfficiënt maal 10. Beide methoden leveren dezelfde vector van 4.000 voorspellingen op:\r\n\r\n\r\n\r\nDat geeft dezelfde resultaten.\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  y_linpred and y_linpred_2\r\nt = Inf, df = 3998, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 1 1\r\nsample estimates:\r\ncor \r\n  1 \r\n\r\nPosterior Predictive Distributies met de onzekerheid in de coëfficiëntschattingen en in sigma\r\nDe laatste voorspellingsmethode voegt nog een extra laag van onzekerheid toe aan onze voorspellingen, door de posterior verdelingen voor sigma mee te nemen in de berekeningen. Deze methode is beschikbaar via de functie posterior_predict, en we gebruiken opnieuw onze matrix van 4.000 simulaties om een vector van 4.000 voorspellingen te berekenen.\r\nDe posterior predict methode volgt de aanpak van de posterior_linpred functie hierboven, maar voegt een extra foutterm toe gebaseerd op onze schattingen van sigma, de standaardafwijking van de residuen. De berekening zoals getoond in het “met de hand” gedeelte van de code hieronder maakt. Het maakt duidelijk waar de willekeurigheid in het spel komt, en vanwege deze willekeurigheid zullen de resultaten van de posterior_predict functie en de “met de hand” berekening niet overeenkomen tenzij we dezelfde seed instellen voordat we elke berekening uitvoeren. Beide methoden leveren een vector van 4.000 voorspellingen op.\r\n\r\n\r\n\r\nOf\r\n\r\n\r\n\r\nDan zien we dezelfde resultaten:\r\n\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  y_post_pred and y_post_pred_2\r\nt = Inf, df = 3998, p-value < 2.2e-16\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n 1 1\r\nsample estimates:\r\ncor \r\n  1 \r\n\r\nVisualisatie van de drie soorten voorspellingen\r\nLaten we een visualisatie maken die de resultaten weergeeft van de voorspellingen die we hierboven deden. We kunnen een enkele cirkel gebruiken om de puntvoorspelling van de regressiecoëfficiënten weer te geven in de modelsamenvatting, en histogrammen om de posterior verdelingen weer te geven die geproduceerd zijn door de lineaire voorspelling met onzekerheid (posterior_linpred) en posterior predictive distribution (posterior_predict) methoden die hierboven beschreven zijn.\r\nWe zetten eerst de vectoren van posterior verdelingen die we hierboven hebben gemaakt in een dataframe. We maken ook een dataframe met de enkele puntvoorspelling van onze lineaire voorspelling. Vervolgens stellen we ons kleurenpalet in (afkomstig uit het NineteenEightyR pakket) en maken dan de plot:\r\n\r\n\r\n\r\nDat geeft de volgende plot:\r\n\r\n\r\n\r\nDeze grafiek is zeer informatief en maakt duidelijk hoe groot de onzekerheid is die we voor elk van onze voorspellingsmethoden krijgen. Hoewel alle drie voorspellingsmethoden op dezelfde plaats op de x-as gecentreerd zijn, verschillen zij sterk wat betreft de onzekerheid rond de voorspellingsramingen.\r\nDe puntvoorspelling is een enkele waarde en bevat als zodanig geen onzekerheid. De lineaire voorspelling met onzekerheid, die rekening houdt met de posterior verdeling van onze interceptie- en temperatuurcoëfficiënten, heeft een zeer scherpe piek, waarbij de modelschattingen binnen een relatief klein bereik variëren. De posterior predictive distributie varieert veel meer, met het laagste bereik van de verdeling onder nul, en het hoogste bereik van de verdeling boven 40.000!\r\nSamenvatting en conclusie\r\nIn dit artikel hebben we een eenvoudig model gemaakt met behulp van het rstanarm pakket in R, om te leren over Bayesiaanse regressie analyse. We gebruikten een dataset bestaande uit mijn geschiedenis van dagelijkse totale stappen, en bouwden een regressie model om het dagelijkse aantal stappen te voorspellen uit de dagelijkse gemiddelde temperatuur in graden Celsius. In tegenstelling tot de gewone kleinste kwadraten benadering die puntschattingen van modelcoëfficiënten oplevert, geeft de Bayesiaanse regressie posterior verdelingen van de coëfficiëntschattingen. Wij hebben een aantal verschillende samenvattingen en visualisaties van deze posterior verdelingen gemaakt om de coëfficiënten en de Bayesiaanse benadering in het algemeen te begrijpen - A) het gebruik van het bayesplot pakket om de posterior verdelingen van onze coëfficiënten te visualiseren\r\nB) het plotten van 500 hellingen van de posterior verdeling, en\r\nC) het uitvoeren van een controle van de posterior predictive distributie.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-11-regressie-en-zo/regressie-en-zo_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-11-multilevelrstan/",
    "title": "Introductie op multilevel modelleren met gebruik van **rstanarm**: Een tutorial voor onderwijsonderzoekers",
    "description": "Een tutorial over multilevel Bayes",
    "author": [
      {
        "name": "JoonHo Lee, Nicholas Sim, Feng Ji, and Sophia Rabe-Hesketh, vertaling HarrieJonkman",
        "url": {}
      }
    ],
    "date": "2022-02-06",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\nEen paar jaar geleden schreven Joonho Lee en collega’s van de Berkley Universiteit onder leiding van Sophia Rabe-Hesketh (van wie ik al eerder veel geleerd heb over multilevel analyse) een mooie tutorial over het gebruik van rstanarm in multilevelanalyse. Omdat deze tutorial in vele opzichten leerzaam is, heb ik deze vertaald. Zo maakte ik mij het werken met rstanarm eigen maar ook hoe je zo’n tutorial in rmarkdown zet. Alle credits gaan natuurlijk naar Joonhoo Lee e.a.. De oorspronkelijke tutorial van 24 april 2018 vind je hier. Later vond ik ook op github de syntax en kon ik diverse eigen fouten weer bijstellen hier.\r\n\r\nIntroductie\r\nEen veel voorkomend kenmerk van datastructuren in het onderwijs is dat analyse-eenheden (b.v. leerlingen) genest zijn in hogere organisatorische clusters (b.v. scholen). Dit soort structuren leidt tot afhankelijkheid tussen de antwoorden die worden waargenomen voor eenheden binnen dezelfde cluster. Leerlingen in dezelfde school hebben in het algemeen de neiging meer gelijkenissen te vertonen in hun academische en attitudinale kenmerken dan leerlingen die willekeurig uit de bevolking worden gekozen.\r\nMultilevel modellen1 zijn ontworpen om dergelijke afhankelijkheid binnen clusters te modelleren. Multilevelmodellen erkennen het bestaan van gegevensclustering (op twee of meer niveaus) door residuele componenten op elk niveau in de hiërarchie toe te staan. Bijvoorbeeld, een model met twee niveaus dat de groepering van leerlingresultaten binnen scholen toelaat, zou residuen op zowel leerling- als schoolniveau omvatten. De residuele variantie wordt dan verdeeld in een tussen-school component (de variantie van de residuen op schoolniveau) en een binnen-school component (de variantie van de residuen op leerlingniveau).\r\nIn deze tutorial laten Joonhoo Lee en collega’s zien hoe je een multilevel lineair model kunt fitten binnen een volledig Bayesiaans raamwerk met rstanarm. Deze handleiding is in de eerste plaats gericht op onderwijsonderzoekers die lme4 in R hebben gebruikt om modellen te fitten op hun gegevens en die mogelijk geïnteresseerd zijn in het leren fitten van Bayesiaanse multilevel modellen. Voor lezers die lme4 nog niet eerder hebben gebruikt, geven we echter een kort overzicht van het gebruik van het pakket voor het fitten van multilevel modellen.\r\nDeze tutorial maakt gebruik van Stan versie 2.17.3 en vereist de volgende R pakketten.\r\n\r\n\r\n# Pakketten die nodig zijn\r\nlibrary(mlmRev)\r\nlibrary(lme4)\r\nlibrary(rstanarm)\r\nlibrary(ggplot2)\r\n\r\n\r\n\r\nData voorbeeld\r\nZe analyseren de Gcsemv dataset (Rasbash et al. 2000) uit het mlmRev pakket in R. De gegevens omvatten de GCSE-examenscores (General Certificate of Secondary Education) van 1.905 leerlingen van 73 scholen in Engeland op een natuurwetenschappelijk vak. De Gcsemv-dataset bestaat uit de volgende 5 variabelen:\r\nschool: schoolidentificatiecode\r\nstudent: identificatiecode student\r\ngender: geslacht van een leerling (M: Man, F: Vrouw)\r\nwritten: totaalscore op schriftelijk werkstuk\r\ncourse: totaalscore op schriftelijk werkstuk\r\n\r\n\r\n# Use example dataset from mlmRev package: GCSE exam score\r\ndata(Gcsemv, package = \"mlmRev\")\r\nsummary(Gcsemv)\r\n\r\n\r\n     school        student     gender      written     \r\n 68137  : 104   77     :  14   F:1128   Min.   : 0.60  \r\n 68411  :  84   83     :  14   M: 777   1st Qu.:37.00  \r\n 68107  :  79   53     :  13            Median :46.00  \r\n 68809  :  73   66     :  13            Mean   :46.37  \r\n 22520  :  65   27     :  12            3rd Qu.:55.00  \r\n 60457  :  54   110    :  12            Max.   :90.00  \r\n (Other):1446   (Other):1827            NA's   :202    \r\n     course      \r\n Min.   :  9.25  \r\n 1st Qu.: 62.90  \r\n Median : 75.90  \r\n Mean   : 73.39  \r\n 3rd Qu.: 86.10  \r\n Max.   :100.00  \r\n NA's   :180     \r\n\r\nTwee onderdelen van het examen werden geregistreerd als uitkomstvariabelen: schriftelijk werkstuk (written) en cursuswerkstuk (course). In deze tutorial wordt alleen de totaalscore op het cursuswerkstuk (course) geanalyseerd. Zoals hierboven te zien is, ontbreken er bij sommige waarnemingen waarden voor bepaalde covariaten. Hoewel we de data niet subsetten om alleen volledige gevallen op te nemen om aan te tonen dat rstanarm deze waarnemingen automatisch laat vallen, is het over het algemeen een goed gebruik om dit handmatig te doen indien nodig.\r\n\r\n\r\n# Maak Male dereferentiecategorie en hernoem de variabele\r\nGcsemv$female <- relevel(Gcsemv$gender, \"M\")\r\n\r\n\r\n# Gebruik alleen de totaalscore op course \r\nGCSE <- subset(x = Gcsemv, \r\n               select = c(school, student, female, course))\r\n\r\n# Tel unieke scholen en studenten\r\nJ <- length(unique(GCSE$school))\r\nN <- nrow(GCSE)\r\n\r\n\r\n\r\nHet pakket rstanarm automatiseert verschillende stappen van datavoorbewerking, waardoor het gebruik ervan sterk lijkt op dat van lme4 en wel op de volgende manier.\r\nInput - rstanarm kan een dataframe als input nemen2.\r\nOntbrekende gegevens - rstanarm verwijdert automatisch waarnemingen met NA waarden voor elke variabele gebruikt in het model3.\r\nIdentifiers - rstanarm vereist niet dat identifiers opeenvolgend zijn4. We stellen voor dat het een goede gewoonte is om alle cluster- en unit-identifiers, evenals categorische variabelen als factoren op te slaan. Dit geldt evenzeer voor lme4 als voor rstanarm. Men kan de structuur van de variabelen controleren met behulp van de str() functie.\r\n\r\n\r\n# Check structure of data frame\r\nstr(GCSE)\r\n\r\n\r\n'data.frame':   1905 obs. of  4 variables:\r\n $ school : Factor w/ 73 levels \"20920\",\"22520\",..: 1 1 1 1 1 1 1 1 1 2 ...\r\n $ student: Factor w/ 649 levels \"1\",\"2\",\"3\",\"4\",..: 16 25 27 31 42 62 101 113 146 1 ...\r\n $ female : Factor w/ 2 levels \"M\",\"F\": 1 2 2 2 1 2 2 1 1 2 ...\r\n $ course : num  NA 71.2 76.8 87.9 44.4 NA 89.8 17.5 32.4 84.2 ...\r\n\r\nLikelihood inferentie met lmer()\r\nIn dit deel bespreken we kort drie lineaire multilevelmodellen die in deze tutorial zullen worden toegepast. We beginnen met een variërend intercept model zonder voorspellers (Model 1), dan gaan we verder met het variërend intercept model met één voorspeller (Model 2), en het variërend intercept en helling model (Model 3).\r\nModel 1: Variërend intercept model zonder voorspellers (Variantiecomponentenmodel)\r\nBeschouw het eenvoudigste multilevel model voor leerlingen \\(i=1,...,n\\) genest binnen scholen \\(j=1,...,J\\) en voor wie we examenresultaten als respons hebben. We kunnen een variabel interceptmodel met twee niveaus zonder voorspellers schrijven met de gebruikelijke tweedelige formulering als\r\n\\[Y_{ij}=\\alpha_{j} + \\epsilon_{ij}, \\text{ where } \\epsilon{ij} \\sim N(0,\\sigma^2_{y}) \\] \\[\\alpha{_j}=u_{\\alpha} + u_{j}, \\text { where } \\epsilon{ij} \\sim N(0,\\sigma^2_{y})\\] waarin \\(yij\\) de examenscore is voor de \\(ith\\) leerling op de \\(jth\\) school, \\(\\alpha_{j}\\) de variërende intercept voor de jde school, en \\(u_{a}\\) het algemene gemiddelde voor alle scholen. Als alternatief kan het model in verkorte vorm worden uitgedrukt als\r\n\\[y_{ij}=u_{a} + u_{j} = \\epsilon_{ij}\\] Als we verder aannemen dat de fouten op leerlingniveau \\(\\epsilon_{ij}\\) normaal verdeeld zijn met gemiddelde 0 en variantie \\(\\sigma^2_{y}\\), en dat de variërende intercepten op schoolniveau \\(\\alpha_j\\) normaal verdeeld zijn met gemiddelde \\(u_{a}\\) en variantie \\(\\sigma^2_{a}\\), dan kan het model worden uitgedrukt als\r\n\\[y_{ij}∼ N(\\alpha_{j},\\sigma^2_{y})\\] \\[a_{j}∼ N(u_{a},\\sigma^2_{a})\\]\r\nDit model kan dan worden aangepast met lmer(). We specificeren een intercept (de voorspeller “1”) en laten deze variëren met de niveau-2-identifier (school). We specificeren ook de REML = FALSE optie om maximum likelihood (ML) schattingen te krijgen in plaats van de standaard restricted maximum likelihood (REML) schattingen.\r\n\r\n\r\nM1 <- lmer(formula = course ~ 1 + (1 | school), \r\n           data = GCSE, \r\n           REML = FALSE)\r\nsummary(M1)\r\n\r\n\r\nLinear mixed model fit by maximum likelihood  ['lmerMod']\r\nFormula: course ~ 1 + (1 | school)\r\n   Data: GCSE\r\n\r\n     AIC      BIC   logLik deviance df.resid \r\n 14111.4  14127.7  -7052.7  14105.4     1722 \r\n\r\nScaled residuals: \r\n    Min      1Q  Median      3Q     Max \r\n-4.9693 -0.5101  0.1116  0.6741  2.7613 \r\n\r\nRandom effects:\r\n Groups   Name        Variance Std.Dev.\r\n school   (Intercept)  75.24    8.674  \r\n Residual             190.77   13.812  \r\nNumber of obs: 1725, groups:  school, 73\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)    73.72       1.11    66.4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOnder Fixed effects zien we dat het intercept \\(u_{a}\\), gemiddeld over de populatie van scholen, wordt geschat op 73,72. Onder Random-effects zien we dat de standaardafwijking tussen de scholen \\(\\sigma_{a}\\) wordt geschat op \\(8.67\\) en de standaardafwijking binnen de scholen \\(\\sigma_{y}\\) op \\(13.81\\).\r\nModel 2: Variërend intercept model met een enkele voorspeller\r\nHet variërende interceptmodel5 met een indicatorvariabele voor het vrouwzijn xij kan worden geschreven als\r\n\\[Y_{ij}∼N(a_{j}+\\beta x{ij}, \\sigma^2_{y}),\\]\\[a_{j}∼N(u_{a}, \\sigma^2_{a})\\].\r\nDe vergelijking van de gemiddelde regressielijn voor alle scholen is \\(u_{ij}=u_{α}+βxij\\). De regressielijnen voor specifieke scholen zullen evenwijdig zijn aan de gemiddelde regressielijn (met dezelfde helling β), maar verschillen wat betreft het intercept \\(a_{j}\\). Dit model kan geschat worden door vrouwelijk toe te voegen aan de formule in de lmer() functie, waardoor alleen het intercept per school varieert en de “helling” voor vrouwelijkheid constant blijft voor alle scholen\r\n\r\n\r\nM2 <- lmer(formula = course ~ 1 + female + (1 | school), \r\n           data = GCSE, \r\n           REML = FALSE)\r\nsummary(M2)\r\n\r\n\r\nLinear mixed model fit by maximum likelihood  ['lmerMod']\r\nFormula: course ~ 1 + female + (1 | school)\r\n   Data: GCSE\r\n\r\n     AIC      BIC   logLik deviance df.resid \r\n 14017.4  14039.2  -7004.7  14009.4     1721 \r\n\r\nScaled residuals: \r\n    Min      1Q  Median      3Q     Max \r\n-4.7809 -0.5401  0.1259  0.6795  2.6753 \r\n\r\nRandom effects:\r\n Groups   Name        Variance Std.Dev.\r\n school   (Intercept)  76.65    8.755  \r\n Residual             179.96   13.415  \r\nNumber of obs: 1725, groups:  school, 73\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)   69.730      1.185   58.87\r\nfemaleF        6.739      0.678    9.94\r\n\r\nCorrelation of Fixed Effects:\r\n        (Intr)\r\nfemaleF -0.338\r\n\r\n\r\n\r\n\r\nDe gemiddelde regressielijn over de scholen wordt dus geschat als \\(\\hat{\\mu}_{ij}=69.73+ 6.74 x_{ij}\\), waarbij \\(\\sigma_\\_alpha\\) en \\(\\sigma_y\\) worden geschat als respectievelijk \\(8.76\\) en \\(13.41\\). Als we deze schattingen van \\(\\mu_alpha\\), \\(\\beta\\), \\(\\sigma^2_{y}\\), en \\(\\sigma^2_{alpha}\\) als de ware parameterwaarden beschouwen, kunnen we de **Best Linear Unbiased Predictions (BLUPs) voor de fouten op schoolniveau \\(\\hat{u}_j = \\hat{\\alpha}_{j} - \\hat{\\mu}_{\\alpha}\\) verkrijgen.\r\nDe BLUPs zijn equivalent met de zogenaamde Empirical Bayes (EB)-voorspelling, die het gemiddelde is van de posterieure verdeling van \\(u_{j}\\) gegeven alle geschatte parameters, alsmede de willekeurige variabelen \\(y_{ij}\\) en \\(x_{ij}\\) voor het cluster. Deze voorspellingen worden “Bayes” genoemd omdat ze gebruik maken van de vooraf gespecificeerde prioriteitsverdeling1 \\(u_j \\sim N(\\mu_\\alpha, \\sigma^2_\\alpha)\\), en bij uitbreiding \\(u_j \\sim N(0, \\sigma^2_\\alpha)\\), en “Empirisch” genoemd omdat de parameters van deze prior, \\(\\mu_\\alpha\\) en \\(\\sigma^2_{\\alpha}\\), naast \\(\\beta\\) en \\(\\sigma^2_{y}\\), geschat worden uit de data.\r\nIn vergelijking met de ML-benadering (Maximum Likelihood - maximale waarschijnlijkheid), waarbij waarden voor \\(u_j\\) worden voorspeld door alleen de geschatte parameters en gegevens van cluster \\(j\\) te gebruiken, houdt de EB-benadering bovendien rekening met de voorafgaande verdeling van \\(u_{j}\\), en levert zij voorspelde waarden op die dichter bij \\(0\\) liggen (een verschijnsel dat wordt beschreven als shrinkage of partial pooling). Om te zien waarom dit verschijnsel shrinkage wordt genoemd, drukken we de uit EB voorspelling verkregen schattingen voor \\(u_j\\) gewoonlijk uit als \\(\\hat{u}_j^{text{EB}} = \\hat{R}_j\\hat{u}_j^{\\text{ML}}\\) waarbij \\(\\hat{u}_j^{\\text{ML}}\\) de ML schattingen zijn, en \\(\\hat{R}_j = \\frac{\\sigma_alpha^2}{n_j}\\) de zogenaamde Shrinkage factor is.\r\n\r\n\r\nhead(ranef(M2)$school)\r\n\r\n\r\n      (Intercept)\r\n20920 -10.1702110\r\n22520 -17.0578149\r\n22710   7.8007260\r\n22738   0.4871012\r\n22908  -8.1940346\r\n23208   4.4304453\r\n\r\nDeze waarden schatten hoeveel het intercept naar boven of beneden verschoven is in bepaalde scholen. Bijvoorbeeld, in de eerste school in de dataset is het geschatte intercept ongeveer 10.17 lager dan gemiddeld, zodat de schoolspecifieke regressielijn \\((69.73 - 10.17) + 6.74 x_{ij}\\) is.\r\nGelman&Hill (2006) karakteriseren multilevel modellering als partial pooling (ook wel shrinkage genoemd), wat een compromis is tussen twee uitersten: complete pooling, waarbij de clustering helemaal niet in het model wordt meegenomen, en no pooling, waarbij voor elke school aparte intercepten worden geschat als coëfficiënten van dummy-variabelen. De geschatte schoolspecifieke regressielijnen in het bovenstaande model zijn gebaseerd op partial pooling schattingen. Om dit aan te tonen, schatten we eerst de intercept en de helling voor elke school op drie manieren:\r\n\r\n\r\n# Complete-pooling regression\r\npooled <- lm(formula = course ~ female,\r\n             data = GCSE)\r\na_pooled <- coef(pooled)[1]   # complete-pooling intercept\r\nb_pooled <- coef(pooled)[2]   # complete-pooling slope\r\n\r\n# No-pooling regression\r\nnopooled <- lm(formula = course ~ 0 + school + female,\r\n               data = GCSE)\r\na_nopooled <- coef(nopooled)[1:J]   # 73 no-pooling intercepts              \r\nb_nopooled <- coef(nopooled)[J+1]\r\n\r\n# Partial pooling (multilevel) regression\r\na_part_pooled <- coef(M2)$school[, 1]\r\nb_part_pooled <- coef(M2)$school[, 2]\r\n\r\n\r\n\r\nVervolgens plotten we de gegevens en schoolspecifieke regressielijnen voor een selectie van acht scholen met behulp van de volgende commando’s.2:\r\n\r\n\r\n# (0) Assen plaatsen & scholen kiezen\r\ny <- GCSE$course\r\nx <- as.numeric(GCSE$female) - 1 + runif(N, -.05, .05)\r\nschid <- GCSE$school\r\nsel.sch <- c(\"65385\",\r\n             \"68207\",\r\n             \"60729\",\r\n             \"67051\",\r\n             \"50631\",\r\n             \"60427\",\r\n             \"64321\",\r\n             \"68137\")\r\n\r\n# (1) Subset 8 van de scholen; genereer data frame\r\ndf <- data.frame(y, x, schid)\r\ndf8 <- subset(df, schid %in% sel.sch)\r\n\r\n# (2) Aangeven van schattingen van volledige-pooling, geen-pooling, gedeeltelijke pooling \r\ndf8$a_pooled <- a_pooled \r\ndf8$b_pooled <- b_pooled\r\ndf8$a_nopooled <- a_nopooled[df8$schid]\r\ndf8$b_nopooled <- b_nopooled\r\ndf8$a_part_pooled <- a_part_pooled[df8$schid]\r\ndf8$b_part_pooled <- b_part_pooled[df8$schid]\r\n\r\n# (3) Plot van hoe regressie fit voor de 8 scholen\r\nggplot(data = df8, \r\n       aes(x = x, y = y)) + \r\n  facet_wrap(facets = ~ schid, \r\n             ncol = 4) + \r\n  theme_bw() +\r\n  geom_jitter(position = position_jitter(width = .05, \r\n                                         height = 0)) +\r\n  geom_abline(aes(intercept = a_pooled, \r\n                  slope = b_pooled), \r\n              linetype = \"solid\", \r\n              color = \"blue\", \r\n              size = 0.5) +\r\n  geom_abline(aes(intercept = a_nopooled, \r\n                  slope = b_nopooled), \r\n              linetype = \"longdash\", \r\n              color = \"red\", \r\n              size = 0.5) + \r\n  geom_abline(aes(intercept = a_part_pooled, \r\n                  slope = b_part_pooled), \r\n              linetype = \"dotted\", \r\n              color = \"purple\", \r\n              size = 0.7) + \r\n  scale_x_continuous(breaks = c(0, 1), \r\n                     labels = c(\"male\", \"female\")) + \r\n  labs(title = \"Schatting van Volledige-pooling, Geen-pooling en Gedeeltelijke pooling\",\r\n       x = \"\", \r\n       y = \"Totale score op cursuswerk paper\")+theme_bw( base_family = \"serif\")\r\n\r\n\r\n\r\n\r\nFigure 1: De blauw-gestreepte, rood-gestreepte en paars-gestippelde lijnen geven respectievelijk de volledige pooling, no-pooling en gedeeltelijke pooling schattingen weer. We zien dat de geschatte schoolspecifieke regressielijn van de gedeeltelijke pooling-schattingen tussen de volledige pooling- en de geenpooling-regressielijn ligt. Er is meer pooling (paarse stippellijn dichter bij blauwe ononderbroken lijn) op scholen met een kleine steekproefomvang.\r\n\r\n\r\n\r\nModel 3: Variërend intercept en slope model met een enkele voorspeller\r\nWe breiden nu het variërende interceptmodel met één voorspeller uit om zowel het intercept als de helling willekeurig te laten variëren tussen scholen met behulp van het volgende model \\[y_{ij} = \\alpha_j + \\beta_j x_{ij} +\\epsilon_{ij},\\] \\[\\alpha_j = \\mu_\\alpha + u_j,\\] \\[\\beta_j = \\mu_\\beta + v_j,\\] or in a reduced form as \\[y_{ij} = \\mu_\\alpha + \\mu_\\beta x_{ij} + u_j + v_j x_{ij} + \\epsilon_{ij}\\] where \\(\\epsilon_{ij} \\sim N(0, \\sigma_{y}^{2})\\) and \\(\\left( \\begin{matrix} u_j \\\\ v_j \\end{matrix} \\right) \\sim N\\left( \\left( \\begin{matrix} 0 \\\\ 0 \\end{matrix} \\right) ,\\left( \\begin{matrix} { \\sigma }_{ \\alpha }^{ 2 } & \\rho { \\sigma }_{ \\alpha }{ \\sigma }_{ \\beta } \\\\ \\rho { \\sigma }_{ \\alpha }{ \\sigma }_{ \\beta } & { \\sigma }_{ \\beta }^{ 2 } \\end{matrix} \\right) \\right)\\).]:\r\n\\[y_{ij}\\sim N(\\alpha_{j}+\\beta_{j}x_{ij} , \\sigma_y ^2 ),\\] \\[\\left( \\begin{matrix} \\alpha _{ j } \\\\ \\beta _{ j } \\end{matrix} \\right) \\sim N\\left( \\left( \\begin{matrix} { \\mu  }_{ \\alpha  } \\\\ { \\mu  }_{ \\beta  } \\end{matrix} \\right) , \\left( \\begin{matrix} { \\sigma  }_{ \\alpha  }^{ 2 } & \\rho { \\sigma  }_{ \\alpha  }{ \\sigma  }_{ \\beta  } \\\\ \\rho { \\sigma  }_{ \\alpha  }{ \\sigma  }_{ \\beta  } & { \\sigma  }_{ \\beta  }^{ 2 } \\end{matrix} \\right)  \\right).\\]\r\nMerk op dat we nu variatie hebben in de \\(\\alpha_{j}\\)’s en de \\(\\beta_{j}\\)’s, en ook een correlatie parameter \\(\\rho\\) tussen \\(\\alpha_{j}\\) en \\(\\beta_{j}\\). Dit model kan gefit worden met gebruik van lmer() en wel als volgt:\r\n\r\n\r\nM3 <- lmer(formula = course ~ 1 + female + (1 + female | school), \r\n           data = GCSE, \r\n           REML = FALSE)\r\nsummary(M3) \r\n\r\n\r\nLinear mixed model fit by maximum likelihood  ['lmerMod']\r\nFormula: course ~ 1 + female + (1 + female | school)\r\n   Data: GCSE\r\n\r\n     AIC      BIC   logLik deviance df.resid \r\n 13983.4  14016.2  -6985.7  13971.4     1719 \r\n\r\nScaled residuals: \r\n    Min      1Q  Median      3Q     Max \r\n-4.6886 -0.5222  0.1261  0.6529  2.6729 \r\n\r\nRandom effects:\r\n Groups   Name        Variance Std.Dev. Corr \r\n school   (Intercept) 102.93   10.146        \r\n          femaleF      47.94    6.924   -0.52\r\n Residual             169.79   13.030        \r\nNumber of obs: 1725, groups:  school, 73\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)   69.425      1.352  51.338\r\nfemaleF        7.128      1.131   6.302\r\n\r\nCorrelation of Fixed Effects:\r\n        (Intr)\r\nfemaleF -0.574\r\n\r\n\r\n\r\n\r\nIn dit model wordt de residuele standaardafwijking binnen de school geschat als \\(\\hat{\\sigma}_{y}=\\) 13.03. De geschatte standaardafwijkingen van de schoolintercepten en de schoolhellingen zijn respectievelijk \\(\\hat{\\sigma}_{\\alpha}= 10.15\\) en \\(\\hat{\\sigma}_{\\beta}= 6.92\\). De geschatte correlatie tussen variërende intercepts en hellingen is \\(\\hat{\\rho} = -0.52\\). We kunnen een soortgelijke code als die in paragraaf 2.2 gebruiken om de gegevens en de schoolspecifieke regressielijnen voor een selectie van acht scholen te plotten.\r\n\r\n\r\n\r\nBayesiaanse inferentie voor Model 1\r\nWe kunnen snel en eenvoudig veel multilevel modellen fitten met behulp van de lmer() functie in R. Zoals eerder vermeld, zijn functies zoals lmer() gebaseerd op een combinatie van maximale waarschijnlijkheid (ML) schatting van de model parameters en empirische Bayes (EB) voorspellingen van de variërende intercepts en/of hellingen. In sommige gevallen, wanneer het aantal groepen klein is of wanneer het model veel variërende coëfficiënten of niet-nested componenten bevat, kan de ML-benadering echter minder goed werken, ten dele omdat er misschien niet genoeg informatie is om de variantieparameters nauwkeurig te schatten. In dergelijke gevallen levert de Restricted Maximum Likelihood (REML) Estimation redelijker gevolgtrekkingen op.\r\nEen volledig Bayesiaanse benadering levert ook in deze gevallen redelijke gevolgtrekkingen op, met als bijkomend voordeel dat bij de voorspelling van de variërende intercepten en hellingen rekening wordt gehouden met alle onzekerheid in de parameterschattingen en met de daarmee samenhangende onzekerheid. Dit is een van de vele redenen waarom men geïnteresseerd zou moeten zijn in volledig Bayesiaanse schattingen. Andere redenen worden besproken in paragraaf 3.4.3. Men kan beginnen met het snel fitten van vele specificaties bij het bouwen van een model met behulp van de lmer() functie, en dan gebruik maken van de flexibiliteit van een volledig Bayesiaanse benadering met rstanarm om simulaties te verkrijgen die de onzekerheid over coëfficiënten, voorspellingen en andere grootheden die van belang zijn samenvatten.\r\nIn dit gedeelte laten we zien hoe model 1 kan worden ingepast en geëvalueerd met behulp van het pakket rstanarm. Als alternatief, maar dat komt niet in deze tutorial aan de orde, kan men ook een handgeschreven programma maken in Stan en het uitvoeren met behulp van het rstan pakket.\r\nGebruik van het rstanarm pakket\r\nVeel relatief eenvoudige modellen kunnen worden aangepast met behulp van het rstanarm pakket zonder enige code te schrijven in de Stan taal. Het rstanarm pakket is een “wrapper” voor het rstan pakket waarmee de meest gebruikte regressiemodellen kunnen worden geschat met behulp van Markov Chain Monte Carlo (MCMC) en toch kunnen worden gespecificeerd met de gebruikelijke R modelleersyntaxis. Onderwijs onderzoekers kunnen Bayesiaanse schatting gebruiken voor multilevel modellen met slechts minimale veranderingen in hun bestaande code met lmer().\r\nBijvoorbeeld, Model 1 met standaard prior verdelingen voor \\(\\mu_{\\alpha}\\), \\(\\sigma_{\\alpha}\\), en \\(\\sigma_{y}\\) kan worden gespecificeerd met het rstanarm pakket door stan_ toe te voegen aan de lmer aanroep:\r\n\r\n\r\nM1_stanlmer <- stan_lmer(formula = course ~ 1 + (1 | school), \r\n                         data = GCSE,\r\n                         seed = 349)\r\n\r\n\r\n\r\nDeze stan_lmer() functie is qua syntax gelijk aan lmer(), maar in plaats van een maximum likelihood schatting uit te voeren, wordt een Bayesiaanse schatting uitgevoerd via MCMC. Omdat elke stap in de MCMC schatting random trekkingen uit de parameter ruimte inhoudt, voegen we een seed optie toe om ervoor te zorgen dat stan_lmer elke keer dat de code wordt uitgevoerd, dezelfde resultaten geeft.\r\nPrior distributies\r\nModel 1 is een variabel interceptiemodel met normaal verdeelde leerlingresiduen en intercepten op schoolniveau: \\(y_{ij} \\sim N(\\alpha_{j}, \\sigma_{y}^{2}),\\) en \\(\\alpha_{j},\\sim N(\\mu_{alpha}, \\sigma_{alpha}^{2})\\). De normale verdeling voor de \\(\\alpha{j}\\)’s kan worden beschouwd als een prioriteitsverdeling voor deze variërende intercepten. De parameters van deze prior verdeling, \\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\), worden geschat uit de gegevens bij gebruik van maximum likelihood schatting. Bij volledige Bayesiaanse inferentie hebben alle hyperparameters (\\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\)), samen met de andere niet-gemodelleerde parameters (in dit geval, \\(\\sigma_{y}\\)) ook een priorverdeling nodig.\r\nHier gebruiken we de standaard prior verdelingen voor de hyperparameters in stan_lmer door geen prior opties op te geven in stan_lmer() functie. De standaard priors zijn bedoeld als zwak informatief in de zin dat ze gematigde regularisatie bieden [Regularisatie kan worden beschouwd als een techniek om ervoor te zorgen dat schattingen binnen een acceptabel bereik van waarden worden begrensd] en helpen bij het stabiliseren van de berekening. Opgemerkt moet worden dat de auteurs van rstanarm suggereren om niet te vertrouwen op rstanarm om de standaard prior voor een model te specificeren, maar eerder om de priors expliciet te specificeren, zelfs als ze inderdaad de huidige standaard zijn, aangezien updates van het pakket andere defaults mee kunnen krijgen.\r\nTen eerste wordt, alvorens rekening te houden met de schaal van de variabelen, \\(\\mu_{alpha}\\) een normale priorverdeling gegeven met gemiddelde 0 en standaardafwijking 10. Dat wil zeggen, \\(mu_{alpha} \\sim N(0, 10^2)\\). De standaardafwijking van deze prioriteitsverdeling, 10, is vijf keer zo groot als de standaardafwijking van de respons indien deze gestandaardiseerd zou zijn. Dit zou een dichte benadering moeten zijn van een niet-informatieve prior over het door de waarschijnlijkheid ondersteunde bereik, die in gevolgtrekkingen zou moeten geven die vergelijkbaar zijn met die verkregen met maximale waarschijnlijkheidsmethoden indien even zwakke priors worden gebruikt voor de andere parameters.\r\nTen tweede wordt de (ongeschaalde) prior voor \\(\\sigma_{y}\\) ingesteld op een exponentiële verdeling met de ‘rate’-parameter op 1.\r\nTen derde, om een prior voor de varianties en covarianties van de variërende (of “willekeurige”) effecten te specificeren, zal rstanarm deze matrix ontbinden in een correlatiematrix van de variërende effecten en een functie van hun varianties. Omdat er in dit voorbeeld slechts één variërend effect is, reduceert de standaard (ongeschaalde) prior voor \\(\\sigma_{\\alpha}\\) die rstanarm gebruikt tot een exponentiële verdeling met de rate parameter op 1.\r\nOok moet worden opgemerkt dat rstanarm de priors zal schalen tenzij de autoscale = FALSE optie wordt gebruikt. Na het fitten van een model met stan_lmer, kunnen we de gebruikte priors controleren door de prior_summary() functie op te roepen.\r\n\r\n\r\n# Een samenvatting krijgen van de priors die gebruikt worden \r\nprior_summary(object = M1_stanlmer)\r\n\r\n\r\nPriors for model 'M1_stanlmer' \r\n------\r\nIntercept (after predictors centered)\r\n  Specified prior:\r\n    ~ normal(location = 73, scale = 2.5)\r\n  Adjusted prior:\r\n    ~ normal(location = 73, scale = 41)\r\n\r\nAuxiliary (sigma)\r\n  Specified prior:\r\n    ~ exponential(rate = 1)\r\n  Adjusted prior:\r\n    ~ exponential(rate = 0.061)\r\n\r\nCovariance\r\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\r\n------\r\nSee help('prior_summary.stanreg') for more details\r\n\r\n\r\n\r\n# Hiermee krijgen we de SD van de uitkomst \r\nsd(GCSE$course, na.rm = TRUE)\r\n\r\n\r\n[1] 16.32096\r\n\r\nZoals hierboven te zien is, worden de schalen van de priors voor \\(\\mu_{\\alpha}\\) en \\(\\sigma_y\\) op respectievelijk \\(163,21\\) en \\(16,32\\) gezet na herschaling. Aangezien de standaard prior voor het intercept normaal is met een schaalparameter van \\(10\\), is de herschaalde prior ook normaal maar met een schaalparameter van \\(\\text{scale} \\times \\text{SD}(y) = 10 \\times 16.321= 163.21\\). Aangezien de standaard prior voor \\(\\sigma_y\\) exponentieel is met een snelheidsparameter van \\(1\\) (of gelijkwaardig, de schaalparameter \\(\\text{scale} = \\frac{1}{text{rate} = 1\\)), is de herschaalde prior eveneens exponentieel met een schaalparameter van \\(\\text{scale} \\maal \\text{SD}(y) = 1 maal 16,321= 16,32\\).\r\nDirecte output van stan_lmer\r\nPosterior medianen en posterior mediaan absolute deviaties\r\nWe kunnen een snelle samenvatting van de fit van Model 1 weergeven door de print methode op de volgende manier te gebruiken:\r\n\r\n\r\nprint(M1_stanlmer, digits = 2)\r\n\r\n\r\nstan_lmer\r\n family:       gaussian [identity]\r\n formula:      course ~ 1 + (1 | school)\r\n observations: 1725\r\n------\r\n            Median MAD_SD\r\n(Intercept) 73.68   1.14 \r\n\r\nAuxiliary parameter(s):\r\n      Median MAD_SD\r\nsigma 13.82   0.24 \r\n\r\nError terms:\r\n Groups   Name        Std.Dev.\r\n school   (Intercept)  8.87   \r\n Residual             13.82   \r\nNum. levels: school 73 \r\n\r\n------\r\n* For help interpreting the printed output see ?print.stanreg\r\n* For info on the priors used see ?prior_summary.stanreg\r\n\r\n\r\n\r\n\r\nHier is de puntschatting van \\(\\mu_{\\alpha}\\) uit stan_lmer \\(73.75\\) en dit komt overeen met de mediaan van de posterior trekkingen. Dit is vergelijkbaar met de ML schatting uit lmer. De puntschatting voor \\(`sigma_{_alpha}\\) van stan_lmer is \\(8.87\\), die groter is dan de ML schatting (\\(8.67\\)). Dit verschil kan deels komen doordat de ML benadering in lmer() geen rekening houdt met de onzekerheid in \\(\\mu_{\\alpha}\\) bij het schatten van \\(\\sigma_{\\alpha}\\). De REML benadering (\\(8.75\\)) in lmer() houdt, zoals eerder vermeld, wel rekening met deze onzekerheid.\r\nBij gebruik van stan_lmer worden standaardfouten verkregen door de mediaan absolute afwijking (MAD) van elke trekking ten opzichte van de mediaan van die trekkingen te beschouwen. Het is bekend dat ML de neiging heeft om onzekerheden te onderschatten, omdat het gebaseerd is op puntschattingen van hyperparameters. Full Bayes daarentegen propageert de onzekerheid in de hyperparameters over alle niveaus van het model en levert adequatere onzekerheidsschattingen op. Zie ook Brown e.a. (2006) voor verdere discussie.\r\nPosterior gemiddelden, posterior standaard deviaties, 95% credible interval en Monte Carlo fouten\r\n\r\n\r\nsummary(M1_stanlmer, \r\n        pars = c(\"(Intercept)\", \"sigma\", \"Sigma[school:(Intercept),(Intercept)]\"),\r\n        probs = c(0.025, 0.975),\r\n        digits = 2)\r\n\r\n\r\n\r\nModel Info:\r\n function:     stan_lmer\r\n family:       gaussian [identity]\r\n formula:      course ~ 1 + (1 | school)\r\n algorithm:    sampling\r\n sample:       4000 (posterior sample size)\r\n priors:       see help('prior_summary')\r\n observations: 1725\r\n groups:       school (73)\r\n\r\nEstimates:\r\n                                        mean   sd     2.5%   97.5%\r\n(Intercept)                            73.67   1.12  71.49  75.92 \r\nsigma                                  13.82   0.24  13.36  14.30 \r\nSigma[school:(Intercept),(Intercept)]  78.67  15.55  53.46 114.10 \r\n\r\nMCMC diagnostics\r\n                                      mcse Rhat n_eff\r\n(Intercept)                           0.05 1.01  572 \r\nsigma                                 0.00 1.00 4579 \r\nSigma[school:(Intercept),(Intercept)] 0.61 1.00  641 \r\n\r\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\r\n\r\nHet is de moeite waard om op te merken dat bij gebruik van de summary methode, de schatting voor de standaardafwijking \\(sigma_y\\) het gemiddelde is van de posterior trekkingen van de parameter. Dit in tegenstelling tot de mediaan van de posterior trekkingen die we krijgen bij gebruik van de print methode. Een voordeel van het gebruik van de mediaan is dat de schatting voor \\(\\sigma_y^2\\) gewoon het kwadraat is van de schatting voor \\(\\sigma_y\\) als het aantal steekproeven oneven is. Dit is niet het geval bij gebruik van het gemiddelde. In dit geval, en meer algemeen wanneer we andere functies van de parameters moeten evalueren, moeten we de posterior trekkingen rechtstreeks benaderen. Dit wordt beschreven in het volgende deel.\r\nOnder Diagnostics, verwijzen we de lezer naar Paragraaf 5 voor meer informatie over Rhat en n_eff. De waarden onder mcse vertegenwoordigen schattingen voor de Monte Carlo standaardfouten, die de willekeurigheid vertegenwoordigen die geassocieerd is met elke MCMC schattingsrun. Dat wil zeggen, met dezelfde dataset, herhaaldelijk gebruik van een MCMC benadering om een parameter te schatten levert schattingen op met een standaardafwijking gelijk aan de Monte Carlo standaardfout.\r\nAndere output van stan_lmer\r\nZoals gezegd, kunnen gebruikers er de voorkeur aan geven om direct met de posterior trekkingen te werken om schattingen van meer complexe parameters te verkrijgen. Om dit te doen, moeten gebruikers ze handmatig benaderen vanuit het stan_lmer object. We laten zien hoe dit moet in de context van het maken van vergelijkingen tussen individuele scholen.\r\nToegang tot de simulaties en samenvattende resultaten\r\nGebaseerd op de standaard instellingen, genereert stan_lmer 4 MCMC-ketens van 2.000 iteraties elk. De helft van deze iteraties in elke keten wordt gebruikt als warming-up/burn-in (om de keten te laten convergeren naar de posterior verdeling), en daarom gebruiken we slechts 1.000 steekproeven per keten. Deze door MCMC gegenereerde steekproeven worden geacht getrokken te zijn uit de posterior verdelingen van de parameters in het model. Wij kunnen deze steekproeven gebruiken voor voorspellingen, om de onzekerheid samen te vatten en ‘çredible intervals’ (geloofwaardige intervallen) te schatten voor elke functie van de parameters.\r\nOm toegang te krijgen tot de posterior trekkingen voor alle parameters, passen we de methode as.matrix() toe op het stanreg object M1_stanlmer. Dit geeft een \\(S\\) bij \\(P\\) matrix, waarbij \\(S\\) de grootte is van de posterior steekproef (of gelijkwaardig, het aantal MCMC iteraties na warm-up) en \\(P\\) het aantal parameters/kwantiteiten. Door deze matrix te manipuleren kunnen we een matrix genereren voor de variërende intercepts \\(\\alpha_{j}\\) en vectoren met de trekkingen voor de within standaardafwijking en de between variantie. Merk op dat om de juiste kolommen voor de parameter van belang te selecteren, het nuttig is om de kolomnamen van de matrix sims te onderzoeken.\r\nEen meer directe benadering voor het verkrijgen van de posterior trekkingen voor specifieke parameters is gebruik te maken van de ingebouwde functionaliteit van de as.matrix methode voor stanreg objecten. Wanneer de as.matrix methode wordt toegepast op een stanreg object, kan de gebruiker ofwel een optionele karaktervector van parameternamen specificeren, of een optionele karaktervector van reguliere expressies3 om de posterior trekkingen van alleen de parameters waarin ze geïnteresseerd zijn te extraheren. Bijvoorbeeld, omdat de parameter die het totale gemiddelde representeert is gelabeld met (Intercept), kunnen we de posterior trekkingen van alleen deze parameter extraheren door de optie pars = \"(Intercept)\" op te nemen. En omdat de parameters die de 73 schoolfouten representeren allemaal de string b[(Intercept) school: bevatten, kunnen we alle parameters die deze string bevatten extraheren door de optie regex_pars = \"b[(Intercept) school:” te gebruiken.\r\n\r\n\r\n# Extraheren van de posterior trekkingen voor alle parameters\r\nsims <- as.matrix(M1_stanlmer)\r\ndim(sims)\r\n\r\n\r\n[1] 4000   76\r\n\r\npara_name <- colnames(sims)\r\npara_name\r\n\r\n\r\n [1] \"(Intercept)\"                          \r\n [2] \"b[(Intercept) school:20920]\"          \r\n [3] \"b[(Intercept) school:22520]\"          \r\n [4] \"b[(Intercept) school:22710]\"          \r\n [5] \"b[(Intercept) school:22738]\"          \r\n [6] \"b[(Intercept) school:22908]\"          \r\n [7] \"b[(Intercept) school:23208]\"          \r\n [8] \"b[(Intercept) school:25241]\"          \r\n [9] \"b[(Intercept) school:30474]\"          \r\n[10] \"b[(Intercept) school:35270]\"          \r\n[11] \"b[(Intercept) school:37224]\"          \r\n[12] \"b[(Intercept) school:47627]\"          \r\n[13] \"b[(Intercept) school:50627]\"          \r\n[14] \"b[(Intercept) school:50631]\"          \r\n[15] \"b[(Intercept) school:60421]\"          \r\n[16] \"b[(Intercept) school:60427]\"          \r\n[17] \"b[(Intercept) school:60437]\"          \r\n[18] \"b[(Intercept) school:60439]\"          \r\n[19] \"b[(Intercept) school:60441]\"          \r\n[20] \"b[(Intercept) school:60455]\"          \r\n[21] \"b[(Intercept) school:60457]\"          \r\n[22] \"b[(Intercept) school:60501]\"          \r\n[23] \"b[(Intercept) school:60729]\"          \r\n[24] \"b[(Intercept) school:60741]\"          \r\n[25] \"b[(Intercept) school:63619]\"          \r\n[26] \"b[(Intercept) school:63833]\"          \r\n[27] \"b[(Intercept) school:64251]\"          \r\n[28] \"b[(Intercept) school:64321]\"          \r\n[29] \"b[(Intercept) school:64327]\"          \r\n[30] \"b[(Intercept) school:64343]\"          \r\n[31] \"b[(Intercept) school:64359]\"          \r\n[32] \"b[(Intercept) school:64428]\"          \r\n[33] \"b[(Intercept) school:65385]\"          \r\n[34] \"b[(Intercept) school:66365]\"          \r\n[35] \"b[(Intercept) school:67051]\"          \r\n[36] \"b[(Intercept) school:67105]\"          \r\n[37] \"b[(Intercept) school:67311]\"          \r\n[38] \"b[(Intercept) school:68107]\"          \r\n[39] \"b[(Intercept) school:68111]\"          \r\n[40] \"b[(Intercept) school:68121]\"          \r\n[41] \"b[(Intercept) school:68125]\"          \r\n[42] \"b[(Intercept) school:68133]\"          \r\n[43] \"b[(Intercept) school:68137]\"          \r\n[44] \"b[(Intercept) school:68201]\"          \r\n[45] \"b[(Intercept) school:68207]\"          \r\n[46] \"b[(Intercept) school:68217]\"          \r\n[47] \"b[(Intercept) school:68227]\"          \r\n[48] \"b[(Intercept) school:68233]\"          \r\n[49] \"b[(Intercept) school:68237]\"          \r\n[50] \"b[(Intercept) school:68241]\"          \r\n[51] \"b[(Intercept) school:68255]\"          \r\n[52] \"b[(Intercept) school:68271]\"          \r\n[53] \"b[(Intercept) school:68303]\"          \r\n[54] \"b[(Intercept) school:68321]\"          \r\n[55] \"b[(Intercept) school:68329]\"          \r\n[56] \"b[(Intercept) school:68405]\"          \r\n[57] \"b[(Intercept) school:68411]\"          \r\n[58] \"b[(Intercept) school:68417]\"          \r\n[59] \"b[(Intercept) school:68531]\"          \r\n[60] \"b[(Intercept) school:68611]\"          \r\n[61] \"b[(Intercept) school:68629]\"          \r\n[62] \"b[(Intercept) school:68711]\"          \r\n[63] \"b[(Intercept) school:68723]\"          \r\n[64] \"b[(Intercept) school:68805]\"          \r\n[65] \"b[(Intercept) school:68809]\"          \r\n[66] \"b[(Intercept) school:71927]\"          \r\n[67] \"b[(Intercept) school:74330]\"          \r\n[68] \"b[(Intercept) school:74862]\"          \r\n[69] \"b[(Intercept) school:74874]\"          \r\n[70] \"b[(Intercept) school:76531]\"          \r\n[71] \"b[(Intercept) school:76631]\"          \r\n[72] \"b[(Intercept) school:77207]\"          \r\n[73] \"b[(Intercept) school:84707]\"          \r\n[74] \"b[(Intercept) school:84772]\"          \r\n[75] \"sigma\"                                \r\n[76] \"Sigma[school:(Intercept),(Intercept)]\"\r\n\r\n# Verkrijgen van school-niveau varyiërende intercept a_j\r\n# trekking voor het algemeen gemiddelde\r\nmu_a_sims <- as.matrix(M1_stanlmer, \r\n                       pars = \"(Intercept)\")\r\n# trekkingen voor 73 scholen van de school-niveua fouten \r\nu_sims <- as.matrix(M1_stanlmer, \r\n                    regex_pars = \"b\\\\[\\\\(Intercept\\\\) school\\\\:\")\r\n# trekkingen van alle 73 school variërende intercepten               \r\na_sims <- as.numeric(mu_a_sims) + u_sims          \r\n\r\n# Verkrijgen van sigma_y en sigma_alpha^2\r\n# trekkingen van sigma_y\r\ns_y_sims <- as.matrix(M1_stanlmer, \r\n                       pars = \"sigma\")\r\n# trekkingen van sigma_alpha^2\r\ns__alpha_sims <- as.matrix(M1_stanlmer, \r\n                       pars = \"Sigma[school:(Intercept),(Intercept)]\")\r\n\r\n\r\n\r\nVerkrijgen van gemiddelden, standaard deviaties, medianen en 95% geloofwaardigheids intervallen\r\nIn a_sims hebben we 4.000 posterior trekkingen (van alle 4 ketens) voor de variërende intercepten \\(\\alpha_{j}\\) van de 73 scholen opgeslagen. De eerste kolom van de matrix van 4.000 bij 73 is bijvoorbeeld een vector van 4.000 posterior simulatietrekkingen voor de variërende intercept van de eerste school (School 20920). Een kwantitatieve manier om de posterior kansverdeling van deze 4.000 schattingen voor \\(1,1pha_{1}\\) samen te vatten is het onderzoeken van hun quantielen.\r\n\r\n\r\n# Computeer gemiddelde, SD, mediaan en 95% geloofwaardigheids interval van de varyiërende intercepten\r\n\r\n# Posterior gemiddelde en SD van elke alpha\r\na_mean <- apply(X = a_sims,     # posterior gemiddelde\r\n                MARGIN = 2,\r\n                FUN = mean)\r\na_sd <- apply(X = a_sims,       # posterior SD\r\n              MARGIN = 2,\r\n              FUN = sd)\r\n\r\n# Posterior mediaan en 95% geloofwaardigheids interval\r\na_quant <- apply(X = a_sims, \r\n                 MARGIN = 2, \r\n                 FUN = quantile, \r\n                 probs = c(0.025, 0.50, 0.975))\r\na_quant <- data.frame(t(a_quant))\r\nnames(a_quant) <- c(\"Q2.5\", \"Q50\", \"Q97.5\")\r\n\r\n# Combineer samenvattende statistieken van posterior simulatie trekkingen\r\na_df <- data.frame(a_mean, a_sd, a_quant)\r\nround(head(a_df), 2)\r\n\r\n\r\n                            a_mean a_sd  Q2.5   Q50 Q97.5\r\nb[(Intercept) school:20920]  63.63 4.40 54.97 63.57 72.17\r\nb[(Intercept) school:22520]  57.15 1.79 53.72 57.15 60.60\r\nb[(Intercept) school:22710]  81.64 3.15 75.47 81.64 87.78\r\nb[(Intercept) school:22738]  73.03 4.27 64.48 73.11 81.50\r\nb[(Intercept) school:22908]  66.51 5.28 56.47 66.47 76.70\r\nb[(Intercept) school:23208]  79.23 2.81 73.60 79.22 85.02\r\n\r\nWij kunnen een rupsplot maken om de volledige Bayes-schattingen voor de schoolafhankelijke intercepts in rangorde te tonen, samen met hun 95% credible intervallen.\r\n\r\n\r\n# Sorteer dataframe die een geschatte alfa gemiddelde en sd voor elke school omvatten\r\na_df <- a_df[order(a_df$a_mean), ]\r\na_df$a_rank <- c(1 : dim(a_df)[1])  # een vector van de schoolranking \r\n\r\n# Plot school-niveau alfas posterior gemiddelde en 95% credible interval\r\nggplot(data = a_df, \r\n       aes(x = a_rank, \r\n           y = a_mean)) +\r\n  geom_pointrange(aes(ymin = Q2.5, \r\n                      ymax = Q97.5),\r\n                  position = position_jitter(width = 0.1, \r\n                                             height = 0)) + \r\n  geom_hline(yintercept = mean(a_df$a_mean), \r\n             size = 0.5, \r\n             col = \"red\") + \r\n  scale_x_continuous(\"Rank\", \r\n                     breaks = seq(from = 0, \r\n                                  to = 80, \r\n                                  by = 5)) + \r\n  scale_y_continuous(expression(paste(\"varying intercept, \", alpha[j]))) + \r\n  theme_bw( base_family = \"serif\")\r\n\r\n\r\n\r\n\r\nDezelfde aanpak kan natuurlijk worden gevolgd om 95% geloofwaardige intervallen te genereren voor \\(\\sigma_y\\) en \\(\\sigma_\\alpha\\).\r\nVergelijkingen maken tussen individuele scholen\r\nHet hebben van steekproeven van alle parameters en variërende intercepten uit hun gezamenlijke posterior verdeling maakt het gemakkelijk om inferenties te trekken over functies van deze parameters.\r\nIn onderwijsonderzoek en in de onderwijspraktijk is het vaak interessant om de scholen in de data met elkaar te vergelijken. Relevante vragen zijn bijvoorbeeld (1) wat is het verschil tussen de gemiddelden van school A en school B, (2) presteert school A beter dan school B en (3) wat is de rangorde van deze scholen binnen de steekproef. Wanneer niet-Bayesiaanse methoden worden gebruikt, kunnen wij proberen dergelijke vergelijkingen te maken op basis van empirische Bayes- (of Best Linear Unbiased-) voorspellingen van de variërende intercepten. Maar het zal in het algemeen onmogelijk zijn om de onzekerheid uit te drukken voor niet-lineaire functies zoals rangschikkingen. Zie ook Goldstein en Spiegelhalter (2006) voor verdere discussie.\r\nHier zullen we twee scholen vergelijken als voorbeeld: Scholen 60501 (de 21ste school) en 68271 (de 51ste school). We hebben al 4.000 posterior simulatietrekkingen voor beide scholen. Om conclusies te trekken over het verschil tussen de gemiddelde scores van de twee scholen, kunnen we eenvoudigweg het verschil nemen tussen de twee vectoren van trekkingen \\(\\alpha_{51} - \\alpha_{21}\\).\r\n\r\n\r\n# Het verschil tussen de twee schoolgemiddelden (school #21 en #51)\r\nschool_diff <- a_sims[, 21] - a_sims[, 51]\r\n\r\n\r\n\r\nWij kunnen de posterior verdeling van het verschil als volgt onderzoeken met beschrijvende statistieken en een histogram:\r\n\r\n\r\n# Onderzoek verschillen van twee distributies \r\nmean <- mean(school_diff)\r\nsd <- sd(school_diff)\r\nquantile <- quantile(school_diff, probs = c(0.025, 0.50, 0.975))\r\nquantile <- data.frame(t(quantile))\r\nnames(quantile) <- c(\"Q2.5\", \"Q50\", \"Q97.5\")\r\ndiff_df <- data.frame(mean, sd, quantile)\r\nround(diff_df, 2)\r\n\r\n\r\n  mean   sd  Q2.5  Q50 Q97.5\r\n1 5.12 4.48 -3.53 5.12 14.02\r\n\r\n\r\n\r\n# Histogram van de verschillen \r\nggplot(data = data.frame(school_diff), \r\n       aes(x = school_diff)) + \r\n  geom_histogram(color = \"black\", \r\n                 fill = \"gray\", \r\n                 binwidth = 0.75) + \r\n  scale_x_continuous(\"Score verschil tussen twee scholen: #21, #51\",\r\n                     breaks = seq(from = -20, \r\n                                  to = 20, \r\n                                  by = 10)) + \r\n  geom_vline(xintercept = c(mean(school_diff),\r\n                            quantile(school_diff, \r\n                                     probs = c(0.025, 0.975))),\r\n             colour = \"red\", \r\n             linetype = \"longdash\") + \r\n  geom_text(aes(5.11, 20, label = \"mean = 5.11\"), \r\n            color = \"red\", \r\n            size = 4) + \r\n  geom_text(aes(9, 50, label = \"SD = 4.46\"), \r\n            color = \"blue\", \r\n            size = 4) + \r\n  theme_bw( base_family = \"serif\") \r\n\r\n\r\n\r\n\r\n\r\n\r\nprop.table(table(a_sims[, 21] > a_sims[, 51]))\r\n\r\n\r\n\r\n  FALSE    TRUE \r\n0.12425 0.87575 \r\n\r\nHet verwachte verschil komt uit op 5,11 met een standaardafwijking van 4,46 en een grote bandbreedte van onzekerheid. Het 95% geloofwaardigheidsinterval is [-3.64, 13.66], dus we zijn er 95% zeker van dat de ware waarde van het verschil tussen de twee scholen binnen het bereik ligt, gegeven de gegevens.\r\nWe kunnen ook het deel van de tijd bepalen dat School 60501 een hoger gemiddelde heeft dan School 68271:\r\n\r\n\r\nprop.table(table(a_sims[, 21] > a_sims[, 51]))\r\n\r\n\r\n\r\n  FALSE    TRUE \r\n0.12425 0.87575 \r\n\r\n\r\n\r\n\r\nDit betekent dat de posterior waarschijnlijkheid dat School 60501 beter is dan School 68271 87.6% is. Elk paar scholen binnen de steekproef van scholen kan op deze manier vergeleken worden.\r\nBayesiaanse inferentie voor Model 2 en 3\r\nModel 2: Een voorspeller op studentenniveau toevoegen\r\nOnderzoekers zouden de variërende interceptmodellen kunnen uitbreiden met waargenomen verklarende variabelen op het niveau van de leerling \\(x_{ij}\\), in dit voorbeeld een indicatorvariabele voor vrouw. Een eenvoudig variërend interceptmodel met één voorspeller op het niveau van de leerling kan worden geschreven als \\(y_{ij} \\N(\\alpha_{j} + \\beta x_{ij}, \\sigma_{y}^{2})\\) en \\(\\alpha_{j} \\N(\\mu_{alpha}, \\sigma_{alpha}^{2})\\). We gebruiken niet-informatieve prioriteitsverdelingen voor de hyperparameters (\\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\)) zoals gespecificeerd in het variërende interceptmodel zonder voorspellers. Bovendien krijgt de regressiecoëfficiënt \\(\\beta\\) een normale prioriteitsverdeling met gemiddelde 0 en standaardafwijking 100. Dit betekent, ruwweg, dat we verwachten dat deze coëfficiënt in het bereik \\((-100, 100)\\) ligt, en als de ML schatting in dit bereik ligt, geeft de prior verdeling zeer weinig informatie voor de inferentie.\r\nHet bovenstaande model kan als volgt worden gefit met de stan_lmer() functie in het rstanarm pakket:\r\n\r\n\r\nM2_stanlmer <- stan_lmer(formula = course ~ female + (1 | school), \r\n                         data = GCSE, \r\n                         prior = normal(location = 0, \r\n                                        scale = 100,\r\n                                        autoscale = FALSE),\r\n                         prior_intercept = normal(location = 0, \r\n                                                  scale = 100, \r\n                                                  autoscale = FALSE),\r\n                         seed = 349)\r\n\r\n\r\n\r\n\r\n\r\nprior_summary(object = M2_stanlmer)\r\n\r\n\r\nPriors for model 'M2_stanlmer' \r\n------\r\nIntercept (after predictors centered)\r\n ~ normal(location = 0, scale = 100)\r\n\r\nCoefficients\r\n ~ normal(location = 0, scale = 100)\r\n\r\nAuxiliary (sigma)\r\n  Specified prior:\r\n    ~ exponential(rate = 1)\r\n  Adjusted prior:\r\n    ~ exponential(rate = 0.061)\r\n\r\nCovariance\r\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\r\n------\r\nSee help('prior_summary.stanreg') for more details\r\n\r\n\r\n\r\nM2_stanlmer\r\n\r\n\r\nstan_lmer\r\n family:       gaussian [identity]\r\n formula:      course ~ female + (1 | school)\r\n observations: 1725\r\n------\r\n            Median MAD_SD\r\n(Intercept) 69.7    1.3  \r\nfemaleF      6.7    0.7  \r\n\r\nAuxiliary parameter(s):\r\n      Median MAD_SD\r\nsigma 13.4    0.2  \r\n\r\nError terms:\r\n Groups   Name        Std.Dev.\r\n school   (Intercept)  9      \r\n Residual             13      \r\nNum. levels: school 73 \r\n\r\n------\r\n* For help interpreting the printed output see ?print.stanreg\r\n* For info on the priors used see ?prior_summary.stanreg\r\n\r\nMerk op dat in plaats van de standaard priors in stan_lmer, \\(\\mu_{\\alpha}\\) en \\(\\beta\\) normale prior verdelingen krijgen met gemiddelde 0 en standaardafwijking 100 door de argumenten prior en prior_intercept op te geven als normal(location = 0, scale = 100, autoscale = FALSE). Om te voorkomen dat stan_lmer de prior schaalt, moeten we ervoor zorgen dat het argument autoscale = FALSE wordt toegevoegd.\r\nDe puntschattingen van \\(_mu_{alpha}\\), \\(\\beta\\), en \\(\\sigma_{y}\\) zijn bijna identiek aan de ML-schattingen van de lmer() fit. Echter, deels omdat ML de onzekerheid over \\(\\mu_{alpha}\\) negeert bij het schatten van \\(\\sigma_{alpha}\\), is de Bayesiaanse schatting voor \\(\\sigma_{alpha}\\) (\\(9,0\\)) groter dan de ML-schatting (\\(8,8\\)), net als bij model 1.\r\nModel 3: Variërende slopes over scholen toevoegen\r\nWe also use stan_lmer to fit Model 3 using the command below. Note that here, we use the default priors which are mostly similar to what was done in Model 1. Additionally, we are also required to specify a prior for the covariance matrix \\(\\Sigma\\) for \\(\\alpha_j\\) and \\(\\beta_j\\) in this Model. stan_lmer decomposes this covariance matrix (up to a factor of \\(\\sigma_y\\)) into (i) a correlation matrix \\(R\\) and (ii) a matrix of variances \\(V\\), and assigns them separate priors as shown below.\r\n\\[\r\n\\begin{aligned}\r\n\\Sigma &= \r\n\\left(\\begin{matrix} \r\n\\sigma_\\alpha^2 & \\rho\\sigma_\\alpha \\sigma_\\beta \\\\ \r\n\\rho\\sigma_\\alpha\\sigma_\\beta&\\sigma_\\beta^2 \r\n\\end{matrix} \\right)\\\\ &= \r\n\\sigma_y^2\\left(\\begin{matrix} \r\n\\sigma_\\alpha^2/\\sigma_y^2 & \\rho\\sigma_\\alpha \\sigma_\\beta/\\sigma_y^2 \\\\ \r\n\\rho\\sigma_\\alpha\\sigma_\\beta/\\sigma_y^2 & \\sigma_\\beta^2/\\sigma_y^2 \r\n\\end{matrix} \\right)\\\\ &= \r\n\\sigma_y^2\\left(\\begin{matrix} \r\n\\sigma_\\alpha/\\sigma_y & 0 \\\\ \r\n0&\\sigma_\\beta/\\sigma_y\r\n\\end{matrix} \\right)\r\n\\left(\\begin{matrix} \r\n1 & \\rho\\\\ \r\n\\rho&1 \r\n\\end{matrix} \\right)\r\n\\left(\\begin{matrix} \r\n\\sigma_\\alpha/\\sigma_y & 0 \\\\ \r\n0&\\sigma_\\beta/\\sigma_y \r\n\\end{matrix} \\right)\\\\ \r\n&= \\sigma_y^2VRV.\r\n\\end{aligned}\r\n\\]\r\nDe correlatiematrix \\(R\\) is een 2 bij 2 matrix met 1-en op de diagonaal en \\(rho\\)’s op de off-diagonaal. stan_lmer kent er een LKJ^[Voor meer details over de LKJ verdeling, zie hier en hier prior aan toe, met regularisatieparameter 1 (Lewandowski et all., 2009). Dit komt overeen met het toekennen van een uniforme prior voor \\(rho\\). Hoe groter de regularisatieparameter is dan 1, hoe meer de verdeling voor \\(\\rho\\) de waarde 0 aanneemt.\r\nDe matrix van (geschaalde) varianties \\(V\\) kan eerst worden samengevat in een vector van (geschaalde) varianties, en vervolgens ontleed in drie delen, \\(J\\), \\(\\tau^2\\) en \\(\\pi\\) zoals hieronder getoond. \\[\r\n\\left(\\begin{matrix} \r\n\\sigma_\\alpha^2/\\sigma_y^2 \\\\ \r\n\\sigma_\\beta^2/\\sigma_y^2 \r\n\\end{matrix} \\right) = \r\n2\\left(\\frac{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2}{2}\\right)\\left(\\begin{matrix} \r\n\\frac{\\sigma_\\alpha^2/\\sigma_y^2}{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2} \\\\ \r\n\\frac{\\sigma_\\beta^2/\\sigma_y^2}{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2} \r\n\\end{matrix} \\right)=\r\nJ\\tau^2 \\pi. \r\n\\]\r\nIn deze formulering is \\(J\\) het aantal variërende effecten in het model (hier \\(J=2\\)), kan \\(Jtau^2\\) worden beschouwd als een gemiddelde (geschaalde) variantie over de variërende effecten \\(Jalpha_j\\) en \\(Jbeta_j\\), en is \\(Jpi\\) een niet-negatieve vector die sommeert tot 1 (een zogenaamde simplex/probabiliteitsvector). Een symmetrische Dirichlet4 verdeling met concentratieparameter ingesteld op 1 wordt dan gebruikt als de prior voor \\(\\pi\\). Standaard impliceert dit een gezamenlijk uniforme prior over alle simplexvectoren van dezelfde grootte. Een schaalinvariante Gamma-voorrang met vorm- en schaalparameters beide op 1 wordt dan toegekend voor \\(\\tau\\). Dit komt overeen met het toekennen van de exponentiële verdeling met de snelheidsparameter op 1 die consistent is met de prioriteit toegekend aan \\(\\sigma_y\\) als prioriteit.\r\n\r\n\r\nM3_stanlmer <- stan_lmer(formula = course ~ female + (1 + female | school), \r\n                         data = GCSE,\r\n                         seed = 349)\r\nprior_summary(object = M3_stanlmer)\r\n\r\n\r\nPriors for model 'M3_stanlmer' \r\n------\r\nIntercept (after predictors centered)\r\n  Specified prior:\r\n    ~ normal(location = 73, scale = 2.5)\r\n  Adjusted prior:\r\n    ~ normal(location = 73, scale = 41)\r\n\r\nCoefficients\r\n  Specified prior:\r\n    ~ normal(location = 0, scale = 2.5)\r\n  Adjusted prior:\r\n    ~ normal(location = 0, scale = 83)\r\n\r\nAuxiliary (sigma)\r\n  Specified prior:\r\n    ~ exponential(rate = 1)\r\n  Adjusted prior:\r\n    ~ exponential(rate = 0.061)\r\n\r\nCovariance\r\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\r\n------\r\nSee help('prior_summary.stanreg') for more details\r\n\r\nM3_stanlmer\r\n\r\n\r\nstan_lmer\r\n family:       gaussian [identity]\r\n formula:      course ~ female + (1 + female | school)\r\n observations: 1725\r\n------\r\n            Median MAD_SD\r\n(Intercept) 69.4    1.3  \r\nfemaleF      7.1    1.1  \r\n\r\nAuxiliary parameter(s):\r\n      Median MAD_SD\r\nsigma 13.0    0.2  \r\n\r\nError terms:\r\n Groups   Name        Std.Dev. Corr \r\n school   (Intercept) 10.3          \r\n          femaleF      7.2     -0.49\r\n Residual             13.0          \r\nNum. levels: school 73 \r\n\r\n------\r\n* For help interpreting the printed output see ?print.stanreg\r\n* For info on the priors used see ?prior_summary.stanreg\r\n\r\n\r\n\r\n\r\nHier zien we dat de puntschattingen voor \\(\\mu_{\\alpha}\\) en \\(\\sigma_{y}\\) identiek zijn aan de ML-schattingen uit lmer() fit. De puntschatting voor $\\(_bèta\\) is iets anders in dit model (7.14 vergeleken met 7.13). Verder is, net als bij de vorige twee modellen, de Bayesiaanse schatting voor \\(\\sigma_{\\alpha}\\) (10.3) groter dan de ML schatting (10.15). Daarnaast zijn de Bayesiaanse schattingen voor \\(\\sigma_{\\beta}\\) (7.2) en \\(\\rho\\) (-0.49) groter dan de overeenkomstige ML schattingen (respectievelijk 6.92 en -0.52).\r\nEvalueren van model convergentie\r\n\r\n\r\n\r\nStandaard zullen alle rstanarm modelleerfuncties 4 willekeurig geïnitialiseerde Markov-ketens laten lopen, elk gedurende 2000 iteraties (inclusief een opwarmperiode van 1000 iteraties). Alle ketens moeten naar de doelverdeling convergeren om geldige conclusies te kunnen trekken. De diagnostica die wij gebruiken om te beoordelen of de ketens naar de posterior verdeling zijn geconvergeerd, zijn de statistieken \\(\\hat{R}\\) en \\(N_{text{eff}}\\) [@gelman1992inference]. Aan elke parameter zijn de statistiek \\(\\hat{R}\\) en \\(N_{text{eff}}\\) verbonden. Zoals eerder gezien, worden deze statistieken automatisch gegenereerd bij gebruik van de summary methode.\r\nDe \\(\\hat{R}\\) is in wezen de verhouding tussen-keten variantie en binnen-keten variantie analoog aan ANOVA. De statistiek \\(\\hat{R}\\) zou kleiner dan 1,1 moeten zijn als de ketens zijn geconvergeerd. Om een plot te zien van de \\(\\hat{R}\\) waarden over de parameters kunnen we de plot methode gebruiken voor stanreg object M1_stanlmer:\r\n\r\n\r\nplot(M1_stanlmer, \"rhat\")\r\n\r\n\r\n\r\n\r\n\r\nplot(M1_stanlmer, \"ess\")\r\n\r\n\r\n\r\nDe statistiek \\(N_{text{eff}}\\) geeft het effectieve aantal trekkingen van de simulatie weer. Als de trekkingen onafhankelijk zouden zijn, zou \\(N_{text{eff}}\\) het aantal opgeslagen trekkingen zijn, 4.000 (4 ketens $ maal$ (2.000 iteraties - 1.000 iteraties voor warmup)), maar \\(N_{text{eff}}\\) is meestal kleiner dan 4.000 omdat Markov chain simulaties de neiging hebben autocorrelatie te vertonen. De \\(N_{text{eff}}\\) statistiek zou typisch minstens 100 moeten zijn voor alle parameters. We kunnen een histogram plotten van de verhouding tussen de effectieve steekproefgrootte en de totale steekproefgrootte:\r\n\r\n\r\nplot(M1_stanlmer, \"ess\")\r\n\r\n\r\n\r\nLiteratuur\r\nBrowne, William J, David Draper, and others. 2006. “A Comparison of Bayesian and Likelihood-Based Methods for Fitting Multilevel Models.” Bayesian Analysis 1 (3): 473–514.\r\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\r\nGelman, Andrew, and Donald B Rubin. 1992. “Inference from Iterative Simulation Using Multiple Sequences.” Statistical Science. JSTOR, 457–72.\r\nGoldstein, Harvey, and David J Spiegelhalter. 1996. “League Tables and Their Limitations: Statistical Issues in Comparisons of Institutional Performance.” Journal of the Royal Statistical Society. Series A (Statistics in Society), 385–443.\r\nLewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. “Generating Random Correlation Matrices Based on Vines and Extended Onion Method.” Journal of Multivariate Analysis 100 (9). Elsevier: 1989–2001.\r\nRasbash, Jon, William Browne, Harvey Goldstein, Min Yang, Ian Plewis, Michael Healy, Geoff Woodhouse, David Draper, Ian Langford, and Toby Lewis. 2000. “A User’s Guide to Mlwin.” University of London, Institute of Education, Centre for Multilevel Modelling.\r\n\r\nWe werken meer over prioriteitsverdelingen uit in hoofdstuk 3. 2↩︎\r\nGebruikers die niet vertrouwd zijn met de syntaxis van ggplot2 verwijzen we naar hier↩︎\r\nVoor meer informatie over reguliere expressies, zie hier↩︎\r\nDe Dirichlet-verdeling is een multivariate veralgemening van de bètaverdeling met één concentratieparameter, die kan worden geïnterpreteerd als voorafgaande tellingen van een multinomiale willekeurige variabele (de simplexvector in onze context), zie voor details hier.↩︎\r\n",
    "preview": "posts/2022-02-11-multilevelrstan/multilevelrstan_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-latente-groei-modeling/",
    "title": "Latente Groei Modeling",
    "description": "Dit is een blog over Latente Groei Modeling van longitudinale data van alcoholgebruik van jongeren",
    "author": [
      {
        "name": "Alexander Cernat, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-01-03",
    "categories": [],
    "contents": "\r\nSchatten en visualiseren van verandering in de tijd met behulp van Latente Groeimodellen met R\r\nLongitudinale gegevens zijn zo interessant omdat ze ons in staat stellen te kijken naar verandering in de tijd, je krijgt er een beter inzicht mee in causale verbanden en je kunt gebeurtenissen en hun timing ermee verklaren. Om gebruik te maken van dit soort gegevens, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA. Dan moeten we gebruik maken van modellen die de extra complexiteit van de gegevens ook echt aan kunnen. Alexandru Cernat schreef ook hier een duidelijke blog over dat ik heb bewerkt en waarbij ik ook de alcoholdata van Singer en Willet heb gebruik.\r\nEen populair model voor de analyse van longitudinale gegevens is het Latente Groei Model (Latent Growth Model, LGM). Hiermee kan de verandering in de tijd worden geschat, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd die genest zijn binnen individuen). Het is vergelijkbaar met het multilevel model van verandering, maar hier wordt de schatting gedaan met behulp van het Structural Equation Modeling (SEM)-raamwerk. Dit raamwerk maakt gebruik van gegevens in het brede formaat (elke rij is een individu en de diverse metingen in de tijd verschijnen als verschillende kolommen).\r\nMeer in het bijzonder kan het LGM helpen\r\n- te begrijpen hoe verandering in de tijd verloopt;\r\n- verandering verklaren met behulp van tijdvariërende en tijdconstante voorspellers;\r\n- variantie ontleden in tussen- en binnenvariatie;\r\n- en het model kan makkelijk worden uitgebreid naar andere analysemodellen.\r\nHieronder volgt een korte inleiding op LGM, hoe de uitkomsten zijn te schatten en hoe de schattingen van verandering zijn te visualiseren.\r\nLaten we eerst de benodigde pakketten eens laden. We zullen tidyverse gebruiken voor het opschonen en visualiseren van de gegevens en lavaan voor het uitvoeren van de LGM in R.\r\n\r\n\r\n\r\nLaten we, voordat we aan de LGM beginnen, eens kijken naar het soort gegevens dat we zouden willen analyseren. Hier gebruik ik alcoholdata van jongeren met de drie metingen van Singer en Willet voor die vrij toegankelijk zijn op internet.\r\nStel dat we geïnteresseerd zijn in hoe alcoholscore in de tijd verandert. Om het preciezer te formuleren willen laten zien hoe alcoholgebruik onder jongeren gemiddeld verandert, en tegelijk willen we een onderscheid maken tussen variatie, hoe jongeren veranderen ten opzichte van anderen. Maar tegelijk willen we ook iets zeggen over binnenvariatie en hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend.\r\nLaten we eerst eens kijken hoe de gegevens eruit zien. Laten we eens kijken naar de brede gegevens, dit zijn de gegevens die gebruikt worden om LGM uit te voeren en laten we ook maar meteen het lange bestand bekijken:\r\n\r\n\r\n\r\nWe beginnen met het lange formaat, waar elke rij een combinatie is van individu en tijd. Dit is het formaat dat we nodig hebben voor visualisatie met ggplot2, en voor andere modellen (zoals het multilevel model voor verandering).\r\n\r\n  id age coa male age_14   alcuse      peer      cpeer  ccoa\r\n1  1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\r\n2  1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\r\n3  1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\r\n4  2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\r\n5  2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\r\n6  2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\r\n\r\nOm een idee te krijgen van wat we gaan modelleren, maken we een eenvoudige grafiek met de gemiddelde verandering in de tijd en de trend voor elk individu.\r\n\r\n\r\n\r\nWe zien hier een gemiddelde verandering in de tijd. Tegelijk willen we ook zichtbaar maken wat variatie is in de manier waarop mensen veranderen. LGM is in staat beide tegelijk te schatten!\r\nWat is Latente Groei Modellering?\r\nNu we een idee hebben van de gegevens en het soort onderzoeksvragen dat we zouden kunnen hebben, kunnen we overgaan tot de uitvoering van LGM. De formule voor het LGM is eigenlijk zeer gelijkaardig aan die voor het multilevel model van verandering:\r\n\\(Y_j=\\alpha_0 + \\alpha_1*\\gamma_j + \\zeta_{00} + \\zeta_{11}*\\gamma_j + \\epsilon_j\\)\r\nWaarbij:\r\n\\(Y_j\\) is de variabele van belang (alcuse, alchoholgebruik van jongeren) die verandert in tijd, j.\r\n\\(\\alpha_0\\) is de gemiddelde waarde bij het begin van de gegevensverzameling (het beginpunt van de rode lijn hierboven).\r\n\\(\\alpha_1*\\gamma_j\\) is de gemiddelde snelheid van verandering in de tijd (de helling van de rode lijn in de grafiek hierboven). Hier is \\(gamma_j\\) gewoon een maat voor de tijd.\r\n\\(\\zeta_{00}\\) is de tussenvariatie aan het begin van de gegevens. Het vat samen hoe verschillend de individuele startpunten zijn ten opzichte van het gemiddelde startpunt.\r\n\\(\\zeta_{11}\\gamma_j\\) is de tussen variatie in de snelheid van verandering. Samenvattend hoe verschillend de individuele veranderingsversnellingen zijn ten opzichte van de gemiddelde verandering (rode lijn hierboven).\r\nDe \\(\\epsilon_j\\) is de binnenvariatie of hoeveel individuen variëren rond hun voorspelde trend. Met de onderstaande grafiek kunnen we een beter idee krijgen van de verschillende variatiebronnen:\r\n\r\n\r\n\r\nDe interne variatie wordt weergegeven door de afstand tussen de lijn en de punten. Dit wordt voor elk individu afzonderlijk gedaan (door de kleur in de grafiek). De tussenvariatie verwijst naar hoe verschillend de lijnen zijn. Dit kan zowel het beginpunt als de helling zijn.\r\nOmdat deze techniek met brede databestanden werkt, zetten we de data over van lang naar wijd\r\n\r\n\r\n\r\n i =~ 1*Meting.0 +  1*Meting.1 +  1*Meting.2 \r\n  s =~ 0*Meting.0 +  1*Meting.1 +  2*Meting.2 \r\n  i ~~ s\r\nStructural Equation Modeling heeft zijn eigen manier om deze statistische relaties weer te geven. Hieronder is afgebeeld hoe we ons het hierboven beschreven model zouden moeten voorstellen:\r\nFig.1, LGM grafisch verbeeldIn de figuur worden de latente variabelen voorgesteld door cirkels (de twee \\(\\eta\\)-variabelen, intercept en slope), terwijl de waargenomen variabelen worden voorgesteld door vierkanten (de vier y-variabelen). Wij krijgen ook de residuen (kleine cirkels die \\(\\epsilon\\) voorstellen). Voor de latente variabelen hebben wij gemiddelden (\\(\\alpha\\)) en varianties (\\(\\zeta\\)). Deze zijn geschat en hebben de hierboven beschreven interpretatie. De pijlen tussen de latente en de geobserveerde variabelen (die gewoon regressiehellingen of ladingen zijn) liggen van tevoren vast. Voor de latente interceptvariabele (weergegeven door \\(\\eta_0\\)) zijn de ladingen vastgesteld op 1 (daarom is er in bovenstaande formule niets vermenigvuldigd met \\(\\alpha_0\\) en \\(\\eta_{00}\\)). De ladingen voor de hellende latente variabele (weergegeven door \\(\\eta_1\\)) worden vastgesteld naar gelang van de verandering in tijd (\\(\\gamma_j\\) in bovenstaande formule). In dit geval gaat het eenvoudig van 0 naar 2. Er is ook een correlatie tussen het beginpunt en de verandering in tijd, weergegeven door de dubbele pijl \\(\\zeta_{01}\\). Dit wordt niet vaak geïnterpreteerd, maar het geeft je in feite een idee of mensen convergeren (of meer op elkaar gaan lijken in de tijd) of divergeren (meer van elkaar gaan verschillen).\r\nNu het technische deel duidelijk is gemaakt, kunnen we wat modelleren en meer grafieken maken!\r\n\r\nlavaan 0.6-9 ended normally after 22 iterations\r\n\r\n  Estimator                                         ML\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                         8\r\n                                                      \r\n  Number of observations                            82\r\n                                                      \r\nModel Test User Model:\r\n                                                      \r\n  Test statistic                                 0.636\r\n  Degrees of freedom                                 1\r\n  P-value (Chi-square)                           0.425\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                             Standard\r\n  Information                                 Expected\r\n  Information saturated (h1) model          Structured\r\n\r\nLatent Variables:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n  i =~                                                         \r\n    age_14            1.000                               0.898\r\n    age_15            1.000                               0.898\r\n    age_16            1.000                               0.898\r\n  s =~                                                         \r\n    age_14            0.000                               0.000\r\n    age_15            1.000                               0.484\r\n    age_16            2.000                               0.967\r\n  Std.all\r\n         \r\n    0.963\r\n    0.862\r\n    0.796\r\n         \r\n    0.000\r\n    0.464\r\n    0.857\r\n\r\nCovariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n  i ~~                                                         \r\n    s                -0.187    0.102   -1.841    0.066   -0.431\r\n  Std.all\r\n         \r\n   -0.431\r\n\r\nIntercepts:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n   .age_14            0.000                               0.000\r\n   .age_15            0.000                               0.000\r\n   .age_16            0.000                               0.000\r\n    i                 0.634    0.103    6.163    0.000    0.706\r\n    s                 0.277    0.062    4.481    0.000    0.573\r\n  Std.all\r\n    0.000\r\n    0.000\r\n    0.000\r\n    0.706\r\n    0.573\r\n\r\nVariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv\r\n   .age_14            0.064    0.147    0.436    0.663    0.064\r\n   .age_15            0.420    0.094    4.463    0.000    0.420\r\n   .age_16            0.280    0.180    1.556    0.120    0.280\r\n    i                 0.807    0.193    4.177    0.000    1.000\r\n    s                 0.234    0.083    2.803    0.005    1.000\r\n  Std.all\r\n    0.073\r\n    0.387\r\n    0.220\r\n    1.000\r\n    1.000\r\n\r\nEr zijn in principe zes soorten coëfficiënten die hier interessant zijn:\r\nintercept i: de waarde 0.634 staat voor de gemiddelde verwachte alcoholgebruik aan het begin van het onderzoek voor alle respondenten wanneer ze 14 jaar zijn.\r\nslope s: de waarde 0.277 vertegenwoordigt de gemiddelde verandering voor alle respondenten. Dus bij elke meting stijgt het gebruik van alcohol met met 0.277.\r\nvariantie i: de waarde 0.807 vertegenwoordigt de tussenvariatie aan het begin van het onderzoek. Dus hoe verschillend zijn mensen vergeleken met het gemiddelde.\r\nvariantie s: de waarde 0.234 staat voor de tussenvariatie in de veranderingssnelheid. Het laat zien hoe verschillend veranderingshellingen zijn voor verschillende mensen.\r\nVariantie alcoholgebruik: de waarden tussen 0.064 en 0.420 geven de interne variatie op elk punt in de tijd weer.\r\ncorrelatie tussen i en s: de waarde -0.187 laat zien dat het alcohol niet in de tijd convergeert (althans niet significant).\r\nHoe kunnen we de verandering visualiseren?\r\nEen goede manier om te begrijpen wat je modelleert, is de voorspelde scores van het model visualiseren. We zullen het predict() commando gebruiken om een nieuw object op te slaan met de voorspelde scores op individueel niveau voor het intercept en de helling.\r\n\r\n\r\n\r\nDit heeft de voorspelde score voor het intercept en de helling voor elk individu:\r\n\r\n              i          s\r\n[1,] 1.67339200 0.15061147\r\n[2,] 0.01577357 0.41987762\r\n[3,] 1.03717373 0.89157072\r\n[4,] 0.15472328 0.80763226\r\n[5,] 0.01182452 0.09588422\r\n[6,] 2.85697940 0.05794456\r\n\r\nDeze zijn gebaseerd op ons model. Wij zouden dus bijvoorbeeld het gemiddelde van deze variabelen kunnen schatten en dat gemiddelde voor intercept en slope zou achtereenvolgens dezelfde resultaten moeten geven als hierboven:\r\n\r\n[1] 0.634417\r\n\r\n\r\n[1] 0.2773233\r\n\r\nOm de resultaten te plotten, willen wij deze gegevens (intercept (\\(\\zeta_0\\)) en helling (\\(\\zeta_1\\))) omzetten in verwachte scores bij elke meting (\\(\\gamma_j\\)j). We kunnen deze transformatie doen op basis van het padmodel dat we hierboven hebben gezien:\r\n\\(Y_1=\\eta_0 + \\eta_1\\gamma_j\\)\r\nVoor de eerste meting (time=0) is de verwachte waarde dus alleen het intercept (\\(\\eta_0\\)) omdat \\(\\gamma_j\\) gelijk is aan 0. Voor de meting zou de verwachte waarde het intercept (\\(\\eta_0\\)) en de helling (\\(\\eta_1\\)) zijn. Voor meting drie zou het intercept + 2*helling zijn, enzovoort.\r\nIn R zouden we al deze metingen met de hand kunnen berekenen of we zouden het automatisch kunnen doen met behulp van functioneel programmeren. Op basis van de bovenstaande formule kunnen we een tegenhanger in R maken:\r\npred_lgm[, 1] + x*pred_lgm[, 2]\r\nVoor de eerste meting (alc_14=0) krijgen we deze scores\r\n\r\n [1] 1.67339200 0.01577357 1.03717373 0.15472328 0.01182452 2.85697940\r\n [7] 1.70002148 0.01182452 0.09353383 0.95999586 0.02431252 0.89485737\r\n[13] 0.01972262 2.74652531 1.69122663 0.98981031 0.07985392 1.88598973\r\n[19] 0.01182452 3.16706021 1.01268775 2.68643747 0.96288677 0.07985392\r\n[25] 2.71888080 0.08543872 2.64724315 0.01182452 0.01577357 1.03091617\r\n[31] 1.77626546 1.62959225 0.08543872 0.08380297 0.95604681 0.96394491\r\n[37] 1.09292161 0.13755300 0.01182452 0.01182452 1.25884647 0.01182452\r\n[43] 0.01182452 0.11987977 2.13501694 0.01740932 0.07985392 0.01182452\r\n[49] 0.01182452 0.01182452 0.01182452 0.07985392 0.01182452 0.01577357\r\n[55] 0.01182452 0.11848083 0.01182452 0.99389867 0.02227273 0.08543872\r\n[61] 1.92422677 0.01182452 0.01182452 0.15671367 0.08543872 1.49845628\r\n[67] 0.88801741 0.07985392 0.01182452 1.83223971 0.01182452 0.01182452\r\n[73] 0.01182452 0.13848524 0.01182452 0.01182452 0.11920222 0.01182452\r\n[79] 1.04572058 0.96288677 0.01182452 0.11198167\r\n\r\nVoor de tweede meting (alc_14=1) ziet het er zo uit\r\n\r\n [1] 1.8240035 0.4356512 1.9287445 0.9623555 0.1077087 2.9149240\r\n [7] 1.5604805 0.1077087 1.3870516 1.0257079 1.1447539 1.1224615\r\n[13] 0.7635937 2.8155774 2.0052428 1.2209099 0.2510256 1.5773629\r\n[19] 0.1077087 2.4340560 1.3706937 2.1045835 1.2657784 0.2510256\r\n[25] 2.4588992 0.7148063 2.0159411 0.1077087 0.4356512 1.4090953\r\n[31] 2.3055104 1.4121079 0.7148063 0.5789681 0.6977654 1.3536504\r\n[37] 1.9968551 1.0118258 0.1077087 0.1077087 1.3953791 0.1077087\r\n[43] 0.1077087 1.2942168 2.4109430 0.5714894 0.2510256 0.1077087\r\n[49] 0.1077087 0.1077087 0.1077087 0.2510256 0.1077087 0.4356512\r\n[55] 0.1077087 1.1780436 0.1077087 1.5604210 0.9753629 0.7148063\r\n[61] 1.9775396 0.1077087 0.1077087 1.1276442 0.7148063 2.2197859\r\n[67] 0.5544485 0.2510256 0.1077087 1.1445052 0.1077087 0.1077087\r\n[73] 0.1077087 1.0892425 0.1077087 0.1077087 1.2379508 0.1077087\r\n[79] 1.3386955 1.2657784 0.1077087 0.6383319\r\n\r\nBij de derde meting ziet het alcoholgebruik er zo uit.\r\n\r\n [1] 1.9746149 0.8555288 2.8203152 1.7699878 0.2035930 2.9728685\r\n [7] 1.4209395 0.2035930 2.6805694 1.0914199 2.2651952 1.3500657\r\n[13] 1.5074647 2.8846296 2.3192590 1.4520095 0.4221974 1.2687361\r\n[19] 0.2035930 1.7010517 1.7286996 1.5227296 1.5686701 0.4221974\r\n[25] 2.1989176 1.3441739 1.3846391 0.2035930 0.8555288 1.7872745\r\n[31] 2.8347553 1.1946236 1.3441739 1.0741332 0.4394841 1.7433558\r\n[37] 2.9007885 1.8860986 0.2035930 0.2035930 1.5319118 0.2035930\r\n[43] 0.2035930 2.4685539 2.6868691 1.1255695 0.4221974 0.2035930\r\n[49] 0.2035930 0.2035930 0.2035930 0.4221974 0.2035930 0.8555288\r\n[55] 0.2035930 2.2376064 0.2035930 2.1269432 1.9284531 1.3441739\r\n[61] 2.0308524 0.2035930 0.2035930 2.0985747 1.3441739 2.9411155\r\n[67] 0.2208797 0.4221974 0.2035930 0.4567708 0.2035930 0.2035930\r\n[73] 0.2035930 2.0399998 0.2035930 0.2035930 2.3566993 0.2035930\r\n[79] 1.6316705 1.5686701 0.2035930 1.1646821\r\n\r\nwaarbij x onze codering van tijd voorstelt (of \\(\\gamma_j\\)). We kunnen deze functie meerdere keren toepassen met het map() commando. De onderstaande syntaxis past deze formule toe voor de getallen 0, 1, 2 (onze codering van meting (variabele alc_14).\r\n\r\n[[1]]\r\n [1] 1.67339200 0.01577357 1.03717373 0.15472328 0.01182452 2.85697940\r\n [7] 1.70002148 0.01182452 0.09353383 0.95999586 0.02431252 0.89485737\r\n[13] 0.01972262 2.74652531 1.69122663 0.98981031 0.07985392 1.88598973\r\n[19] 0.01182452 3.16706021 1.01268775 2.68643747 0.96288677 0.07985392\r\n[25] 2.71888080 0.08543872 2.64724315 0.01182452 0.01577357 1.03091617\r\n[31] 1.77626546 1.62959225 0.08543872 0.08380297 0.95604681 0.96394491\r\n[37] 1.09292161 0.13755300 0.01182452 0.01182452 1.25884647 0.01182452\r\n[43] 0.01182452 0.11987977 2.13501694 0.01740932 0.07985392 0.01182452\r\n[49] 0.01182452 0.01182452 0.01182452 0.07985392 0.01182452 0.01577357\r\n[55] 0.01182452 0.11848083 0.01182452 0.99389867 0.02227273 0.08543872\r\n[61] 1.92422677 0.01182452 0.01182452 0.15671367 0.08543872 1.49845628\r\n[67] 0.88801741 0.07985392 0.01182452 1.83223971 0.01182452 0.01182452\r\n[73] 0.01182452 0.13848524 0.01182452 0.01182452 0.11920222 0.01182452\r\n[79] 1.04572058 0.96288677 0.01182452 0.11198167\r\n\r\n[[2]]\r\n [1] 1.8240035 0.4356512 1.9287445 0.9623555 0.1077087 2.9149240\r\n [7] 1.5604805 0.1077087 1.3870516 1.0257079 1.1447539 1.1224615\r\n[13] 0.7635937 2.8155774 2.0052428 1.2209099 0.2510256 1.5773629\r\n[19] 0.1077087 2.4340560 1.3706937 2.1045835 1.2657784 0.2510256\r\n[25] 2.4588992 0.7148063 2.0159411 0.1077087 0.4356512 1.4090953\r\n[31] 2.3055104 1.4121079 0.7148063 0.5789681 0.6977654 1.3536504\r\n[37] 1.9968551 1.0118258 0.1077087 0.1077087 1.3953791 0.1077087\r\n[43] 0.1077087 1.2942168 2.4109430 0.5714894 0.2510256 0.1077087\r\n[49] 0.1077087 0.1077087 0.1077087 0.2510256 0.1077087 0.4356512\r\n[55] 0.1077087 1.1780436 0.1077087 1.5604210 0.9753629 0.7148063\r\n[61] 1.9775396 0.1077087 0.1077087 1.1276442 0.7148063 2.2197859\r\n[67] 0.5544485 0.2510256 0.1077087 1.1445052 0.1077087 0.1077087\r\n[73] 0.1077087 1.0892425 0.1077087 0.1077087 1.2379508 0.1077087\r\n[79] 1.3386955 1.2657784 0.1077087 0.6383319\r\n\r\n[[3]]\r\n [1] 1.9746149 0.8555288 2.8203152 1.7699878 0.2035930 2.9728685\r\n [7] 1.4209395 0.2035930 2.6805694 1.0914199 2.2651952 1.3500657\r\n[13] 1.5074647 2.8846296 2.3192590 1.4520095 0.4221974 1.2687361\r\n[19] 0.2035930 1.7010517 1.7286996 1.5227296 1.5686701 0.4221974\r\n[25] 2.1989176 1.3441739 1.3846391 0.2035930 0.8555288 1.7872745\r\n[31] 2.8347553 1.1946236 1.3441739 1.0741332 0.4394841 1.7433558\r\n[37] 2.9007885 1.8860986 0.2035930 0.2035930 1.5319118 0.2035930\r\n[43] 0.2035930 2.4685539 2.6868691 1.1255695 0.4221974 0.2035930\r\n[49] 0.2035930 0.2035930 0.2035930 0.4221974 0.2035930 0.8555288\r\n[55] 0.2035930 2.2376064 0.2035930 2.1269432 1.9284531 1.3441739\r\n[61] 2.0308524 0.2035930 0.2035930 2.0985747 1.3441739 2.9411155\r\n[67] 0.2208797 0.4221974 0.2035930 0.4567708 0.2035930 0.2035930\r\n[73] 0.2035930 2.0399998 0.2035930 0.2035930 2.3566993 0.2035930\r\n[79] 1.6316705 1.5686701 0.2035930 1.1646821\r\n\r\nConclusies\r\nHopelijk geeft dit je een idee over wat LGM is, hoe je het kan schatten in R en hoe je deze verandering kan visualiseren.\r\n[Met dank aan Alexandru Cernat}(https://www.alexcernat.com/estimating-and-visualizing-change-in-time-using-latent-growth-models-with-r/)\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-multilevel-modeling/",
    "title": "Multilevel modeling",
    "description": "Dit is een post over multilevel analyse van longitudinale data met betrekking tot alcoholgebruik van jongeren.",
    "author": [
      {
        "name": "Alexander Cernat, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2022-01-03",
    "categories": [],
    "contents": "\r\nHet schatten van multilevel-modellen voor verandering in R\r\nLongitudinale gegevens zijn heel boeiend omdat je ermee kunt kijken naar verandering in de tijd, een beter begrip krijgt van causale verbanden en gebeurtenissen en hun timing ermee kunt verklaren. Om dit te kunnen doen, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA, en modellen gebruiken die beter kunnen omgaan met complexiteit van de gegevens. Alexander Cernat schreef er een blog over hier die ik hier in het Nederlands overzet en waarbij ik alcoholdata van Willet en Singer gebruik/\r\nEen populair model voor de analyse van longitudinale gegevens is het Multilevel Model voor Verandering; MultiLevel Model for Change (MLMC). Dit model maakt de schatting van verandering in de tijd mogelijk, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd genest binnen individuen). Het is vergelijkbaar met het Latente Groei Model; Latent Growth Model zie deze post, maar hier wordt geschat met behulp van het multilevel model raamwerk (ook bekend als hiërarchische modellering of random effecten). Deze techniek maakt gebruik van het lange dataformaat (elke rij is is een rij gegevens op een specifiek tijdstip voor een individu).\r\nMeer in het bijzonder kan het MLMC helpen:\r\n- te begrijpen hoe de individuele en geaggregeerde verandering in de tijd verlopen;\r\n- verandering te verklaren met behulp van tijdsvariërende (bv. tijd) en tijdsconstante (b.v. geslacht) voorspellers;\r\n- variantie te ontleden in tussen- en binnen- variatie;\r\n- gemakkelijk om te gaan met continue tijd, onevenwichtige gegevens (niet alle individuen zijn op alle tijdstippen aanwezig) en verschillende timings (niet iedereen geeft gegevens op precies hetzelfde moment).\r\nHier volgt een korte inleiding op MLMC, hoe hiermee te werken in R en hoe veranderingen zijn te visualiseren.\r\nLaten we eerst de R-pakketten laden (deze moeten dus wel geïnstalleerd zijn). We zullen tidyverse gebruiken voor het opschonen en visualiseren van de data, lme4 voor het uitvoeren van de MLMC in R en sjstats voor het schatten van intra class correlation (icc)\r\nJe kunt pakketten installeren met het install.packages() commando.\r\nLaten we vervolgens de pakketten binnenhalen die wij bij deze analyse zullen gebruiken.\r\n\r\n\r\n\r\nLaten we, voordat we aan de MLMC beginnen, eerst kijken naar de data die we willen analyseren. Hier gebruik ik alcuse (alcoholgebruik) met drie metingen. Dit is een longitudinale dataset van 82 jongeren.\r\nWe willen weten hoe alcoholgebruik in de tijd verandert en we willen die verandering begrijpen. Hierbij maken we een onderscheid tussen tussenvariatie (hoe de jongeren ten opzichte van elkaar veranderen) en binnenvariatie (hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend).\r\nLaten we deze gegevens eens onderzoeken. We kijken hiervoor naar de gegevens in lang formaat, die we zullen gebruiken voor de modellering en de grafieken. Hieronder zie je de eerste tien gegevens:\r\n\r\n   id age coa male age_14   alcuse      peer      cpeer  ccoa\r\n1   1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\r\n2   1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\r\n3   1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\r\n4   2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\r\n5   2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\r\n6   2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\r\n7   3  14   1    1      0 1.000000 0.8944272 -0.1235728 0.549\r\n8   3  15   1    1      1 2.000000 0.8944272 -0.1235728 0.549\r\n9   3  16   1    1      2 3.316625 0.8944272 -0.1235728 0.549\r\n10  4  14   1    1      0 0.000000 1.7888544  0.7708544 0.549\r\n\r\nWe zien dat elke rij een combinatie is van een jongere (variabele id (van 1 tot en met 4, die we in de analyse gebruiken)) en tijd (variabele age_14, alcoholgebruik op 14-jarige leeftijd). Dit is ook het formaat dat we nodig hebben voor een visualisatie met ggplot2.\r\nOm te zien wat we gaan modelleren, kunnen we een eenvoudige grafiek maken met een gemiddelde veranderingslijn in tijd voor de hele dataset en een wirwar van lijnen voor de verandering van elk individu:\r\n\r\n\r\n\r\nWe zien dus een hele lichte positieve en constante verandering in de tijd, maar vooral ook heel wat variatie in de manier waarop jongeren alcohol gebruiken (grijze lijnen). MLMC is in staat om beide dingen (het structurele en individuele) tegelijk te schatten!\r\nWat is multilevel modellering?\r\nMultilevel modellering is een uitbreiding van regressie modellering (‘just regression’) waarin we als het ware verschillende bronnen van variatie uit elkaar trekken. Waarom is dit belangrijk?\r\nTraditioneel gaat OLS-regressie er vanuit dat alle gevallen onafhankelijk zijn. Dit impliceert dat er geen correlatie is tussen de geobserveerde metingen als gevolg van zaken als clustering. Dit is vaak niet waar bij sociaal-wetenschappelijke gegevens. Individuen zijn bijvoorbeeld genest in huishoudens, klassen, buurten, regio’s en landen. Studenten zijn genest in klassen, scholen, regio’s en landen. Deze geneste structuur zorgt ervoor dat individuen op elkaar lijken. Zo zullen de gezondheidsuitkomsten waarschijnlijk vergelijkbaar zijn voor mensen die in dezelfde buurt wonen, vanwege bijvoorbeeld zelfselectie (vergelijkbaar inkomen en opleiding), kwaliteit van de gezondheidszorg of luchtkwaliteit. Als dit waar is, dan kunnen we mensen die uit dezelfde buurt komen niet als onafhankelijk behandelen.\r\nMultilevel modellering lost dit probleem op door willekeurige effecten (d.w.z. variatie) te schatten voor de verschillende niveaus die in de gegevens aanwezig zijn. Op die manier worden de regressiecoëfficiënten (zoals standaardfouten) gecorrigeerd voor deze geneste structuur. Bovendien kan met dit soort modellen worden geschat hoeveel variatie van elk niveau afkomstig is. Dit kan zeer informatief zijn. Door bijvoorbeeld de variatie van de leerlingresultaten uit elkaar te halen op leerlingniveau, klasniveau en schoolniveau, kunnen we begrijpen wat de belangrijkste factoren zijn die de uitkomsten van de leerlingen beïnvloeden. Dit kan informatie opleveren voor theorie en beleid.\r\nMultilevel modellering en longitudinale data\r\nLongitudinale data zijn ook genest. Metingen op verschillende tijdstippen zijn genest binnen een individu. Deze waarnemingen zijn niet onafhankelijk, omdat stabiele individuele kenmerken (zoals genen, persoonlijkheid of context) leiden tot consistente uitkomsten binnen de individuen en over de tijd. Multilevel modellering kan dus helpen corrigeren voor dit geklusterd karakter, maar helpt ons ook om twee belangrijke bronnen van variatie in longitudinale gegevens uit elkaar te halen: tussen-variatie (tussen individuen) en binnen-variatie (binnen het individu).\r\nTussen-variatie heeft betrekking op de manier waarop individuen van elkaar verschillen wat de belangrijke uitkomst betreft (bv. in dit geval dat het ene individu gemiddeld een hoger alcuse score heeft dan het andere individu). Terwijl binnen-variatie, de tweede vorm van variatie, betrekking heeft op de manier waarop alcuse op een bepaalde meting verschilt van het individuele gemiddelde (bv. hebben zij een lager of hoger score op alcoholgebruik in vergelijking met hun normale inkomen).\r\nLaten we, om deze vormen van variatie beter te begrijpen, eens naar de onderstaande grafieken kijken (een voor individu met idn=1 en de ander met idn=10).\r\n\r\n\r\n\r\n\r\n\r\n\r\nHier zien we de twee alcuse-scores van twee individuen en de individuele trends over de tijd (drie metingen). We kunnen de variatie tussen de lijnen zien als het verschil tussen de lijnen. Dit vertelt ons of de trend voor het ene individu anders is dan voor het andere. Als deze bron van variatie 0 zou zijn, dan zouden alle lijnen gelijk zijn. Als individuen zeer verschillende lijnen hebben, dan zal dit een belangrijke bron van variatie zijn. Met interne variatie wordt het verschil bedoeld tussen de individuele trend en de bijbehorende waargenomen scores. Als deze bron van variatie 0 zou zijn, zouden we alcuse bij elke meting perfect voorspellen en zou er geen interne variatie zijn. variatie zijn. Hoe groter deze bron van variatie, hoe meer de individuele waarden buiten hun eigen trend schommelen.\r\nHet onvoorwaardelijke verandermodel (ook wel random effects genoemd)\r\nNu we wat basiskennis hebben van multilevel modellering en longitudinale gegevens kunnen we het uitproberen. Wij gaan gewoonlijk uit van een eenvoudig model dat enkel de binnen- en tussenvariatie wil scheiden. We kunnen het model definiëren als:\r\n\\(Y_ij=\\gamma_{00}+u_{01}+e_{ij}\\)\r\nWaarbij:\r\n\\(Y_{ij}\\) is de variabele van belang (bijvoorbeeld alcuse) en varieert per individu (i) en tijd (j)\r\n\\(\\gamma_{00}\\) is het intercept in de regressie. We kunnen dit interpreteren als het grote gemiddelde of het gemiddelde van de uitkomst over alle individuen en tijdstippen.\r\n\\(u_{0i}\\) is de tussenvariatie en vertelt ons hoe verschillend jongeren van elkaar zijn in hun alcoholgebruik-score. Als iedereen hetzelfde scoort op alcuse, zou dit 0 zijn. Hoe groter de verschillen, hoe groter deze coëfficiënt zal zijn en dus de variatie is.\r\n\\(e_{ij}\\) dit is het residu, maar heeft ook hier de interpretatie van binnenvariatie en vertelt ons hoeveel elk individu varieert rond zijn eigen gemiddelde. Hoe groter deze coëfficiënt, hoe meer individuen in hun uitkomsten op alcoholgebruik schommelen.\r\nNu we weten wat we willen modelleren, laten we eens kijken hoe we dat kunnen doen in R en met het pakket lme4. Voor het schatten van multilevel modellen zullen we het lmer()commando gebruiken. We moeten de data en de formule opgeven. Onze uitkomst is alcuse, dus dat staat aan de linkerkant van ~. Aan de rechterkant hebben we “1”, dat staat voor het intercept. Tussen haakjes definiëren we de willekeurige effecten. Hier zeggen we dat we het intercept (“1”) willen laten variëren per individu (| id). Dit alles leidt tot deze syntaxis:\r\n\r\nLinear mixed model fit by REML ['lmerMod']\r\nFormula: alcuse ~ 1 + (1 | id)\r\n   Data: alcohol1\r\n\r\nREML criterion at convergence: 673\r\n\r\nScaled residuals: \r\n    Min      1Q  Median      3Q     Max \r\n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \r\n\r\nRandom effects:\r\n Groups   Name        Variance Std.Dev.\r\n id       (Intercept) 0.5731   0.7571  \r\n Residual             0.5617   0.7495  \r\nNumber of obs: 246, groups:  id, 82\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)   0.9220     0.0963   9.574\r\n\r\nLaten we de belangrijkste coëfficienten eens interpreteren:\r\nonder “Fixed effects” hebben we het “(Intercept)”, dat het grote gemiddelde is (\\(\\gamma_{00}\\) en ons vertelt dat over alle tijdstippen en individuen de gemiddelde score 0.922 is.\r\n“Random effects” vertegenwoordigt alles dat varieert met id, de tussen-variatie. In dit geval is de tussenvariatie voor het intercept (\\(\\beta_{0i}\\)) 0.573\r\nonder “Random effects” vertegenwoordigt de “Residual”-coëfficiënt de binnen-variatie. In dit geval is de binnen variatie voor het intercept \\(e_{ij}\\) 0.562\r\nOm de tussen en binnen-variantie beter te begrijpen, berekenen we InterClass Coefficient (ICC), dat is de tussen-variatie gedeeld door de totale variatie (tussen en binnen-variatie opgeteld). Dit berekenen we met het sjstats-pakket waarmee we het percentage tussenvariatie berekenen van het m0-model (51%), het percentage variatie dat door de groep kan worden verklaard.\r\n\r\n# Intraclass Correlation Coefficient\r\n\r\n     Adjusted ICC: 0.505\r\n  Conditional ICC: 0.505\r\n\r\nDit is dus gewoon een verhouding van de tussen-variatie op de totale variatie en vertelt ons welk deel van de variatie tussen individuen is. In ons geval blijkt dat ongeveer 51% van de variatie in alcuse tussen jongeren komt, terwijl de resterende (~ 49%) binnen jongeren ligt. In wezen zou dit erop wijzen dat de verschillen op alcoholgebruik tussen jongeren net iets belangrijker zijn dan de verschillen op alcoholgebruik binnen jongeren.\r\nOm beter te begrijpen wat het model doet, kunnen we de scores voorspellen en een grafiek maken met de voorspelde individuele score (lijnen) en de waargenomen scores (punten) voor vijf individuen:\r\n\r\n\r\n\r\nHet model van onvoorwaardelijke verandering\r\nHet vorige model is nuttig om ons een idee te geven van hoeveel variatie we op elk niveau hebben, maar we willen ook kijken naar verandering in de tijd! Laten we het model dus uitbreiden:\r\n\\(Y_{ij}=\\gamma_{00}+ \\gamma_{10}*Meting_{ij}+ u_{0i}+e_{ij}\\)\r\nIn dit model voegen we tijd (metingen van veertienjarige leeftijd gemeten) toe als voorspeller. Nu stelt \\(\\gamma_{00}\\) de gemiddelde score op alcoholgebruik voor wanneer de tijd 0 (alcoholgebruik op veertien jarige leeftijd) is, terwijl \\(\\gamma_{10}\\) het veranderingspercentage van alcoholgebruik voorstelt wanneer de tijd met 1 toeneemt.\r\nOm de zaken gemakkelijker te interpreteren, is het belangrijk te beginnen bij 0 zodat de \\(\\gamma_{00}\\) de mooie interpretatie heeft van verwacht alcoholgebruik aan het begin van de studie. De tijdsvariabele ‘age_14’ (alcoholgebruik op 14-jarige leeftijd) kent al de waarden 0, 1 en 2\r\nNu kunnen we ons model uitvoeren. We voegen gewoon de nieuwe tijdsvariabele toe als een fixed effect:\r\n\r\nLinear mixed model fit by REML ['lmerMod']\r\nFormula: alcuse ~ 1 + age_14 + (1 | id)\r\n   Data: alcohol1\r\n\r\nREML criterion at convergence: 654.1\r\n\r\nScaled residuals: \r\n     Min       1Q   Median       3Q      Max \r\n-2.19816 -0.66940  0.03001  0.44728  2.66167 \r\n\r\nRandom effects:\r\n Groups   Name        Variance Std.Dev.\r\n id       (Intercept) 0.5966   0.7724  \r\n Residual             0.4915   0.7011  \r\nNumber of obs: 246, groups:  id, 82\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)  0.65130    0.11077   5.880\r\nage_14       0.27065    0.05474   4.944\r\n\r\nCorrelation of Fixed Effects:\r\n       (Intr)\r\nage_14 -0.494\r\n\r\nEen belangrijk verschil met het vorig model zit in het fixed effects-deel. Nu interpreteren wij het intercept (0.651) als het verwachte alcoholgebruik-score aan het begin van de studie (wanneer ze veertien jaar zijn, age_14=0). Het effect van een jaar extra, 0.271, vertelt ons de gemiddelde veranderingssnelheid bij ieder van de twee metingen. De score op alcoholgebruik neemt dus elk jaar met 0.271 toe.\r\nLaten we de resultaten opnieuw visualiseren op basis van het nieuwe model:\r\n\r\n\r\n\r\nNu zien we dat we twee vormen van tussenvariatie hebben. De coëfficiënt \\(u_{0i}\\) vertegenwoordigt de tussenvariatie aan het begin van het onderzoek terwijl \\(u_{1i}\\) de tussenvariatie in het tempo van de verandering vertegenwoordigt. Dit betekent dat we toestaan dat individuen verschillende alcoholgebruik-scores aan het begin hebben (age_14=0), maar ook verschillende trends laten zien.\r\nDit model willen we met lme4 uitvoeren. We kunnen eenvoudigweg “meting” of in dit geval “jaartal” toevoegen aan het willekeurige deel van het model:\r\nWe zien nu dus een positieve trend die te wijten is aan de tijdscoëfficiënt. De individuele lijnen lopen, algemeen gezegd, hier parallel aan. We nemen nu aan dat de verandering in de tijd voor alle individuen gelijk is. In meer technische termen nemen we aan dat er geen tussenvariatie is in de snelheid van verandering. Dat is een vrij sterke veronderstelling. In ons geval zouden we dat niet verwachten gezien de eerste grafiek die we hebben gemaakt (en die individuele lijnen die verschillende kanten opgingen). Laten we het model dus uitbreiden met de tussenvariatie in de veranderingssnelheid:\r\n\\(Y_{ij}=\\gamma_{00}+\\gamma_{10}*Meting_{ij}+u_{0i}+u_{1i}*Meting_{ij}+e{ij}\\)\r\nNu zien we dat we twee bronnen van tussenvariatie hebben. De coëfficiënt \\(u_0i\\) vertegenwoordigt de tussenvariatie aan het begin van de studie terwijl \\(u_1i\\) de tussenvariatie in het tempo van verandering vertegenwoordigt. Dit betekent dat we toestaan dat individuen verschillen in alcoholgebruik aan het begin, maar ook verschillende trends kunnen kunnen laten zien.\r\nOm een dergelijk model in lme4 uit te voeren. We kunnen eenvoudigweg “tijd” (age_14) toevoegen aan het willekeurige deel van het model:\r\n\r\nLinear mixed model fit by REML ['lmerMod']\r\nFormula: alcuse ~ 1 + age_14 + (1 + age_14 | id)\r\n   Data: alcohol1\r\n\r\nREML criterion at convergence: 643.2\r\n\r\nScaled residuals: \r\n     Min       1Q   Median       3Q      Max \r\n-2.48287 -0.37933 -0.07858  0.38876  2.49284 \r\n\r\nRandom effects:\r\n Groups   Name        Variance Std.Dev. Corr \r\n id       (Intercept) 0.6355   0.7972        \r\n          age_14      0.1552   0.3939   -0.23\r\n Residual             0.3373   0.5808        \r\nNumber of obs: 246, groups:  id, 82\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)  0.65130    0.10573   6.160\r\nage_14       0.27065    0.06284   4.307\r\n\r\nCorrelation of Fixed Effects:\r\n       (Intr)\r\nage_14 -0.441\r\n\r\nIn de resultaten zien we dat het random deel van het model nu twee coëfficiënten heeft die variëren naar id. Het “(Intercept)”, 0.65130, staat voor de variatie tussen het beginpunt van het onderzoek (\\(\\gamma_{0i})\\), terwijl de coëfficiënt voor age_14, 0.271, staat voor de variatie tussen de veranderingssnelheden ( \\(\\gamma_{1i}\\) ).\r\nAls wij nu de voorspellingen onderzoeken, zien wij dat individuen zowel verschillende beginpunten als verschillende trends mogen hebben:\r\n\r\n\r\n\r\nHoe groter de \\(e_{0i}\\)-coëfficiënt, hoe groter het verschil tussen de mensen aan het begin van het onderzoek, terwijl een grotere \\(e_{1i}\\) op meer uiteenlopende veranderingssnelheden wijst.\r\nConclusies\r\nDit geeft een idee wat multilevel model voor verandering ons in onderzoek kan bieden, hoe je het kunt schatten in R en hoe je deze verandering kunt visualiseren. Dit model is vergelijkbaar met het Latent Growth Model (Latent Groei Model) en daar zal ik ook een stukje over schrijven.\r\nMet dank aan Alexander Cernat\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-03-multilevel-modeling/multilevel-modeling_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-03-ontrafelen/",
    "title": "Opschonen en ontrafelen",
    "description": "Een post over opschonen en ontrafelen van data",
    "author": [
      {
        "name": "Bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-10-03",
    "categories": [],
    "contents": "\r\n\r\nAls je data krijgt is het belangrijk om over enkele technieken te beschikken de data goed te ontrafelen en schoon te maken. Het pakket tidyverse is een standaardpakket waarmee je goed uit de voeten kunt. Ook het pakket janitor hoort thuis in de gereedschapskist van de analyticus. Een aantal codes van dat janitorpakket wilde ik goed leren kennen. Vandaar deze blog. Daarbij bouw ik voort op deze blog hier.\r\nJanitor\r\nEerst maar eens janitor en tidyverse laden en de data binnenhalen (wel eerst installeren als je deze pakketten niet hebt).\r\n\r\n\r\nlibrary(janitor)\r\nlibrary(tidyverse)\r\nplace_names = read.csv(\"data/GNIS Query Result.csv\")\r\n\r\n\r\n\r\nEerst eens kijken hoe deze data eruit zien?\r\n\r\n\r\nView(place_names)\r\n\r\n\r\n\r\nMee werken\r\nLaten we een beetje met deze data werken. Eerst geven we de kolommen de naam columns om te voorkomen dat het rommelig wordt. Vervolgens gebruiken we de separate() functie om de kolommen te scheiden. Vervolgens filteren we de gegevens tot Berkshire County, omdat bij nadere inspectie van de gegevens duidelijk wordt dat er een paar items van buiten dit gebied zijn opgenomen. Dan gebruiken we mutate() om verder op te ruimen. str_replace() wordt gebruikt om de ID “598673” te vervangen door “598712,” een ID nummer dat al bestaat in de dataset, om zo een duplicaat ID te maken. Tenslotte wordt een extra kolom genaamd “extra_column” aangemaakt met NAs in elke rij:\r\n\r\n\r\ncolnames(place_names) = \"columns\"\r\n\r\nplace_names = \r\n  place_names %>% \r\n  separate(columns, c(\"Feature Name\", \"ID\", \"Class\", \"County\", \"State\", \"Latitude\", \"Longitude\", \"Ele(ft)\", \"Map\", \"BGN Date\", \"Entry Date\"), sep = \"[|]\") %>%\r\n  filter(County == \"Berkshire\") %>% \r\n  mutate(\r\n    ID = str_replace(ID, \"598673\", \"598712\"),\r\n    extra_column = NA\r\n  )\r\n\r\n\r\n\r\nCreëer non_ma_names\r\nVoor we verder gaan, maken we snel ook een tweede dataset aan. Deze noemen we “non_ma_names”, met data die niet van Berkshire County afkomstig zijn. Nogmaals lezen we het “GNIS Query Result.csv” bestand in en scheiden de kolomnamen. Vervolgens gebruiken we de clean_names() functie uit het janitor pakket, die we in de volgende sectie uitvoerig zullen behandelen. Tenslotte gebruiken we as.numeric() en as.factor() in een mutate stap om onze ele_ft variabele te transformeren naar een numerieke variabele en onze map variabele naar een factor:\r\n\r\n\r\nnon_ma_names = read.csv(\"data/GNIS Query Result.csv\")\r\n\r\ncolnames(non_ma_names) = \"columns\"\r\n\r\nnon_ma_names = \r\n  non_ma_names %>% \r\n  separate(columns, c(\"Feature Name\", \"ID\", \"Class\", \"County\", \"State\", \"Latitude\", \"Longitude\", \"Ele(ft)\", \"Map\", \"BGN Date\", \"Entry Date\"), sep = \"[|]\") %>% \r\n  filter(County != \"Berkshire\") %>% \r\n  clean_names() %>% \r\n  mutate(\r\n    ele_ft = as.numeric(ele_ft),\r\n    map = as.factor(map)\r\n  )\r\n\r\n\r\n\r\nLaten we nu eens zien wat janitor kan.\r\nGebruik janitor\r\nrow_to_names()\r\nJe hebt waarschijnlijk al heel wat databestanden ontvangen, waarschijnlijk in .xlsx-formaat. Bovenaan de spreadsheet staan er dan niet zelden een aantal rijen voordat de eigenlijke gegevens beginnen. Deze rijen kunnen leeg zijn of zijn gevuld met informatie en bedrijfslogo’s. Wanneer je dergelijke gegevens in R laadt, kan de inhoud van deze eerste rijen automatisch jouw kolomkoppen en eerste rijen worden. De functie row_to_names() in het janitor-pakket geeft joun de gelegenheid om aan te geven welke rij in jouw dataframe de eigenlijke kolomnamen bevat en om al het andere dat aan die rij voorafgaat te verwijderen. In onze dataset hadden de kolomnamen al de juiste plaats. Maar laten we deze functie toch eens proberen. We doen alsof de kolomnamen in de derde rij staan. We gebruiken de row_to_names()-functie om een nieuw data frame te maken genaamd “test_names”. De row_to_names() functie neemt de volgende argumenten: de gegevensbron, het rijnummer waar de kolomnamen vandaan moeten komen, of die rij moet worden verwijderd uit de gegevens, en of de rijen erboven moeten worden verwijderd uit de gegevens:\r\n\r\n\r\ntest_names = row_to_names(place_names, 3, remove_row = TRUE, remove_rows_above = TRUE)\r\n\r\n\r\n\r\nclean_names()\r\nclean_names() functie wordt vaak gebruikt als je een nieuwe dataset in R laadt. Als je deze functie nog niet gebruikt, raad ik je sterk aan om deze in jouw workflow op te nemen. Het is niet voor niets de meest populaire functie uit het janitor- pakket - het is uiterst nuttig! Laten we even terugkijken naar onze kolomnamen. Er zijn allerlei hoofdletters en spaties (b.v. “Feature Name”, “BGN Date”) en ook symbolen (“Ele(ft)”). De clean_names() functie zet deze allemaal voor ons om naar kleine letters.\r\nHet gebruik van clean_names() is eenvoudig en kan als volgt worden uitgevoerd:\r\n\r\n\r\nplace_names = clean_names(place_names)\r\n\r\n#OR\r\n\r\nplace_names = \r\n  place_names %>% \r\n  clean_names()\r\n\r\n\r\n\r\nZoals je ziet, heeft deze ene functie alle soorten rommelige kolomnamen verwerkt. Alles ziet er nu netjes en opgeruimd uit. Kijk maar eens\r\n\r\n\r\nhead(place_names)\r\n\r\n\r\n            feature_name     id     class    county state latitude\r\n1    A-Z Shopping Center 603538    Locale Berkshire    MA  422755N\r\n2             Abbey Hill 607260    Summit Berkshire    MA  420822N\r\n3             Abbey Lake 617758 Reservoir Berkshire    MA  420818N\r\n4         Abbey Lake Dam 604930       Dam Berkshire    MA  420806N\r\n5            Abbey Swamp 607261     Swamp Berkshire    MA  420822N\r\n6 Abbott Memorial School 598712    School Berkshire    MA  424032N\r\n  longitude ele_ft             map bgn_date  entry_date extra_column\r\n1  0731254W   1037 Pittsfield East          27-AUG-2002           NA\r\n2  0730930W   1798        Monterey          24-FEB-1974           NA\r\n3  0730908W   1463        Monterey          24-FEB-1974           NA\r\n4  0730910W   1486        Monterey          27-AUG-2002           NA\r\n5  0730930W   1798        Monterey          24-FEB-1974           NA\r\n6  0730213W   1696     North Adams          27-AUG-2002           NA\r\n\r\nremove_empty()\r\nDe remove_empty() functie verwijdert, zoals de naam al zegt, kolommen die leeg zijn. We hebben een lege kolom gemaakt in ons “place_names” dataframe tijdens het voorbereiden van onze gegevens, dus we weten dat ten minste één kolom door deze functie zou moeten worden beïnvloed. Laten we het eens uitproberen:\r\n\r\n\r\nplace_names = \r\n  place_names %>% \r\n  remove_empty()\r\n\r\n\r\n\r\nZoals je kunt zien is de lege kolom (‘extra_column’) verdwenen en zijn er niet meer 12 maar 11 variabelen over.\r\nDe bgn_date-kolom lijkt leeg, maar het feit dat deze niet is verwijderd door remove_empty() vertelt ons dat er in ieder geval in één rij gegevens moeten zitten. Scroll maar in de dataset naar beneden en dan zie je het.\r\nremove_constant()\r\nDe remove_constant() functie verwijdert kolommen met dezelfde waarde in alle rijen. Onze dataset heeft er momenteel twee - omdat we de data hebben gefilterd tot Berkshire County, en heel Berkshire County in Massachusetts ligt, is county = “Berkshire” en staat = “MA” voor alle rijen. Deze rijen zijn niet bijzonder nuttig om in de dataset te houden omdat ze geen rij-specifieke informatie geven. We zouden simpelweg select() kunnen gebruiken om deze kolommen te verwijderen, maar het voordeel van remove_constant() is dat deze functie de aanname alle gegevens hetzelfde zijn, dubbel controleert. In feite, door het gebruik van remove_constant() werd ook duidelijk dat 38 van de 1968 items in de ruwe data eigenlijk niet van Berkshire Country waren! Net als remove_empty(), is alle informatie die de remove_constant() functie nodig heeft, de dataset waarop het moet werken:\r\n\r\n\r\nplace_names = \r\n  place_names %>% \r\n  remove_constant()\r\n\r\n\r\n\r\nZoals je kunt zien, zijn de variabelen Berkshire(county) en MA(staat) nu er uit en zijn er nog negen variabelen over.\r\ncompare_df_cols()\r\nOoit geprobeerd om rbind() te gebruiken om twee data frames te stapelen en tegen een onverwachte fout aangelopen? De compare_df_cols() functie vergelijkt direct de kolommen in twee dataframes en is ongelooflijk handig voor het oplossen van dit probleem. Laten we het eens proberen door ons “place_names” data frame te vergelijken met het data frame dat we hebben gemaakt met gegevens buiten Berkshire County, “non_ma_names”:\r\n\r\n\r\ncompare_df_cols(place_names, non_ma_names)\r\n\r\n\r\n    column_name place_names non_ma_names\r\n1      bgn_date   character    character\r\n2         class   character    character\r\n3        county        <NA>    character\r\n4        ele_ft   character      numeric\r\n5    entry_date   character    character\r\n6  feature_name   character    character\r\n7            id   character    character\r\n8      latitude   character    character\r\n9     longitude   character    character\r\n10          map   character       factor\r\n11        state        <NA>    character\r\n\r\nDe output is een handige tabel waarin de twee dataframes worden vergeleken. We zien “NA” voor county en state in place_names en “character” voor deze variabelen in non_ma_names. Dit komt omdat we deze kolommen met remove_constant() uit place_names hebben verwijderd, maar nooit iets hebben gedaan met de standaard karaktervariabelen in non_ma_names. We zien ook ele_ft als numeriek en map als een factor variabele in non_ma_names, die we specifiek hebben aangewezen tijdens datavoorbereiding. Als we deze dataframes zouden proberen samen te voegen, zou het nuttig zijn te weten welke kolommen ontbreken en welke kolommen inconsistente types hebben in de dataframes. In dataframes met veel kolommen kan compare_df_cols() de tijd die nodig is om deze vergelijkingen te maken, aanzienlijk verminderen.\r\nget_dupes()\r\nIk heb vaak gewerkt aan projecten met unieke patiënt-ID’s waarvan je niet verwacht dat ze dubbel voorkomen in je dataset. Er zijn tal van andere gevallen waarin je ervoor zou willen zorgen dat een ID-variabele volledig unieke waarden heeft, waaronder onze GNIS-gegevens. Zoals je je zult herinneren, hebben we een dubbele ID aangemaakt toen we onze data voorbereidden. Laten we eens kijken hoe get_dupes() dit detecteert. De functie heeft enkel de naam van ons dataframe nodig en de naam van de kolom die als identifier fungeert:\r\n\r\n\r\nget_dupes(place_names, id)\r\n\r\n\r\n      id dupe_count           feature_name  class latitude longitude\r\n1 598712          2 Abbott Memorial School School  424032N  0730213W\r\n2 598712          2      Abby Lodge School School  422440N  0731503W\r\n  ele_ft             map bgn_date  entry_date\r\n1   1696     North Adams          27-AUG-2002\r\n2   1076 Pittsfield West          27-AUG-2002\r\n\r\nZoals hieronder getoond, wordt het dataframe gefilterd tot de rijen met dubbele waarden in de kolom ID, zodat eventuele problemen gemakkelijk kunnen worden onderzocht:\r\ntabyl()\r\nDe tabyl() functie is vergelijkbaar met de table() functie van tidyverse. Het is ook compatibel met het knitr pakket, en is erg handig voor data exploratie.Laten we het eerst uitproberen met een enkele variabele. Stel dat we geïnteresseerd zijn in hoeveel scholen er zijn in elk van de steden in Berkshire County. We filteren eerst onze klasse variabele op “School”, en gebruiken dan de tabyl() functie met onze map(locatie) variabele. Tenslotte pijpen we dat in knitr::kable() om de uitvoer in een mooie tabel te formatteren:\r\n\r\n\r\nplace_names %>% \r\n  filter(class %in% \"School\") %>% \r\n  tabyl(map) %>% \r\n  knitr::kable()\r\n\r\n\r\nmap\r\nn\r\npercent\r\nBash Bish Falls\r\n2\r\n0.0143885\r\nBecket\r\n4\r\n0.0287770\r\nCheshire\r\n3\r\n0.0215827\r\nEast Lee\r\n3\r\n0.0215827\r\nEgremont\r\n3\r\n0.0215827\r\nGreat Barrington\r\n8\r\n0.0575540\r\nHancock\r\n1\r\n0.0071942\r\nMonterey\r\n2\r\n0.0143885\r\nNorth Adams\r\n15\r\n0.1079137\r\nPeru\r\n1\r\n0.0071942\r\nPittsfield East\r\n41\r\n0.2949640\r\nPittsfield West\r\n18\r\n0.1294964\r\nStockbridge\r\n21\r\n0.1510791\r\nTolland Center\r\n1\r\n0.0071942\r\nWilliamstown\r\n10\r\n0.0719424\r\nWindsor\r\n6\r\n0.0431655\r\n\r\nHet uitvoeren van deze zeer eenvoudige code levert de volgende uitvoertabel op:\r\nWanneer we ons Rmd bestand ‘knitten’, zal de kable() functie de tabel mooi opmaken, zoals hierboven getoond. We krijgen een aantal scholen in elke stad, evenals het percentage van alle scholen in die stad. Het is gemakkelijk om opmerkingen te maken over deze gegevens, zoals dat 29,5% van alle scholen in Pittsfield East zijn, dat 41 scholen telt. Of dat 3 steden zo klein zijn dat ze maar 1 school hebben:\r\nLaten we nu de kruistabellen van twee variabelen proberen. Laten we eens kijken hoeveel herkenningspunten van elk type aanwezig zijn in elke stad:\r\n\r\n\r\nplace_names %>% \r\n  tabyl(map, class) %>% \r\n knitr::kable()\r\n\r\n\r\nmap\r\nAirport\r\nArch\r\nBasin\r\nBay\r\nBench\r\nBridge\r\nBuilding\r\nCape\r\nCemetery\r\nCensus\r\nChurch\r\nCivil\r\nCliff\r\nCrossing\r\nDam\r\nFalls\r\nFlat\r\nForest\r\nGap\r\nHospital\r\nIsland\r\nLake\r\nLocale\r\nMilitary\r\nPark\r\nPopulated Place\r\nPost Office\r\nRange\r\nRapids\r\nReserve\r\nReservoir\r\nRidge\r\nSchool\r\nSpring\r\nStream\r\nSummit\r\nSwamp\r\nTower\r\nTrail\r\nValley\r\nWoods\r\nAshley Falls\r\n1\r\n0\r\n0\r\n0\r\n0\r\n2\r\n7\r\n0\r\n5\r\n0\r\n1\r\n1\r\n0\r\n0\r\n2\r\n1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n2\r\n6\r\n3\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n8\r\n8\r\n1\r\n0\r\n0\r\n1\r\n0\r\nBash Bish Falls\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n1\r\n1\r\n0\r\n0\r\n5\r\n3\r\n0\r\n1\r\n0\r\n0\r\n1\r\n6\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\n0\r\n0\r\n5\r\n0\r\n2\r\n0\r\n8\r\n9\r\n1\r\n2\r\n3\r\n2\r\n0\r\nBecket\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n8\r\n0\r\n1\r\n0\r\n1\r\n1\r\n0\r\n0\r\n8\r\n0\r\n0\r\n2\r\n0\r\n0\r\n0\r\n2\r\n0\r\n0\r\n2\r\n8\r\n1\r\n0\r\n0\r\n0\r\n7\r\n0\r\n4\r\n0\r\n13\r\n5\r\n0\r\n0\r\n0\r\n0\r\n0\r\nBerlin\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n3\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n2\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\nBristol\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\nCanaan\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\nCheshire\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n12\r\n0\r\n5\r\n1\r\n4\r\n3\r\n1\r\n0\r\n2\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n4\r\n6\r\n0\r\n2\r\n11\r\n3\r\n0\r\n0\r\n0\r\n2\r\n0\r\n3\r\n0\r\n14\r\n15\r\n0\r\n0\r\n1\r\n0\r\n0\r\nChester\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\nCopake\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\nEast Lee\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n6\r\n0\r\n1\r\n1\r\n4\r\n3\r\n0\r\n1\r\n8\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n4\r\n3\r\n0\r\n0\r\n6\r\n1\r\n0\r\n0\r\n0\r\n12\r\n0\r\n3\r\n0\r\n14\r\n5\r\n1\r\n0\r\n0\r\n0\r\n0\r\nEgremont\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n8\r\n0\r\n3\r\n0\r\n1\r\n2\r\n1\r\n0\r\n3\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n4\r\n1\r\n0\r\n3\r\n4\r\n1\r\n0\r\n0\r\n0\r\n5\r\n0\r\n3\r\n0\r\n6\r\n10\r\n0\r\n1\r\n0\r\n0\r\n0\r\nGreat Barrington\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n6\r\n0\r\n3\r\n2\r\n6\r\n1\r\n0\r\n0\r\n7\r\n0\r\n0\r\n2\r\n2\r\n1\r\n0\r\n6\r\n5\r\n0\r\n2\r\n7\r\n1\r\n0\r\n0\r\n0\r\n9\r\n0\r\n8\r\n1\r\n4\r\n9\r\n1\r\n1\r\n1\r\n1\r\n0\r\nHancock\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n3\r\n0\r\n2\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n2\r\n0\r\n2\r\n2\r\n1\r\n0\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\n8\r\n13\r\n0\r\n0\r\n0\r\n3\r\n0\r\nMonterey\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n5\r\n0\r\n2\r\n0\r\n3\r\n2\r\n0\r\n0\r\n11\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n3\r\n4\r\n0\r\n1\r\n5\r\n2\r\n1\r\n0\r\n0\r\n13\r\n0\r\n2\r\n0\r\n6\r\n13\r\n6\r\n0\r\n0\r\n1\r\n0\r\nNorth Adams\r\n0\r\n1\r\n0\r\n0\r\n2\r\n0\r\n19\r\n0\r\n9\r\n1\r\n7\r\n4\r\n1\r\n0\r\n7\r\n1\r\n0\r\n1\r\n1\r\n1\r\n0\r\n6\r\n4\r\n0\r\n14\r\n11\r\n1\r\n1\r\n0\r\n0\r\n8\r\n0\r\n15\r\n0\r\n22\r\n6\r\n1\r\n3\r\n1\r\n0\r\n0\r\nOtis\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n4\r\n1\r\n2\r\n0\r\n0\r\n1\r\n0\r\n0\r\n9\r\n1\r\n0\r\n0\r\n0\r\n0\r\n2\r\n10\r\n4\r\n0\r\n0\r\n6\r\n2\r\n0\r\n0\r\n0\r\n11\r\n1\r\n0\r\n0\r\n8\r\n7\r\n0\r\n0\r\n0\r\n0\r\n0\r\nPeru\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n5\r\n0\r\n2\r\n0\r\n0\r\n2\r\n0\r\n0\r\n4\r\n1\r\n0\r\n1\r\n0\r\n0\r\n0\r\n3\r\n1\r\n0\r\n4\r\n6\r\n0\r\n0\r\n0\r\n2\r\n4\r\n0\r\n1\r\n0\r\n15\r\n7\r\n0\r\n0\r\n0\r\n0\r\n0\r\nPittsfield East\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n29\r\n0\r\n6\r\n0\r\n37\r\n1\r\n0\r\n0\r\n7\r\n0\r\n0\r\n0\r\n1\r\n2\r\n0\r\n2\r\n19\r\n0\r\n16\r\n21\r\n59\r\n0\r\n0\r\n0\r\n16\r\n1\r\n41\r\n0\r\n14\r\n9\r\n0\r\n2\r\n0\r\n0\r\n0\r\nPittsfield West\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n24\r\n1\r\n7\r\n0\r\n19\r\n2\r\n1\r\n0\r\n2\r\n1\r\n0\r\n1\r\n0\r\n1\r\n0\r\n5\r\n17\r\n0\r\n9\r\n9\r\n1\r\n1\r\n0\r\n0\r\n3\r\n1\r\n18\r\n0\r\n25\r\n14\r\n1\r\n7\r\n3\r\n2\r\n0\r\nPlainfield\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n2\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n8\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\nRowe\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n3\r\n0\r\n1\r\n2\r\n0\r\n0\r\n0\r\n0\r\n2\r\n1\r\n0\r\n0\r\n13\r\n2\r\n0\r\n0\r\n0\r\n0\r\n0\r\nSouth Sandisfield\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n2\r\n0\r\n5\r\n0\r\n2\r\n1\r\n0\r\n0\r\n6\r\n1\r\n0\r\n2\r\n0\r\n0\r\n0\r\n3\r\n1\r\n0\r\n0\r\n6\r\n1\r\n0\r\n0\r\n0\r\n4\r\n0\r\n0\r\n0\r\n2\r\n7\r\n2\r\n0\r\n0\r\n0\r\n0\r\nSouthbury\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\nState Line\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n4\r\n0\r\n0\r\n1\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n6\r\n2\r\n0\r\n0\r\n7\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n5\r\n4\r\n0\r\n0\r\n0\r\n0\r\n0\r\nStockbridge\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n35\r\n0\r\n7\r\n2\r\n6\r\n2\r\n0\r\n1\r\n7\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\n10\r\n7\r\n0\r\n7\r\n18\r\n8\r\n0\r\n0\r\n0\r\n6\r\n0\r\n21\r\n0\r\n20\r\n10\r\n1\r\n4\r\n0\r\n1\r\n1\r\nTolland Center\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n3\r\n0\r\n1\r\n0\r\n0\r\n1\r\n0\r\n0\r\n4\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n2\r\n1\r\n0\r\n0\r\n3\r\n2\r\n0\r\n0\r\n0\r\n3\r\n0\r\n1\r\n0\r\n6\r\n6\r\n0\r\n0\r\n0\r\n0\r\n0\r\nWilliamstown\r\n3\r\n0\r\n1\r\n0\r\n0\r\n2\r\n19\r\n1\r\n4\r\n1\r\n14\r\n1\r\n1\r\n0\r\n3\r\n1\r\n0\r\n1\r\n1\r\n0\r\n0\r\n1\r\n13\r\n2\r\n13\r\n13\r\n1\r\n1\r\n1\r\n0\r\n5\r\n1\r\n10\r\n2\r\n20\r\n19\r\n1\r\n3\r\n6\r\n4\r\n0\r\nWindsor\r\n2\r\n0\r\n0\r\n0\r\n0\r\n0\r\n10\r\n0\r\n4\r\n0\r\n4\r\n2\r\n0\r\n0\r\n1\r\n1\r\n0\r\n2\r\n0\r\n2\r\n0\r\n0\r\n2\r\n0\r\n5\r\n6\r\n2\r\n0\r\n0\r\n4\r\n1\r\n0\r\n6\r\n0\r\n12\r\n9\r\n0\r\n1\r\n0\r\n0\r\n0\r\nWorthington\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n6\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n\r\nEen deel van onze tabel (eenmaal ‘geknit’) is hierboven afgebeeld. Voor elke stad kunnen we duidelijk zien hoeveel van elk oriëntatiepunttype er in de database zitten:\r\nHoewel eenvoudige tellingen als deze heel nuttig kunnen zijn, geven we misschien meer om kolompercentages. Met andere woorden, hoeveel procent van de items voor elk oriëntatiepunttype zijn er in elke stad? Dit is gemakkelijk te onderzoeken met tabyl() via de adorn_percentages() functie:\r\n\r\n\r\nplace_names %>% \r\n  tabyl(map, class) %>% \r\n  adorn_percentages(\"col\") %>% \r\n  knitr::kable()\r\n\r\n\r\nmap\r\nAirport\r\nArch\r\nBasin\r\nBay\r\nBench\r\nBridge\r\nBuilding\r\nCape\r\nCemetery\r\nCensus\r\nChurch\r\nCivil\r\nCliff\r\nCrossing\r\nDam\r\nFalls\r\nFlat\r\nForest\r\nGap\r\nHospital\r\nIsland\r\nLake\r\nLocale\r\nMilitary\r\nPark\r\nPopulated Place\r\nPost Office\r\nRange\r\nRapids\r\nReserve\r\nReservoir\r\nRidge\r\nSchool\r\nSpring\r\nStream\r\nSummit\r\nSwamp\r\nTower\r\nTrail\r\nValley\r\nWoods\r\nAshley Falls\r\n0.0909091\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.4\r\n0.0341463\r\n0.0000000\r\n0.0649351\r\n0.000\r\n0.0090090\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0204082\r\n0.0833333\r\n1\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0235294\r\n0.03750\r\n0.0326087\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0306513\r\n0.0421053\r\n0.0625\r\n0.0000000\r\n0.0000000\r\n0.0625\r\n0\r\nBash Bish Falls\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0129870\r\n0.000\r\n0.0090090\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0510204\r\n0.2500000\r\n0\r\n0.0625\r\n0.0000000\r\n0.000\r\n0.25\r\n0.0769231\r\n0.0000000\r\n0\r\n0.0000000\r\n0.00625\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0431034\r\n0.0000000\r\n0.0143885\r\n0.00\r\n0.0306513\r\n0.0473684\r\n0.0625\r\n0.0833333\r\n0.1764706\r\n0.1250\r\n0\r\nBecket\r\n0.0000000\r\n0\r\n0.3333333\r\n0\r\n0\r\n0.0\r\n0.0390244\r\n0.0000000\r\n0.0129870\r\n0.000\r\n0.0090090\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0816327\r\n0.0000000\r\n0\r\n0.1250\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0256410\r\n0.0000000\r\n0\r\n0.0235294\r\n0.05000\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0603448\r\n0.0000000\r\n0.0287770\r\n0.00\r\n0.0498084\r\n0.0263158\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nBerlin\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0090090\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0625\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0303030\r\n0\r\n0.0117647\r\n0.00000\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.25\r\n0.0076628\r\n0.0000000\r\n0.0000\r\n0.0000000\r\n0.0588235\r\n0.0625\r\n0\r\nBristol\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000000\r\n0.00000\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0.0000000\r\n0.0588235\r\n0.0000\r\n0\r\nCanaan\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0101010\r\n0\r\n0.0000000\r\n0.00000\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0000000\r\n0.0052632\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nCheshire\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0585366\r\n0.0000000\r\n0.0649351\r\n0.125\r\n0.0360360\r\n0.0909091\r\n0.2\r\n0.0000000\r\n0.0204082\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.25\r\n0.0512821\r\n0.0606061\r\n0\r\n0.0235294\r\n0.06875\r\n0.0326087\r\n0.00\r\n0\r\n0.0000000\r\n0.0172414\r\n0.0000000\r\n0.0215827\r\n0.00\r\n0.0536398\r\n0.0789474\r\n0.0000\r\n0.0000000\r\n0.0588235\r\n0.0000\r\n0\r\nChester\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000000\r\n0.00000\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0038314\r\n0.0000000\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nCopake\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000000\r\n0.00000\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0038314\r\n0.0052632\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nEast Lee\r\n0.0909091\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0292683\r\n0.0000000\r\n0.0129870\r\n0.125\r\n0.0360360\r\n0.0909091\r\n0.0\r\n0.3333333\r\n0.0816327\r\n0.0000000\r\n0\r\n0.0625\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0512821\r\n0.0303030\r\n0\r\n0.0000000\r\n0.03750\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.1034483\r\n0.0000000\r\n0.0215827\r\n0.00\r\n0.0536398\r\n0.0263158\r\n0.0625\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nEgremont\r\n0.0909091\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0390244\r\n0.0000000\r\n0.0389610\r\n0.000\r\n0.0090090\r\n0.0606061\r\n0.2\r\n0.0000000\r\n0.0306122\r\n0.0000000\r\n0\r\n0.0625\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0512821\r\n0.0101010\r\n0\r\n0.0352941\r\n0.02500\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0431034\r\n0.0000000\r\n0.0215827\r\n0.00\r\n0.0229885\r\n0.0526316\r\n0.0000\r\n0.0416667\r\n0.0000000\r\n0.0000\r\n0\r\nGreat Barrington\r\n0.0000000\r\n0\r\n0.3333333\r\n0\r\n0\r\n0.0\r\n0.0292683\r\n0.0000000\r\n0.0389610\r\n0.250\r\n0.0540541\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0714286\r\n0.0000000\r\n0\r\n0.1250\r\n0.3333333\r\n0.125\r\n0.00\r\n0.0769231\r\n0.0505051\r\n0\r\n0.0235294\r\n0.04375\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0775862\r\n0.0000000\r\n0.0575540\r\n0.25\r\n0.0153257\r\n0.0473684\r\n0.0625\r\n0.0416667\r\n0.0588235\r\n0.0625\r\n0\r\nHancock\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0146341\r\n0.0000000\r\n0.0259740\r\n0.000\r\n0.0000000\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0128205\r\n0.0202020\r\n0\r\n0.0235294\r\n0.01250\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.1666667\r\n0.0071942\r\n0.00\r\n0.0306513\r\n0.0684211\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.1875\r\n0\r\nMonterey\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0243902\r\n0.0000000\r\n0.0259740\r\n0.000\r\n0.0270270\r\n0.0606061\r\n0.0\r\n0.0000000\r\n0.1122449\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0384615\r\n0.0404040\r\n0\r\n0.0117647\r\n0.03125\r\n0.0217391\r\n0.25\r\n0\r\n0.0000000\r\n0.1120690\r\n0.0000000\r\n0.0143885\r\n0.00\r\n0.0229885\r\n0.0684211\r\n0.3750\r\n0.0000000\r\n0.0000000\r\n0.0625\r\n0\r\nNorth Adams\r\n0.0000000\r\n1\r\n0.0000000\r\n0\r\n1\r\n0.0\r\n0.0926829\r\n0.0000000\r\n0.1168831\r\n0.125\r\n0.0630631\r\n0.1212121\r\n0.2\r\n0.0000000\r\n0.0714286\r\n0.0833333\r\n0\r\n0.0625\r\n0.1666667\r\n0.125\r\n0.00\r\n0.0769231\r\n0.0404040\r\n0\r\n0.1647059\r\n0.06875\r\n0.0108696\r\n0.25\r\n0\r\n0.0000000\r\n0.0689655\r\n0.0000000\r\n0.1079137\r\n0.00\r\n0.0842912\r\n0.0315789\r\n0.0625\r\n0.1250000\r\n0.0588235\r\n0.0000\r\n0\r\nOtis\r\n0.0000000\r\n0\r\n0.0000000\r\n1\r\n0\r\n0.0\r\n0.0195122\r\n0.3333333\r\n0.0259740\r\n0.000\r\n0.0000000\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0918367\r\n0.0833333\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.50\r\n0.1282051\r\n0.0404040\r\n0\r\n0.0000000\r\n0.03750\r\n0.0217391\r\n0.00\r\n0\r\n0.0000000\r\n0.0948276\r\n0.1666667\r\n0.0000000\r\n0.00\r\n0.0306513\r\n0.0368421\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nPeru\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0243902\r\n0.0000000\r\n0.0259740\r\n0.000\r\n0.0000000\r\n0.0606061\r\n0.0\r\n0.0000000\r\n0.0408163\r\n0.0833333\r\n0\r\n0.0625\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0384615\r\n0.0101010\r\n0\r\n0.0470588\r\n0.03750\r\n0.0000000\r\n0.00\r\n0\r\n0.3333333\r\n0.0344828\r\n0.0000000\r\n0.0071942\r\n0.00\r\n0.0574713\r\n0.0368421\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nPittsfield East\r\n0.0909091\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.1414634\r\n0.0000000\r\n0.0779221\r\n0.000\r\n0.3333333\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0714286\r\n0.0000000\r\n0\r\n0.0000\r\n0.1666667\r\n0.250\r\n0.00\r\n0.0256410\r\n0.1919192\r\n0\r\n0.1882353\r\n0.13125\r\n0.6413043\r\n0.00\r\n0\r\n0.0000000\r\n0.1379310\r\n0.1666667\r\n0.2949640\r\n0.00\r\n0.0536398\r\n0.0473684\r\n0.0000\r\n0.0833333\r\n0.0000000\r\n0.0000\r\n0\r\nPittsfield West\r\n0.0909091\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.1170732\r\n0.3333333\r\n0.0909091\r\n0.000\r\n0.1711712\r\n0.0606061\r\n0.2\r\n0.0000000\r\n0.0204082\r\n0.0833333\r\n0\r\n0.0625\r\n0.0000000\r\n0.125\r\n0.00\r\n0.0641026\r\n0.1717172\r\n0\r\n0.1058824\r\n0.05625\r\n0.0108696\r\n0.25\r\n0\r\n0.0000000\r\n0.0258621\r\n0.1666667\r\n0.1294964\r\n0.00\r\n0.0957854\r\n0.0736842\r\n0.0625\r\n0.2916667\r\n0.1764706\r\n0.1250\r\n0\r\nPlainfield\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0259740\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0833333\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000000\r\n0.00625\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0306513\r\n0.0052632\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nRowe\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.2\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.3333333\r\n0.0102041\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0303030\r\n0\r\n0.0117647\r\n0.01250\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0172414\r\n0.1666667\r\n0.0000000\r\n0.00\r\n0.0498084\r\n0.0105263\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nSouth Sandisfield\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0097561\r\n0.0000000\r\n0.0649351\r\n0.000\r\n0.0180180\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0612245\r\n0.0833333\r\n0\r\n0.1250\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0384615\r\n0.0101010\r\n0\r\n0.0000000\r\n0.03750\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0344828\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0076628\r\n0.0368421\r\n0.1250\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nSouthbury\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0117647\r\n0.00000\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nState Line\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0519481\r\n0.000\r\n0.0000000\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0102041\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0769231\r\n0.0202020\r\n0\r\n0.0000000\r\n0.04375\r\n0.0108696\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0191571\r\n0.0210526\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nStockbridge\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.1707317\r\n0.0000000\r\n0.0909091\r\n0.250\r\n0.0540541\r\n0.0606061\r\n0.0\r\n0.3333333\r\n0.0714286\r\n0.0000000\r\n0\r\n0.0000\r\n0.1666667\r\n0.125\r\n0.00\r\n0.1282051\r\n0.0707071\r\n0\r\n0.0823529\r\n0.11250\r\n0.0869565\r\n0.00\r\n0\r\n0.0000000\r\n0.0517241\r\n0.0000000\r\n0.1510791\r\n0.00\r\n0.0766284\r\n0.0526316\r\n0.0625\r\n0.1666667\r\n0.0000000\r\n0.0625\r\n1\r\nTolland Center\r\n0.0000000\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0146341\r\n0.0000000\r\n0.0129870\r\n0.000\r\n0.0000000\r\n0.0303030\r\n0.0\r\n0.0000000\r\n0.0408163\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0256410\r\n0.0101010\r\n0\r\n0.0000000\r\n0.01875\r\n0.0217391\r\n0.00\r\n0\r\n0.0000000\r\n0.0258621\r\n0.0000000\r\n0.0071942\r\n0.00\r\n0.0229885\r\n0.0315789\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\nWilliamstown\r\n0.2727273\r\n0\r\n0.3333333\r\n0\r\n0\r\n0.4\r\n0.0926829\r\n0.3333333\r\n0.0519481\r\n0.125\r\n0.1261261\r\n0.0303030\r\n0.2\r\n0.0000000\r\n0.0306122\r\n0.0833333\r\n0\r\n0.0625\r\n0.1666667\r\n0.000\r\n0.00\r\n0.0128205\r\n0.1313131\r\n1\r\n0.1529412\r\n0.08125\r\n0.0108696\r\n0.25\r\n1\r\n0.0000000\r\n0.0431034\r\n0.1666667\r\n0.0719424\r\n0.50\r\n0.0766284\r\n0.1000000\r\n0.0625\r\n0.1250000\r\n0.3529412\r\n0.2500\r\n0\r\nWindsor\r\n0.1818182\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0487805\r\n0.0000000\r\n0.0519481\r\n0.000\r\n0.0360360\r\n0.0606061\r\n0.0\r\n0.0000000\r\n0.0102041\r\n0.0833333\r\n0\r\n0.1250\r\n0.0000000\r\n0.250\r\n0.00\r\n0.0000000\r\n0.0202020\r\n0\r\n0.0588235\r\n0.03750\r\n0.0217391\r\n0.00\r\n0\r\n0.6666667\r\n0.0086207\r\n0.0000000\r\n0.0431655\r\n0.00\r\n0.0459770\r\n0.0473684\r\n0.0000\r\n0.0416667\r\n0.0000000\r\n0.0000\r\n0\r\nWorthington\r\n0.0909091\r\n0\r\n0.0000000\r\n0\r\n0\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0129870\r\n0.000\r\n0.0000000\r\n0.0000000\r\n0.0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000\r\n0.0000000\r\n0.000\r\n0.00\r\n0.0000000\r\n0.0000000\r\n0\r\n0.0000000\r\n0.00625\r\n0.0000000\r\n0.00\r\n0\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.0000000\r\n0.00\r\n0.0229885\r\n0.0000000\r\n0.0000\r\n0.0000000\r\n0.0000000\r\n0.0000\r\n0\r\n\r\nNu zien we deze kolompercentages in plaats van tellingen, maar de tabel is nogal moeilijk te lezen.\r\nWe kunnen dit een beetje opschonen met de adorn_pct_formatting() functie, die de gebruiker toestaat het aantal decimalen op te geven dat in de uitvoer moet worden opgenomen. Precisie is niet bijzonder belangrijk voor deze verkennende tabel, dus laten we 0 decimalen gebruiken om deze tabel makkelijker leesbaar te maken:\r\n\r\n\r\nplace_names %>% \r\n  tabyl(map, class) %>% \r\n  adorn_percentages(\"col\") %>% \r\n  adorn_pct_formatting(digits = 0) %>% \r\n  knitr::kable()\r\n\r\n\r\nmap\r\nAirport\r\nArch\r\nBasin\r\nBay\r\nBench\r\nBridge\r\nBuilding\r\nCape\r\nCemetery\r\nCensus\r\nChurch\r\nCivil\r\nCliff\r\nCrossing\r\nDam\r\nFalls\r\nFlat\r\nForest\r\nGap\r\nHospital\r\nIsland\r\nLake\r\nLocale\r\nMilitary\r\nPark\r\nPopulated Place\r\nPost Office\r\nRange\r\nRapids\r\nReserve\r\nReservoir\r\nRidge\r\nSchool\r\nSpring\r\nStream\r\nSummit\r\nSwamp\r\nTower\r\nTrail\r\nValley\r\nWoods\r\nAshley Falls\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n40%\r\n3%\r\n0%\r\n6%\r\n0%\r\n1%\r\n3%\r\n0%\r\n0%\r\n2%\r\n8%\r\n100%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n4%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n4%\r\n6%\r\n0%\r\n0%\r\n6%\r\n0%\r\nBash Bish Falls\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n1%\r\n3%\r\n0%\r\n0%\r\n5%\r\n25%\r\n0%\r\n6%\r\n0%\r\n0%\r\n25%\r\n8%\r\n0%\r\n0%\r\n0%\r\n1%\r\n1%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n1%\r\n0%\r\n3%\r\n5%\r\n6%\r\n8%\r\n18%\r\n12%\r\n0%\r\nBecket\r\n0%\r\n0%\r\n33%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n1%\r\n0%\r\n1%\r\n3%\r\n0%\r\n0%\r\n8%\r\n0%\r\n0%\r\n12%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n2%\r\n5%\r\n1%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n3%\r\n0%\r\n5%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nBerlin\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n25%\r\n1%\r\n0%\r\n0%\r\n0%\r\n6%\r\n6%\r\n0%\r\nBristol\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\nCanaan\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nCheshire\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n6%\r\n12%\r\n4%\r\n9%\r\n20%\r\n0%\r\n2%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n25%\r\n5%\r\n6%\r\n0%\r\n2%\r\n7%\r\n3%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n2%\r\n0%\r\n5%\r\n8%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\nChester\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nCopake\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nEast Lee\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n12%\r\n4%\r\n9%\r\n0%\r\n33%\r\n8%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n5%\r\n3%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n10%\r\n0%\r\n2%\r\n0%\r\n5%\r\n3%\r\n6%\r\n0%\r\n0%\r\n0%\r\n0%\r\nEgremont\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n4%\r\n0%\r\n1%\r\n6%\r\n20%\r\n0%\r\n3%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n5%\r\n1%\r\n0%\r\n4%\r\n2%\r\n1%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n2%\r\n0%\r\n2%\r\n5%\r\n0%\r\n4%\r\n0%\r\n0%\r\n0%\r\nGreat Barrington\r\n0%\r\n0%\r\n33%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n4%\r\n25%\r\n5%\r\n3%\r\n0%\r\n0%\r\n7%\r\n0%\r\n0%\r\n12%\r\n33%\r\n12%\r\n0%\r\n8%\r\n5%\r\n0%\r\n2%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n8%\r\n0%\r\n6%\r\n25%\r\n2%\r\n5%\r\n6%\r\n4%\r\n6%\r\n6%\r\n0%\r\nHancock\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n3%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n2%\r\n0%\r\n2%\r\n1%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n17%\r\n1%\r\n0%\r\n3%\r\n7%\r\n0%\r\n0%\r\n0%\r\n19%\r\n0%\r\nMonterey\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n3%\r\n0%\r\n3%\r\n6%\r\n0%\r\n0%\r\n11%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n4%\r\n4%\r\n0%\r\n1%\r\n3%\r\n2%\r\n25%\r\n0%\r\n0%\r\n11%\r\n0%\r\n1%\r\n0%\r\n2%\r\n7%\r\n38%\r\n0%\r\n0%\r\n6%\r\n0%\r\nNorth Adams\r\n0%\r\n100%\r\n0%\r\n0%\r\n100%\r\n0%\r\n9%\r\n0%\r\n12%\r\n12%\r\n6%\r\n12%\r\n20%\r\n0%\r\n7%\r\n8%\r\n0%\r\n6%\r\n17%\r\n12%\r\n0%\r\n8%\r\n4%\r\n0%\r\n16%\r\n7%\r\n1%\r\n25%\r\n0%\r\n0%\r\n7%\r\n0%\r\n11%\r\n0%\r\n8%\r\n3%\r\n6%\r\n12%\r\n6%\r\n0%\r\n0%\r\nOtis\r\n0%\r\n0%\r\n0%\r\n100%\r\n0%\r\n0%\r\n2%\r\n33%\r\n3%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n9%\r\n8%\r\n0%\r\n0%\r\n0%\r\n0%\r\n50%\r\n13%\r\n4%\r\n0%\r\n0%\r\n4%\r\n2%\r\n0%\r\n0%\r\n0%\r\n9%\r\n17%\r\n0%\r\n0%\r\n3%\r\n4%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nPeru\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n3%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n4%\r\n8%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n5%\r\n4%\r\n0%\r\n0%\r\n0%\r\n33%\r\n3%\r\n0%\r\n1%\r\n0%\r\n6%\r\n4%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nPittsfield East\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n14%\r\n0%\r\n8%\r\n0%\r\n33%\r\n3%\r\n0%\r\n0%\r\n7%\r\n0%\r\n0%\r\n0%\r\n17%\r\n25%\r\n0%\r\n3%\r\n19%\r\n0%\r\n19%\r\n13%\r\n64%\r\n0%\r\n0%\r\n0%\r\n14%\r\n17%\r\n29%\r\n0%\r\n5%\r\n5%\r\n0%\r\n8%\r\n0%\r\n0%\r\n0%\r\nPittsfield West\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n12%\r\n33%\r\n9%\r\n0%\r\n17%\r\n6%\r\n20%\r\n0%\r\n2%\r\n8%\r\n0%\r\n6%\r\n0%\r\n12%\r\n0%\r\n6%\r\n17%\r\n0%\r\n11%\r\n6%\r\n1%\r\n25%\r\n0%\r\n0%\r\n3%\r\n17%\r\n13%\r\n0%\r\n10%\r\n7%\r\n6%\r\n29%\r\n18%\r\n12%\r\n0%\r\nPlainfield\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n8%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nRowe\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n20%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n33%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n17%\r\n0%\r\n0%\r\n5%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nSouth Sandisfield\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n6%\r\n0%\r\n2%\r\n3%\r\n0%\r\n0%\r\n6%\r\n8%\r\n0%\r\n12%\r\n0%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n0%\r\n1%\r\n4%\r\n12%\r\n0%\r\n0%\r\n0%\r\n0%\r\nSouthbury\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nState Line\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n5%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n8%\r\n2%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n2%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nStockbridge\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n17%\r\n0%\r\n9%\r\n25%\r\n5%\r\n6%\r\n0%\r\n33%\r\n7%\r\n0%\r\n0%\r\n0%\r\n17%\r\n12%\r\n0%\r\n13%\r\n7%\r\n0%\r\n8%\r\n11%\r\n9%\r\n0%\r\n0%\r\n0%\r\n5%\r\n0%\r\n15%\r\n0%\r\n8%\r\n5%\r\n6%\r\n17%\r\n0%\r\n6%\r\n100%\r\nTolland Center\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n1%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n4%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n1%\r\n0%\r\n0%\r\n2%\r\n2%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n0%\r\n2%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nWilliamstown\r\n27%\r\n0%\r\n33%\r\n0%\r\n0%\r\n40%\r\n9%\r\n33%\r\n5%\r\n12%\r\n13%\r\n3%\r\n20%\r\n0%\r\n3%\r\n8%\r\n0%\r\n6%\r\n17%\r\n0%\r\n0%\r\n1%\r\n13%\r\n100%\r\n15%\r\n8%\r\n1%\r\n25%\r\n100%\r\n0%\r\n4%\r\n17%\r\n7%\r\n50%\r\n8%\r\n10%\r\n6%\r\n12%\r\n35%\r\n25%\r\n0%\r\nWindsor\r\n18%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n5%\r\n0%\r\n5%\r\n0%\r\n4%\r\n6%\r\n0%\r\n0%\r\n1%\r\n8%\r\n0%\r\n12%\r\n0%\r\n25%\r\n0%\r\n0%\r\n2%\r\n0%\r\n6%\r\n4%\r\n2%\r\n0%\r\n0%\r\n67%\r\n1%\r\n0%\r\n4%\r\n0%\r\n5%\r\n5%\r\n0%\r\n4%\r\n0%\r\n0%\r\n0%\r\nWorthington\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n\r\nVeel beter! Nu is het veel gemakkelijker om de tabel te lezen en onze kolompercentages te begrijpen:\r\nHet is net zo eenvoudig om adorn_percentages() te gebruiken om in plaats daarvan naar rij percentages te kijken (in ons geval, het percentage vermeldingen van elke stad dat behoort tot elk oriëntatiepunt type):\r\n\r\n\r\nplace_names %>% \r\n  tabyl(map, class) %>% \r\n  adorn_percentages(\"col\") %>% \r\n  adorn_pct_formatting(digits = 0) %>% \r\n  knitr::kable()\r\n\r\n\r\nmap\r\nAirport\r\nArch\r\nBasin\r\nBay\r\nBench\r\nBridge\r\nBuilding\r\nCape\r\nCemetery\r\nCensus\r\nChurch\r\nCivil\r\nCliff\r\nCrossing\r\nDam\r\nFalls\r\nFlat\r\nForest\r\nGap\r\nHospital\r\nIsland\r\nLake\r\nLocale\r\nMilitary\r\nPark\r\nPopulated Place\r\nPost Office\r\nRange\r\nRapids\r\nReserve\r\nReservoir\r\nRidge\r\nSchool\r\nSpring\r\nStream\r\nSummit\r\nSwamp\r\nTower\r\nTrail\r\nValley\r\nWoods\r\nAshley Falls\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n40%\r\n3%\r\n0%\r\n6%\r\n0%\r\n1%\r\n3%\r\n0%\r\n0%\r\n2%\r\n8%\r\n100%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n4%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n4%\r\n6%\r\n0%\r\n0%\r\n6%\r\n0%\r\nBash Bish Falls\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n1%\r\n3%\r\n0%\r\n0%\r\n5%\r\n25%\r\n0%\r\n6%\r\n0%\r\n0%\r\n25%\r\n8%\r\n0%\r\n0%\r\n0%\r\n1%\r\n1%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n1%\r\n0%\r\n3%\r\n5%\r\n6%\r\n8%\r\n18%\r\n12%\r\n0%\r\nBecket\r\n0%\r\n0%\r\n33%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n1%\r\n0%\r\n1%\r\n3%\r\n0%\r\n0%\r\n8%\r\n0%\r\n0%\r\n12%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n2%\r\n5%\r\n1%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n3%\r\n0%\r\n5%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nBerlin\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n25%\r\n1%\r\n0%\r\n0%\r\n0%\r\n6%\r\n6%\r\n0%\r\nBristol\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\nCanaan\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nCheshire\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n6%\r\n0%\r\n6%\r\n12%\r\n4%\r\n9%\r\n20%\r\n0%\r\n2%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n25%\r\n5%\r\n6%\r\n0%\r\n2%\r\n7%\r\n3%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n2%\r\n0%\r\n5%\r\n8%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\nChester\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nCopake\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nEast Lee\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n12%\r\n4%\r\n9%\r\n0%\r\n33%\r\n8%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n5%\r\n3%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n10%\r\n0%\r\n2%\r\n0%\r\n5%\r\n3%\r\n6%\r\n0%\r\n0%\r\n0%\r\n0%\r\nEgremont\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n4%\r\n0%\r\n1%\r\n6%\r\n20%\r\n0%\r\n3%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n5%\r\n1%\r\n0%\r\n4%\r\n2%\r\n1%\r\n0%\r\n0%\r\n0%\r\n4%\r\n0%\r\n2%\r\n0%\r\n2%\r\n5%\r\n0%\r\n4%\r\n0%\r\n0%\r\n0%\r\nGreat Barrington\r\n0%\r\n0%\r\n33%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n4%\r\n25%\r\n5%\r\n3%\r\n0%\r\n0%\r\n7%\r\n0%\r\n0%\r\n12%\r\n33%\r\n12%\r\n0%\r\n8%\r\n5%\r\n0%\r\n2%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n8%\r\n0%\r\n6%\r\n25%\r\n2%\r\n5%\r\n6%\r\n4%\r\n6%\r\n6%\r\n0%\r\nHancock\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n3%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n2%\r\n0%\r\n2%\r\n1%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n17%\r\n1%\r\n0%\r\n3%\r\n7%\r\n0%\r\n0%\r\n0%\r\n19%\r\n0%\r\nMonterey\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n3%\r\n0%\r\n3%\r\n6%\r\n0%\r\n0%\r\n11%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n4%\r\n4%\r\n0%\r\n1%\r\n3%\r\n2%\r\n25%\r\n0%\r\n0%\r\n11%\r\n0%\r\n1%\r\n0%\r\n2%\r\n7%\r\n38%\r\n0%\r\n0%\r\n6%\r\n0%\r\nNorth Adams\r\n0%\r\n100%\r\n0%\r\n0%\r\n100%\r\n0%\r\n9%\r\n0%\r\n12%\r\n12%\r\n6%\r\n12%\r\n20%\r\n0%\r\n7%\r\n8%\r\n0%\r\n6%\r\n17%\r\n12%\r\n0%\r\n8%\r\n4%\r\n0%\r\n16%\r\n7%\r\n1%\r\n25%\r\n0%\r\n0%\r\n7%\r\n0%\r\n11%\r\n0%\r\n8%\r\n3%\r\n6%\r\n12%\r\n6%\r\n0%\r\n0%\r\nOtis\r\n0%\r\n0%\r\n0%\r\n100%\r\n0%\r\n0%\r\n2%\r\n33%\r\n3%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n9%\r\n8%\r\n0%\r\n0%\r\n0%\r\n0%\r\n50%\r\n13%\r\n4%\r\n0%\r\n0%\r\n4%\r\n2%\r\n0%\r\n0%\r\n0%\r\n9%\r\n17%\r\n0%\r\n0%\r\n3%\r\n4%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nPeru\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n3%\r\n0%\r\n0%\r\n6%\r\n0%\r\n0%\r\n4%\r\n8%\r\n0%\r\n6%\r\n0%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n5%\r\n4%\r\n0%\r\n0%\r\n0%\r\n33%\r\n3%\r\n0%\r\n1%\r\n0%\r\n6%\r\n4%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nPittsfield East\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n14%\r\n0%\r\n8%\r\n0%\r\n33%\r\n3%\r\n0%\r\n0%\r\n7%\r\n0%\r\n0%\r\n0%\r\n17%\r\n25%\r\n0%\r\n3%\r\n19%\r\n0%\r\n19%\r\n13%\r\n64%\r\n0%\r\n0%\r\n0%\r\n14%\r\n17%\r\n29%\r\n0%\r\n5%\r\n5%\r\n0%\r\n8%\r\n0%\r\n0%\r\n0%\r\nPittsfield West\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n12%\r\n33%\r\n9%\r\n0%\r\n17%\r\n6%\r\n20%\r\n0%\r\n2%\r\n8%\r\n0%\r\n6%\r\n0%\r\n12%\r\n0%\r\n6%\r\n17%\r\n0%\r\n11%\r\n6%\r\n1%\r\n25%\r\n0%\r\n0%\r\n3%\r\n17%\r\n13%\r\n0%\r\n10%\r\n7%\r\n6%\r\n29%\r\n18%\r\n12%\r\n0%\r\nPlainfield\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n8%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nRowe\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n20%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n33%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n17%\r\n0%\r\n0%\r\n5%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nSouth Sandisfield\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n6%\r\n0%\r\n2%\r\n3%\r\n0%\r\n0%\r\n6%\r\n8%\r\n0%\r\n12%\r\n0%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n0%\r\n1%\r\n4%\r\n12%\r\n0%\r\n0%\r\n0%\r\n0%\r\nSouthbury\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nState Line\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n5%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n8%\r\n2%\r\n0%\r\n0%\r\n4%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n2%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nStockbridge\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n17%\r\n0%\r\n9%\r\n25%\r\n5%\r\n6%\r\n0%\r\n33%\r\n7%\r\n0%\r\n0%\r\n0%\r\n17%\r\n12%\r\n0%\r\n13%\r\n7%\r\n0%\r\n8%\r\n11%\r\n9%\r\n0%\r\n0%\r\n0%\r\n5%\r\n0%\r\n15%\r\n0%\r\n8%\r\n5%\r\n6%\r\n17%\r\n0%\r\n6%\r\n100%\r\nTolland Center\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n1%\r\n0%\r\n0%\r\n3%\r\n0%\r\n0%\r\n4%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n3%\r\n1%\r\n0%\r\n0%\r\n2%\r\n2%\r\n0%\r\n0%\r\n0%\r\n3%\r\n0%\r\n1%\r\n0%\r\n2%\r\n3%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\nWilliamstown\r\n27%\r\n0%\r\n33%\r\n0%\r\n0%\r\n40%\r\n9%\r\n33%\r\n5%\r\n12%\r\n13%\r\n3%\r\n20%\r\n0%\r\n3%\r\n8%\r\n0%\r\n6%\r\n17%\r\n0%\r\n0%\r\n1%\r\n13%\r\n100%\r\n15%\r\n8%\r\n1%\r\n25%\r\n100%\r\n0%\r\n4%\r\n17%\r\n7%\r\n50%\r\n8%\r\n10%\r\n6%\r\n12%\r\n35%\r\n25%\r\n0%\r\nWindsor\r\n18%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n5%\r\n0%\r\n5%\r\n0%\r\n4%\r\n6%\r\n0%\r\n0%\r\n1%\r\n8%\r\n0%\r\n12%\r\n0%\r\n25%\r\n0%\r\n0%\r\n2%\r\n0%\r\n6%\r\n4%\r\n2%\r\n0%\r\n0%\r\n67%\r\n1%\r\n0%\r\n4%\r\n0%\r\n5%\r\n5%\r\n0%\r\n4%\r\n0%\r\n0%\r\n0%\r\nWorthington\r\n9%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n1%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n2%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n0%\r\n\r\nAndere Functies\r\nIn dit blog zijn de functies uit het janitor-pakket beschreven die nuttig zijn voor het dagelijkse werk. Dit is echter geen uitputtende lijst van janitor functies en ik raad aan om de documentatie te raadplegen voor meer informatie over dit pakket.\r\nEr zijn nog een paar andere functies die op zijn minst de moeite waard zijn om hier te vermelden: excel_numeric_to_date(): Deze functie is ontworpen om veel van Excel’s datum formaten te verwerken en om deze numerieke variabelen om te zetten naar datum variabelen. Het lijkt een grote tijdsbesparing voor diegenen die vaak met gegevens in Excel werken. Als niet frequent gebruiker van Excel, vertrouw ik in plaats daarvan zwaar op het lubridate pakket voor het werken met datum variabelen.\r\nround_to_fraction(): Met deze functie kun je decimale getallen afronden naar een precieze breuknoemer. Wil je al je waarden afgerond hebben naar het dichtstbijzijnde kwartier, of gebruik je decimalen om minuten in een uur weer te geven? Dan kan de round_to_fraction()-functie jou waarschijnlijk helpen.\r\ntop_levels(): Deze functie genereert een frequentietabel die een categorische variabele samenbrengt in hoge, middelste en lage niveaus. Veelgebruikte gevallen zijn onder andere het vereenvoudigen van Likert-achtige schalen.\r\nConclusie\r\nHet is op dit punt algemeen bekend dat de meeste dataanalisten en -wetenschappers het grootste deel van hun tijd besteden aan het opschonen en verkennen van gegevens. Daarom is het goed om nieuwe pakketten en functies te ontdekken die deze processen een beetje efficiënter maken.\r\nOf je het janitor-pakket nu wel of niet eerder hebt gebruikt, ik hoop dat deze blog jouw kennis heeft laten maken met enkele functies die nuttige toevoegingen zullen blijken te zijn aan je datawetenschapsgereedschapskist.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-01-rmarkdown-en-officedown/",
    "title": "Rmarkdown en Officedown",
    "description": "Een blog over documentatie en hoe ze dat in de farmaceutische industrie kunnen doen",
    "author": [
      {
        "name": "Jakub Sobolewski, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-10-01",
    "categories": [],
    "contents": "\r\nOnlangs kwam ik op R-bloggers een blog tegen waar mijn oog op viel. Hierin werden enkele mogelijkheden van rmarkdown en officedown met elkaar vergeleken. Ik dacht ik zet deze blog eens over als manier om voor en nadelen van beide pakketten te vergelijken. Dank je wel, Jakub.\r\nAccuraatheid, betrouwbaarheid en zichtbaarheid van data staat centraal in het farmaceutische kwaliteitssysteem. Daarvoor zijn er allerlei richlijnen die weer onder controle staan van FDA in Amerika en EMA in Europa.\r\nGoede documentatie hoort daarbij. De eisen die daaraan gesteld worden richten zich op 3 belangrijke gebieden van naleving:\r\nTraceerbaarheid: de mogelijkheid om de hele levenscyclus van het product te traceren, compleet met alle veranderingen. Verantwoordingsplicht:de mogelijkheid om alle personen die bijdragen te identificeren en vast te kunnen stellen wanneer belangrijke wijzigingen zijn doorgevoerd. Gegevensintegriteit:de betrouwbaarheid van de door het systeem gegenereerde gegevens.\r\nVolgens de richtlijnen voor kwaliteit moet elk verslag niet alleen de resultaten en conclusies bevatten, maar ook alle gegevens en gegevensbronnen die zijn gebruikt om ze op te stellen. Daarom is rapportage erg foutgevoelig, vooral als documentatie elke keer met de hand wordt gemaakt op een copy-paste manier. Bovendien kost handmatige rapportage veel tijd en moeite. De invoering van een documentbeheersysteem en programmatisch opgestelde rapporten kunnen deze problemen beperken.\r\nRMarkdown en de eisen\r\nRMarkdown is een bestandsformaat voor het maken van dynamische documenten met R. Onder de motorkap wordt een R Markdown bestand door het knitrpakket verwerkt tot een Markdown document. Het wordt dan verwerkt door Pandoc dat het uiteindelijke document rendert. Zowel Markdown als Pandoc zijn onafhankelijk van de R-omgeving.\r\nRmarkdown workflowRMarkdown bestanden laten je toe om rijke en reproduceerbare documenten te maken. Dit automatiseringselement maakt van R Markdown een krachtig instrument in de handen van klinisch-farmaceutische wetenschappers en technologen.\r\nVoordelen van het gebruik van R Markdown\r\nMet behulp van R Markdown worden uw documenten met voorschriften, zoals klinische onderzoeksrapporten:\r\nGeautomatiseerd: vermijd tijd te besteden aan herhaalbare taken en alledaagse handmatige rapportcreatie. Reproduceerbaar: iedereen kan het rapport regenereren met dezelfde of nieuwe gegevens. Toetsbaar: er kunnen tests worden ingevoerd om te garanderen dat de gegevens geldig zijn of dat de rapportstructuur de richtlijnen volgt. Beschikbaar in meerdere formaten: Word-, PowerPoint-, PDF- en HTML-documenten kunnen gemakkelijk worden aangemaakt. In overeenstemming met kwaliteitseisen: automatiseringsopties omvatten een standaardvereiste om een auteur toe te wijzen aan elk gegenereerd rapport.\r\nBovendien kunt u RStudio Connect() gebruiken om RMarkdown-bestanden te implementeren met een klik op een knop. De bestanden worden dan automatisch gerenderd en kunnen gemakkelijk worden gedeeld als view-only of downloadables. Zowel render- als toegangsgebeurtenissen kunnen gemonitord en gevolgd worden. Als er een nieuwe versie van de RMarkdown-bestanden beschikbaar is, kunnen ze snel opnieuw worden gegenereerd. Hetzelfde automatische proces kan ook periodiek worden toegepast.\r\nBeperkingen van RMarkdown\r\nOp een bepaald moment kan u gevraagd worden om uw rapporten aan te passen ivm kwaliteitseisen:\r\n“Maak aub een pagina met informatie over auteur, datum, bronnen, beoordeling. Het moet de eerste pagina van het rapport zijn voor de inhoudsopgave”\r\n“Pagina 50 moet worden gepresenteerd als een landschap, we moeten alle kolommen met gegevens bevatten en het zal niet passen in portret modus.”\r\n“Alle veranderlijke gegevens moeten cursief blauw worden geformatteerd, ongeacht of ze in een tabel of in de tekst staan.”\r\nHet kan erg moeilijk worden om aan zulke gedetailleerde richtlijnen te voldoen met RMarkdown, omdat het niet altijd even flexibel is.\r\nOfficedown en de eisen\r\nMet het officedown-pakket kunt u de beperkingen van RMarkdown overwinnen, zoals het stylen van specifieke Word elementen. Het is beter geschikt voor het genereren van Office-documenten en biedt meer mogelijkheden voor het maken en opmaken van Microsoft Word- of PowerPoint-documenten.\r\nOfficedown bevat enkele van de belangrijkste functies van het officer-pakket, dat speciaal gebouwd is voor het werken met Word en PowerPoint documenten. Onder andere kunt u met officer blokken tekst opmaken, stijlen uit Microsoft Word toepassen en automatisch kruisverwijzingen maken.\r\nZowel officedown als officer pakketten zijn een onderdeel van een groter officeverse, dat een ecosysteem is van 4 pakketten.\r\nOfficeverseMet mschart kunt u bewerkbare grafieken maken, vergelijkbaar met die in een Excel spreadsheet. De flextable interface is een uiterst handige manier om zeer aanpasbare tabellen te maken.\r\nWanneer je de kwaliteitseisen de sleutel is tot uw onderzoeks- en ontwikkelingsproces, kunt u officedown gebruiken om uw documentatie workflow te stroomlijnen. De officedown functies die hieronder worden beschreven, pakken enkele van de grootste uitdagingen aan wanneer je wilt voldoen aan Goede Documentatie Praktijken.\r\nNauwkeurige controle van de documentstructuur\r\nVanuit een kwaltiteitsperspectief kan het van cruciaal belang zijn dat de documentatie een specifieke volgorde van secties heeft. Met RMarkdown kun je een inhoudsopgave toevoegen, maar je hebt weinig controle over waar die verschijnt. Met pagedown kun je niet alleen een TOC toevoegen, maar ook een lijst van tabellen en een lijst van figuren. Het is echter moeilijk om de volgorde van deze secties te veranderen en het is niet mogelijk om het rapport te exporteren als een Word Document.\r\nDit is waar officedown om de hoek komt kijken, het geeft je veel meer flexibiliteit. Het helpt je om secties van het rapport precies te plaatsen waar je ze wilt in het document door gebruik te maken van HTML commentaar tags.\r\nStijl van tekst en tabellen\r\nHet maken van bewerkbare documenten in een onderzoeksomgeving kan extra opmaakopties vereisen. U wilt bijvoorbeeld belangrijke gegevens markeren, of dat nu tekst is of cellen in tabellen. Terwijl de opmaak van tabellen met verschillende pakketten kan worden gedaan, is de opmaak van tekstparagrafen alleen mogelijk met officedown. Door een eenvoudige variabele toe te voegen, kunt u ook de opmaak automatiseren, bijv. de oplichtingskleur in cellen alleen veranderen als de waarde lager of hoger is dan de referentiewaarde.\r\nTekst en tabellenStijldocumenten als templates\r\nMet officedown is het mogelijk om stijlen toe te passen die bekend zijn van Microsoft Word. In plaats van eigen CSS-styl te gebruiken, kun je een template .docx bestand maken met daarin gestylde elementen, dat vervolgens wordt doorgegeven aan de YAML header van het RMarkdown bestand. Dezelfde stijlen zullen worden toegepast op onderdelen in het resulterende .docx bestand. Op deze manier zal de aangepaste opmaak die u nodig hebt voor uw GxP-rapporten of -documentatie automatisch worden opgenomen in alle nieuw gegenereerde bestanden.\r\nTemplates gebruikenMeerdere documenten combineren\r\nWat als u liever een paar kleinere documenten of rapporten maakt en ze later samenvoegt in één bestand? Met officedown kunt u gemakkelijk meerdere documenten samenvoegen tot één. Bovendien blijven alle referenties behouden, terwijl de nummering van secties, figuren en tabellen automatisch wordt bijgewerkt en in een nieuwe inhoudsopgave wordt opgenomen.\r\nCombineren van documentenPagina in landschap\r\nHet gebruik van officedown geeft je ook meer flexibiliteit in termen van het veranderen van pagina oriëntatie voor geselecteerde delen van het rapport. Met een enkele opmerking in de code, kunt u de liggende modus toepassen op elk blok in het document. Deze eenvoudige functionaliteit komt zeer goed van pas wanneer u brede tabellen of grotere figuren moet opnemen.\r\nLandschapbreedofficedown voor betere documentatiemanagement\r\nMet het naleven van de kwaliteitseisen bereikt het automatiseren van het creatieproces en bijwerken van documenten een nieuw niveau in de farmaceutische industrie. Waar handmatige processen en andere pakketten tekort schieten, biedt officedown een levensvatbare oplossing. Het is een eenvoudige en effectieve manier om de functies van Microsoft Word naar R Markdown te brengen en aangepaste opmaakopties te ontsluiten.\r\nLaten we eens samenvatten hoe officedown u kan helpen te voldoen aan de kwaliteitseisen:\r\nmeer controle over document lay-out en structuur\r\nrobuuste opmaak van tekst en tabellen\r\nglobale toepassing van aangepaste stijlen in rapporten\r\nhandige compilatie van meerdere documenten\r\nDeze eigenschappen van officedown vertalen zich in een sterker kwaliteitssysteem, beter risico- en wijzigingsbeheer en nauwkeuriger versiebeheer. Het is een win-win situatie voor zowel uw R&D team als alle mensen die vertrouwen op de kwaliteit van uw medische product om te herstellen of een gezonder leven te leiden.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-21-kaart-van-zwitserland/",
    "title": "Kaart van Zwitserland",
    "description": "Dit is een mooie blog van Giulia Ruggeri over kaarten maken met `ggplot2` en `sf`, in dit geval van Zwitserland",
    "author": [
      {
        "name": "Giulia Ruggeri, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-21",
    "categories": [],
    "contents": "\r\nKaarten maken\r\nOp Medium verscheen eind 2020 dit duidelijke blog van Giulia Ruggeri Hier de link. Ik wilde weer eens met ggplot2 en sf werken en Giulia’s blog vond ik interessant en heb ik vervolgens bewerkt.\r\nIn de afgelopen jaren is het maken van mooie kaarten in R vrij eenvoudig geworden, dankzij het {sf} pakket. In dit artikel gaan we de ruimtelijke verspreiding van de COVID-19 incidentie van de laatste 14 dagen in Zwitserland visualiseren door een thematische kaart te maken, een choropleth kaart zoals dat heet. We maken daarbij gebruik van {sf} en {ggplot2} als onze belangrijkste hulpmiddelen.\r\n{sf}, wat staat voor simple feature (eenvoudige eigenschap), is de ‘go-to’ bibliotheek om om te gaan met ruimtelijke vectoriële gegevens, dat zijn gegevens die geografische geometrieën beschrijven als een reeks van punten, die worden beschreven door hun lengteg- en breedtegraad coördinaten. Hiermee kunnen geografische vormen worden geïmporteerd, gemanipuleerd en geplot en kunnen we gegevens verwerken in een tabel-achtig formaat, net als een data.frame. Wat een opluchting!\r\nIn deze kleine oefening gebruiken we {readxl} om het Excel bestand te importeren en binnen te halen van de website van het Zwitsers Federaal Bureau van Publieke Gezondheid.\r\n{rcartocolor} is de R bibliotheek die mooi uitziende kleurschalen bevat, die zijn ontwikkeld voor cartografie. Zoals David Letterman zou zeggen, {tidyverse}‘needs no introduction’. Dit zijn de programma’s die we hier binnenhalen.\r\n\r\n\r\n\r\nLaten we beginnen met het laden van de gegevens, met read_excel(), waarin we de exacte naam van het blad dat we willen laden kunnen opgeven, hoeveel regels we mogen overslaan en hoeveel regels we in totaal willen behouden.\r\nWe hebben een rij per kanton en een rij voor de titel, wat betekent dat we slechts 27 rijen hoeven te behouden (er zijn 26 Zwitserse kantons).\r\nWe schonen ook de kolomnamen een beetje op met clean_names uit het {janitor} pakket en gebruiken transmute om de gewenste kolommen te hernoemen en de andere te laten vallen.\r\n\r\nTot nu toe, eenvoudige dataimport en manipulatie.\r\n\r\n\r\n# A tibble: 6 x 2\r\n  canton incidence\r\n  <chr>      <dbl>\r\n1 AG          45.1\r\n2 AI          99.1\r\n3 AR          92.3\r\n4 BE          73  \r\n5 BL          33  \r\n6 BS          47.7\r\n\r\nWij hebben nu een variabele die de kantoncodes bevat en een variabele die de incidentie per 100.000, per kanton, van COVID-19 in de laatste 14 dagen bevat.\r\nWe zijn nu klaar om de shapefiles te laden.\r\n\r\nWacht even, wat zijn de shapefiles?\r\n\r\nShapefiles, zijn de bestanden die de geografische vormen bevatten die we willen plotten. We willen de data van de kantons plotten, dus hebben we de Zwitserse kantons nodig, die kunnen worden gedownload van [hier] (https://www.bfs.admin.ch/bfs/en/home/services/geostat/swiss-federal-statistics-geodata/administrative-boundaries/generalized-boundaries-local-regional-authorities.html).\r\nShapefiles zijn eigenlijk een set van bestanden, die verschillende geografische informatie bevatten (b.v. info over de projecties). Een van deze bestanden heeft de extensie .shp en dit is het bestand dat we gaan laden.\r\n\r\nLet op dat je alle andere bestanden in dezelfde map hebt staan.\r\n\r\nNu kunnen we dus 2 shapefiles laden, één die de vormen van de kantongrenzen bevat en één die de vorm van de grote meren van Zwitserland bevat.\r\nLaten we ze eens laden en kijken hoe ze eruit zien.\r\n\r\nReading layer `g2s15' from data source `/Users/harriejonkman/Desktop/GithubHarrie/HarriesHoekje/_posts/2021-07-21-kaart-van-zwitserland/resources/g2s15.shp' using driver `ESRI Shapefile'\r\nSimple feature collection with 22 features and 9 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: 500253.8 ymin: 63872.4 xmax: 774495.3 ymax: 297632.2\r\nprojected CRS:  CH1903 / LV03\r\nReading layer `G1K09' from data source `/Users/harriejonkman/Desktop/GithubHarrie/HarriesHoekje/_posts/2021-07-21-kaart-van-zwitserland/resources/G1K09.shp' using driver `ESRI Shapefile'\r\nSimple feature collection with 26 features and 3 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: 485414 ymin: 75286 xmax: 833837 ymax: 295935\r\nprojected CRS:  CH1903 / LV03\r\n\r\n\r\n[1] \"sf\"         \"data.frame\"\r\n\r\nswiss_cantons en swiss_lakes, worden opgeslagen als sf data.frames (als dataframe van sf dus), zodat we ze kunnen manipuleren, net zoals we tibbles (of data.frames) kunnen manipuleren. Dit is mogelijk omdat geometrieën worden opgeslagen op een zeer nette manier: als een geneste variabele meestal genaamd geometry. Dit zal je enige speciale variabele zijn, de andere (die attributen worden genoemd) zullen gewoon normale variabelen zijn. Bijvoorbeeld, elk kanton heeft zijn naam en code gekoppeld aan de geometrie die het beschrijft.\r\n\r\nSimple feature collection with 6 features and 3 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: 546871 ymin: 130593 xmax: 768722 ymax: 295935\r\nprojected CRS:  CH1903 / LV03\r\n  KT         NAME KURZ                       geometry\r\n1 17   St. Gallen   SG MULTIPOLYGON (((738559 1968...\r\n2 12  Basel-Stadt   BS MULTIPOLYGON (((608728 2681...\r\n3  7    Nidwalden   NW MULTIPOLYGON (((671030 1822...\r\n4  2         Bern   BE MULTIPOLYGON (((572954 1936...\r\n5 14 Schaffhausen   SH MULTIPOLYGON (((684561 2726...\r\n6 10     Fribourg   FR MULTIPOLYGON (((584435 1976...\r\n\r\n\r\nHet voordeel van {sf} te gebruiken als ons hoofdgereedschap om met deze datatypes om te gaan? We kunnen {ggplot2} gebruiken om ze te plotten!\r\n\r\n\r\n\r\n\r\nEn, net als met elke {ggplot2} grafiek, kunnen we het bouwen van de kaart laag voor laag opbouwen. Laten we nu de Zwitserse meren toevoegen bovenop de kantonvormen en theme_void() gebruiken om de achtergrond en de as te verwijderen. In deze stap kunnen we ook de kantons transparant maken door het fill argument op NA te zetten en een lichte groenblauwe kleur toe te voegen om de meren te vullen. geom_sf() werkt inderdaad net als elke andere geom_ functie, geen alarmen en geen verrassingen hier.\r\n\r\n\r\n\r\nHoe kunnen we nu elk kanton kleuren naar de grootte van de COVID-19 incidentie per 100’000 mensen?\r\nWe hoeven alleen maar de covid_incidence tabel en de swiss_cantons tabel samen te voegen, met de kantoncode als verbindingsvariabele. Hiermee kunnen we de variabele incidentie in kaart brengen naar de fill esthetiek en ereen choropleth kaart van maken, d.w.z. een thematische kaart.\r\n\r\n\r\n\r\nOm onze kaart er mooi te laten uitzien, verdelen wij, in plaats van een numerieke variabele te gebruiken, de incidentie in categorieën. Zo is het voor de gebruiker gemakkelijker te zien in welke categorie elk kanton valt.\r\nDit is een typische praktijk voor choropleth kaarten en het kan op verschillende manieren worden gedaan. In dit geval kiezen we voor een brute kracht aanpak, we doen het handmatig.\r\n\r\n\r\n\r\nNu kunnen we de kleur toewijzen aan de incidence_cat variabele en de eerste choropleth kaart maken.\r\n\r\n\r\n\r\nEn we hebben onze eerste choropleth kaart, gebouwd met alleen {sf} en {ggplot2}. Laten we nog wat puntjes op de i zetten: we zijn niet tevreden met hoe de legende eruit ziet en we kunnen die veranderen met guide_legend().\r\n\r\n\r\n\r\nNu kunnen we een titel, een ondertitel en labels toevoegen aan de bovenkant van elke kanton. We zullen {ggrepel} gebruiken om ervoor te zorgen dat de labels elkaar niet overlappen, we zullen ook {ggtext} gebruiken zodat we markdown syntax kunnen gebruiken voor onze titel en ondertitel.\r\n\r\n\r\n\r\nWe hebben nu de code om een choropleth kaart te maken en we hebben gezien hoe we die stap voor stap kunnen bouwen met {ggplot2}. Met een beetje maatwerk hebben we nu een statische kaart die we in een ander formaat opslaan en delen.\r\nAls je geïnteresseerd bent in het omgaan met geografische gegevens, is een van de beste vrij beschikbare bronnen het Geocomputation with R-boek. De auteurs van het boek maken veel gebruik van verschillende pakketten voor het plotten van thematische kaarten, met name {tmap}, dat ook de moeite waard is om te onderzoeken. Als u pakketten zoals {ggtext} wilt gebruiken om uw plots aan te passen, is {ggplot2} de bibliotheek waarop u wilt vertrouwen, vooral als u al gewend bent om ermee te werken.\r\nIk hoop dat je dit artikel met plezier las en blijf op de hoogte van meer voorbeelden over hoe kaarten in R zijn te maken.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-21-kaart-van-zwitserland/kaart-van-zwitserland_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-18-classificeren-van-palmer-penguins/",
    "title": "Classificeren van Palmer penguins",
    "description": "De laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het `tidymodels` raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met `tidymodels`. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.",
    "author": [
      {
        "name": "Julia Silge, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-18",
    "categories": [],
    "contents": "\r\nPalmer penguins classificatie\r\nHier kun je overigen haar opnmame vinden. Julia Silge on youtube\r\nDe laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het tidymodels raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met tidymodels. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.\r\nHier kun je haar opnmame vinden. Julia Silge on youtube\r\nEerst maar eens enkele pakketten laden en het databestand openen.\r\n\r\n# A tibble: 344 x 8\r\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\r\n   <fct>   <fct>              <dbl>         <dbl>             <int>\r\n 1 Adelie  Torgersen           39.1          18.7               181\r\n 2 Adelie  Torgersen           39.5          17.4               186\r\n 3 Adelie  Torgersen           40.3          18                 195\r\n 4 Adelie  Torgersen           NA            NA                  NA\r\n 5 Adelie  Torgersen           36.7          19.3               193\r\n 6 Adelie  Torgersen           39.3          20.6               190\r\n 7 Adelie  Torgersen           38.9          17.8               181\r\n 8 Adelie  Torgersen           39.2          19.6               195\r\n 9 Adelie  Torgersen           34.1          18.1               193\r\n10 Adelie  Torgersen           42            20.2               190\r\n# … with 334 more rows, and 3 more variables: body_mass_g <int>,\r\n#   sex <fct>, year <int>\r\n\r\nAls je een classificatiemodel voor soorten pinquins probeert op te stellen, zul je waarschijnlijk een bijna perfecte pasvorm vinden, omdat dit soort waarnemingen in feite de verschillende soorten onderscheiden. sex (geslacht) daarentegen geeft een wat rommeliger beeld, vandaar dat hier deze uitkomstvariabelen op basis van predictoren wordt voorspeld.\r\n\r\n\r\n\r\nHet ziet er naar uit dat de vrouwelijke pinguïnflippers kleiner zijn met kleinere snavels, maar laten we ons klaarmaken voor het modelleren om meer te weten te komen! De informatie over het eiland of het jaar zullen we niet gebruiken in ons model. Die halen we eruit.\r\n\r\n\r\n\r\nEen modelopbouwen\r\nWe zullen ook het tidymodels metapakket laden en vervolgens onze gegevens splitsen in een trainings- en testingssets.\r\n\r\n\r\n\r\nOmdat het een relatieve kleine dataset betreft (zeker de testset), maken we vervolgens hier gebruik van bootstrap-resamples van de trainingsgegevens, om onze modellen te evalueren.\r\n\r\n# Bootstrap sampling \r\n# A tibble: 25 x 2\r\n   splits            id         \r\n   <list>            <chr>      \r\n 1 <rsplit [250/93]> Bootstrap01\r\n 2 <rsplit [250/92]> Bootstrap02\r\n 3 <rsplit [250/90]> Bootstrap03\r\n 4 <rsplit [250/92]> Bootstrap04\r\n 5 <rsplit [250/86]> Bootstrap05\r\n 6 <rsplit [250/88]> Bootstrap06\r\n 7 <rsplit [250/96]> Bootstrap07\r\n 8 <rsplit [250/89]> Bootstrap08\r\n 9 <rsplit [250/96]> Bootstrap09\r\n10 <rsplit [250/90]> Bootstrap10\r\n# … with 15 more rows\r\n\r\nLaten we eens twee verschillende modellen vergelijken, een logistisch regressiemodel en een random forest model. We beginnen met het maken van de modelspecificaties voor beide modellen.\r\n\r\nLogistic Regression Model Specification (classification)\r\n\r\nComputational engine: glm \r\n\r\n\r\nRandom Forest Model Specification (classification)\r\n\r\nComputational engine: ranger \r\n\r\nLaten we nu beginnen met het samenstellen van een tidymodels workflow(), een object dat helpt om modelleer-pijplijnen te beheren met stukjes die in elkaar passen als Lego-blokjes. Merk op dat er nog geen model is:\r\n\r\n══ Workflow ══════════════════════════════════════════════════════════\r\nPreprocessor: Formula\r\nModel: None\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\nsex ~ .\r\n\r\nNu kunnen we een model toevoegen, en de fit voor elk van de resamples. Eerst kunnen we het logistische regressiemodel passen.\r\n\r\n# Resampling results\r\n# Bootstrap sampling \r\n# A tibble: 25 x 5\r\n   splits         id        .metrics      .notes       .predictions   \r\n   <list>         <chr>     <list>        <list>       <list>         \r\n 1 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [93 × …\r\n 2 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\r\n 3 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\r\n 4 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\r\n 5 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [86 × …\r\n 6 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [88 × …\r\n 7 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\r\n 8 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [89 × …\r\n 9 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\r\n10 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\r\n# … with 15 more rows\r\n\r\nTen tweede kunnen we het random forest model toepassen.\r\n\r\n# Resampling results\r\n# Bootstrap sampling \r\n# A tibble: 25 x 5\r\n   splits         id        .metrics      .notes       .predictions   \r\n   <list>         <chr>     <list>        <list>       <list>         \r\n 1 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [93 × …\r\n 2 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\r\n 3 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\r\n 4 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\r\n 5 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [86 × …\r\n 6 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [88 × …\r\n 7 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\r\n 8 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [89 × …\r\n 9 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\r\n10 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\r\n# … with 15 more rows\r\n\r\nWij hebben elk van onze kandidaat-modellen aangepast aan onze opnieuw bemonsterde trainingsreeks!\r\nHet model evalueren.\r\nLaten we nu eens kijken hoe we het gedaan hebben. Eerst het logistisch regressiemodel.\r\n\r\n# A tibble: 2 x 6\r\n  .metric  .estimator  mean     n std_err .config             \r\n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 accuracy binary     0.897    25 0.00631 Preprocessor1_Model1\r\n2 roc_auc  binary     0.964    25 0.00368 Preprocessor1_Model1\r\n\r\nGoed zo! De functie collect_metrics() extraheert en formatteert de .metrics kolom van resampling resultaten zoals hierboven voor het glm-model. Nu het random-forest model.\r\n\r\n# A tibble: 2 x 6\r\n  .metric  .estimator  mean     n std_err .config             \r\n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 accuracy binary     0.890    25 0.00595 Preprocessor1_Model1\r\n2 roc_auc  binary     0.959    25 0.00342 Preprocessor1_Model1\r\n\r\nDus… ook geweldig! Als ik in een situatie zit waarin een complexer model (zoals een random forest) hetzelfde presteert als een eenvoudiger model (zoals logistische regressie), dan kies ik het eenvoudiger model. Laten we eens dieper ingaan op hoe het het doet. Bijvoorbeeld, hoe voorspelt het glm-model de twee klassen?\r\n\r\n# A tibble: 4 x 3\r\n  Prediction Truth   Freq\r\n  <fct>      <fct>  <dbl>\r\n1 female     female 40.6 \r\n2 female     male    4.48\r\n3 male       female  4.92\r\n4 male       male   41.4 \r\n\r\nOngeveer hetzelfde, wat goed is. We kunnen ook een ROC curve maken.\r\n\r\n\r\n\r\nDeze ROC-curve is grilliger dan andere die u wellicht hebt gezien omdat de dataset klein is.\r\nHet is eindelijk tijd om terug te keren naar de testset. Merk op dat we de testset tijdens deze hele analyse nog niet hebben gebruikt; de testset is kostbaar en kan alleen worden gebruikt om de prestaties op nieuwe gegevens in te schatten. Laten we nog een keer passen op de trainingsgegevens en evalueren op de testgegevens met behulp van de functie last_fit().\r\n\r\n# Resampling results\r\n# Manual resampling \r\n# A tibble: 1 x 6\r\n  splits      id         .metrics    .notes    .predictions  .workflow\r\n  <list>      <chr>      <list>      <list>    <list>        <list>   \r\n1 <rsplit [2… train/tes… <tibble [2… <tibble … <tibble [83 … <workflo…\r\n\r\nDe metriek en voorspellingen hier zijn op de testgegevens.\r\n\r\n# A tibble: 2 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.940 Preprocessor1_Model1\r\n2 roc_auc  binary         0.991 Preprocessor1_Model1\r\n\r\n\r\n          Truth\r\nPrediction female male\r\n    female     39    3\r\n    male        2   39\r\n\r\nDe coëfficiënten (die we eruit kunnen halen met tidy()) zijn geschat met behulp van de trainingsdata. Als we exponentiate = TRUE gebruiken, hebben we odds ratio’s.\r\n\r\n# A tibble: 7 x 5\r\n  term              estimate std.error statistic       p.value\r\n  <chr>                <dbl>     <dbl>     <dbl>         <dbl>\r\n1 (Intercept)       3.12e-35  13.5         -5.90 0.00000000369\r\n2 speciesChinstrap  1.34e- 3   1.70        -3.89 0.000101     \r\n3 speciesGentoo     1.08e- 4   2.89        -3.16 0.00159      \r\n4 bill_length_mm    1.78e+ 0   0.137        4.20 0.0000268    \r\n5 bill_depth_mm     3.89e+ 0   0.373        3.64 0.000273     \r\n6 flipper_length_mm 1.07e+ 0   0.0538       1.31 0.189        \r\n7 body_mass_g       1.01e+ 0   0.00108      4.70 0.00000260   \r\n\r\nDe grootste kansverhouding geldt voor de snaveldiepte, en de op één na grootste voor de snavellengte. Een toename van 1 mm snaveldiepte komt overeen met bijna 4x meer kans om een mannetje te zijn. De kenmerken van de bek van een pinguïn moeten geassocieerd zijn met het geslacht.\r\nWe hebben geen sterke aanwijzingen dat de lengte van de vleugels verschillend is tussen mannelijke en vrouwelijke pinguïns, als we de andere maten controleren; misschien moeten we dat onderzoeken door de eerste grafiek te veranderen!\r\n\r\n\r\n\r\nJa, de mannetjes- en vrouwtjespinguïns zijn nu veel meer gescheiden.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-18-classificeren-van-palmer-penguins/classificeren-van-palmer-penguins_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-18-verzekeringskosten/",
    "title": "Verzekeringskosten",
    "description": "Dit is de blog die [Arta Seyedan op 14 februari 2021 R-bloggers](https://www.r-bloggers.com/2021/02/using-tidymodels-to-predict-health-insurance-cost/) schreef en die ik wat bewerkt en vertaald heb.",
    "author": [
      {
        "name": "Arta Seyadan, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-18",
    "categories": [],
    "contents": "\r\nVerzekeringskosten voorspellen met behulp van lineaire regressie\r\nDit is de blog die Arta Seyedan op 14 februari 2021 R-bloggers schreef en die ik wat bewerkt en vertaald heb.\r\nRond eind oktober 2020 woonde AS de Open Data Science Conferentie bij, voornamelijk voor de workshops en trainingssessies die daar werden aangeboden. De eerste workshop die hij bijwoonde was een demonstratie door Jared Lander over hoe je machine learning methoden in R kunt implementeren met behulp van een nieuw pakket genaamd tidymodels. Hij ging die training in en wist bijna niets over machine learning en heeft vervolgens uitsluitend gebruik gemaakt van gratis online materiaal om te begrijpen hoe je data analyseert met behulp van dit “meta-pakket”.\r\ntidymodels is net als tidyverse niet een enkel pakket. Het is eerder een verzameling van data science pakketten (een suite zeg maar) ontworpen volgens principes van tidyverse. Er is overeenkomst tussen tidymodels en tidyverse. Wat tidymodels echter anders maakt dan tidyverse, is dat veel van deze pakketten bedoeld zijn voor voorspellend modelleren. Het biedt een universele standaard interface voor alle verschillende machine learning methoden die beschikbaar zijn in R.\r\nOm te laten zien hoe het werkt, wordt hier een dataset aangeboden met informatie van ziektekostenverzekering van ~1300 klanten van een ziektekostenverzekeringsmaatschappij. Deze dataset is afkomstig uit een boek getiteld Machine Learning with R van Brett Lantz. Laten we tegelijk enkele pakketten openen.\r\n\r\n\r\n\r\nDit zijn de zeven variabelen die erin zitten.\r\n\r\n[1] \"age\"      \"sex\"      \"bmi\"      \"children\" \"smoker\"   \"region\"  \r\n[7] \"charges\" \r\n\r\nZo zien de variabelen er vervolgens uit.\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  18.00   27.00   39.00   39.21   51.00   64.00 \r\n\r\n\r\n.\r\nfemale   male \r\n   662    676 \r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  15.96   26.30   30.40   30.66   34.69   53.13 \r\n\r\n\r\n.\r\n  no  yes \r\n1064  274 \r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   1122    4740    9382   13270   16640   63770 \r\n\r\nHierboven zie je dat je pakketten als parsnip en recipes hebt geladen. Deze pakketten vormen dus, samen met andere pakketten, het meta-pakket tidymodels, dat gebruikt wordt voor modelleren en statistische analyse en machine learning.\r\nZoals je kunt zien, zijn er 7 verschillende, relatief voor zichzelf sprekende variabelen in deze dataset, waarvan sommige vermoedelijk worden gebruikt door de particuliere ziektekostenverzekeraar in kwestie om te bepalen hoeveel een bepaald individu uiteindelijk in rekening wordt gebracht. age(Leeftijd), sex (geslacht) en region (regio) lijken demografische achtergrondvariabelen te zijn, waarbij de leeftijd niet lager dan 18 en niet hoger dan 64 jaar is, met een gemiddelde van ongeveer 40 jaar. Het aantal mannen en vrouwen is vrijwel hetzelfde.\r\nErvan uitgaande dat de variabele bmi overeenkomt met Body Mass Index, wordt een BMI van 30 of hoger als klinisch zwaarlijvig beschouwd. In onze huidige gegevensverzameling ligt het gemiddelde net boven de grens van zwaarlijvigheid.\r\nVervolgens hebben we het aantal rokers versus niet-rokers. Nu kan ik je zeker al vertellen dat het al of nietroker zijn belangrijk zal zijn bij het bepalen van de kosten van een bepaalde ziektekostenverzekeraar.\r\nTenslotte, hebben we charge (kosten). De gemiddelde jaarlijkse kosten voor een ziektekostenverzekering zijn een bescheiden 13.000 dollar.\r\nExploratieve Data Analyse\r\n\r\nTable 1: Data summary\r\nName\r\ninsur_dt\r\nNumber of rows\r\n1338\r\nNumber of columns\r\n7\r\nKey\r\nNULL\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n3\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nsex\r\n0\r\n1\r\n4\r\n6\r\n0\r\n2\r\n0\r\nsmoker\r\n0\r\n1\r\n2\r\n3\r\n0\r\n2\r\n0\r\nregion\r\n0\r\n1\r\n9\r\n9\r\n0\r\n4\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n39.21\r\n14.05\r\n18.00\r\n27.00\r\n39.00\r\n51.00\r\n64.00\r\n▇▅▅▆▆\r\nbmi\r\n0\r\n1\r\n30.66\r\n6.10\r\n15.96\r\n26.30\r\n30.40\r\n34.69\r\n53.13\r\n▂▇▇▂▁\r\nchildren\r\n0\r\n1\r\n1.09\r\n1.21\r\n0.00\r\n0.00\r\n1.00\r\n2.00\r\n5.00\r\n▇▂▂▁▁\r\ncharges\r\n0\r\n1\r\n13270.42\r\n12110.01\r\n1121.87\r\n4740.29\r\n9382.03\r\n16639.91\r\n63770.43\r\n▇▂▁▁▁\r\n\r\nDe dataset ziet er vrij schoon uit; je zult waarschijnlijk nooit een dataset als deze zo tegenkomen. Er zijn geen NAs (missende waarden) en, zoals al eerder opgemerkt, geslacht is evenwichtig verdeeld. Laten we eens kijken naar de verdeling van de children (kinderen):\r\n\r\n\r\n  0   1   2   3   4   5 \r\n574 324 240 157  25  18 \r\n\r\nVrij standaard toch; de overgrote meerderheid van de mensen in deze set heeft geen kinderen. Het volgende hoogste aantal is 1, het op een na hoogste 2, enz.\r\n\r\n\r\n\r\nGGally is een pakket dat het proces van exploratieve data analyse vergemakkelijkt door automatisch ggplots te genereren met de variabelen die in het data frame zitten. Het helpt je om een beter inzicht te krijgen in de relaties die er tussen variabelen zouden kunnen bestaan. De meeste van deze plots zijn gewoon ruis, maar er zijn een paar interessante. Kijk maar eens naar de twee linksonder, die charge vs age en charge vs bmi beoordelen. Verder naar rechts, is er ook charge vs smoker. Laten we een aantal van deze verbanden eens nader bekijken:\r\n\r\n\r\n\r\nIk wilde zien of er regio’s zijn die op de een of andere manier anders belast worden dan de andere, maar deze plots zien er allemaal hetzelfde uit. Zoals je ziet, zijn er ongeveer twee verschillende blobs die van 0,0 naar het centrum van de plot gaan. We komen daar later op terug.\r\n\r\n\r\n\r\nHier, wilde ik zien of er een soort van herkenbaar verband was tussen leeftijd en kosten. In de vier regio’s lijken de meeste op een helling bij de X-as te liggen, die licht toeneemt met de leeftijd. Er is echter een patroon dat lijkt te bestaan uit twee niveaus die van die basislijn afkomen. Aangezien we geen variabele hebben voor het soort ziektekostenverzekering dat deze mensen hebben, moeten we voorlopig maar even wachten met een oordeel over wat dit zou kunnen zijn.\r\nLaten we overgaan tot wat ongetwijfeld het ‘pièce de résistance’ is van de ziektekostenverzekeringsdekking: rokers.\r\n\r\n\r\n\r\nWow. Wat een groot verschil. Hier zie je dat rokers bijna een hele nieuwe klodder punten creëert los van niet-rokers… en die klodder stijgt sterk na ‘bmi = 30’. Zeg, wat was de officiële cutoff-score van de CDC voor obesitas ook alweer?\r\n\r\n\r\n\r\nJe kunt zien dat leeftijd een rol speelt bij kosten, maar het is nog steeds gestratificeerd binnen de 3 clusters van punten. Dus zelfs onder de hoge bmi-rokers, betalen jongere mensen nog steeds minder geld dan oudere mensen op een consistente manier, dus het is logisch. Het lijkt er echter niet op dat leeftijd een wisselwerking heeft met bmi of roker, wat betekent dat het onafhankelijk effect heeft op de prijs`.\r\nTenslotte, kinderen heeft geen significant effect op de lading, zie maar.\r\n\r\n\r\n\r\nIk denk dat we genoeg verkennende analyses hebben gedaan om vast te stellen dat bmi en roker samen een synergetisch effect hebben op de prijs, en dat leeftijd ook invloed heeft op de prijs.\r\nModel bouwen\r\nMet deze kennis in ons achterhoofd gaan we een model bouwen.\r\n\r\n\r\n\r\nWe splitsen eerst onze gegevens in training- en testsets. We stratificeren de steekproeftrekking op basis van de status van “roker”, omdat er daar een onevenwicht bestaat en we willen dat ze gelijk vertegenwoordigd zijn in zowel de trainings- als de testdatasets. Dit wordt bereikt door eerst aselecte steekproeven uit te voeren binnen deze klassen.\r\nEen uitleg van recipe (het pakket recept, zeg maar):\r\nWe gaan het effect van bmi, leeftijd en roker op prijs modelleren. We specificeren in deze stap geen interacties omdat het recept interacties als stap afhandelt.\r\nWe maken dummy variabelen (step_dummy) voor alle nominale voorspellers, dus roker wordt roker_ja en roker_nee wordt “geïmpliceerd” door weglating (dus als een rij roker_ja == 0 heeft) omdat in sommige modellen niet alle dummy variabelen als kolom aanwezig kunnen zijn. Om alle dummy variabelen op te nemen, kunt u one_hot = TRUE gebruiken.\r\nVervolgens normaliseren we alle numerieke voorspellers behalve onze uitkomstvariabele(step_normalize(all_numeric(), -all_outcomes())), omdat je over het algemeen transformaties op uitkomsten wilt vermijden bij het trainen en ontwikkelen van een model, omdat anders een andere dataset die niet consistent is met de dataset die je gebruikt langskomt en je model kapot maakt. Het is het beste om transformaties op uitkomsten te doen voordat je een recipe maakt.\r\nWe stellen een interactieterm in; bmi en smoker_yes (de dummy variabele voor smoker), hebben allemaal een wisselwerking met elkaar bij het beïnvloeden van de uitkomst. Eerder zagen we al dat oudere patiënten meer moeten betalen, en dat oudere patiënten met een hogere bmi zelfs nog meer moeten betalen. Welnu, oudere patiënten met een hogere bmi die roken worden het meest aangerekend van iedereen in onze dataset. We hebben dit visueel waargenomen toen we naar de plot keken, dus we gaan dit ook testen in het model dat we zullen ontwikkelen.\r\nLaten we het model specificeren. We gaan werken met een k-Nearest Neighbors model om het later te vergelijken met een ander model. Het KNN-model is eenvoudig als volgt gedefinieerd:\r\n\r\nKNN regressie is een niet-parametrische methode die op een intuïtieve manier de associatie tussen onafhankelijke variabelen en de continue uitkomst benadert door het gemiddelde te nemen van de waarnemingen in dezelfde buurt. De grootte van de buurt moet worden ingesteld door de analist of kan worden gekozen met behulp van crossvalidatie (dit komt later aan de orde) om de grootte te kiezen die de gemiddelde kwadratische fout zo klein mogelijk maakt.\r\n\r\nOm het eenvoudig te houden, gaan we geen kruisvalidatie gebruiken om de optimale k te vinden. In plaats daarvan zeggen we gewoon k = 10.\r\n\r\n\r\n\r\nWe hebben het model knn_spec gespecificeerd door het model zelf aan te roepen vanuit parsnip. Daarna hebben we set_engine en de modus op regressie gezet. Let op de neighbors parameter in nearest_neighbor. Dat komt overeen met de k in knn.\r\nVervolgens passen we het model met behulp van de modelspecificatie toe op onze gegevens. Omdat we al kolommen hebben berekend voor de bmi en smoker_yes interactie, hoeven we de interactie niet opnieuw formeel weer te geven.\r\nLaten we dit model evalueren om te zien hoe het het doet.\r\n\r\n# A tibble: 2 x 6\r\n  .metric .estimator     mean     n  std_err .config             \r\n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \r\n1 rmse    standard   4916.       10 274.     Preprocessor1_Model1\r\n2 rsq     standard      0.827    10   0.0194 Preprocessor1_Model1\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   1122    4740    9382   13270   16640   63770 \r\n\r\nWe stellen vfold_cv in (dat is de crossvalidatie waar de meeste mensen bekend mee zijn, waarbij de trainingsdata wordt verdeeld in V vouwen en dan wordt getraind op V-1 vouwen om een voorspelling te doen op de laatste vouw, en wordt herhaald zodat alle vouwen worden getraind en gebruikt als voorspellingsvouw) op een prop van 0.9, wat hetzelfde is als het specificeren van 9 trainingsvouwen en 1 testvouw (binnen onze trainingsdata).\r\nTenslotte voeren we de crossvalidatie uit door fit_resamples te gebruiken. Zoals je kunt zien, hebben we ons workflow object als invoer gebruikt.\r\nTenslotte roepen we collect_metrics op om de effectiviteit van het model te onderzoeken. We eindigen met een rmse van 4,915 en een rsq van 0,82. De RMSE zou suggereren dat, gemiddeld, onze voorspellingen verschilden van de waargenomen waarden met een absolute maatstaf van 4.915, in dit geval, dollars in charges. De R^2 zou suggereren dat onze regressie een fit heeft van ~82%, hoewel een hoge R^2 niet altijd betekent dat het model een goede fit heeft en een lage R^2 niet altijd betekent dat een model een slechte fit heeft.\r\n\r\n\r\n\r\nHierboven is een demonstratie van onze regressie op een lijn. Er is een grote cluster van waarden die ons model gewoon niet weergeeft, en we zouden meer kunnen leren over deze punten, maar in plaats daarvan gaan we verder met het toepassen van ons model op onze testgegevens, die we veel eerder in dit project hebben gedefinieerd.\r\n\r\n# A tibble: 334 x 2\r\n    .pred charges\r\n    <dbl>   <dbl>\r\n 1  4339.   3757.\r\n 2 27038.  27809.\r\n 3  2231.   1837.\r\n 4  6500.   6204.\r\n 5  2794.   4688.\r\n 6  6057.   6314.\r\n 7 14335.  12630.\r\n 8  1663.   2211.\r\n 9  5655.   3580.\r\n10 39401.  37743.\r\n# … with 324 more rows\r\n\r\nWe hebben nu ons model toegepast op test_proc, de test set nadat we de recipes voorbewerkingsstappen erop hebben toegepast om ze op dezelfde manier te transformeren als we onze trainingsdata hebben getransformeerd. We verbinden de resulterende voorspellingen met de werkelijke charges gevonden in de training data om een twee-koloms tabel te maken met onze voorspellingen en de overeenkomstige werkelijke waarden die we probeerden te voorspellen.\r\n\r\n\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard       4985.\r\n# A tibble: 2 x 6\r\n  .metric .estimator     mean     n  std_err .config             \r\n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \r\n1 rmse    standard   4916.       10 274.     Preprocessor1_Model1\r\n2 rsq     standard      0.827    10   0.0194 Preprocessor1_Model1\r\n\r\nMooi zo. De RMSE gegenereerd door onze testgegevens verschilt niet significant van die gegenereerd door onze crossvalidatie! Dat betekent dat ons model op betrouwbare wijze voorspellingen kan reproduceren met ongeveer hetzelfde foutenniveau.\r\nEen ander groot voordeel van tidymodels is dat het het proces van het vergelijken van voorspellende prestaties tussen twee verschillende modellen stroomlijnt. Laat mij jou dat demonstreren.\r\nLineaire Regressie\r\nWe hebben het recept (recipe) al. Nu moeten we alleen nog een lineair model specificeren en het model kruisvalideren om het te testen op de testgegevens.\r\n\r\n\r\n\r\nWe herhalen sommige van dezelfde stappen die we voor KNN deden, maar dan nu voor het lineaire model. We kunnen zelfs crossvalideren door (bijna) hetzelfde commando te gebruiken:\r\n\r\n# A tibble: 2 x 6\r\n  .metric .estimator     mean     n  std_err .config             \r\n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \r\n1 rmse    standard   4866.       10 251.     Preprocessor1_Model1\r\n2 rsq     standard      0.832    10   0.0162 Preprocessor1_Model1\r\n# A tibble: 2 x 6\r\n  .metric .estimator     mean     n  std_err .config             \r\n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \r\n1 rmse    standard   4916.       10 274.     Preprocessor1_Model1\r\n2 rsq     standard      0.827    10   0.0194 Preprocessor1_Model1\r\n\r\nFascinerend! Het blijkt dat het goede, ouderwetse lineaire model k-Nearest Neighbors verslaat zowel in termen van RMSE als van R^2 over 10 kruisvalidatie-voudigingen.\r\n\r\n# A tibble: 334 x 2\r\n    .pred charges\r\n    <dbl>   <dbl>\r\n 1  6335.   3757.\r\n 2 31938.  27809.\r\n 3  3171.   1837.\r\n 4  7878.   6204.\r\n 5  3081.   4688.\r\n 6  7815.   6314.\r\n 7 14070.  12630.\r\n 8  2656.   2211.\r\n 9  3498.   3580.\r\n10 36293.  37743.\r\n# … with 324 more rows\r\n\r\nNu we onze voorspellingen hebben, laten we eens kijken hoe goed het lineaire model het deed:\r\n\r\n\r\n\r\nHet lijkt erop dat het gebied linksonder de grootste concentratie ladingen had, en het grootste deel van de lm fit verklaart. Kijkend naar deze beide plots vraag ik me af of er een beter model was dat we hadden kunnen gebruiken, maar ons model voldeed gezien onze doelstellingen en nauwkeurigheidsniveau.\r\n\r\n\r\n\r\nHierboven is een vergelijking van de twee methoden met hun respectieve voorspellingen, en met de stippellijn die de “juiste” waarden weergeeft. In dit geval verschilden de twee modellen niet zo veel van elkaar dat hun verschillen gemakkelijk konden worden waargenomen wanneer ze tegen elkaar werden uitgezet. Maar er zullen zich in de toekomst gevallen voordoen waarin uw twee modellen toch aanzienlijk verschillen. Het zo doen zal je helpen het ene model boven het andere te verkiezen.\r\nConclusie\r\nHier konden wij een KNN-model bouwen met onze trainingsgegevens en het gebruiken om waarden in onze testgegevens te voorspellen. Om dit te doen, hebben we:\r\neen EDA uitgevoerd;\r\nonze gegevens voorbewerkt en met recipe ons model gespecificeerd als KNN;\r\nhet toegepast op onze trainingsgegevens;\r\ncrossvalidatie uitgevoerd om nauwkeurige foutstatistieken te produceren;\r\nvoorspelde waarden vastgesteld in onze testset;\r\nde waargenomen testwaarden met onze voorspellingen vergeleken;\r\neen ander model gespecificeerd (lm);\r\nook hier een crossvalidatie uitgevoerd;\r\nontdekt dat lm het betere model was.\r\nHij is zeer enthousiast om door te gaan met het gebruik van tidymodels in R als een manier om machine-learning methoden toe te passen. Als je geïnteresseerd bent, raad ik je aan om Tidy Modeling with R by Max Kuhn and Julia Silge te bekijken.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-18-classificeren-met-tidymodels/",
    "title": "Classificeren met Tidymodels",
    "description": "Dit is bewerking van een blog die [Rahul Raoniar, Towards data science](https://towardsdatascience.com/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055) begin 2021 schreef.",
    "author": [
      {
        "name": "Rahul Raoniar, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\r\nEen gids om stap voor stap een logissche regressie uit te voeren met gebruik van het tidymodels pakket\r\nDit is bewerking van een blog die Rahul Raoniar, Towards data science begin 2021 schreef.\r\nIn de wereld van ‘supervised machine learning’ worden vaak twee soorten analyses uitgevoerd. De ene heet regressie (voorspellen van continue waarden), de andere heet classificatie (voorspellen van discrete waarden). In deze blog geef ik een voorbeeld van een binair classificatiealgoritme, “Binaire Logistische Regressie” genaamd. Dat valt onder de Binomiale familie met een logit koppelingsfunctie. Binaire logistische regressie wordt gebruikt voor het voorspellen van binaire klassen. Bijvoorbeeld in gevallen waarin je ja/nee, winst/verlies, negatief/positief, waar/onwaar enzovoort wilt voorspellen.\r\nDeze blog leidt jou door een proces van hoe het ‘tidymodels’-pakket te gebruiken om een model toe te passen en te evalueren met heel weinig en eenvoudige stappen.\r\nAchtergrond van de data\r\nIn dit voorbeeld maak je gebruik maken van de Pima Indian Diabetes 2 data, verkregen uit de UCI Repository van de machine learning data (Newman et al. 1998).\r\nDeze data zijn oorspronkelijk afkomstig van het ‘National Institute of Diabetes and Digestive and Kidney Diseases’. Het doel van de dataset is diagnostisch te voorspellen of een patiënt al dan niet diabetes heeft, op basis bepaalde diagnostische metingen die in de dataset zijn opgenomen. Bij de selectie van deze data uit een grotere databank werden verschillende beperkingen opgelegd. In het bijzonder zijn alle patiënten hier vrouwen van ten minste 21 jaar oud van Pima Indiaanse afkomst. De Pima Indian Diabetes 2-data is de verfijnde versie (alle ontbrekende waarden zijn toegewezen als NA) van de Pima Indian diabetes-gegevens. De dataset bevat de volgende onafhankelijke en afhankelijke variabelen.\r\nOnafhankelijke variabelen (met symbool: O) - O1: pregnant: Aantal keren zwanger\r\n- O2: glucose: Plasma glucose concentratie (glucose tolerantie test)\r\n- O3: pressure: Diastolische bloed druk (mm Hg)\r\n- O4: triceps: Triceps huidplooidikte (mm)\r\n- O5: insulin: 2-uur serum insuline (mu U/ml)\r\n- O6: mass: Body mass index (gewicht in kg/(lengte in m)\\²)\r\n- O7: pedigree: Diabetes pedigree functie\r\n- O8: age: Leeftijd (jaren)\r\nDependent Variable (met symbool: A)\r\n- A1: diabetes: diabetes geval (pos/neg)\r\nDoel van de modellering\r\naanpassen van een binair logistisch regressie-machineleermodel met behulp van de bibliotheek tidymodels\r\nhet testen van de voorspellingskracht van het getrainde model (evaluatie van het model) op de ongeziene/geteste dataset met behulp van verschillende evaluatiemetrieken.\r\nBibliotheken en Datasets laden\r\nStap1: Eerst moeten we de volgende pakketten worden geïnstalleerd met de install.packages( ) functie (als ze al niet zijn geïnstalleerd en ze laden met de library( ) functie.\r\n\r\n\r\n\r\nStap2: Vervolgens moet je de dataset binnen halen uit het mlbench pakket met behulp van de data( ) functie.\r\nNa het laden van de data, is de volgende essentiële stap het uitvoeren van een verkennende data-analyse, die zal helpen bij het vertrouwd raken met de data. Gebruik de head( ) functie om de bovenste zes rijen van de data te bekijken.\r\n\r\n  pregnant glucose pressure triceps insulin mass pedigree age\r\n1        6     148       72      35      NA 33.6    0.627  50\r\n2        1      85       66      29      NA 26.6    0.351  31\r\n3        8     183       64      NA      NA 23.3    0.672  32\r\n4        1      89       66      23      94 28.1    0.167  21\r\n5        0     137       40      35     168 43.1    2.288  33\r\n6        5     116       74      NA      NA 25.6    0.201  30\r\n  diabetes\r\n1      pos\r\n2      neg\r\n3      pos\r\n4      neg\r\n5      pos\r\n6      neg\r\n\r\nDe Diabetes-gegevensreeks telt 768 waarnemingen en negen variabelen. De eerste acht variabelen zijn van het numerieke type en de afhankelijke/output variabele (diabetes) is een factor/categorische variabele. Het is ook merkbaar dat veel variabelen NA waarden bevatten (missende waarde). Onze volgende taak is het de gegevens te verfijnen/wijzigen, zodat ze compatibel worden met het modelleeralgoritme. Eerst nog eens beter naar de data kijken.\r\n\r\nRows: 768\r\nColumns: 9\r\n$ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, …\r\n$ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110,…\r\n$ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80,…\r\n$ triceps  <dbl> 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA,…\r\n$ insulin  <dbl> NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, N…\r\n$ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.…\r\n$ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.…\r\n$ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57,…\r\n$ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, n…\r\n\r\nVoorbereiding van de gegevens\r\nDe eerste stap is het verwijderen van data rijen met NA waarden met behulp van na.omit( ) functie. De volgende stap is nogmaals het controleren van de gegevens met behulp van de glimpse( ) functie.\r\n\r\nRows: 392\r\nColumns: 9\r\n$ pregnant <dbl> 1, 0, 3, 2, 1, 5, 0, 1, 1, 3, 11, 10, 1, 13, 3, 3, …\r\n$ glucose  <dbl> 89, 137, 78, 197, 189, 166, 118, 103, 115, 126, 143…\r\n$ pressure <dbl> 66, 40, 50, 70, 60, 72, 84, 30, 70, 88, 94, 70, 66,…\r\n$ triceps  <dbl> 23, 35, 32, 45, 23, 19, 47, 38, 30, 41, 33, 26, 15,…\r\n$ insulin  <dbl> 94, 168, 88, 543, 846, 175, 230, 83, 96, 235, 146, …\r\n$ mass     <dbl> 28.1, 43.1, 31.0, 30.5, 30.1, 25.8, 45.8, 43.3, 34.…\r\n$ pedigree <dbl> 0.167, 2.288, 0.248, 0.158, 0.398, 0.587, 0.551, 0.…\r\n$ age      <dbl> 21, 33, 26, 53, 59, 51, 31, 33, 32, 27, 51, 41, 22,…\r\n$ diabetes <fct> neg, pos, pos, pos, pos, pos, pos, neg, pos, neg, p…\r\n\r\nDe uiteindelijke (voorbereide) gegevens bevatten 392 waarnemingen en 9 kolommen. De onafhankelijke variabelen zijn van het type numeriek/dubbel, terwijl de afhankelijke/uitgaande binaire variabele van het type factor/categorie is (neg/ pos).\r\nGegevensniveaus\r\nWe kunnen het referentieniveau van de afhankelijke variabele controleren met de functie levels( ). We kunnen zien dat het referentieniveau neg is (het allereerste niveau).\r\n\r\n[1] \"neg\" \"pos\"\r\n\r\nInstellen referentieniveau\r\nVoor een betere interpretatie (later voor het uitzetten van de ROC curve) moeten we het referentieniveau van onze afhankelijke variabele “diabetes” op positief (pos) zetten met de relevel( ) functie.\r\n\r\n[1] \"pos\" \"neg\"\r\n\r\nSplitsing training en testset\r\nDe volledige dataset wordt in het algemeen opgesplitst in 75% train en 25% test data set (algemene vuistregel). 75% van de trainingsdata wordt gebruikt om het model te trainen, terwijl de overige 25% wordt gebruikt om te controleren hoe het model generaliseerde op ongeziene/test data set.\r\nOm een split object te maken kun je de initial_split( ) functie gebruiken waar je de dataset, proportie en een strata argument voor moet opgeven. Door de afhankelijke variabele in het strata-attribuut op te geven, wordt gestratificeerde steekproeftrekking uitgevoerd. Gestratificeerde steekproeftrekking is nuttig als je afhankelijke variabele een ongelijke klasse heeft.\r\nDe volgende stap is het aanroepen van de training( ) en testing( ) functies op het split object (d.w.z. diabetes_split) om de trainings- (diabetes_train) en test- (diabetes_test) datasets op te slaan.\r\nDe training set bevat 295 waarnemingen, terwijl de test set 97 waarnemingen bevat.\r\n\r\n[1] 295\r\n[1] 97\r\n\r\nFitten van logistische regressie\r\nJe kunt met tidymodels elk type model pasklaar maken met behulp van de volgende stappen. l Stap 1: roep de modelfunctie op: hier gebruiken we logistic_reg( ) omdat we een logistisch regressiemodel willen draaien.\r\nStap 2: gebruik de set_engine( ) functie om de familie van het model op te geven. We geven het glm argument op, omdat logistische regressie onder de ‘Generalized Linear Regression’-familie valt.\r\nStap 3: gebruik de set_mode( ) functie en geef het type model op dat je wilt toepassen. Hier willen we pos vs neg classificeren, dus het is een classificatie.\r\nStap 4: Vervolgens moet je de fit( ) functie gebruiken om het model te fitten en daarbinnen moet je de formule notatie en de dataset (diabetes_train) opgeven.\r\nplus notatie → diabetes ~ ind_variable 1 + ind_variable 2 + …….so on\r\ntilde punt notatioe →\r\ndiabetes~. betekent dat diabetes wordt voorspeld door de rest van de variabelen in het gegevensbestand (d.w.z. alle onafhankelijke variabelen), behalve de afhankelijke variabele, d.w.z. diabetes.\r\nNa het draaien van het model is de volgende stap het genereren van de modeloverzichtstabel. Je kunt een mooie tabel maken met behulp van de tidy( ) functie van de broom bibliotheek (die is ingebouwd in de tidymodels bibliotheek). De gerapporteerde coëfficiënten zijn in log-odds termen.\r\n\r\n# A tibble: 9 x 5\r\n  term         estimate std.error statistic  p.value\r\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)  8.64       1.35       6.42   1.36e-10\r\n2 pregnant    -0.0669     0.0591    -1.13   2.57e- 1\r\n3 glucose     -0.0352     0.00642   -5.49   4.05e- 8\r\n4 pressure     0.00931    0.0130     0.715  4.74e- 1\r\n5 triceps      0.00123    0.0196     0.0629 9.50e- 1\r\n6 insulin      0.000802   0.00151    0.530  5.96e- 1\r\n7 mass        -0.0715     0.0302    -2.37   1.79e- 2\r\n8 pedigree    -0.840      0.452     -1.86   6.31e- 2\r\n9 age         -0.0375     0.0199    -1.88   6.01e- 2\r\n\r\nOpgelet: Het teken en de waarde van de coëfficiënten veranderen afhankelijk van de referentie die u voor de afhankelijke variabele hebt ingesteld (in ons geval is pos het referentieniveau) en de waarneming die u op basis van de aselecte steekproefselectie in de opleidingssteekproef hebt opgenomen [bovenstaande resultaten zijn slechts een voorbeeld].\r\nDe interpretatie van coëfficiënten in de log-odds term heeft niet veel zin als je die moet rapporteren in je artikel of publicatie. Daarom werd het begrip odds ratio geïntroduceerd.\r\nDe ODDS is de verhouding van de kans dat een gebeurtenis zich voordoet tot de kans dat de gebeurtenis zich niet voordoet. Wanneer we een verhouding van twee zulke kansen nemen, noemen we dat Odds Ratio.\r\nOdds ratioWiskundig kan men de odds ratio berekenen door de exponent van de geschatte coëfficiënten te nemen. Je kunt bijvoorbeeld direct de odds ratio’s van de coëfficiënten krijgen door de exponentiate = True mee te geven in de tidy( ) functie.\r\nHet resultaat is alleen afhankelijk van de steekproeven die we hebben verkregen tijdens het splitsen. Je kunt een ander resultaat krijgen (odds ratio waarden).\r\n\r\n# A tibble: 9 x 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept) 5660.      1.35       6.42   1.36e-10\r\n2 pregnant       0.935   0.0591    -1.13   2.57e- 1\r\n3 glucose        0.965   0.00642   -5.49   4.05e- 8\r\n4 pressure       1.01    0.0130     0.715  4.74e- 1\r\n5 triceps        1.00    0.0196     0.0629 9.50e- 1\r\n6 insulin        1.00    0.00151    0.530  5.96e- 1\r\n7 mass           0.931   0.0302    -2.37   1.79e- 2\r\n8 pedigree       0.432   0.452     -1.86   6.31e- 2\r\n9 age            0.963   0.0199    -1.88   6.01e- 2\r\n\r\nSignificante kansen\r\nDe tabel geproduceerd door tidy( ) functie kan worden gefilterd. Hier hebben we de variabelen uitgefilterd waarvan de p-waarden lager zijn dan 0.05 (5%) significant niveau. Voor onze steekproef hebben glucose en massa een significante invloed op diabetes.\r\n\r\n# A tibble: 3 x 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept) 5660.      1.35         6.42 1.36e-10\r\n2 glucose        0.965   0.00642     -5.49 4.05e- 8\r\n3 mass           0.931   0.0302      -2.37 1.79e- 2\r\n\r\nModel voorspelling\r\nVoorspelling van de testgegevensklasse\r\nDe volgende stap is het genereren van de testvoorspellingen die we kunnen gebruiken voor de evaluatie van het model. Om de klassevoorspelling (pos/neg) te genereren kunnen wij de predict-functie gebruiken en het getrainde modelobject, de testdataset en het type opgeven, dat hier “klasse” is, aangezien wij de klassevoorspelling willen, geen waarschijnlijkheden.\r\n\r\n# A tibble: 5 x 1\r\n  .pred_class\r\n  <fct>      \r\n1 pos        \r\n2 neg        \r\n3 neg        \r\n4 pos        \r\n5 pos        \r\n\r\nTestdata klasse waarschijnlijkheden\r\nWe kunnen ook voorspellingen genereren voor de klassenwaarschijnlijkheden door het argument “prob” in het type-attribuut mee te geven.\r\n\r\n# A tibble: 5 x 2\r\n  .pred_pos .pred_neg\r\n      <dbl>     <dbl>\r\n1     0.820    0.180 \r\n2     0.352    0.648 \r\n3     0.150    0.850 \r\n4     0.932    0.0683\r\n5     0.875    0.125 \r\n\r\nVoorbereiding van de uiteindelijke gegevens voor de evaluatie van het model\r\nDe volgende stap is het voorbereiden van een gegevensframe dat de kolom diabetes uit de oorspronkelijke testdataset, de voorspelde klasse en de klassevoorspellingswaarschijnlijkheid bevat. We gaan dit dataframe gebruiken voor de evaluatie van het model.\r\n\r\n   diabetes .pred_class .pred_pos  .pred_neg\r\n14      pos         pos 0.8197657 0.18023425\r\n17      pos         neg 0.3520320 0.64796802\r\n36      neg         neg 0.1499556 0.85004441\r\n44      pos         pos 0.9317037 0.06829634\r\n54      pos         pos 0.8752335 0.12476646\r\n\r\nModelevaluatie\r\nConfusiematrix\r\nWe kunnen een confusiematrix genereren met de conf_mat( )-functie door het uiteindelijke dataframe, diabetes_results, de waarheidskolom, diabetes en voorspelde klasse (.pred_class) in het schattingsattribuut op te geven.\r\nUit de confusiematrix blijkt dat de testdataset 65 gevallen van negatieve (neg) en 32 gevallen van positieve (pos) waarnemingen bevat. Het getrainde model classificeert 61 negatieven (neg) en 18 positieven (pos) accuraat.\r\n\r\n          Truth\r\nPrediction pos neg\r\n       pos  18   4\r\n       neg  14  61\r\n\r\nWe kunnen ook het yardstick pakket gebruiken dat bij het tidymodels pakket hoort om verschillende evaluatie metrieken te genereren voor de testdata set.\r\nNauwkeurigheid\r\nWe kunnen de classificatienauwkeurigheid berekenen met de accuracy( )-functie door het uiteindelijke dataframe, diabetes_results, de waarheidskolom, diabetes en voorspelde klasse (.pred_class) in het schattingsattribuut op te geven. De classificatienauwkeurigheid van het model op de testdataset is ongeveer 81,4%.\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.814\r\n\r\nSensitiviteit\r\nDe sensitiviteit van een classificator is de verhouding tussen het aantal dat correct als positief wordt geïdentificeerd (TP) en het aantal dat daadwerkelijk positief is (FN+TP).\r\nSensitivity = TP / FN+TP\r\nDe geschatte sensitiviteitswaarde is 0,562, wat wijst op een slechte detectie van positieve klassen in de testdataset.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 sens    binary         0.562\r\n\r\nSpecificiteit\r\nSpecificiteit van een classificator is de verhouding tussen het aantal dat correct als negatief werd geclassificeerd (TN) en het aantal dat werkelijk negatief was (FP+TN).\r\nSpecificity = TN/FP+TN\r\nDe geschatte specificiteitswaarde is 0,938, wat wijst op een algemeen goede detectie van negatieve klassen in de testdataset.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 spec    binary         0.938\r\n\r\nPrecisie\r\nHoeveel van alle positieven werden correct als positief geclassificeerd?\r\nPrecisie = TP/TP+FP\r\nDe geschatte precisie waarde is 0.818.\r\n\r\n# A tibble: 1 x 3\r\n  .metric   .estimator .estimate\r\n  <chr>     <chr>          <dbl>\r\n1 precision binary         0.818\r\n\r\nRecall\r\nRecall en sensitiviteit zijn hetzelfde.\r\nRecall = TP / FN+TP\r\nDe geschatte recall-waarde is 0.562.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 recall  binary         0.562\r\n\r\nF-maat\r\nF-maat is een gewogen harmonisch gemiddelde van precisie en recall met de beste score 1 en de slechtste score 0. De F-maatscore geeft het evenwicht tussen precisie en recall weer. De F1-score is ongeveer 0,667, wat betekent dat het getrainde model een classificatiekracht van 66,7% heeft.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 f_meas  binary         0.667\r\n\r\nKappa\r\nCohen Kappa geeft informatie over hoeveel beter een model is dan de willekeurige classificator. Kappa kan gaan van -1 tot +1. De waarde <0 betekent geen overeenstemming, terwijl 1,0 een perfecte overeenstemming aangeeft. Uit de geschatte kappastatistieken bleek een matige overeenkomst.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 kap     binary         0.544\r\n\r\nMatthews Correlatie Coefficient (MCC)\r\nDe Matthews correlatiecoëfficiënt (MCC) wordt gebruikt als maatstaf voor de kwaliteit van een binaire classificator. De waarde varieert van -1 tot +1.\r\nMCC: -1 wijst op totale onenigheid MCC: 0 wijst op geen overeenstemming MCC: +1 wijst op totale overeenstemming\r\nUit de geschatte MCC-statistieken bleek een matige overeenstemming.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 mcc     binary         0.562\r\n\r\nEvaluatiematen genereren\r\nWe kunnen de custom_metrics( )-functie gebruiken om verschillende metrieken tegelijk te genereren.\r\nStap 1: laat eerst zien wat je wilt laten zien door metric_set( ) te gebruiken Step 2: gebruik decustom_metrics( ) functie en betrek dit op de diabetes_results dataframe, diabaets kolom en op de voorspelde klasse (.pred_class).\r\n\r\n# A tibble: 8 x 3\r\n  .metric   .estimator .estimate\r\n  <chr>     <chr>          <dbl>\r\n1 accuracy  binary         0.814\r\n2 sens      binary         0.562\r\n3 spec      binary         0.938\r\n4 precision binary         0.818\r\n5 recall    binary         0.562\r\n6 f_meas    binary         0.667\r\n7 kap       binary         0.544\r\n8 mcc       binary         0.562\r\n\r\nROC-AUC\r\nROC-AUC is a performance measurement for the classification problem at various thresholds settings. ROC_AUC tells how much the model is capable of distinguishing between classes. The trained logistic regression model has a ROC-AUC of 0.921 indicating overall good predictive performance.\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 roc_auc binary         0.921\r\n\r\nROC-curve\r\nROC-AUC is een evaluatiemaat voor het classificatieprobleem bij verschillende drempelinstellingen. ROC-AUC geeft aan in welke mate het model in staat is een onderscheid te maken tussen de klassen. Het getrainde logistische regressiemodel heeft een ROC-AUC van 0,921, wat wijst op een algemeen goede voorspellende prestatie.\r\nDe ROC-curve wordt uitgezet met TPR (Sensitiviteit) tegen de FPR/ (1- Specificiteit), waarbij Sensitiviteit op de y-as staat en 1-Specificiteit op de x-as. Een lijn wordt diagonaal getrokken om de 50-50 verdeling van de grafiek aan te geven. Als de kromme dichter bij de lijn ligt, is de prestatie van de classificeerder lager en dan niet beter dan een toevallige gok.\r\nJe kunt een ROC Curve genereren met de roc_curve( ) functie waarbij je de waarheidskolom (diabetes) en de voorspelde kansen voor de positieve klasse (.pred_pos) moet opgeven.\r\nOns model heeft een ROC-AUC score van 0.921 wat aangeeft dat het een goed model is dat onderscheid kan maken tussen patiënten met diabetes en zonder diabetes.\r\n\r\n\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction pos neg\r\n       pos  18   4\r\n       neg  14  61\r\n                                          \r\n               Accuracy : 0.8144          \r\n                 95% CI : (0.7227, 0.8862)\r\n    No Information Rate : 0.6701          \r\n    P-Value [Acc > NIR] : 0.001173        \r\n                                          \r\n                  Kappa : 0.5441          \r\n                                          \r\n Mcnemar's Test P-Value : 0.033895        \r\n                                          \r\n            Sensitivity : 0.5625          \r\n            Specificity : 0.9385          \r\n         Pos Pred Value : 0.8182          \r\n         Neg Pred Value : 0.8133          \r\n             Prevalence : 0.3299          \r\n         Detection Rate : 0.1856          \r\n   Detection Prevalence : 0.2268          \r\n      Balanced Accuracy : 0.7505          \r\n                                          \r\n       'Positive' Class : pos             \r\n                                          \r\n\r\nBinaire logistische regressie is nog steeds een enorm populair ML-algoritme (voor binaire classificatie) in het bèta/technische onderzoeksdomein. Het is nog steeds zeer eenvoudig te trainen en te interpreteren, in vergelijking met veel complexere modellen.\r\nReferenties\r\nNewman, C. B. D. & Merz, C. (1998). UCI Repository of machine learning databases, Technical report, University of California, Irvine, Dept. of Information and Computer Sciences.\r\nShrikant I. Bangdiwala (2018). Regression: binary logistic, International Journal of Injury Control and Safety Promotion, DOI: 10.1080/17457300.2018.1486503\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-22-tidymodels-opnieuw/",
    "title": "Tidymodels opnieuw",
    "description": "Blog van Rebecca Barter onder de titel 'Tidymodels: tidy machine learning in R'",
    "author": [
      {
        "name": "Rebecca Barter, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [],
    "contents": "\r\nTidymodels: een nette en consistente manier om met machine learning in R te werken\r\nTidyverse is misschien wel een van de grootste successen van R de laatste jaren. Het is een basispakket (een suite van pakketten) waarmee je heel veel statistiscche bewerkingen goed en betrekkelijk eenvoudig kunt uitvoeren. De laatste jaren is tidymodels ontwikkeld dat voor het modelleren van data het basispakket moet worden en het ontwikkelt zich vergelijkbaar de gereedschapskist van tidyverse maar dan op het gebied van machine learning.\r\nWaarom tidymodels? Nou, het blijkt dat R een consistentieprobleem heeft. Omdat alles rondom machine learning door verschillende mensen is gemaakt, allemaal met verschillende principes, heeft alles een net iets andere interface gekregenen. Om de boel in lijn te houden is onderhand een frustrerende bezigheid. Enkele jaren geleden ontwikkelde Max Kuhn (nu bij RStudio in dienst) het caret R-pakket, dat is zo’n uniforme interface voor een groot aantal machine learning-modellen die er in R zijn. Het programma caret bestaat nog steeds, was in veel opzichten geweldig en is nog steeds goed te gebruiken. Maar in andere opzichten is het beperkt. Zo kan het vrij traag zijn, zelfs bij gebuik van data in bescheiden omvang.\r\ncaret was een geweldig uitgangspunt, dus RStudio heeft Max Kuhn ingehuurd om te werken aan een tidy versie van caret. Hij en veel anderen ontwikkelden de afgelopen jaren tidymodels.tidymodels is al een paar jaar in ontwikkeling en delen ervan waren al eerder uitgebracht. Die volledige versie is in het voorjaar van 2020 gepresenteerd en Barter schreef vlak daarvoor deze tutoriol. Ondertussen is het voldoende ontwikkeld als je het wil leren! Terwijl caret niet verder ontwikkeld wordt (je kunt caret blijven gebruiken en je bestaande caret-code werkt nog steeds, het pakket wordt alleen niet onderhouden), zal tidymodels het uiteindelijk overbodig maken.\r\nDeze tutorial van Barter is gebaseerd op Alison Hill’s dia’s van Introduction to Machine Learning with the Tidyverse, die alle dia’s bevat voor de cursus die ze met Garrett Grolemund voor RStudio heeft voorbereid::conf(2020), en Edgar Ruiz’s Gentle introduction to tidymodels op de website van RStudio. In deze tutorial gaat zij ervan uit dat de gebruiker bepaalde basiskennis heeft, voornamelijk omgaan met dplyr (b.v. piping %>% en een functie zoals mutate()).\r\nWat is tidymodels?\r\nNet als tidyverse, dat uit verschillende pakketten bestaat zoals ggplot2 en dplyr, zitten er ook in tidymodels enkele kernpakketten, zoals\r\nrsample: voor het uit elkaar halen van een datasample (b.v. train/test of cross-validatie);\r\nrecipes: voor pre-procesfuncties;\r\nparsnip: voor het specificeren van het model;\r\nyardstick: voor het evalueren van van het model;\r\ntune: voor het afstemmen van parameters;\r\nworkflow: om alles samen te brengen.\r\nNet zoals je de hele suite aan pakketten van tidyverse kunt binnenhalen door library(tidyverse) in te tikken. tidymodels bestaat dus uit verschillende pakketten en soms zal ik hieronder individuele pakketten noemen.\r\nEerst maar eens de boel klaarzetten\r\nAls je deze pakketten nog niet hebt geïnstalleerd, moet je dat wel eerst doen (slechts één keer) door install.packages(\"tidymodels\") te gebruiken. Vervolgens laad je bepaalde bibliotheken: tidymodels en tidyverse.\r\n\r\n\r\n\r\n\r\nRows: 768\r\nColumns: 9\r\n$ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, …\r\n$ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110,…\r\n$ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, …\r\n$ triceps  <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19…\r\n$ insulin  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 1…\r\n$ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.…\r\n$ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.…\r\n$ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57,…\r\n$ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, n…\r\n\r\nWe zullen gebruik maken van de Pima Indian Women’s diabetes-dataset dat informatie bevat over de diabetes status van 768 Pima Indian vrouwen(diabetes). In de dataset zitten daarnaast enkele predictoren zoals het aantal zwangerschappen (pregnant), concentratie glucose (glucose), diastolische bloeddruk (pressure), triceps huidplooidikte (triceps), 2 uur serum insuline (insuline), BMI (mass), diabetes stamboom functie (pedigree) en hun leeftijd (age). Voor het geval je het je afvraagt, de Pima Indianen zijn een groep indianen die leven in een gebied dat bestaat uit wat nu centraal en zuidelijk Arizona is. De korte naam “Pima” zou afkomstig zijn van een zinsnede die “ik weet het niet” betekent, die ze herhaaldelijk gebruikten in hun eerste ontmoetingen met Spaanse kolonisten. Wikipedia bedankt!\r\n\r\n\r\n\r\n\r\n    pregnant glucose pressure triceps insulin mass pedigree age\r\n1          6     148       72      35       0 33.6    0.627  50\r\n2          1      85       66      29       0 26.6    0.351  31\r\n3          8     183       64       0       0 23.3    0.672  32\r\n4          1      89       66      23      94 28.1    0.167  21\r\n5          0     137       40      35     168 43.1    2.288  33\r\n6          5     116       74       0       0 25.6    0.201  30\r\n7          3      78       50      32      88 31.0    0.248  26\r\n8         10     115        0       0       0 35.3    0.134  29\r\n9          2     197       70      45     543 30.5    0.158  53\r\n10         8     125       96       0       0  0.0    0.232  54\r\n11         4     110       92       0       0 37.6    0.191  30\r\n12        10     168       74       0       0 38.0    0.537  34\r\n13        10     139       80       0       0 27.1    1.441  57\r\n14         1     189       60      23     846 30.1    0.398  59\r\n15         5     166       72      19     175 25.8    0.587  51\r\n16         7     100        0       0       0 30.0    0.484  32\r\n17         0     118       84      47     230 45.8    0.551  31\r\n18         7     107       74       0       0 29.6    0.254  31\r\n19         1     103       30      38      83 43.3    0.183  33\r\n20         1     115       70      30      96 34.6    0.529  32\r\n21         3     126       88      41     235 39.3    0.704  27\r\n22         8      99       84       0       0 35.4    0.388  50\r\n23         7     196       90       0       0 39.8    0.451  41\r\n24         9     119       80      35       0 29.0    0.263  29\r\n25        11     143       94      33     146 36.6    0.254  51\r\n26        10     125       70      26     115 31.1    0.205  41\r\n27         7     147       76       0       0 39.4    0.257  43\r\n28         1      97       66      15     140 23.2    0.487  22\r\n29        13     145       82      19     110 22.2    0.245  57\r\n30         5     117       92       0       0 34.1    0.337  38\r\n31         5     109       75      26       0 36.0    0.546  60\r\n32         3     158       76      36     245 31.6    0.851  28\r\n33         3      88       58      11      54 24.8    0.267  22\r\n34         6      92       92       0       0 19.9    0.188  28\r\n35        10     122       78      31       0 27.6    0.512  45\r\n36         4     103       60      33     192 24.0    0.966  33\r\n37        11     138       76       0       0 33.2    0.420  35\r\n38         9     102       76      37       0 32.9    0.665  46\r\n39         2      90       68      42       0 38.2    0.503  27\r\n40         4     111       72      47     207 37.1    1.390  56\r\n41         3     180       64      25      70 34.0    0.271  26\r\n42         7     133       84       0       0 40.2    0.696  37\r\n43         7     106       92      18       0 22.7    0.235  48\r\n44         9     171      110      24     240 45.4    0.721  54\r\n45         7     159       64       0       0 27.4    0.294  40\r\n46         0     180       66      39       0 42.0    1.893  25\r\n47         1     146       56       0       0 29.7    0.564  29\r\n48         2      71       70      27       0 28.0    0.586  22\r\n49         7     103       66      32       0 39.1    0.344  31\r\n50         7     105        0       0       0  0.0    0.305  24\r\n51         1     103       80      11      82 19.4    0.491  22\r\n52         1     101       50      15      36 24.2    0.526  26\r\n53         5      88       66      21      23 24.4    0.342  30\r\n54         8     176       90      34     300 33.7    0.467  58\r\n55         7     150       66      42     342 34.7    0.718  42\r\n56         1      73       50      10       0 23.0    0.248  21\r\n57         7     187       68      39     304 37.7    0.254  41\r\n58         0     100       88      60     110 46.8    0.962  31\r\n59         0     146       82       0       0 40.5    1.781  44\r\n60         0     105       64      41     142 41.5    0.173  22\r\n61         2      84        0       0       0  0.0    0.304  21\r\n62         8     133       72       0       0 32.9    0.270  39\r\n63         5      44       62       0       0 25.0    0.587  36\r\n64         2     141       58      34     128 25.4    0.699  24\r\n65         7     114       66       0       0 32.8    0.258  42\r\n66         5      99       74      27       0 29.0    0.203  32\r\n67         0     109       88      30       0 32.5    0.855  38\r\n68         2     109       92       0       0 42.7    0.845  54\r\n69         1      95       66      13      38 19.6    0.334  25\r\n70         4     146       85      27     100 28.9    0.189  27\r\n71         2     100       66      20      90 32.9    0.867  28\r\n72         5     139       64      35     140 28.6    0.411  26\r\n73        13     126       90       0       0 43.4    0.583  42\r\n74         4     129       86      20     270 35.1    0.231  23\r\n75         1      79       75      30       0 32.0    0.396  22\r\n76         1       0       48      20       0 24.7    0.140  22\r\n77         7      62       78       0       0 32.6    0.391  41\r\n78         5      95       72      33       0 37.7    0.370  27\r\n79         0     131        0       0       0 43.2    0.270  26\r\n80         2     112       66      22       0 25.0    0.307  24\r\n81         3     113       44      13       0 22.4    0.140  22\r\n82         2      74        0       0       0  0.0    0.102  22\r\n83         7      83       78      26      71 29.3    0.767  36\r\n84         0     101       65      28       0 24.6    0.237  22\r\n85         5     137      108       0       0 48.8    0.227  37\r\n86         2     110       74      29     125 32.4    0.698  27\r\n87        13     106       72      54       0 36.6    0.178  45\r\n88         2     100       68      25      71 38.5    0.324  26\r\n89        15     136       70      32     110 37.1    0.153  43\r\n90         1     107       68      19       0 26.5    0.165  24\r\n91         1      80       55       0       0 19.1    0.258  21\r\n92         4     123       80      15     176 32.0    0.443  34\r\n93         7      81       78      40      48 46.7    0.261  42\r\n94         4     134       72       0       0 23.8    0.277  60\r\n95         2     142       82      18      64 24.7    0.761  21\r\n96         6     144       72      27     228 33.9    0.255  40\r\n97         2      92       62      28       0 31.6    0.130  24\r\n98         1      71       48      18      76 20.4    0.323  22\r\n99         6      93       50      30      64 28.7    0.356  23\r\n100        1     122       90      51     220 49.7    0.325  31\r\n101        1     163       72       0       0 39.0    1.222  33\r\n102        1     151       60       0       0 26.1    0.179  22\r\n103        0     125       96       0       0 22.5    0.262  21\r\n104        1      81       72      18      40 26.6    0.283  24\r\n105        2      85       65       0       0 39.6    0.930  27\r\n106        1     126       56      29     152 28.7    0.801  21\r\n107        1      96      122       0       0 22.4    0.207  27\r\n108        4     144       58      28     140 29.5    0.287  37\r\n109        3      83       58      31      18 34.3    0.336  25\r\n110        0      95       85      25      36 37.4    0.247  24\r\n111        3     171       72      33     135 33.3    0.199  24\r\n112        8     155       62      26     495 34.0    0.543  46\r\n113        1      89       76      34      37 31.2    0.192  23\r\n114        4      76       62       0       0 34.0    0.391  25\r\n115        7     160       54      32     175 30.5    0.588  39\r\n116        4     146       92       0       0 31.2    0.539  61\r\n117        5     124       74       0       0 34.0    0.220  38\r\n118        5      78       48       0       0 33.7    0.654  25\r\n119        4      97       60      23       0 28.2    0.443  22\r\n120        4      99       76      15      51 23.2    0.223  21\r\n121        0     162       76      56     100 53.2    0.759  25\r\n122        6     111       64      39       0 34.2    0.260  24\r\n123        2     107       74      30     100 33.6    0.404  23\r\n124        5     132       80       0       0 26.8    0.186  69\r\n125        0     113       76       0       0 33.3    0.278  23\r\n126        1      88       30      42      99 55.0    0.496  26\r\n127        3     120       70      30     135 42.9    0.452  30\r\n128        1     118       58      36      94 33.3    0.261  23\r\n129        1     117       88      24     145 34.5    0.403  40\r\n130        0     105       84       0       0 27.9    0.741  62\r\n131        4     173       70      14     168 29.7    0.361  33\r\n132        9     122       56       0       0 33.3    1.114  33\r\n133        3     170       64      37     225 34.5    0.356  30\r\n134        8      84       74      31       0 38.3    0.457  39\r\n135        2      96       68      13      49 21.1    0.647  26\r\n136        2     125       60      20     140 33.8    0.088  31\r\n137        0     100       70      26      50 30.8    0.597  21\r\n138        0      93       60      25      92 28.7    0.532  22\r\n139        0     129       80       0       0 31.2    0.703  29\r\n140        5     105       72      29     325 36.9    0.159  28\r\n141        3     128       78       0       0 21.1    0.268  55\r\n142        5     106       82      30       0 39.5    0.286  38\r\n143        2     108       52      26      63 32.5    0.318  22\r\n144       10     108       66       0       0 32.4    0.272  42\r\n145        4     154       62      31     284 32.8    0.237  23\r\n146        0     102       75      23       0  0.0    0.572  21\r\n147        9      57       80      37       0 32.8    0.096  41\r\n148        2     106       64      35     119 30.5    1.400  34\r\n149        5     147       78       0       0 33.7    0.218  65\r\n150        2      90       70      17       0 27.3    0.085  22\r\n151        1     136       74      50     204 37.4    0.399  24\r\n152        4     114       65       0       0 21.9    0.432  37\r\n153        9     156       86      28     155 34.3    1.189  42\r\n154        1     153       82      42     485 40.6    0.687  23\r\n155        8     188       78       0       0 47.9    0.137  43\r\n156        7     152       88      44       0 50.0    0.337  36\r\n157        2      99       52      15      94 24.6    0.637  21\r\n158        1     109       56      21     135 25.2    0.833  23\r\n159        2      88       74      19      53 29.0    0.229  22\r\n160       17     163       72      41     114 40.9    0.817  47\r\n161        4     151       90      38       0 29.7    0.294  36\r\n162        7     102       74      40     105 37.2    0.204  45\r\n163        0     114       80      34     285 44.2    0.167  27\r\n164        2     100       64      23       0 29.7    0.368  21\r\n165        0     131       88       0       0 31.6    0.743  32\r\n166        6     104       74      18     156 29.9    0.722  41\r\n167        3     148       66      25       0 32.5    0.256  22\r\n168        4     120       68       0       0 29.6    0.709  34\r\n169        4     110       66       0       0 31.9    0.471  29\r\n170        3     111       90      12      78 28.4    0.495  29\r\n171        6     102       82       0       0 30.8    0.180  36\r\n172        6     134       70      23     130 35.4    0.542  29\r\n173        2      87        0      23       0 28.9    0.773  25\r\n174        1      79       60      42      48 43.5    0.678  23\r\n175        2      75       64      24      55 29.7    0.370  33\r\n176        8     179       72      42     130 32.7    0.719  36\r\n177        6      85       78       0       0 31.2    0.382  42\r\n178        0     129      110      46     130 67.1    0.319  26\r\n179        5     143       78       0       0 45.0    0.190  47\r\n180        5     130       82       0       0 39.1    0.956  37\r\n181        6      87       80       0       0 23.2    0.084  32\r\n182        0     119       64      18      92 34.9    0.725  23\r\n183        1       0       74      20      23 27.7    0.299  21\r\n184        5      73       60       0       0 26.8    0.268  27\r\n185        4     141       74       0       0 27.6    0.244  40\r\n186        7     194       68      28       0 35.9    0.745  41\r\n187        8     181       68      36     495 30.1    0.615  60\r\n188        1     128       98      41      58 32.0    1.321  33\r\n189        8     109       76      39     114 27.9    0.640  31\r\n190        5     139       80      35     160 31.6    0.361  25\r\n191        3     111       62       0       0 22.6    0.142  21\r\n192        9     123       70      44      94 33.1    0.374  40\r\n193        7     159       66       0       0 30.4    0.383  36\r\n194       11     135        0       0       0 52.3    0.578  40\r\n195        8      85       55      20       0 24.4    0.136  42\r\n196        5     158       84      41     210 39.4    0.395  29\r\n197        1     105       58       0       0 24.3    0.187  21\r\n198        3     107       62      13      48 22.9    0.678  23\r\n199        4     109       64      44      99 34.8    0.905  26\r\n200        4     148       60      27     318 30.9    0.150  29\r\n201        0     113       80      16       0 31.0    0.874  21\r\n202        1     138       82       0       0 40.1    0.236  28\r\n203        0     108       68      20       0 27.3    0.787  32\r\n204        2      99       70      16      44 20.4    0.235  27\r\n205        6     103       72      32     190 37.7    0.324  55\r\n206        5     111       72      28       0 23.9    0.407  27\r\n207        8     196       76      29     280 37.5    0.605  57\r\n208        5     162      104       0       0 37.7    0.151  52\r\n209        1      96       64      27      87 33.2    0.289  21\r\n210        7     184       84      33       0 35.5    0.355  41\r\n211        2      81       60      22       0 27.7    0.290  25\r\n212        0     147       85      54       0 42.8    0.375  24\r\n213        7     179       95      31       0 34.2    0.164  60\r\n214        0     140       65      26     130 42.6    0.431  24\r\n215        9     112       82      32     175 34.2    0.260  36\r\n216       12     151       70      40     271 41.8    0.742  38\r\n217        5     109       62      41     129 35.8    0.514  25\r\n218        6     125       68      30     120 30.0    0.464  32\r\n219        5      85       74      22       0 29.0    1.224  32\r\n220        5     112       66       0       0 37.8    0.261  41\r\n221        0     177       60      29     478 34.6    1.072  21\r\n222        2     158       90       0       0 31.6    0.805  66\r\n223        7     119        0       0       0 25.2    0.209  37\r\n224        7     142       60      33     190 28.8    0.687  61\r\n225        1     100       66      15      56 23.6    0.666  26\r\n226        1      87       78      27      32 34.6    0.101  22\r\n227        0     101       76       0       0 35.7    0.198  26\r\n228        3     162       52      38       0 37.2    0.652  24\r\n229        4     197       70      39     744 36.7    2.329  31\r\n230        0     117       80      31      53 45.2    0.089  24\r\n231        4     142       86       0       0 44.0    0.645  22\r\n232        6     134       80      37     370 46.2    0.238  46\r\n233        1      79       80      25      37 25.4    0.583  22\r\n234        4     122       68       0       0 35.0    0.394  29\r\n235        3      74       68      28      45 29.7    0.293  23\r\n236        4     171       72       0       0 43.6    0.479  26\r\n237        7     181       84      21     192 35.9    0.586  51\r\n238        0     179       90      27       0 44.1    0.686  23\r\n239        9     164       84      21       0 30.8    0.831  32\r\n240        0     104       76       0       0 18.4    0.582  27\r\n241        1      91       64      24       0 29.2    0.192  21\r\n242        4      91       70      32      88 33.1    0.446  22\r\n243        3     139       54       0       0 25.6    0.402  22\r\n244        6     119       50      22     176 27.1    1.318  33\r\n245        2     146       76      35     194 38.2    0.329  29\r\n246        9     184       85      15       0 30.0    1.213  49\r\n247       10     122       68       0       0 31.2    0.258  41\r\n248        0     165       90      33     680 52.3    0.427  23\r\n249        9     124       70      33     402 35.4    0.282  34\r\n250        1     111       86      19       0 30.1    0.143  23\r\n251        9     106       52       0       0 31.2    0.380  42\r\n252        2     129       84       0       0 28.0    0.284  27\r\n253        2      90       80      14      55 24.4    0.249  24\r\n254        0      86       68      32       0 35.8    0.238  25\r\n255       12      92       62       7     258 27.6    0.926  44\r\n256        1     113       64      35       0 33.6    0.543  21\r\n257        3     111       56      39       0 30.1    0.557  30\r\n258        2     114       68      22       0 28.7    0.092  25\r\n259        1     193       50      16     375 25.9    0.655  24\r\n260       11     155       76      28     150 33.3    1.353  51\r\n261        3     191       68      15     130 30.9    0.299  34\r\n262        3     141        0       0       0 30.0    0.761  27\r\n263        4      95       70      32       0 32.1    0.612  24\r\n264        3     142       80      15       0 32.4    0.200  63\r\n265        4     123       62       0       0 32.0    0.226  35\r\n266        5      96       74      18      67 33.6    0.997  43\r\n267        0     138        0       0       0 36.3    0.933  25\r\n268        2     128       64      42       0 40.0    1.101  24\r\n269        0     102       52       0       0 25.1    0.078  21\r\n270        2     146        0       0       0 27.5    0.240  28\r\n271       10     101       86      37       0 45.6    1.136  38\r\n272        2     108       62      32      56 25.2    0.128  21\r\n273        3     122       78       0       0 23.0    0.254  40\r\n274        1      71       78      50      45 33.2    0.422  21\r\n275       13     106       70       0       0 34.2    0.251  52\r\n276        2     100       70      52      57 40.5    0.677  25\r\n277        7     106       60      24       0 26.5    0.296  29\r\n278        0     104       64      23     116 27.8    0.454  23\r\n279        5     114       74       0       0 24.9    0.744  57\r\n280        2     108       62      10     278 25.3    0.881  22\r\n281        0     146       70       0       0 37.9    0.334  28\r\n282       10     129       76      28     122 35.9    0.280  39\r\n283        7     133       88      15     155 32.4    0.262  37\r\n284        7     161       86       0       0 30.4    0.165  47\r\n285        2     108       80       0       0 27.0    0.259  52\r\n286        7     136       74      26     135 26.0    0.647  51\r\n287        5     155       84      44     545 38.7    0.619  34\r\n288        1     119       86      39     220 45.6    0.808  29\r\n289        4      96       56      17      49 20.8    0.340  26\r\n290        5     108       72      43      75 36.1    0.263  33\r\n291        0      78       88      29      40 36.9    0.434  21\r\n292        0     107       62      30      74 36.6    0.757  25\r\n293        2     128       78      37     182 43.3    1.224  31\r\n294        1     128       48      45     194 40.5    0.613  24\r\n295        0     161       50       0       0 21.9    0.254  65\r\n296        6     151       62      31     120 35.5    0.692  28\r\n297        2     146       70      38     360 28.0    0.337  29\r\n298        0     126       84      29     215 30.7    0.520  24\r\n299       14     100       78      25     184 36.6    0.412  46\r\n300        8     112       72       0       0 23.6    0.840  58\r\n301        0     167        0       0       0 32.3    0.839  30\r\n302        2     144       58      33     135 31.6    0.422  25\r\n303        5      77       82      41      42 35.8    0.156  35\r\n304        5     115       98       0       0 52.9    0.209  28\r\n305        3     150       76       0       0 21.0    0.207  37\r\n306        2     120       76      37     105 39.7    0.215  29\r\n307       10     161       68      23     132 25.5    0.326  47\r\n308        0     137       68      14     148 24.8    0.143  21\r\n309        0     128       68      19     180 30.5    1.391  25\r\n310        2     124       68      28     205 32.9    0.875  30\r\n311        6      80       66      30       0 26.2    0.313  41\r\n312        0     106       70      37     148 39.4    0.605  22\r\n313        2     155       74      17      96 26.6    0.433  27\r\n314        3     113       50      10      85 29.5    0.626  25\r\n315        7     109       80      31       0 35.9    1.127  43\r\n316        2     112       68      22      94 34.1    0.315  26\r\n317        3      99       80      11      64 19.3    0.284  30\r\n318        3     182       74       0       0 30.5    0.345  29\r\n319        3     115       66      39     140 38.1    0.150  28\r\n320        6     194       78       0       0 23.5    0.129  59\r\n321        4     129       60      12     231 27.5    0.527  31\r\n322        3     112       74      30       0 31.6    0.197  25\r\n323        0     124       70      20       0 27.4    0.254  36\r\n324       13     152       90      33      29 26.8    0.731  43\r\n325        2     112       75      32       0 35.7    0.148  21\r\n326        1     157       72      21     168 25.6    0.123  24\r\n327        1     122       64      32     156 35.1    0.692  30\r\n328       10     179       70       0       0 35.1    0.200  37\r\n329        2     102       86      36     120 45.5    0.127  23\r\n330        6     105       70      32      68 30.8    0.122  37\r\n331        8     118       72      19       0 23.1    1.476  46\r\n332        2      87       58      16      52 32.7    0.166  25\r\n333        1     180        0       0       0 43.3    0.282  41\r\n334       12     106       80       0       0 23.6    0.137  44\r\n335        1      95       60      18      58 23.9    0.260  22\r\n336        0     165       76      43     255 47.9    0.259  26\r\n337        0     117        0       0       0 33.8    0.932  44\r\n338        5     115       76       0       0 31.2    0.343  44\r\n339        9     152       78      34     171 34.2    0.893  33\r\n340        7     178       84       0       0 39.9    0.331  41\r\n341        1     130       70      13     105 25.9    0.472  22\r\n342        1      95       74      21      73 25.9    0.673  36\r\n343        1       0       68      35       0 32.0    0.389  22\r\n344        5     122       86       0       0 34.7    0.290  33\r\n345        8      95       72       0       0 36.8    0.485  57\r\n346        8     126       88      36     108 38.5    0.349  49\r\n347        1     139       46      19      83 28.7    0.654  22\r\n348        3     116        0       0       0 23.5    0.187  23\r\n349        3      99       62      19      74 21.8    0.279  26\r\n350        5       0       80      32       0 41.0    0.346  37\r\n351        4      92       80       0       0 42.2    0.237  29\r\n352        4     137       84       0       0 31.2    0.252  30\r\n353        3      61       82      28       0 34.4    0.243  46\r\n354        1      90       62      12      43 27.2    0.580  24\r\n355        3      90       78       0       0 42.7    0.559  21\r\n356        9     165       88       0       0 30.4    0.302  49\r\n357        1     125       50      40     167 33.3    0.962  28\r\n358       13     129        0      30       0 39.9    0.569  44\r\n359       12      88       74      40      54 35.3    0.378  48\r\n360        1     196       76      36     249 36.5    0.875  29\r\n361        5     189       64      33     325 31.2    0.583  29\r\n362        5     158       70       0       0 29.8    0.207  63\r\n363        5     103      108      37       0 39.2    0.305  65\r\n364        4     146       78       0       0 38.5    0.520  67\r\n365        4     147       74      25     293 34.9    0.385  30\r\n366        5      99       54      28      83 34.0    0.499  30\r\n367        6     124       72       0       0 27.6    0.368  29\r\n368        0     101       64      17       0 21.0    0.252  21\r\n369        3      81       86      16      66 27.5    0.306  22\r\n370        1     133      102      28     140 32.8    0.234  45\r\n371        3     173       82      48     465 38.4    2.137  25\r\n372        0     118       64      23      89  0.0    1.731  21\r\n373        0      84       64      22      66 35.8    0.545  21\r\n374        2     105       58      40      94 34.9    0.225  25\r\n375        2     122       52      43     158 36.2    0.816  28\r\n376       12     140       82      43     325 39.2    0.528  58\r\n377        0      98       82      15      84 25.2    0.299  22\r\n378        1      87       60      37      75 37.2    0.509  22\r\n379        4     156       75       0       0 48.3    0.238  32\r\n380        0      93      100      39      72 43.4    1.021  35\r\n381        1     107       72      30      82 30.8    0.821  24\r\n382        0     105       68      22       0 20.0    0.236  22\r\n383        1     109       60       8     182 25.4    0.947  21\r\n384        1      90       62      18      59 25.1    1.268  25\r\n385        1     125       70      24     110 24.3    0.221  25\r\n386        1     119       54      13      50 22.3    0.205  24\r\n387        5     116       74      29       0 32.3    0.660  35\r\n388        8     105      100      36       0 43.3    0.239  45\r\n389        5     144       82      26     285 32.0    0.452  58\r\n390        3     100       68      23      81 31.6    0.949  28\r\n391        1     100       66      29     196 32.0    0.444  42\r\n392        5     166       76       0       0 45.7    0.340  27\r\n393        1     131       64      14     415 23.7    0.389  21\r\n394        4     116       72      12      87 22.1    0.463  37\r\n395        4     158       78       0       0 32.9    0.803  31\r\n396        2     127       58      24     275 27.7    1.600  25\r\n397        3      96       56      34     115 24.7    0.944  39\r\n398        0     131       66      40       0 34.3    0.196  22\r\n399        3      82       70       0       0 21.1    0.389  25\r\n400        3     193       70      31       0 34.9    0.241  25\r\n401        4      95       64       0       0 32.0    0.161  31\r\n402        6     137       61       0       0 24.2    0.151  55\r\n403        5     136       84      41      88 35.0    0.286  35\r\n404        9      72       78      25       0 31.6    0.280  38\r\n405        5     168       64       0       0 32.9    0.135  41\r\n406        2     123       48      32     165 42.1    0.520  26\r\n407        4     115       72       0       0 28.9    0.376  46\r\n408        0     101       62       0       0 21.9    0.336  25\r\n409        8     197       74       0       0 25.9    1.191  39\r\n410        1     172       68      49     579 42.4    0.702  28\r\n411        6     102       90      39       0 35.7    0.674  28\r\n412        1     112       72      30     176 34.4    0.528  25\r\n413        1     143       84      23     310 42.4    1.076  22\r\n414        1     143       74      22      61 26.2    0.256  21\r\n415        0     138       60      35     167 34.6    0.534  21\r\n416        3     173       84      33     474 35.7    0.258  22\r\n417        1      97       68      21       0 27.2    1.095  22\r\n418        4     144       82      32       0 38.5    0.554  37\r\n419        1      83       68       0       0 18.2    0.624  27\r\n420        3     129       64      29     115 26.4    0.219  28\r\n421        1     119       88      41     170 45.3    0.507  26\r\n422        2      94       68      18      76 26.0    0.561  21\r\n423        0     102       64      46      78 40.6    0.496  21\r\n424        2     115       64      22       0 30.8    0.421  21\r\n425        8     151       78      32     210 42.9    0.516  36\r\n426        4     184       78      39     277 37.0    0.264  31\r\n427        0      94        0       0       0  0.0    0.256  25\r\n428        1     181       64      30     180 34.1    0.328  38\r\n429        0     135       94      46     145 40.6    0.284  26\r\n430        1      95       82      25     180 35.0    0.233  43\r\n431        2      99        0       0       0 22.2    0.108  23\r\n432        3      89       74      16      85 30.4    0.551  38\r\n433        1      80       74      11      60 30.0    0.527  22\r\n434        2     139       75       0       0 25.6    0.167  29\r\n435        1      90       68       8       0 24.5    1.138  36\r\n436        0     141        0       0       0 42.4    0.205  29\r\n437       12     140       85      33       0 37.4    0.244  41\r\n438        5     147       75       0       0 29.9    0.434  28\r\n439        1      97       70      15       0 18.2    0.147  21\r\n440        6     107       88       0       0 36.8    0.727  31\r\n441        0     189      104      25       0 34.3    0.435  41\r\n442        2      83       66      23      50 32.2    0.497  22\r\n443        4     117       64      27     120 33.2    0.230  24\r\n444        8     108       70       0       0 30.5    0.955  33\r\n445        4     117       62      12       0 29.7    0.380  30\r\n446        0     180       78      63      14 59.4    2.420  25\r\n447        1     100       72      12      70 25.3    0.658  28\r\n448        0      95       80      45      92 36.5    0.330  26\r\n449        0     104       64      37      64 33.6    0.510  22\r\n450        0     120       74      18      63 30.5    0.285  26\r\n451        1      82       64      13      95 21.2    0.415  23\r\n452        2     134       70       0       0 28.9    0.542  23\r\n453        0      91       68      32     210 39.9    0.381  25\r\n454        2     119        0       0       0 19.6    0.832  72\r\n455        2     100       54      28     105 37.8    0.498  24\r\n456       14     175       62      30       0 33.6    0.212  38\r\n457        1     135       54       0       0 26.7    0.687  62\r\n458        5      86       68      28      71 30.2    0.364  24\r\n459       10     148       84      48     237 37.6    1.001  51\r\n460        9     134       74      33      60 25.9    0.460  81\r\n461        9     120       72      22      56 20.8    0.733  48\r\n462        1      71       62       0       0 21.8    0.416  26\r\n463        8      74       70      40      49 35.3    0.705  39\r\n464        5      88       78      30       0 27.6    0.258  37\r\n465       10     115       98       0       0 24.0    1.022  34\r\n466        0     124       56      13     105 21.8    0.452  21\r\n467        0      74       52      10      36 27.8    0.269  22\r\n468        0      97       64      36     100 36.8    0.600  25\r\n469        8     120        0       0       0 30.0    0.183  38\r\n470        6     154       78      41     140 46.1    0.571  27\r\n471        1     144       82      40       0 41.3    0.607  28\r\n472        0     137       70      38       0 33.2    0.170  22\r\n473        0     119       66      27       0 38.8    0.259  22\r\n474        7     136       90       0       0 29.9    0.210  50\r\n475        4     114       64       0       0 28.9    0.126  24\r\n476        0     137       84      27       0 27.3    0.231  59\r\n477        2     105       80      45     191 33.7    0.711  29\r\n478        7     114       76      17     110 23.8    0.466  31\r\n479        8     126       74      38      75 25.9    0.162  39\r\n480        4     132       86      31       0 28.0    0.419  63\r\n481        3     158       70      30     328 35.5    0.344  35\r\n482        0     123       88      37       0 35.2    0.197  29\r\n483        4      85       58      22      49 27.8    0.306  28\r\n484        0      84       82      31     125 38.2    0.233  23\r\n485        0     145        0       0       0 44.2    0.630  31\r\n486        0     135       68      42     250 42.3    0.365  24\r\n487        1     139       62      41     480 40.7    0.536  21\r\n488        0     173       78      32     265 46.5    1.159  58\r\n489        4      99       72      17       0 25.6    0.294  28\r\n490        8     194       80       0       0 26.1    0.551  67\r\n491        2      83       65      28      66 36.8    0.629  24\r\n492        2      89       90      30       0 33.5    0.292  42\r\n493        4      99       68      38       0 32.8    0.145  33\r\n494        4     125       70      18     122 28.9    1.144  45\r\n495        3      80        0       0       0  0.0    0.174  22\r\n496        6     166       74       0       0 26.6    0.304  66\r\n497        5     110       68       0       0 26.0    0.292  30\r\n498        2      81       72      15      76 30.1    0.547  25\r\n499        7     195       70      33     145 25.1    0.163  55\r\n500        6     154       74      32     193 29.3    0.839  39\r\n501        2     117       90      19      71 25.2    0.313  21\r\n502        3      84       72      32       0 37.2    0.267  28\r\n503        6       0       68      41       0 39.0    0.727  41\r\n504        7      94       64      25      79 33.3    0.738  41\r\n505        3      96       78      39       0 37.3    0.238  40\r\n506       10      75       82       0       0 33.3    0.263  38\r\n507        0     180       90      26      90 36.5    0.314  35\r\n508        1     130       60      23     170 28.6    0.692  21\r\n509        2      84       50      23      76 30.4    0.968  21\r\n510        8     120       78       0       0 25.0    0.409  64\r\n511       12      84       72      31       0 29.7    0.297  46\r\n512        0     139       62      17     210 22.1    0.207  21\r\n513        9      91       68       0       0 24.2    0.200  58\r\n514        2      91       62       0       0 27.3    0.525  22\r\n515        3      99       54      19      86 25.6    0.154  24\r\n516        3     163       70      18     105 31.6    0.268  28\r\n517        9     145       88      34     165 30.3    0.771  53\r\n518        7     125       86       0       0 37.6    0.304  51\r\n519       13      76       60       0       0 32.8    0.180  41\r\n520        6     129       90       7     326 19.6    0.582  60\r\n521        2      68       70      32      66 25.0    0.187  25\r\n522        3     124       80      33     130 33.2    0.305  26\r\n523        6     114        0       0       0  0.0    0.189  26\r\n524        9     130       70       0       0 34.2    0.652  45\r\n525        3     125       58       0       0 31.6    0.151  24\r\n526        3      87       60      18       0 21.8    0.444  21\r\n527        1      97       64      19      82 18.2    0.299  21\r\n528        3     116       74      15     105 26.3    0.107  24\r\n529        0     117       66      31     188 30.8    0.493  22\r\n530        0     111       65       0       0 24.6    0.660  31\r\n531        2     122       60      18     106 29.8    0.717  22\r\n532        0     107       76       0       0 45.3    0.686  24\r\n533        1      86       66      52      65 41.3    0.917  29\r\n534        6      91        0       0       0 29.8    0.501  31\r\n535        1      77       56      30      56 33.3    1.251  24\r\n536        4     132        0       0       0 32.9    0.302  23\r\n537        0     105       90       0       0 29.6    0.197  46\r\n538        0      57       60       0       0 21.7    0.735  67\r\n539        0     127       80      37     210 36.3    0.804  23\r\n540        3     129       92      49     155 36.4    0.968  32\r\n541        8     100       74      40     215 39.4    0.661  43\r\n542        3     128       72      25     190 32.4    0.549  27\r\n543       10      90       85      32       0 34.9    0.825  56\r\n544        4      84       90      23      56 39.5    0.159  25\r\n545        1      88       78      29      76 32.0    0.365  29\r\n546        8     186       90      35     225 34.5    0.423  37\r\n547        5     187       76      27     207 43.6    1.034  53\r\n548        4     131       68      21     166 33.1    0.160  28\r\n549        1     164       82      43      67 32.8    0.341  50\r\n550        4     189      110      31       0 28.5    0.680  37\r\n551        1     116       70      28       0 27.4    0.204  21\r\n552        3      84       68      30     106 31.9    0.591  25\r\n553        6     114       88       0       0 27.8    0.247  66\r\n554        1      88       62      24      44 29.9    0.422  23\r\n555        1      84       64      23     115 36.9    0.471  28\r\n556        7     124       70      33     215 25.5    0.161  37\r\n557        1      97       70      40       0 38.1    0.218  30\r\n558        8     110       76       0       0 27.8    0.237  58\r\n559       11     103       68      40       0 46.2    0.126  42\r\n560       11      85       74       0       0 30.1    0.300  35\r\n561        6     125       76       0       0 33.8    0.121  54\r\n562        0     198       66      32     274 41.3    0.502  28\r\n563        1      87       68      34      77 37.6    0.401  24\r\n564        6      99       60      19      54 26.9    0.497  32\r\n565        0      91       80       0       0 32.4    0.601  27\r\n566        2      95       54      14      88 26.1    0.748  22\r\n567        1      99       72      30      18 38.6    0.412  21\r\n568        6      92       62      32     126 32.0    0.085  46\r\n569        4     154       72      29     126 31.3    0.338  37\r\n570        0     121       66      30     165 34.3    0.203  33\r\n571        3      78       70       0       0 32.5    0.270  39\r\n572        2     130       96       0       0 22.6    0.268  21\r\n573        3     111       58      31      44 29.5    0.430  22\r\n574        2      98       60      17     120 34.7    0.198  22\r\n575        1     143       86      30     330 30.1    0.892  23\r\n576        1     119       44      47      63 35.5    0.280  25\r\n577        6     108       44      20     130 24.0    0.813  35\r\n578        2     118       80       0       0 42.9    0.693  21\r\n579       10     133       68       0       0 27.0    0.245  36\r\n580        2     197       70      99       0 34.7    0.575  62\r\n581        0     151       90      46       0 42.1    0.371  21\r\n582        6     109       60      27       0 25.0    0.206  27\r\n583       12     121       78      17       0 26.5    0.259  62\r\n584        8     100       76       0       0 38.7    0.190  42\r\n585        8     124       76      24     600 28.7    0.687  52\r\n586        1      93       56      11       0 22.5    0.417  22\r\n587        8     143       66       0       0 34.9    0.129  41\r\n588        6     103       66       0       0 24.3    0.249  29\r\n589        3     176       86      27     156 33.3    1.154  52\r\n590        0      73        0       0       0 21.1    0.342  25\r\n591       11     111       84      40       0 46.8    0.925  45\r\n592        2     112       78      50     140 39.4    0.175  24\r\n593        3     132       80       0       0 34.4    0.402  44\r\n594        2      82       52      22     115 28.5    1.699  25\r\n595        6     123       72      45     230 33.6    0.733  34\r\n596        0     188       82      14     185 32.0    0.682  22\r\n597        0      67       76       0       0 45.3    0.194  46\r\n598        1      89       24      19      25 27.8    0.559  21\r\n599        1     173       74       0       0 36.8    0.088  38\r\n600        1     109       38      18     120 23.1    0.407  26\r\n601        1     108       88      19       0 27.1    0.400  24\r\n602        6      96        0       0       0 23.7    0.190  28\r\n603        1     124       74      36       0 27.8    0.100  30\r\n604        7     150       78      29     126 35.2    0.692  54\r\n605        4     183        0       0       0 28.4    0.212  36\r\n606        1     124       60      32       0 35.8    0.514  21\r\n607        1     181       78      42     293 40.0    1.258  22\r\n608        1      92       62      25      41 19.5    0.482  25\r\n609        0     152       82      39     272 41.5    0.270  27\r\n610        1     111       62      13     182 24.0    0.138  23\r\n611        3     106       54      21     158 30.9    0.292  24\r\n612        3     174       58      22     194 32.9    0.593  36\r\n613        7     168       88      42     321 38.2    0.787  40\r\n614        6     105       80      28       0 32.5    0.878  26\r\n615       11     138       74      26     144 36.1    0.557  50\r\n616        3     106       72       0       0 25.8    0.207  27\r\n617        6     117       96       0       0 28.7    0.157  30\r\n618        2      68       62      13      15 20.1    0.257  23\r\n619        9     112       82      24       0 28.2    1.282  50\r\n620        0     119        0       0       0 32.4    0.141  24\r\n621        2     112       86      42     160 38.4    0.246  28\r\n622        2      92       76      20       0 24.2    1.698  28\r\n623        6     183       94       0       0 40.8    1.461  45\r\n624        0      94       70      27     115 43.5    0.347  21\r\n625        2     108       64       0       0 30.8    0.158  21\r\n626        4      90       88      47      54 37.7    0.362  29\r\n627        0     125       68       0       0 24.7    0.206  21\r\n628        0     132       78       0       0 32.4    0.393  21\r\n629        5     128       80       0       0 34.6    0.144  45\r\n630        4      94       65      22       0 24.7    0.148  21\r\n631        7     114       64       0       0 27.4    0.732  34\r\n632        0     102       78      40      90 34.5    0.238  24\r\n633        2     111       60       0       0 26.2    0.343  23\r\n634        1     128       82      17     183 27.5    0.115  22\r\n635       10      92       62       0       0 25.9    0.167  31\r\n636       13     104       72       0       0 31.2    0.465  38\r\n637        5     104       74       0       0 28.8    0.153  48\r\n638        2      94       76      18      66 31.6    0.649  23\r\n639        7      97       76      32      91 40.9    0.871  32\r\n640        1     100       74      12      46 19.5    0.149  28\r\n641        0     102       86      17     105 29.3    0.695  27\r\n642        4     128       70       0       0 34.3    0.303  24\r\n643        6     147       80       0       0 29.5    0.178  50\r\n644        4      90        0       0       0 28.0    0.610  31\r\n645        3     103       72      30     152 27.6    0.730  27\r\n646        2     157       74      35     440 39.4    0.134  30\r\n647        1     167       74      17     144 23.4    0.447  33\r\n648        0     179       50      36     159 37.8    0.455  22\r\n649       11     136       84      35     130 28.3    0.260  42\r\n650        0     107       60      25       0 26.4    0.133  23\r\n651        1      91       54      25     100 25.2    0.234  23\r\n652        1     117       60      23     106 33.8    0.466  27\r\n653        5     123       74      40      77 34.1    0.269  28\r\n654        2     120       54       0       0 26.8    0.455  27\r\n655        1     106       70      28     135 34.2    0.142  22\r\n656        2     155       52      27     540 38.7    0.240  25\r\n657        2     101       58      35      90 21.8    0.155  22\r\n658        1     120       80      48     200 38.9    1.162  41\r\n659       11     127      106       0       0 39.0    0.190  51\r\n660        3      80       82      31      70 34.2    1.292  27\r\n661       10     162       84       0       0 27.7    0.182  54\r\n662        1     199       76      43       0 42.9    1.394  22\r\n663        8     167      106      46     231 37.6    0.165  43\r\n664        9     145       80      46     130 37.9    0.637  40\r\n665        6     115       60      39       0 33.7    0.245  40\r\n666        1     112       80      45     132 34.8    0.217  24\r\n667        4     145       82      18       0 32.5    0.235  70\r\n668       10     111       70      27       0 27.5    0.141  40\r\n669        6      98       58      33     190 34.0    0.430  43\r\n670        9     154       78      30     100 30.9    0.164  45\r\n671        6     165       68      26     168 33.6    0.631  49\r\n672        1      99       58      10       0 25.4    0.551  21\r\n673       10      68      106      23      49 35.5    0.285  47\r\n674        3     123      100      35     240 57.3    0.880  22\r\n675        8      91       82       0       0 35.6    0.587  68\r\n676        6     195       70       0       0 30.9    0.328  31\r\n677        9     156       86       0       0 24.8    0.230  53\r\n678        0      93       60       0       0 35.3    0.263  25\r\n679        3     121       52       0       0 36.0    0.127  25\r\n680        2     101       58      17     265 24.2    0.614  23\r\n681        2      56       56      28      45 24.2    0.332  22\r\n682        0     162       76      36       0 49.6    0.364  26\r\n683        0      95       64      39     105 44.6    0.366  22\r\n684        4     125       80       0       0 32.3    0.536  27\r\n685        5     136       82       0       0  0.0    0.640  69\r\n686        2     129       74      26     205 33.2    0.591  25\r\n687        3     130       64       0       0 23.1    0.314  22\r\n688        1     107       50      19       0 28.3    0.181  29\r\n689        1     140       74      26     180 24.1    0.828  23\r\n690        1     144       82      46     180 46.1    0.335  46\r\n691        8     107       80       0       0 24.6    0.856  34\r\n692       13     158      114       0       0 42.3    0.257  44\r\n693        2     121       70      32      95 39.1    0.886  23\r\n694        7     129       68      49     125 38.5    0.439  43\r\n695        2      90       60       0       0 23.5    0.191  25\r\n696        7     142       90      24     480 30.4    0.128  43\r\n697        3     169       74      19     125 29.9    0.268  31\r\n698        0      99        0       0       0 25.0    0.253  22\r\n699        4     127       88      11     155 34.5    0.598  28\r\n700        4     118       70       0       0 44.5    0.904  26\r\n701        2     122       76      27     200 35.9    0.483  26\r\n702        6     125       78      31       0 27.6    0.565  49\r\n703        1     168       88      29       0 35.0    0.905  52\r\n704        2     129        0       0       0 38.5    0.304  41\r\n705        4     110       76      20     100 28.4    0.118  27\r\n706        6      80       80      36       0 39.8    0.177  28\r\n707       10     115        0       0       0  0.0    0.261  30\r\n708        2     127       46      21     335 34.4    0.176  22\r\n709        9     164       78       0       0 32.8    0.148  45\r\n710        2      93       64      32     160 38.0    0.674  23\r\n711        3     158       64      13     387 31.2    0.295  24\r\n712        5     126       78      27      22 29.6    0.439  40\r\n713       10     129       62      36       0 41.2    0.441  38\r\n714        0     134       58      20     291 26.4    0.352  21\r\n715        3     102       74       0       0 29.5    0.121  32\r\n716        7     187       50      33     392 33.9    0.826  34\r\n717        3     173       78      39     185 33.8    0.970  31\r\n718       10      94       72      18       0 23.1    0.595  56\r\n719        1     108       60      46     178 35.5    0.415  24\r\n720        5      97       76      27       0 35.6    0.378  52\r\n721        4      83       86      19       0 29.3    0.317  34\r\n722        1     114       66      36     200 38.1    0.289  21\r\n723        1     149       68      29     127 29.3    0.349  42\r\n724        5     117       86      30     105 39.1    0.251  42\r\n725        1     111       94       0       0 32.8    0.265  45\r\n726        4     112       78      40       0 39.4    0.236  38\r\n727        1     116       78      29     180 36.1    0.496  25\r\n728        0     141       84      26       0 32.4    0.433  22\r\n729        2     175       88       0       0 22.9    0.326  22\r\n730        2      92       52       0       0 30.1    0.141  22\r\n731        3     130       78      23      79 28.4    0.323  34\r\n732        8     120       86       0       0 28.4    0.259  22\r\n733        2     174       88      37     120 44.5    0.646  24\r\n734        2     106       56      27     165 29.0    0.426  22\r\n735        2     105       75       0       0 23.3    0.560  53\r\n736        4      95       60      32       0 35.4    0.284  28\r\n737        0     126       86      27     120 27.4    0.515  21\r\n738        8      65       72      23       0 32.0    0.600  42\r\n739        2      99       60      17     160 36.6    0.453  21\r\n740        1     102       74       0       0 39.5    0.293  42\r\n741       11     120       80      37     150 42.3    0.785  48\r\n742        3     102       44      20      94 30.8    0.400  26\r\n743        1     109       58      18     116 28.5    0.219  22\r\n744        9     140       94       0       0 32.7    0.734  45\r\n745       13     153       88      37     140 40.6    1.174  39\r\n746       12     100       84      33     105 30.0    0.488  46\r\n747        1     147       94      41       0 49.3    0.358  27\r\n748        1      81       74      41      57 46.3    1.096  32\r\n749        3     187       70      22     200 36.4    0.408  36\r\n750        6     162       62       0       0 24.3    0.178  50\r\n751        4     136       70       0       0 31.2    1.182  22\r\n752        1     121       78      39      74 39.0    0.261  28\r\n753        3     108       62      24       0 26.0    0.223  25\r\n754        0     181       88      44     510 43.3    0.222  26\r\n755        8     154       78      32       0 32.4    0.443  45\r\n756        1     128       88      39     110 36.5    1.057  37\r\n757        7     137       90      41       0 32.0    0.391  39\r\n758        0     123       72       0       0 36.3    0.258  52\r\n759        1     106       76       0       0 37.5    0.197  26\r\n760        6     190       92       0       0 35.5    0.278  66\r\n761        2      88       58      26      16 28.4    0.766  22\r\n762        9     170       74      31       0 44.0    0.403  43\r\n763        9      89       62       0       0 22.5    0.142  33\r\n764       10     101       76      48     180 32.9    0.171  63\r\n765        2     122       70      27       0 36.8    0.340  27\r\n766        5     121       72      23     112 26.2    0.245  30\r\n767        1     126       60       0       0 30.1    0.349  47\r\n768        1      93       70      31       0 30.4    0.315  23\r\n    diabetes\r\n1        pos\r\n2        neg\r\n3        pos\r\n4        neg\r\n5        pos\r\n6        neg\r\n7        pos\r\n8        neg\r\n9        pos\r\n10       pos\r\n11       neg\r\n12       pos\r\n13       neg\r\n14       pos\r\n15       pos\r\n16       pos\r\n17       pos\r\n18       pos\r\n19       neg\r\n20       pos\r\n21       neg\r\n22       neg\r\n23       pos\r\n24       pos\r\n25       pos\r\n26       pos\r\n27       pos\r\n28       neg\r\n29       neg\r\n30       neg\r\n31       neg\r\n32       pos\r\n33       neg\r\n34       neg\r\n35       neg\r\n36       neg\r\n37       neg\r\n38       pos\r\n39       pos\r\n40       pos\r\n41       neg\r\n42       neg\r\n43       neg\r\n44       pos\r\n45       neg\r\n46       pos\r\n47       neg\r\n48       neg\r\n49       pos\r\n50       neg\r\n51       neg\r\n52       neg\r\n53       neg\r\n54       pos\r\n55       neg\r\n56       neg\r\n57       pos\r\n58       neg\r\n59       neg\r\n60       neg\r\n61       neg\r\n62       pos\r\n63       neg\r\n64       neg\r\n65       pos\r\n66       neg\r\n67       pos\r\n68       neg\r\n69       neg\r\n70       neg\r\n71       pos\r\n72       neg\r\n73       pos\r\n74       neg\r\n75       neg\r\n76       neg\r\n77       neg\r\n78       neg\r\n79       pos\r\n80       neg\r\n81       neg\r\n82       neg\r\n83       neg\r\n84       neg\r\n85       pos\r\n86       neg\r\n87       neg\r\n88       neg\r\n89       pos\r\n90       neg\r\n91       neg\r\n92       neg\r\n93       neg\r\n94       pos\r\n95       neg\r\n96       neg\r\n97       neg\r\n98       neg\r\n99       neg\r\n100      pos\r\n101      pos\r\n102      neg\r\n103      neg\r\n104      neg\r\n105      neg\r\n106      neg\r\n107      neg\r\n108      neg\r\n109      neg\r\n110      pos\r\n111      pos\r\n112      pos\r\n113      neg\r\n114      neg\r\n115      pos\r\n116      pos\r\n117      pos\r\n118      neg\r\n119      neg\r\n120      neg\r\n121      pos\r\n122      neg\r\n123      neg\r\n124      neg\r\n125      pos\r\n126      pos\r\n127      neg\r\n128      neg\r\n129      pos\r\n130      pos\r\n131      pos\r\n132      pos\r\n133      pos\r\n134      neg\r\n135      neg\r\n136      neg\r\n137      neg\r\n138      neg\r\n139      neg\r\n140      neg\r\n141      neg\r\n142      neg\r\n143      neg\r\n144      pos\r\n145      neg\r\n146      neg\r\n147      neg\r\n148      neg\r\n149      neg\r\n150      neg\r\n151      neg\r\n152      neg\r\n153      pos\r\n154      neg\r\n155      pos\r\n156      pos\r\n157      neg\r\n158      neg\r\n159      neg\r\n160      pos\r\n161      neg\r\n162      neg\r\n163      neg\r\n164      neg\r\n165      pos\r\n166      pos\r\n167      neg\r\n168      neg\r\n169      neg\r\n170      neg\r\n171      pos\r\n172      pos\r\n173      neg\r\n174      neg\r\n175      neg\r\n176      pos\r\n177      neg\r\n178      pos\r\n179      neg\r\n180      pos\r\n181      neg\r\n182      neg\r\n183      neg\r\n184      neg\r\n185      neg\r\n186      pos\r\n187      pos\r\n188      pos\r\n189      pos\r\n190      pos\r\n191      neg\r\n192      neg\r\n193      pos\r\n194      pos\r\n195      neg\r\n196      pos\r\n197      neg\r\n198      pos\r\n199      pos\r\n200      pos\r\n201      neg\r\n202      neg\r\n203      neg\r\n204      neg\r\n205      neg\r\n206      neg\r\n207      pos\r\n208      pos\r\n209      neg\r\n210      pos\r\n211      neg\r\n212      neg\r\n213      neg\r\n214      pos\r\n215      pos\r\n216      pos\r\n217      pos\r\n218      neg\r\n219      pos\r\n220      pos\r\n221      pos\r\n222      pos\r\n223      neg\r\n224      neg\r\n225      neg\r\n226      neg\r\n227      neg\r\n228      pos\r\n229      neg\r\n230      neg\r\n231      pos\r\n232      pos\r\n233      neg\r\n234      neg\r\n235      neg\r\n236      pos\r\n237      pos\r\n238      pos\r\n239      pos\r\n240      neg\r\n241      neg\r\n242      neg\r\n243      pos\r\n244      pos\r\n245      neg\r\n246      pos\r\n247      neg\r\n248      neg\r\n249      neg\r\n250      neg\r\n251      neg\r\n252      neg\r\n253      neg\r\n254      neg\r\n255      pos\r\n256      pos\r\n257      neg\r\n258      neg\r\n259      neg\r\n260      pos\r\n261      neg\r\n262      pos\r\n263      neg\r\n264      neg\r\n265      pos\r\n266      neg\r\n267      pos\r\n268      neg\r\n269      neg\r\n270      pos\r\n271      pos\r\n272      neg\r\n273      neg\r\n274      neg\r\n275      neg\r\n276      neg\r\n277      pos\r\n278      neg\r\n279      neg\r\n280      neg\r\n281      pos\r\n282      neg\r\n283      neg\r\n284      pos\r\n285      pos\r\n286      neg\r\n287      neg\r\n288      pos\r\n289      neg\r\n290      neg\r\n291      neg\r\n292      pos\r\n293      pos\r\n294      pos\r\n295      neg\r\n296      neg\r\n297      pos\r\n298      neg\r\n299      pos\r\n300      neg\r\n301      pos\r\n302      pos\r\n303      neg\r\n304      pos\r\n305      neg\r\n306      neg\r\n307      pos\r\n308      neg\r\n309      pos\r\n310      pos\r\n311      neg\r\n312      neg\r\n313      pos\r\n314      neg\r\n315      pos\r\n316      neg\r\n317      neg\r\n318      pos\r\n319      neg\r\n320      pos\r\n321      neg\r\n322      pos\r\n323      pos\r\n324      pos\r\n325      neg\r\n326      neg\r\n327      pos\r\n328      neg\r\n329      pos\r\n330      neg\r\n331      neg\r\n332      neg\r\n333      pos\r\n334      neg\r\n335      neg\r\n336      neg\r\n337      neg\r\n338      pos\r\n339      pos\r\n340      pos\r\n341      neg\r\n342      neg\r\n343      neg\r\n344      neg\r\n345      neg\r\n346      neg\r\n347      neg\r\n348      neg\r\n349      neg\r\n350      pos\r\n351      neg\r\n352      neg\r\n353      neg\r\n354      neg\r\n355      neg\r\n356      pos\r\n357      pos\r\n358      pos\r\n359      neg\r\n360      pos\r\n361      pos\r\n362      neg\r\n363      neg\r\n364      pos\r\n365      neg\r\n366      neg\r\n367      pos\r\n368      neg\r\n369      neg\r\n370      pos\r\n371      pos\r\n372      neg\r\n373      neg\r\n374      neg\r\n375      neg\r\n376      pos\r\n377      neg\r\n378      neg\r\n379      pos\r\n380      neg\r\n381      neg\r\n382      neg\r\n383      neg\r\n384      neg\r\n385      neg\r\n386      neg\r\n387      pos\r\n388      pos\r\n389      pos\r\n390      neg\r\n391      neg\r\n392      pos\r\n393      neg\r\n394      neg\r\n395      pos\r\n396      neg\r\n397      neg\r\n398      pos\r\n399      neg\r\n400      pos\r\n401      pos\r\n402      neg\r\n403      pos\r\n404      neg\r\n405      pos\r\n406      neg\r\n407      pos\r\n408      neg\r\n409      pos\r\n410      pos\r\n411      neg\r\n412      neg\r\n413      neg\r\n414      neg\r\n415      pos\r\n416      pos\r\n417      neg\r\n418      pos\r\n419      neg\r\n420      pos\r\n421      neg\r\n422      neg\r\n423      neg\r\n424      neg\r\n425      pos\r\n426      pos\r\n427      neg\r\n428      pos\r\n429      neg\r\n430      pos\r\n431      neg\r\n432      neg\r\n433      neg\r\n434      neg\r\n435      neg\r\n436      pos\r\n437      neg\r\n438      neg\r\n439      neg\r\n440      neg\r\n441      pos\r\n442      neg\r\n443      neg\r\n444      pos\r\n445      pos\r\n446      pos\r\n447      neg\r\n448      neg\r\n449      pos\r\n450      neg\r\n451      neg\r\n452      pos\r\n453      neg\r\n454      neg\r\n455      neg\r\n456      pos\r\n457      neg\r\n458      neg\r\n459      pos\r\n460      neg\r\n461      neg\r\n462      neg\r\n463      neg\r\n464      neg\r\n465      neg\r\n466      neg\r\n467      neg\r\n468      neg\r\n469      pos\r\n470      neg\r\n471      neg\r\n472      neg\r\n473      neg\r\n474      neg\r\n475      neg\r\n476      neg\r\n477      pos\r\n478      neg\r\n479      neg\r\n480      neg\r\n481      pos\r\n482      neg\r\n483      neg\r\n484      neg\r\n485      pos\r\n486      pos\r\n487      neg\r\n488      neg\r\n489      neg\r\n490      neg\r\n491      neg\r\n492      neg\r\n493      neg\r\n494      pos\r\n495      neg\r\n496      neg\r\n497      neg\r\n498      neg\r\n499      pos\r\n500      neg\r\n501      neg\r\n502      neg\r\n503      pos\r\n504      neg\r\n505      neg\r\n506      neg\r\n507      pos\r\n508      neg\r\n509      neg\r\n510      neg\r\n511      pos\r\n512      neg\r\n513      neg\r\n514      neg\r\n515      neg\r\n516      pos\r\n517      pos\r\n518      neg\r\n519      neg\r\n520      neg\r\n521      neg\r\n522      neg\r\n523      neg\r\n524      pos\r\n525      neg\r\n526      neg\r\n527      neg\r\n528      neg\r\n529      neg\r\n530      neg\r\n531      neg\r\n532      neg\r\n533      neg\r\n534      neg\r\n535      neg\r\n536      pos\r\n537      neg\r\n538      neg\r\n539      neg\r\n540      pos\r\n541      pos\r\n542      pos\r\n543      pos\r\n544      neg\r\n545      neg\r\n546      pos\r\n547      pos\r\n548      neg\r\n549      neg\r\n550      neg\r\n551      neg\r\n552      neg\r\n553      neg\r\n554      neg\r\n555      neg\r\n556      neg\r\n557      neg\r\n558      neg\r\n559      neg\r\n560      neg\r\n561      pos\r\n562      pos\r\n563      neg\r\n564      neg\r\n565      neg\r\n566      neg\r\n567      neg\r\n568      neg\r\n569      neg\r\n570      pos\r\n571      neg\r\n572      neg\r\n573      neg\r\n574      neg\r\n575      neg\r\n576      neg\r\n577      neg\r\n578      pos\r\n579      neg\r\n580      pos\r\n581      pos\r\n582      neg\r\n583      neg\r\n584      neg\r\n585      pos\r\n586      neg\r\n587      pos\r\n588      neg\r\n589      pos\r\n590      neg\r\n591      pos\r\n592      neg\r\n593      pos\r\n594      neg\r\n595      neg\r\n596      pos\r\n597      neg\r\n598      neg\r\n599      pos\r\n600      neg\r\n601      neg\r\n602      neg\r\n603      neg\r\n604      pos\r\n605      pos\r\n606      neg\r\n607      pos\r\n608      neg\r\n609      neg\r\n610      neg\r\n611      neg\r\n612      pos\r\n613      pos\r\n614      neg\r\n615      pos\r\n616      neg\r\n617      neg\r\n618      neg\r\n619      pos\r\n620      pos\r\n621      neg\r\n622      neg\r\n623      neg\r\n624      neg\r\n625      neg\r\n626      neg\r\n627      neg\r\n628      neg\r\n629      neg\r\n630      neg\r\n631      pos\r\n632      neg\r\n633      neg\r\n634      neg\r\n635      neg\r\n636      pos\r\n637      neg\r\n638      neg\r\n639      pos\r\n640      neg\r\n641      neg\r\n642      neg\r\n643      pos\r\n644      neg\r\n645      neg\r\n646      neg\r\n647      pos\r\n648      pos\r\n649      pos\r\n650      neg\r\n651      neg\r\n652      neg\r\n653      neg\r\n654      neg\r\n655      neg\r\n656      pos\r\n657      neg\r\n658      neg\r\n659      neg\r\n660      pos\r\n661      neg\r\n662      pos\r\n663      pos\r\n664      pos\r\n665      pos\r\n666      neg\r\n667      pos\r\n668      pos\r\n669      neg\r\n670      neg\r\n671      neg\r\n672      neg\r\n673      neg\r\n674      neg\r\n675      neg\r\n676      pos\r\n677      pos\r\n678      neg\r\n679      pos\r\n680      neg\r\n681      neg\r\n682      pos\r\n683      neg\r\n684      pos\r\n685      neg\r\n686      neg\r\n687      neg\r\n688      neg\r\n689      neg\r\n690      pos\r\n691      neg\r\n692      pos\r\n693      neg\r\n694      pos\r\n695      neg\r\n696      pos\r\n697      pos\r\n698      neg\r\n699      neg\r\n700      neg\r\n701      neg\r\n702      pos\r\n703      pos\r\n704      neg\r\n705      neg\r\n706      neg\r\n707      pos\r\n708      neg\r\n709      pos\r\n710      pos\r\n711      neg\r\n712      neg\r\n713      pos\r\n714      neg\r\n715      neg\r\n716      pos\r\n717      pos\r\n718      neg\r\n719      neg\r\n720      pos\r\n721      neg\r\n722      neg\r\n723      pos\r\n724      neg\r\n725      neg\r\n726      neg\r\n727      neg\r\n728      neg\r\n729      neg\r\n730      neg\r\n731      pos\r\n732      pos\r\n733      pos\r\n734      neg\r\n735      neg\r\n736      neg\r\n737      neg\r\n738      neg\r\n739      neg\r\n740      pos\r\n741      pos\r\n742      neg\r\n743      neg\r\n744      pos\r\n745      neg\r\n746      neg\r\n747      pos\r\n748      neg\r\n749      pos\r\n750      pos\r\n751      pos\r\n752      neg\r\n753      neg\r\n754      pos\r\n755      pos\r\n756      pos\r\n757      neg\r\n758      pos\r\n759      neg\r\n760      pos\r\n761      neg\r\n762      pos\r\n763      neg\r\n764      neg\r\n765      neg\r\n766      neg\r\n767      pos\r\n768      neg\r\n\r\nEen snelle verkenning van de dataset toont aan dat er meer nullen in de gegevens zitten dan verwacht (vooral omdat een BMI of tricep huiddikte van 0 onmogelijk is), wat betekent dat ontbrekende waarden als nullen worden geregistreerd. Zie bijvoorbeeld het histogram van de tricep huidplooidikte, waar de nullen voor dikte opvallen.\r\n\r\n\r\n\r\nDit fenomeen is ook te zien in de glucose-, druk-, insuline- en massavariabelen. We zetten eerst de 0-scores in alle variabelen (behalve “zwanger”) over naar NA (missende waarde). Daarvoor gebruiken we de mutate_at()functie (die binnenkort wordt vervangen door mutate() met across()) om aan te geven op welke variabelen we onze muterende functie willen toepassen. We gebruiken de if_else()functie om aan te geven waar we de waarde mee moeten vervangen als de voorwaarde waar of onwaar is.\r\n\r\n\r\n\r\nOnze gegevens zijn klaar. Laten we beginnen met het maken van een aantal tidymodels!\r\nHaal train/test sets uit elkaar\r\nLaten we onze data verdelen in trainings- en testdata. De trainingsdata worden gebruikt om ons model te vinden en de parameters in te stellen (tune). De testdata gebruiken we alleen om de werking van het finale model vast te stellen. Dat splitten kunnen we doen door de inital_split() functie (van het rsample pakket). Dat creëert een speciaal “split” object.\r\n\r\n<Analysis/Assess/Total>\r\n<576/192/768>\r\n\r\ndiabetes_split, ons gesplitste object, vertelt ons hoeveel waarnemingen we hebben in de trainingsset, de testset en de gehele dataset: <train/test/totaal> (576/192/768).\r\nDe trainings- en testsets kunnen uit het “split”-object worden gehaald met behulp van de training() en testing() functies. Hoewel we deze objecten niet echt zullen gebruiken in de pipeline (daarvoor zullen we het diabetes_split-object zelf gebruiken).\r\n\r\n\r\n\r\nOp een gegeven moment zullen we de parameters hiervan wat willen tuenen (afstemmen). Dat doen we met cross-validatie. Zo ontstaat er met vfold_cv() een cross-validatie versie van de trainingsset waar we zo op terugkomen.\r\n\r\n\r\n\r\nDefineeer een recipe\r\nMet het pakket recipes kun je de variabelen een rol geven, als uitkomst of voorspellende variabele (gebruik een “formule”) b.v.. Maar met recipe kun je ook andere voorbereidingsstappen zetten die je nodig acht (zoals standaardiseren, imputeren, PCA, etc). Een recipe voer je uit in delen (gelaagd op elkaar door pipes %>% te gebruiken):\r\nSpecificeer de formule (recipe()): specificeer eerst wat is de uitkomstvariabele en wat zijn de predictoren;\r\nSpecificeer pre-processing steps (step_zzz()): defineer voorbereidingsstappen, zoals imputatie, creëren van dummy variabelen, schalen en wat al niet meer\r\nZo kunnen we bijvoorbeeld de volgende recipe maken.\r\n\r\n\r\n\r\nAls je ooit eerder formules hebt gezien (bijvoorbeeld met behulp van de lm() functie in R), dan weet je misschien dat we onze formule veel efficiënter hadden kunnen schrijven met behulp van een shortcut, waarbij de . alle variabelen in de gegevens vertegenwoordigt: outcome ~ .\r\nDe volledige lijst van beschikbare voorbewerkingsstappen is hier te vinden. In de bovenstaande chunck hebben we de functies all_numeric() en all_predictors() gebruikt als argumenten van voorbereiding. Deze worden “rolselecties” genoemd en geven aan dat we de stap willen toepassen op “alle numerieke” variabelen of “alle predictoren”. De lijst van alle potentiële rolselectoren kan worden gevonden door ?selectis in je console te typen.\r\nMerk op dat we het originele diabetes_clean data-object hebben gebruikt (we stellen recipe(..., data = diabetes_clean)), in plaats van het diabetes_train-object of het diabetes_split-object. Het blijkt dat we deze allemaal hadden kunnen gebruiken. Alle recipes die op dit punt uit het dataobject worden gehaald zijn de namen en rollen van de uitkomst en de voorspellende variabelen. We zullen deze recipe later toepassen op specifieke datasets. Dit betekent dat voor grote datasets een kleinere dataset gebruikt wordt om tijd en geheugen te besparen.\r\nInderdaad, als we een samenvatting van het diabetes_recipe object printen, dan laat het ons gewoon zien hoeveel voorspellingsvariabelen we hebben gespecificeerd en welke stappen we hebben gespecificeerd (maar het implementeert ze eigenlijk nog niet!).\r\n\r\nData Recipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor          8\r\n\r\nOperations:\r\n\r\nCentering and scaling for all_numeric()\r\nK-nearest neighbor imputation for all_predictors()\r\n\r\nAls je de voorbewerkte dataset zelf wilt extraheren, kunt je eerst prep() het recept voor een specifieke dataset en juice() het voorbewerkte recept om de voorbewerkte gegevens te extraheren. Het blijkt dat het extraheren van de voorbewerkte data eigenlijk niet nodig is voor de pipeline, omdat dit onder de motorkap gebeurt als het model geschikt is. Soms is het toch nuttig.\r\n\r\n# A tibble: 576 x 9\r\n   pregnant glucose pressure triceps insulin   mass pedigree     age\r\n      <dbl>   <dbl>    <dbl>   <dbl>   <dbl>  <dbl>    <dbl>   <dbl>\r\n 1   0.673   0.837   -0.0581  0.616   0.328   0.187    0.531  1.37  \r\n 2  -0.824  -1.21    -0.537   0.0354 -0.770  -0.831   -0.359 -0.205 \r\n 3   1.27    1.98    -0.697   0.229   1.33   -1.31     0.676 -0.122 \r\n 4  -1.12    0.478   -2.61    0.616   0.0669  1.57     5.89  -0.0396\r\n 5   0.374  -0.205    0.102  -0.700  -0.404  -0.977   -0.843 -0.287 \r\n 6   1.87   -0.238    0.229   0.229   0.558   0.435   -1.06  -0.370 \r\n 7  -0.525   2.43    -0.218   1.58    3.15   -0.264   -0.982  1.61  \r\n 8   1.27    0.0877   1.86   -0.178   0.863   0.318   -0.743  1.70  \r\n 9   0.0743 -0.401    1.54    0.674   0.139   0.769   -0.875 -0.287 \r\n10   1.87    0.544    0.581  -0.448   0.666  -0.758    3.16   1.94  \r\n# … with 566 more rows, and 1 more variable: diabetes <fct>\r\n\r\nSpecificeer het model\r\nTot nu toe hebben we onze data verdeeld in training en test-sets en onze pre-proces stappen gespecificeerd door een recipe te gebruiken. Nu willen we ons model definiëren en daarvoor gebruiken we het parsnip pakket dat in tidymodels zit.\r\nParsnip biedt een uniforme interface voor de enorme verscheidenheid aan modellen die er in R bestaan. Dit betekent dat je slechts één manier hoeft te leren om een model te specificeren en dan kun je dit gebruiken voor allerlei verschillende modellen, vaak met enkele coderegel.\r\nEr zijn een paar primaire componenten in de modelspecificatie opgeslagen:\r\nHet model type: wat voor soort model wil je gebruiken, zoals rand_forest() voor het random forest-model, logistic_reg() voor het logistisch regressie-model, svm_poly() voor een polynomiaal SVM-model, enz. De volledige lijst van modellen die beschikbaar zijn via parsnip kan [hier] (link naar website) vinden.\r\nDe arguments: de model parameter waarden (de benaming is consistent over verschillende modellen), door het gebruik van set_args().\r\nDe engine: het onderliggende pakket waar het model van wegkomt (bv. “ranger” voor implementatie van Random Forest), door het gebuik van set_engine().\r\nDe mode: het type voorspelling - omdat verschillende pakketten zowel classificatie (binaire/categoriale voorspelling) en regressie (continue voorspelling) kunnen uitvoeren, door het gebruik van set_mode().\r\nAls we bijvoorbeeld een random forest model willen gebruiken, zoals dat in het ranger pakket zit, met als doel classificatie en we willen de try parameter tunen (het afstemmen van het aantal willekeurig gekozen variabelen dat bij elke splitsing in aanmerking moet worden genomen), dan moeten we de volgende modelspecificatie definiëren:\r\n\r\n\r\n\r\nAls je later het variabele belang van jouw uiteindelijke model wilt kunnen onderzoeken, moet je het engine argument opnieuw instellen. De volgende code specificeert bijvoorbeeld een logistisch regressiemodel uit het glm pakket.\r\n\r\n\r\n\r\nDeze code draait niet het model. Net als de recipe, is het veel meer een beschrijving van het model. Echter, wanneer je een parameter op tune() zet wordt het later gestemd in de stemfase van de pipeline (bv. om de waarde vast te stellen van de parameter die de beste performance geeft). Je kunt ook zelf een bepaalde waarde aan de parameter geven wanneer je het niet wilt afstemmen, bv door set_args(mtry = 4) te gebruiken. Een ander ding om op te merken is dat niets wat deze modelspecificatie betreft specifiek is voor de diabetes-dataset.\r\nAlles in een workflow samenbrengen\r\nWe zijn klaar om het model en de recipes in een workflow te plaatsen. Een workflow zet je op door het gebruik van workflow() (van het workflows pakket) en dan kun je een recipe en een model toevoegen.\r\n\r\n\r\n\r\nMerk op dat we de voorbewerkingsstappen nog niet in de recipe hebben geïmplementeerd noch dat we het model hebben gepast. We hebben alleen maar het raamwerk geschreven. Pas als we de parameters hebben afgestemd of in het model hebben gepast, worden het recept en het model daadwerkelijk geïmplementeerd.\r\nAfstemmen van de parameters\r\nOmdat er een parameter is ontwikkeld om af te stemmen (mtry), moeten we dat daar voor gebruiken (bv. de waarde kiezen die de beste performance laat zien) voordat we het model passen. Als je geen parameters hebt om af te stemmen, kun je dit deel overslaan.\r\nDat afstemmen doen we door een cross-validation object (diabetes_cv) te kiezen. Om dat te doen specificeren we de range van mtry waarden die we willen gebruiken en dan voegen we een stemmingslaag toe aan onze workflow door tune_grid() te gebruiken (van het tune pakket). We richten ons op twee maten: accuracy en roc_auc (van het yardstick pakket). Die vertellen ons welke maten we het beste kunnen gebruiken.\r\n\r\n\r\n\r\nJe kunt verschillende parameters afstemmen door verschillende parameters aan de expand.grid() functie toe te voegen, bv. expand.grid(mtry = c(3, 4, 5), trees = c(100, 500)).\r\nHet is altijd goed om de resultaten van de cross-validatie goed te onderzoeken. collect_metrics() is echt een handige functie die in verschillende omstandigheden kan worden gebruikt om te vergelijken die zijn berekend in het object dat is gebruikt. In dit geval komen de maten van de cross-validatie performance over de verschillende waarden van de performance.\r\n\r\n# A tibble: 6 x 7\r\n   mtry .metric  .estimator  mean     n std_err .config             \r\n  <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \r\n1     3 accuracy binary     0.750    10  0.0225 Preprocessor1_Model1\r\n2     3 roc_auc  binary     0.829    10  0.0154 Preprocessor1_Model1\r\n3     4 accuracy binary     0.758    10  0.0226 Preprocessor1_Model2\r\n4     4 roc_auc  binary     0.828    10  0.0149 Preprocessor1_Model2\r\n5     5 accuracy binary     0.753    10  0.0232 Preprocessor1_Model3\r\n6     5 roc_auc  binary     0.826    10  0.0153 Preprocessor1_Model3\r\n\r\nTen opzichte van accuracy en AUC laat mtry = 4 de beste performance zien (hoogste gemiddelde waarden).\r\nAfronden van de workflow\r\nWe willen een laag aan onze workflow toevoegen die overeenkomt met de afgestemde parameter, d.w.z. dat we mtry instellen als de waarde die de beste resultaten opleverde. Als je geen parameters hebt afgestemd, kun je deze stap overslaan.\r\nWe kunnen de beste waarde voor de nauwkeurigheidsmetriek extraheren door de select_best()functie toe te passen op het afstemmingsobject.\r\n\r\n# A tibble: 1 x 2\r\n   mtry .config             \r\n  <dbl> <chr>               \r\n1     4 Preprocessor1_Model2\r\n\r\nDan kunnen we deze parameter aan de workflow toevoegen door de finalize_workflow() functie te gebruiken.\r\n\r\n\r\n\r\nEvalueren van het model op de test set\r\nNu we ons recipe en ons model hebben gedefinieerd en de parameters van het model hebben getuned, zijn we klaar om daadwerkelijk het uiteindelijke model te draaien. Aangezien al deze informatie in het workflow-object zit, zullen we de last_fit() functie toepassen op onze workflow en ons train/test-splitsingsobject. Dit zal automatisch het door de workflow gespecificeerde model trainen met behulp van de trainingsgegevens en evaluaties produceren op basis van de testset.\r\n\r\n\r\n\r\nMerk op dat het object dat wordt gecreëerd een data-frame-achtig object is; het is een tibble met listkolommen.\r\n\r\n# Resampling results\r\n# Manual resampling \r\n# A tibble: 1 x 6\r\n  splits      id        .metrics    .notes    .predictions   .workflow\r\n  <list>      <chr>     <list>      <list>    <list>         <list>   \r\n1 <rsplit [5… train/te… <tibble [2… <tibble … <tibble [192 … <workflo…\r\n\r\nDit is echt een aardige eigenschap van tidymodels (en ook waarom je zo goed kunt werken met tidyverse) omdat je al je nette handelingen op het modelobject kunt uitvoeren.\r\nAangezien we het trainings/testobject al hebben geleverd op het moment dat we in de workflow werken, worden de maten geëvalueerd op de testset. Wanneer we nu de collect_metrics() functie gebruiken (herinner ons dat we deze hebben gebruikt bij het afstemmen van onze parameters), haalt deze de prestaties van het uiteindelijke model (aangezien rf_fit nu bestaat uit een enkel definitief model) toegepast op de test set.\r\n\r\n# A tibble: 2 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.745 Preprocessor1_Model1\r\n2 roc_auc  binary         0.831 Preprocessor1_Model1\r\n\r\nOverall is de performance heel goed, met een accuracy van 0.74 en een AUC van 0.82. Maar deze waarden zijn vaak lager dan in de trainingsset.\r\nJe kunt de test set voorspellingen zelf gebruiken met de collect_predictions() functie. Let op dat er 192 rijen in het voorspellingsobject zitten dat overeenkomt met de test set observaties (juist om jou te laten zien dat deze gebaseerd zijn op de testset meer dan op de trainingsset).\r\n\r\n# A tibble: 192 x 7\r\n   id        .pred_neg .pred_pos  .row .pred_class diabetes .config   \r\n   <chr>         <dbl>     <dbl> <int> <fct>       <fct>    <chr>     \r\n 1 train/te…    1.00     0.00045     4 neg         neg      Preproces…\r\n 2 train/te…    0.967    0.0330      7 neg         pos      Preproces…\r\n 3 train/te…    0.0393   0.961      12 pos         pos      Preproces…\r\n 4 train/te…    0.708    0.292      19 neg         neg      Preproces…\r\n 5 train/te…    0.636    0.364      21 neg         neg      Preproces…\r\n 6 train/te…    0.541    0.459      25 neg         pos      Preproces…\r\n 7 train/te…    0.482    0.518      27 pos         pos      Preproces…\r\n 8 train/te…    0.692    0.308      29 neg         neg      Preproces…\r\n 9 train/te…    0.951    0.0493     34 neg         neg      Preproces…\r\n10 train/te…    0.369    0.631      35 pos         neg      Preproces…\r\n# … with 182 more rows\r\n\r\nOmndat dit een normaal data frame/tibble object is, kunnen we de samenvattingen genereren en een confusie matrix plotten.\r\n\r\n          Truth\r\nPrediction neg pos\r\n       neg 106  33\r\n       pos  16  37\r\n\r\nWe kunnen ook de voorspelde kansverdelingen voor elke klasse in kaart brengen.\r\n\r\n\r\n\r\nDe voorspellingen kun je ook als volgt laten zien:\r\n\r\n[[1]]\r\n# A tibble: 192 x 6\r\n   .pred_neg .pred_pos  .row .pred_class diabetes .config             \r\n       <dbl>     <dbl> <int> <fct>       <fct>    <chr>               \r\n 1    1.00     0.00045     4 neg         neg      Preprocessor1_Model1\r\n 2    0.967    0.0330      7 neg         pos      Preprocessor1_Model1\r\n 3    0.0393   0.961      12 pos         pos      Preprocessor1_Model1\r\n 4    0.708    0.292      19 neg         neg      Preprocessor1_Model1\r\n 5    0.636    0.364      21 neg         neg      Preprocessor1_Model1\r\n 6    0.541    0.459      25 neg         pos      Preprocessor1_Model1\r\n 7    0.482    0.518      27 pos         pos      Preprocessor1_Model1\r\n 8    0.692    0.308      29 neg         neg      Preprocessor1_Model1\r\n 9    0.951    0.0493     34 neg         neg      Preprocessor1_Model1\r\n10    0.369    0.631      35 pos         neg      Preprocessor1_Model1\r\n# … with 182 more rows\r\n\r\nHet laatste model\r\nIn de vorige paragraaf is het model dat is getraind op de trainingsgegevens geëvalueerd aan de hand van de testgegevens. Maar als je eenmaal jouw definitieve model hebt bepaald, wil je het vaak trainen op je volledige dataset en het dan gebruiken om de respons voor nieuwe gegevens te voorspellen.\r\nAls je jouw model wilt gebruiken om de respons voor nieuwe waarnemingen te voorspellen, moet je de fit()functie op jouw workflow gebruiken en de dataset waarop je het uiteindelijke model wilt laten passen (bijvoorbeeld de volledige training + testdataset).\r\n\r\n\r\n\r\nHet final_model object bevat een aantal zaken, waaronder het ranger-object dat getraind is met de parameters die via de workflow in rf_workflow zijn vastgelegd op basis van de gegevens in diabetes_clean (de gecombineerde trainings- en testgegevens).\r\n\r\n══ Workflow [trained] ════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: rand_forest()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n2 Recipe Steps\r\n\r\n• step_normalize()\r\n• step_knnimpute()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\nRanger result\r\n\r\nCall:\r\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \r\n\r\nType:                             Probability estimation \r\nNumber of trees:                  500 \r\nSample size:                      768 \r\nNumber of independent variables:  8 \r\nMtry:                             4 \r\nTarget node size:                 10 \r\nVariable importance mode:         impurity \r\nSplitrule:                        gini \r\nOOB prediction error (Brier s.):  0.1576627 \r\n\r\nAls we de diabetes status van een nieuwe vrouw willen voorspellen, kunnen we de predict() functie gebruiken.\r\nBijvoorbeeld, definieren we de data voor een nieuwe vrouw.\r\n\r\n# A tibble: 1 x 8\r\n  pregnant glucose pressure triceps insulin  mass pedigree   age\r\n     <dbl>   <dbl>    <dbl>   <dbl>   <dbl> <dbl>    <dbl> <dbl>\r\n1        2      95       70      31     102  28.2     0.67    47\r\n\r\nDe voorspelde diabetes status van deze nieuwe vrouw is “negatief”.\r\n\r\n# A tibble: 1 x 1\r\n  .pred_class\r\n  <fct>      \r\n1 neg        \r\n\r\nVariabele belang\r\nAls je de belangrijkheid van een variabele uit je model wilt vaststellen, voor zover je dat kan zien, moet je het modelobject uit het fit() object halen (dat voor ons final_model heet). De functie die het model extraheert is pull_workflow_fit() en dan moet je het fit-object pakken dat de output bevat.\r\n\r\nRanger result\r\n\r\nCall:\r\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \r\n\r\nType:                             Probability estimation \r\nNumber of trees:                  500 \r\nSample size:                      768 \r\nNumber of independent variables:  8 \r\nMtry:                             4 \r\nTarget node size:                 10 \r\nVariable importance mode:         impurity \r\nSplitrule:                        gini \r\nOOB prediction error (Brier s.):  0.1576627 \r\n\r\nVervolgens kun je het belang van de variabele uit het ranger-object zelf halen (variable.importance is een specifiek object in de ranger-output - dit zal moeten worden aangepast voor het specifieke objecttype van andere modellen).\r\n\r\npregnant  glucose pressure  triceps  insulin     mass pedigree \r\n16.33289 80.62800 17.08757 21.43870 51.76331 42.41799 30.79204 \r\n     age \r\n34.36002 \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-10-machine-learning/",
    "title": "Machine Learning met Tidymodels",
    "description": "Enkele blogs zal ik aan Machine Learning besteden. Ik zal enkele tutorials bewerken die mij veel geleerd hebben. Lisa Lendway, van wie ik al twee keer eerder materiaal gebruikte, schreef een goede blog over tidymodels. Zie hieronder.",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\r\nLaad tidyverse, tidymodels en enkele andere pakketten en zet theme (optioneel) voor een bepaalde vormgeving van de figuren.\r\n\r\n\r\n\r\nLees de King County Housing-data in en kijk eens naar de eerste vijf rijen.\r\n\r\n# A tibble: 5 x 21\r\n  id         date        price bedrooms bathrooms sqft_living sqft_lot\r\n  <chr>      <date>      <dbl>    <int>     <dbl>       <int>    <int>\r\n1 7129300520 2014-10-13 221900        3      1           1180     5650\r\n2 6414100192 2014-12-09 538000        3      2.25        2570     7242\r\n3 5631500400 2015-02-25 180000        2      1            770    10000\r\n4 2487200875 2014-12-09 604000        4      3           1960     5000\r\n5 1954400510 2015-02-18 510000        3      2           1680     8080\r\n# … with 14 more variables: floors <dbl>, waterfront <lgl>,\r\n#   view <int>, condition <fct>, grade <fct>, sqft_above <int>,\r\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\r\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\r\n#   sqft_lot15 <int>\r\n\r\nOver de house_prices data lezen we het volgende:\r\n\r\n“Deze dataset bevat huizenverkoopprijzen voor King County, waar Seattle deel van uitmaakt. Het omvat huizen verkocht tussen mei 2014 en mei 2015. Deze dataset is verkregen via Kaggle.com.” De beschrijving van de variabelen in de dataset in de documentatie lijkt niet helemaal te kloppen. Een meer accurate beschrijving is hieronder te vinden. In ieder geval willen we hier de prijs van woningen modelleren.\r\n\r\n\r\nExploratie\r\nKijk eerst eens naar de verdelingen van alle variabelen om te zien of er iets onregelmatigs aan de hand is.\r\nKwantitatieve variabelen:\r\n\r\n\r\n\r\nDingen die opvielen en eerste gedachten over het opstartproces: * ‘Right-skewness’ in de variabele price en alle variabelen betreffende vierkante meters –> log transformeren indien lineaire regressie wordt gebruikt. * Veel 0’s in sqft_basement, view, en yr_renovated –> maak indicator variabelen van het hebben van dat kenmerk vs. niet hebben , dat wil zeggen een variabele genaamd basement waar een 0 aangeeft geen kelder (sqft_basement = 0) en wel een kelder bij (sqft_basement > 0).\r\n* Leeftijd van huis is misschien een betere, interpreteerbare variabele dan bouwjaar –> age_at_sale = year(date) - yr_built.\r\n\r\n\r\n\r\nData splitsen (in train- en test-sets)\r\nEerst splitsen we de gegevens op in training- en test-datasets. We gebruiken de trainingsgegevens om verschillende soorten modellen te proberen en de parameters van die modellen zo nodig aan te passen. De testdataset wordt bewaard voor het allerlaatst om een kleine subset van modellen te vergelijken. De initial_split() functie uit de rsample bibliotheek (onderdeel van tidymodels) wordt gebruikt om tot deze splitsing te komen. We splitsen deze dataset random, maar er zijn andere mogelijkheden om tot gestratificeerde steekproeven te komen. Daarna gebruiken we training() en testing() om de twee datasets, house_training en house_testing, te extraheren.\r\n\r\n<Analysis/Assess/Total>\r\n<16210/5403/21613>\r\n\r\nLater zullen we 5-voudige cross-validatie gebruiken om het model te evalueren en de modelparameters aan te passen. We zetten de vijfvoud van de trainingsdata op met de vfold_cv() functie. We zullen dit later in meer detail uitleggen.\r\n\r\n\r\n\r\nData voorspel: recipe() en step_xxx()\r\nWe gebruiken de recipe()-functie om de uitkomstvariabele en de predictoren te definiëren.\r\nEen verscheidenheid van step_xxx() functies kan worden gebruikt om data te bewerken/transformeren. Vind ze allemaal hier. Ik heb er een paar gebruikt, met korte beschrijvingen in de code. Ik heb ook een aantal selectiefuncties gebruikt, zoals all_predictors() en all_nominal() om de juiste variabelen te selecteren.\r\nWe gebruiken ook update_roles() om de rollen van sommige variabelen te veranderen. Voor ons zijn dit variabelen die we misschien willen meenemen voor evaluatiedoeleinden, maar die niet gebruikt zullen worden bij het bouwen van het model. Ik heb gekozen voor de rol evaluative, maar je kunt die rol elke naam geven die je maar wilt, bijvoorbeeld id, extra, junk (misschien een slecht idee?).\r\n\r\n\r\n\r\nPas het toe op de trainings-dataset, gewoon om te zien wat er gebeurt. Let op de namen van de variabelen.\r\n\r\n# A tibble: 16,210 x 36\r\n   id        date       bedrooms bathrooms sqft_living sqft_lot floors\r\n   <fct>     <date>        <int>     <dbl>       <dbl>    <dbl>  <dbl>\r\n 1 71293005… 2014-10-13        3      1           3.07     3.75      1\r\n 2 64141001… 2014-12-09        3      2.25        3.41     3.86      2\r\n 3 56315004… 2015-02-25        2      1           2.89     4         1\r\n 4 24872008… 2014-12-09        4      3           3.29     3.70      1\r\n 5 19544005… 2015-02-18        3      2           3.23     3.91      1\r\n 6 72375503… 2014-05-12        4      4.5         3.73     5.01      1\r\n 7 13214000… 2014-06-27        3      2.25        3.23     3.83      2\r\n 8 20080002… 2015-01-15        3      1.5         3.03     3.99      1\r\n 9 24146001… 2015-04-15        3      1           3.25     3.87      1\r\n10 37935001… 2015-03-12        3      2.5         3.28     3.82      2\r\n# … with 16,200 more rows, and 29 more variables: waterfront <dbl>,\r\n#   view <dbl>, sqft_above <dbl>, zipcode <fct>, lat <dbl>,\r\n#   long <dbl>, price <dbl>, basement <dbl>, renovated <dbl>,\r\n#   age_at_sale <dbl>, condition_X2 <dbl>, condition_X3 <dbl>,\r\n#   condition_X4 <dbl>, condition_X5 <dbl>, grade_X7 <dbl>,\r\n#   grade_X8 <dbl>, grade_X9 <dbl>, grade_high <dbl>,\r\n#   date_month_Feb <dbl>, date_month_Mar <dbl>, date_month_Apr <dbl>,\r\n#   date_month_May <dbl>, date_month_Jun <dbl>, date_month_Jul <dbl>,\r\n#   date_month_Aug <dbl>, date_month_Sep <dbl>, date_month_Oct <dbl>,\r\n#   date_month_Nov <dbl>, date_month_Dec <dbl>\r\n\r\nHet model definiëren en workflows creëren\r\nNu we de gegevens hebben opgesplitst en voorbewerkt, zijn we klaar om te modelleren! Eerst zullen we price (die nu eigenlijk log(price) is) modelleren met eenvoudige lineaire regressie.\r\nWe zullen dit doen met behulp van enkele modelleringsfuncties uit het parsnip pakket. Vind alle beschikbare functies hier. Hier is lineaire regressie meer in detail.\r\nOm ons model te definiëren, moeten we de volgende stappen zetten:\r\nBepaal het modeltype, dat is het algemene moeltype dat u wilt draaien.\r\nStel de motor in, die het pakket/de functie bepaalt die zal worden gebruikt om het model te draaien.\r\nStel de modus in, die ofwel “regressie” is voor continue uitkomstvariabelen of “classificatie” voor binaire/categorische uitkomstvariabelen. (Merk op dat voor lineaire regressie, het alleen “regressie” kan zijn, dus we hebben deze stap in dit geval niet NODIG).\r\n(OPTIONEEL) Stel argumenten in om af te stemmen (‘tunen’). We zullen hier later een voorbeeld van zien.\r\n\r\n\r\n\r\nDit is slechts het opzetten van het proces. We hebben het model nog niet aan de gegevens aangepast en er is nog één stap voordat we dat doen - een workflow maken! Hier wordt de voorbewerking en de stappen in het model gecombineerd.\r\n\r\n══ Workflow ══════════════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n6 Recipe Steps\r\n\r\n● step_rm()\r\n● step_log()\r\n● step_mutate()\r\n● step_rm()\r\n● step_date()\r\n● step_dummy()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nModelleren en evalueren\r\nNu zijn we eindelijk klaar om het model te draaien! Na al dat werk, lijkt dit deel eenvoudig. We gebruiken eerst de fit() functie om het model te fitten, door te vertellen op welke dataset we het model willen draaien. Daarna gebruiken we enkele andere functies om de resultaten mooi weer te geven.\r\n\r\n# A tibble: 31 x 5\r\n   term        estimate std.error statistic p.value\r\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n 1 (Intercept)    4.01      0.048     83.4        0\r\n 2 bedrooms      -0.018     0.002    -11.2        0\r\n 3 bathrooms      0.036     0.003     14.3        0\r\n 4 sqft_living    0.294     0.025     11.7        0\r\n 5 sqft_lot      -0.038     0.003    -11.0        0\r\n 6 floors         0.021     0.003      6.87       0\r\n 7 waterfront     0.194     0.013     15.0        0\r\n 8 view          -0.061     0.004    -15.6        0\r\n 9 sqft_above     0.149     0.025      5.99       0\r\n10 basement      -0.042     0.005     -9.14       0\r\n# … with 21 more rows\r\n\r\nOm het model te evalueren, gebruiken we crossvalidatie (CV), specifiek de 5-voudige CV. (Ik veronderstel dat we niet zowel de vorige stap van het passen van een model op de trainingsgegevens EN deze stap moeten doen, maar ik kon er niet achter komen hoe we het uiteindelijke model uit de CV-gegevens kunnen halen … dus dit was mijn oplossing voor nu). We passen het model dus aan met de 5-voudige dataset die we in het begin hebben gemaakt. Voor een diepere discussie over crossvalidatie, raad ik Bradley Boehmke’s Resampling sectie van Hands on Machine Learning with R aan.\r\n\r\n# A tibble: 10 x 5\r\n   id    .metric .estimator .estimate .config             \r\n   <chr> <chr>   <chr>          <dbl> <chr>               \r\n 1 Fold1 rmse    standard       0.135 Preprocessor1_Model1\r\n 2 Fold1 rsq     standard       0.662 Preprocessor1_Model1\r\n 3 Fold2 rmse    standard       0.137 Preprocessor1_Model1\r\n 4 Fold2 rsq     standard       0.644 Preprocessor1_Model1\r\n 5 Fold3 rmse    standard       0.137 Preprocessor1_Model1\r\n 6 Fold3 rsq     standard       0.638 Preprocessor1_Model1\r\n 7 Fold4 rmse    standard       0.133 Preprocessor1_Model1\r\n 8 Fold4 rsq     standard       0.655 Preprocessor1_Model1\r\n 9 Fold5 rmse    standard       0.135 Preprocessor1_Model1\r\n10 Fold5 rsq     standard       0.642 Preprocessor1_Model1\r\n# A tibble: 2 x 6\r\n  .metric .estimator  mean     n  std_err .config             \r\n  <chr>   <chr>      <dbl> <int>    <dbl> <chr>               \r\n1 rmse    standard   0.135     5 0.000668 Preprocessor1_Model1\r\n2 rsq     standard   0.648     5 0.00437  Preprocessor1_Model1\r\n# A tibble: 2 x 5\r\n# Groups:   .metric [2]\r\n  .metric .estimator  mean     n  std_err\r\n  <chr>   <chr>      <dbl> <int>    <dbl>\r\n1 rmse    standard   0.135     5 0.000668\r\n2 rsq     standard   0.648     5 0.00437 \r\n\r\nVoorspellen en evalueren van testgegevens\r\nIn dit eenvoudige scenario zijn we wellicht geïnteresseerd in hoe het model presteert op de testgegevens die werden weggelaten. De onderstaande code past het model toe op de trainingsgegevens en past het toe op de testgegevens. Er zijn andere manieren waarop we dit hadden kunnen doen, maar de manier waarop we het hier doen zal nuttig zijn wanneer we complexere modellen gaan gebruiken waarbij we de modelparameters moeten afstellen.\r\nNadat het model is aangepast en toegepast, verzamelen we de prestatiecijfers en geven we ze weer en tonen we de voorspellingen van de testgegevens.\r\n\r\n# A tibble: 2 x 4\r\n  .metric .estimator .estimate .config             \r\n  <chr>   <chr>          <dbl> <chr>               \r\n1 rmse    standard       0.135 Preprocessor1_Model1\r\n2 rsq     standard       0.655 Preprocessor1_Model1\r\n# A tibble: 5,403 x 5\r\n   id               .pred  .row price .config             \r\n   <chr>            <dbl> <int> <dbl> <chr>               \r\n 1 train/test split  5.58    12  5.67 Preprocessor1_Model1\r\n 2 train/test split  5.53    17  5.60 Preprocessor1_Model1\r\n 3 train/test split  5.90    27  5.97 Preprocessor1_Model1\r\n 4 train/test split  5.58    29  5.64 Preprocessor1_Model1\r\n 5 train/test split  5.67    31  5.76 Preprocessor1_Model1\r\n 6 train/test split  5.88    38  5.81 Preprocessor1_Model1\r\n 7 train/test split  5.69    40  5.78 Preprocessor1_Model1\r\n 8 train/test split  5.79    41  5.80 Preprocessor1_Model1\r\n 9 train/test split  5.77    42  5.89 Preprocessor1_Model1\r\n10 train/test split  5.66    44  5.84 Preprocessor1_Model1\r\n# … with 5,393 more rows\r\n\r\nDe onderstaande code maakt een eenvoudige plot om de voorspelde vs. de werkelijke prijs van de huisgegevens te onderzoeken.\r\n\r\n\r\n\r\n\r\n\r\n\r\nHoe zal het model worden gebruikt?\r\nWanneer we modellen creëren is het belangrijk na te denken over hoe het model zal worden gebruikt en met name hoe het model schade zou kunnen berokkenen. Wat opvalt in de bovenstaande grafieken is dat de prijs van woningen met een lagere prijs gemiddeld wordt overschat, terwijl de prijs van woningen met een hogere prijs gemiddeld wordt onderschat.\r\nWat als dit model werd gebruikt om de prijs van woningen te bepalen voor de onroerendgoedbelasting? Dan zouden lager geprijsde huizen te zwaar worden belast en hoger geprijsde huizen te weinig.\r\nMer complexe modellen met tuning parameters\r\nNu gaan we de Least Absolute Shrinkage and Selection Operator (LASSO) regressie proberen. Deze methode krimpt sommige coëfficiënten tot 0 op basis van een strafterm. We zullen crossvalidatie gebruiken om ons te helpen de beste strafterm te vinden.\r\nHet model opzetten\r\nWe zetten het model op zoals we het lineaire model hebben opgezet, maar voegen nu een set_args() functie toe. We vertellen het model dat we de penalty parameter later gaan aanpassen.\r\n\r\n\r\n\r\nDe workflow updaten\r\nEn dan creëren we een LASSO workflow.\r\n\r\n══ Workflow ══════════════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n6 Recipe Steps\r\n\r\n● step_rm()\r\n● step_log()\r\n● step_mutate()\r\n● step_rm()\r\n● step_date()\r\n● step_dummy()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\nLinear Regression Model Specification (regression)\r\n\r\nMain Arguments:\r\n  penalty = tune()\r\n  mixture = 1\r\n\r\nComputational engine: glmnet \r\n\r\nAfstemmen van de strafparameter\r\nWe gebruiken de grid_regular() functie uit de dials bibliotheek om een aantal waarden van de penalty parameter voor ons te kiezen. Als alternatief kunnen we ook een vector van waarden opgeven die we willen proberen.\r\n\r\n# A tibble: 20 x 1\r\n    penalty\r\n      <dbl>\r\n 1 1.00e-10\r\n 2 3.36e-10\r\n 3 1.13e- 9\r\n 4 3.79e- 9\r\n 5 1.27e- 8\r\n 6 4.28e- 8\r\n 7 1.44e- 7\r\n 8 4.83e- 7\r\n 9 1.62e- 6\r\n10 5.46e- 6\r\n11 1.83e- 5\r\n12 6.16e- 5\r\n13 2.07e- 4\r\n14 6.95e- 4\r\n15 2.34e- 3\r\n16 7.85e- 3\r\n17 2.64e- 2\r\n18 8.86e- 2\r\n19 2.98e- 1\r\n20 1.00e+ 0\r\n\r\nGebruik de tune_grid() functie om het model te draaien met behulp van crossvalidatie voor alle penalty_grid waarden en evalueer op alle vouwen.\r\n\r\n# Tuning results\r\n# 5-fold cross-validation \r\n# A tibble: 5 x 4\r\n  splits                id    .metrics          .notes          \r\n  <list>                <chr> <list>            <list>          \r\n1 <rsplit [12968/3242]> Fold1 <tibble [40 × 5]> <tibble [2 × 1]>\r\n2 <rsplit [12968/3242]> Fold2 <tibble [40 × 5]> <tibble [2 × 1]>\r\n3 <rsplit [12968/3242]> Fold3 <tibble [40 × 5]> <tibble [2 × 1]>\r\n4 <rsplit [12968/3242]> Fold4 <tibble [40 × 5]> <tibble [2 × 1]>\r\n5 <rsplit [12968/3242]> Fold5 <tibble [40 × 5]> <tibble [2 × 1]>\r\n\r\nBekijk de resultaten van de cross-validatie.\r\n\r\n# A tibble: 100 x 6\r\n   id     penalty .metric .estimator .estimate .config              \r\n   <chr>    <dbl> <chr>   <chr>          <dbl> <chr>                \r\n 1 Fold1 1.00e-10 rmse    standard       0.135 Preprocessor1_Model01\r\n 2 Fold1 3.36e-10 rmse    standard       0.135 Preprocessor1_Model02\r\n 3 Fold1 1.13e- 9 rmse    standard       0.135 Preprocessor1_Model03\r\n 4 Fold1 3.79e- 9 rmse    standard       0.135 Preprocessor1_Model04\r\n 5 Fold1 1.27e- 8 rmse    standard       0.135 Preprocessor1_Model05\r\n 6 Fold1 4.28e- 8 rmse    standard       0.135 Preprocessor1_Model06\r\n 7 Fold1 1.44e- 7 rmse    standard       0.135 Preprocessor1_Model07\r\n 8 Fold1 4.83e- 7 rmse    standard       0.135 Preprocessor1_Model08\r\n 9 Fold1 1.62e- 6 rmse    standard       0.135 Preprocessor1_Model09\r\n10 Fold1 5.46e- 6 rmse    standard       0.135 Preprocessor1_Model10\r\n# … with 90 more rows\r\n# A tibble: 20 x 7\r\n    penalty .metric .estimator  mean     n  std_err .config           \r\n      <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             \r\n 1 1.00e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 2 3.36e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 3 1.13e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 4 3.79e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 5 1.27e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 6 4.28e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 7 1.44e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 8 4.83e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n 9 1.62e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n10 5.46e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n11 1.83e- 5 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\r\n12 6.16e- 5 rmse    standard   0.135     5 0.000640 Preprocessor1_Mod…\r\n13 2.07e- 4 rmse    standard   0.135     5 0.000623 Preprocessor1_Mod…\r\n14 6.95e- 4 rmse    standard   0.135     5 0.000591 Preprocessor1_Mod…\r\n15 2.34e- 3 rmse    standard   0.136     5 0.000522 Preprocessor1_Mod…\r\n16 7.85e- 3 rmse    standard   0.142     5 0.000513 Preprocessor1_Mod…\r\n17 2.64e- 2 rmse    standard   0.161     5 0.000515 Preprocessor1_Mod…\r\n18 8.86e- 2 rmse    standard   0.191     5 0.000896 Preprocessor1_Mod…\r\n19 2.98e- 1 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\r\n20 1.00e+ 0 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\r\n\r\n# A tibble: 1 x 2\r\n   penalty .config              \r\n     <dbl> <chr>                \r\n1 0.000207 Preprocessor1_Model13\r\n\r\nUpdate de workflow voor best afgestemde parameter\r\nPas de workflow aan om de beste afstemparameter (kleinste rmse, met select_best() in vorige stap) in het model op te nemen. Er zijn andere manieren om modellen te selecteren, zoals select_by_one_std_error() die “het meest eenvoudige model selecteert dat binnen één standaardfout van de numeriek optimale resultaten ligt”.\r\n\r\n══ Workflow ══════════════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n6 Recipe Steps\r\n\r\n● step_rm()\r\n● step_log()\r\n● step_mutate()\r\n● step_rm()\r\n● step_date()\r\n● step_dummy()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\nLinear Regression Model Specification (regression)\r\n\r\nMain Arguments:\r\n  penalty = 0.000206913808111479\r\n  mixture = 1\r\n\r\nComputational engine: glmnet \r\n\r\nPas de beste afstelling toe op de trainingsgegevens\r\nNu kunnen we dit toepassen op de trainingsgegevens en het resulterende model bekijken. De uitvoer van het model was niet wat ik verwachtte. Volgens Julia Silge’s antwoord op mijn vraag hier, zou dit verholpen moeten zijn als je parsnip installeert vanaf GitHub] met devtools::install_github(\"tidymodels/parsnip\") van de devtools bibliotheek.\r\n\r\n══ Workflow [trained] ════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n6 Recipe Steps\r\n\r\n● step_rm()\r\n● step_log()\r\n● step_mutate()\r\n● step_rm()\r\n● step_date()\r\n● step_dummy()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\n\r\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \r\n\r\n   Df  %Dev   Lambda\r\n1   0  0.00 0.153800\r\n2   1  7.71 0.140200\r\n3   1 14.11 0.127700\r\n4   1 19.42 0.116400\r\n5   1 23.83 0.106000\r\n6   1 27.49 0.096610\r\n7   1 30.53 0.088030\r\n8   1 33.06 0.080210\r\n9   1 35.15 0.073080\r\n10  2 37.42 0.066590\r\n11  2 39.41 0.060670\r\n12  2 41.06 0.055280\r\n13  2 42.43 0.050370\r\n14  3 43.93 0.045900\r\n15  3 45.24 0.041820\r\n16  3 46.32 0.038100\r\n17  4 47.48 0.034720\r\n18  5 48.59 0.031640\r\n19  6 49.53 0.028830\r\n20  7 50.48 0.026260\r\n21  7 51.84 0.023930\r\n22  8 52.98 0.021810\r\n23  8 54.00 0.019870\r\n24  9 54.93 0.018100\r\n25 11 56.10 0.016490\r\n26 10 57.40 0.015030\r\n27 11 58.29 0.013690\r\n28 11 59.06 0.012480\r\n29 12 59.70 0.011370\r\n30 12 60.25 0.010360\r\n31 13 60.71 0.009439\r\n32 13 61.10 0.008600\r\n33 13 61.43 0.007836\r\n34 13 61.71 0.007140\r\n35 14 61.95 0.006506\r\n36 15 62.18 0.005928\r\n37 15 62.40 0.005401\r\n38 16 62.57 0.004921\r\n39 17 62.96 0.004484\r\n40 18 63.28 0.004086\r\n41 18 63.56 0.003723\r\n42 20 63.80 0.003392\r\n43 20 64.00 0.003091\r\n44 20 64.16 0.002816\r\n45 20 64.30 0.002566\r\n46 21 64.42 0.002338\r\n\r\n...\r\nand 32 more lines.\r\n# A tibble: 31 x 3\r\n   term        estimate  penalty\r\n   <chr>          <dbl>    <dbl>\r\n 1 (Intercept)   4.12   0.000207\r\n 2 bedrooms     -0.0174 0.000207\r\n 3 bathrooms     0.0355 0.000207\r\n 4 sqft_living   0.307  0.000207\r\n 5 sqft_lot     -0.0376 0.000207\r\n 6 floors        0.0210 0.000207\r\n 7 waterfront    0.191  0.000207\r\n 8 view         -0.0615 0.000207\r\n 9 sqft_above    0.139  0.000207\r\n10 basement     -0.0405 0.000207\r\n# … with 21 more rows\r\n\r\nWe kunnen het belang van de variabele visualiseren\r\n\r\n\r\n\r\nEvalueren op testgegevens\r\nTen slotte passen we het model toe op de testgegevens en onderzoeken we enkele definitieve metrieken. We tonen ook de metriek van het gewone lineaire model. Het lijkt erop dat de prestaties van het LASSO-model iets beter zijn, maar het scheelt niet veel.\r\n\r\n# A tibble: 2 x 4\r\n  .metric .estimator .estimate .config             \r\n  <chr>   <chr>          <dbl> <chr>               \r\n1 rmse    standard       0.135 Preprocessor1_Model1\r\n2 rsq     standard       0.655 Preprocessor1_Model1\r\n# A tibble: 2 x 4\r\n  .metric .estimator .estimate .config             \r\n  <chr>   <chr>          <dbl> <chr>               \r\n1 rmse    standard       0.135 Preprocessor1_Model1\r\n2 rsq     standard       0.655 Preprocessor1_Model1\r\n\r\nBronnen\r\nDank gaat uit naar verschillende mensen voor het delen van materiaal over tidymodels, waaronder\r\n En natuurlijk Lisa zelf, haar voeg ik hier zelf aan toe\r\n\r\nDit zijn de bronnen die bij deze blog ondersteuning boden:\r\nRebecca Barter’s blog\r\ntidymodels website (Alison Hill, Max Kuhn, Desirée De Leon, Julia Silge)\r\nJulia Silge’s tidymodels example\r\nUiteraard vooral Lisa Lendway via:\r\nLisa Lendway/2020_north_tidymodels\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-10-machine-learning/images/house_prices_variables.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-10-github/",
    "title": "GitHub voor samenwerking",
    "description": "Lisa Lendway heeft een aantal interessante repositories op haar GitHub account staan, [zie hier](https://github.com/llendway). Ze zijn vaak kort, maar helder en concreet. Haar stijl en de consistentie daarin bevallen mij zeer. Van haar manier van doen leer ik veel. Zij maakt haar stukken vaak voor haar statistieklessen en deelt zo haar kennis met haar studenten en anderen buiten haar klas. Ik heb mij voorgenomen om er een aantal goed te lezen, te vertalen en te bewerken waar nodig, en deze op mijn website over te nemen. Vorige maand deed ik dat al met een blof over Distill en nu een over GitHub.",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\r\nGitHub\r\nIk was al vaker van plan hier een stukje over te schrijven. Lisa Lendways tekst hierover vind ik heel duidelijk. Lisa, ik hoop dat je het goed vindt dat ik mij aan jou op trek zo. Dank je wel.\r\nMijn eigen GitHub accountLisa Lendway heeft veel van haar materiaal weghaald uit Happy git with R by Jenny Bryan. Dat is inderdaad een uitstekende bron, maar bevat ook veel informatie die we niet altijd nodig hebben. Als je Git en GitHub op meer geavanceerde manieren wilt gebruiken of als dit stuk onduidelijk is voor je, dan moet je het zeker bekijken. Het is trouwens überhaupt een goede bron.\r\nVideo uitleg\r\n\r\n\r\nVoicethread tutorial\r\nGit en GitHub\r\nGit is een versie controle systeem. Het is net zoiets als Googledocs, maar het biedt ruimte aan veel soorten bestanden, ook bestanden waar Google docs niets mee kan … zoals .rmd bestanden! GitHub is een online interface om met Git te werken.\r\nWaarom leren we deze dingen?!\r\nGitHub is goed geïntegreerd met R Studio. Dus, we zullen geen command-line functies hoeven te gebruiken, tenminste niet nadat we alles hebben ingesteld.\r\nJe bent verplicht om R te gebruiken voor je eindproject. De presentatie of paper moet worden opgeslagen als een .rmd document dat kan worden ‘geknit’ tot een html document. Door GitHub te gebruiken, kun je gemakkelijk met je groep samenwerken, ook als je niet bij elkaar bent.\r\nGitHub leert je een aantal goede gewoontes aan. Je wordt gedwongen om na te denken over wanneer je ieets opslaat en om notities te maken over welke wijzigingen je hebt gemaakt, bijvoorbeeld.\r\nMaak eerst een GitHub account aan\r\nGa naar http://github.com\r\nGebruik een username … zie Jenny Bryan’s tips. Incorporeer jouw eigen naam, maar gebruik een andere usernaam die je verder gebrukkt, neem iets waar jouw toekomstige baas zich prettig bij voelt. De username van de universiteit is misschien een goede optie.\r\nInstalleer Git\r\n1. Controleer of je Git al geïnstalleerd hebt. Dit zal alleen het geval zijn als je het ergens anders gebruikt hebt. Om dit te doen, open je de commandoregel of, in R Studio, vouw je de Console uit. Er zou een tabblad moeten zijn dat Terminal zegt. In dat gebied type je\r\nwhich git\r\nHet geeft iets terug als\r\n/usr/bin/git\r\ndan ben je klaar en hoef je Git niet meer te installeren. Op een Windows machine kun je misschien niet eens het which git commando succesvol intypen. Dit zou verholpen moeten zijn door git te installeren. Of je zult de shell moeten gebruiken.\r\nAls je Git niet geïnstalleerd hebt, moet je het installeren. De instructies zijn iets anders voor Windows en Macs.\r\nVoor een Windows machine:\r\nInstalleer Git for Windows. Als er gevraagd wordt naar “Aanpassen van uw PATH omgeving”, zorg er dan voor dat u “Git vanaf de commandoregel en ook van software van derden” selecteert. Anders denken we dat het goed is om de standaardinstellingen te accepteren.\r\nR Studio voor Windows geeft er de voorkeur aan dat Git geïnstalleerd wordt onder C:/Program Files en dit lijkt de standaard te zijn. Dit houdt bijvoorbeeld in dat de Git executable op mijn Windows systeem te vinden is in C:/Program Files/Git/bin/git.exe. Tenzij je specifieke redenen hebt om anders te doen, volg deze conventie.\r\nVoor een Mac machine:\r\nGa naar jouw shell/terminal en voer één van deze commando’s in om een aanbod te krijgen om developer command line tools te installeren. Accepteer het aanbod … klik op installeren.\r\ngit --version\r\ngit config\r\nSommigen van jullie die op een Mac werken moeten misschien eerst het volgende doen in de terminal als je een project zonder succes probeert te openen.\r\nxcode-select --install\r\nJe komt er zo achter of dit het geval is.\r\nGa nu terug naar de Console in R Studio en installeer het usethis pakket in R Studio. Sluit vervolgens R Studio en open het opnieuw.\r\nLaad de usethis bibliotheek door het volgende stukje code in de console uit te voeren:\r\nlibrary(usethis)\r\nVoer de volgende code uit in de console met enkele kleine wijzigingen. De user.name is je Git gebruikersnaam. Dit kan anders zijn dan je GitHub gebruikersnaam, hoewel het misschien een goed idee is om het gewoon hetzelfde te houden. De user.email MOET hetzelfde zijn als je GitHub gebruikers email.\r\nuse_git_config(user.name = \"Jane Doe\", user.email =       \"jane@example.org\")\r\nMaak een eerste repo (repository) en gebruik RStudio daarbij\r\nHet woord “repo” is een afkorting van “repository”, en dat is precies wat het is: een plaats waar dingen (onze bestanden, in dit geval) worden opgeslagen. Het is als de map die je gemaakt hebt om al je werk voor deze les in op te slaan.\r\nLaten we naar GitHub gaan en inloggen. Nadat je ingelogd bent, zou je een klein icoontje in de rechter bovenhoek moeten zien. Het mijne is een afbeelding van mij. Als ik daar op klik verschijnt er een drop-down en kan ik “Your repositories” kiezen. Doe dat. Je zou nu zoiets als dit moeten zien:\r\n\r\nKlik op de “New” knop. Geef jouw repository een naam, bv NAME_test_repo, waar NAME eigenlijk jouw naam is. Kies Public en klik de README file aan. Klik dan op Create repository.\r\n\r\nEr zijn dingen die je direct binnen GitHub kunt doen, maar we zullen ons richten op de integratie met R Studio.\r\nKlonen van een repo\r\nDenk aan het klonen van een repo als het “kopiëren” van de repository naar je computer. Maar als met het kopiëren doet, houdt het de verbinding met de online repo.\r\nLaten we dit doen. Op je mijn_test_repo pagina, kies je de groene knop met Code en kopieer je het pad door het icoontje met een pijl erop te selecteren en erop te klikken.\r\nGa nu naar R Studio. Klik op Bestand –> Nieuw Project … Je zou nu een venster moeten zien dat er als volgt uitziet:\r\n\r\nKies Version Control. Dan zie je een scherm dat er zo’n beetje zo uitziet:\r\n\r\nKies Git. Dan zou je een scherm moeten zien dat er uitziet als dit, zonder alle details ingevuld. De Repository URL is waar je de repo URL moet plakken die je gekloond hebt van github. Het zal ook de Project mapnaam invullen. Laat die gewoon staan. Let op waar de project directory zich bevindt en verander het naar een betere directory indien nodig. Klik op Create Project.\r\n\r\nAls je in de Bestanden tab kijkt in het rechter ondervenster van R Studio, dan zou je het .gitignore bestand moeten zien, het project bestand (eindigt op .Rproj), en het README.md bestand. Je zou ook een Git tab moeten zien in het rechter bovenvenster van R Studio. Als je nu op de Git tab klikt, zul je daar niets zien.\r\nMet de Git tab open, laten we het README.md bestand in R Studio openen. Maak een kleine wijziging in het bestand door de zin “Ik verander iets in dit bestand.” toe te voegen. Klik dan op het save icoon. Als je dit doet, zul je README.md zien verschijnen in de Git tab.\r\nKlik nu op de Commit knop in de Git tab. Zet een vinkje in het vakje naast het README.md bestand onder het woord Staged (in de toekomst kun je meerdere bestanden tegelijk stagen door de vakjes naast meerdere bestanden aan te vinken) en voeg een commentaar toe aan het commit vakje.\r\nHet zou er ongeveer zo uit moeten zien:\r\n\r\nKlik tenslotte op commit. Je krijgt dan een bericht dat het voltooid is. Het bericht kan cryptisch overkomen als je er niet aan gewend bent. Het ziet er ongeveer zo uit:\r\n\r\nDe wijziging die je hebt gemaakt is nu gecommit in het lokale geheugen. Het gewijzigde bestand is alleen gewijzigd op je computer, NIET online als je op GitHub kijkt … ga maar eens kijken. Klik op de Diff knop in de Git tab en je kunt de geschiedenis van je commits zien.\r\nVervolgens gaan we die wijzigingen naar GitHub pushen door op de groene pijl omhoog in de Git tab te klikken. Dit zal je een bericht geven dat er ongeveer zo uitziet:\r\n\r\nWerd je gevraagd om een gebruikersnaam en wachtwoord? Probeer een andere wijziging te maken, vast te leggen en te pushen. Wordt er nog steeds om een gebruikersnaam en wachtwoord gevraagd? Zo ja, dan kun je hier zien hoe je dat doet Jenny Bryans bron.\r\nPROBEER HET EENS!!\r\nVoeg een .rmd bestand toe aan je project. Doe dit door te gaan naar Bestand –> Nieuw bestand –> R Markdown … Voeg wat woorden en een R code chunk toe aan het .rmd bestand. Sla het op, commit het (vergeet het bericht niet!), en push het. Controleer GitHub online om er zeker van te zijn dat je het .rmd bestand daar ziet.\r\nNu, knit je het bestand lokaal. Commit de wijzigingen (zorg ervoor dat je een vinkje zet naast alles wat je ge-staged wilt hebben - .rmd, .html, etc.) en push ze naar GitHub. Controleer GitHub online om er zeker van te zijn dat je alles ziet wat je verwacht.\r\nPartners toevoegen\r\nTot nu toe hebben we eigenlijk alleen technieken geleerd om GitHub te gebruiken om onze eigen bestanden te beheren, maar het coolste eraan zijn de samenwerkingsmogelijkheden. De manier waarop we dit gaan leren is door medewerkers aan de repo toe te voegen.\r\nZoek iemand om mee samen te werken. Als er een oneven aantal is, maak dan een groepje van drie. In je groepje, voeg elkaar toe als medewerkers aan je project. In GitHub, op de repo pagina, ga naar Instellingen. Een van de opties aan de linkerkant is Collaborators. Klik daarop en doe wat er staat.\r\nDe persoon die is uitgenodigd om samen te werken zal een email ontvangen en zou ook in staat moeten zijn om de uitnodiging op GitHub te zien. Ze moeten deze accepteren. Eenmaal geaccepteerd, zouden jullie beiden (of alle drie) toegang moeten hebben om wijzigingen in het bestand vast te leggen.\r\nCommit –> Push –> Pull –> … (en Communicatie)\r\nZodra je medewerkers hebt toegevoegd, kunnen alle medewerkers committen en pushen. Maar, wat gebeurt er als iemand iets commit en terugzet en jij gaat er dan aan werken op je computer… hoe krijg je dan die wijzigingen? … PULL!\r\nProbeer in jullie groepen het volgende. Jullie moeten allemaal meewerken aan elkaars projecten, dus je kunt van rol wisselen nadat je het één keer gedaan hebt.\r\nDe medewerker moet eerst de repo klonen waaraan hij gevraagd is om mee te werken. Als ze een ander project open hebben, sla dan op, commit, en push alle wijzigingen. Sluit dan dat project en open het project waar ze om gevraagd is om aan mee te werken door de GitHub repo te klonen. De medewerker moet het project open hebben in R Studio.\r\nDe medewerker moet proberen te trekken (pullen) door op de aqua pijl omlaag te klikken in de Git tab. Je zou een bericht moeten krijgen dat er als volgt uitziet:\r\n\r\nDe persoon die de repo heeft aangemaakt, maakt een wijziging in zijn .rmd bestand. Het kan een kleine wijziging zijn, zoals het toevoegen van een zin. Diezelfde persoon slaat het bestand op, commit (staged en schrijft een commit boodschap), en pushed het naar GitHub. Controleer online om er zeker van te zijn dat de meest recente wijzigingen zijn gepushed.\r\nDe medewerker haalt nu die wijzigingen naar zijn lokale map (naar zijn computer). Klik op het pull icoon. Je zou een bericht moeten zien dat er ongeveer zo uitziet:\r\n\r\nEn controleer het bestand waarin een wijziging is aangebracht om er zeker van te zijn dat de wijziging wordt weerspiegeld in het bestand op uw computer.\r\nGa nog een paar keer heen en weer en breng kleine wijzigingen aan. Degene die eigenaar is van de repo zou de wijziging moeten maken en de medewerker zou het moeten binnenhalen. Wissel dan van rol. Als je wisselt, wees er dan zeker van dat je aan het juiste project werkt.\r\nConflicten samenvoegen\r\nAls je samen aan een project werkt, is de kans groot dat je tegen een moment aanloopt waarop twee van jullie hetzelfde bestand tegelijkertijd aan het bewerken zijn. Soms, als je allebei je wijzigingen probeert te pushen, zul je wat genoemd wordt een “merge conflict” krijgen. GitHub zal niet weten welke te gebruiken. Dus, zal het je dwingen om te beslissen.\r\nAls je probeert je wijzigingen naar GitHub te pushen en iemand anders heeft zijn wijzigingen met betrekking tot hetzelfde bestand al gepushed, dan zul je een bericht als dit krijgen:\r\n\r\nDan, wanneer je de wijzigingen binnenhaalt, krijg je een bericht zoals dit:\r\n\r\nMerk op dat het je vertelt in welk bestand het samenvoegconflict zich voordeed. Je moet dat bestand openen en beslissen hoe de conflicterende informatie samengevoegd moet worden. In het begin zal het er ongeveer zo uitzien:\r\n\r\nHet deel na het woord HEAD is wat in je lokale bestand staat. Alles na de ====== is wat in het bestand op afstand staat (d.w.z. de wijzigingen die je medewerker heeft gemaakt). Je kunt besluiten om dit op te lossen op elke manier die je wilt: combineer de twee ideeën, verwijder ze allebei, houd er maar een over, etc. Als je klaar bent, zorg er dan voor dat je de <<<<<<< HEAD en >>>>>>> verwijdert, gevolgd door de alfanumerieke string, plus alle andere vreemde tekens.\r\nSla het bestand dan op en doe de gebruikelijke commit en push. Je zou de wijzigingen naar GitHub gepushed moeten zien worden.\r\nLaten we dit eens proberen!\r\nIn groepjes van 3-4, oefen je GitHub vaardigheden.\r\nKies iemand om een nieuwe repo aan te maken op GitHub genaamd our_collaborative_graph.\r\nDe maker voegt de anderen toe als collaborators.\r\nDe medewerkers moeten hun e-mail controleren en accepteren dat ze medewerkers zijn.\r\nDe maker en de medewerkers klonen de repo lokaal.\r\nEen medewerker voegt lokaal een .rmd bestand aan het project toe. De titel moet zijn “Onze grafiek” en voeg alle groepsleden toe als auteurs. Voeg een R code stuk toe dat de tidyverse bibliotheek laadt. Sla het bestand op, commit met bericht, en push naar GitHub. Controleer online om er zeker van te zijn dat het goed gepushed is.\r\nAlle andere groepsleden halen de wijzigingen lokaal op.\r\nEen andere medewerker voegt een ander R code stuk toe. Maak met de mpg dataset een scatterplot met hwy op de y-as, displ op de x-as en kleur de punten met drv. Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de wijzigingen ziet.\r\nAlle andere groepsleden halen de wijzigingen lokaal op.\r\nEen ander groepslid (medewerker of maker als je maar 3 groepsleden hebt) wijzigt de R code chunk die de grafiek maakt, door mooie x en y labels toe te voegen en te veranderen naar theme_minimal(). Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de veranderingen ziet.\r\nAlle grop leden trekken. Degene die net gepushed heeft zou moeten zien dat ze al up to date zijn. Alle anderen zouden de wijzigingen lokaal terug moeten zien.\r\nNu moeten alle groepsleden iets toevoegen aan het .rmd bestand. Vertel elkaar niet wat je toevoegt. Als je klaar bent, sla op, brei, commit, en push naar GitHub. Ten minste één van jullie zal een samenvoeg conflict krijgen, dus zal het je vragen om wijzigingen van GitHub binnen te halen en het conflict op te lossen. Doe dat. Deze keer zul je het .rmd bestand moeten aanpassen in plaats van de README zoals ik eerder liet zien.\r\nAls je klaar bent, moeten 112 leerlingen een project opzetten met hun eigenlijke groepsprojectleden. Neem een .rmd-bestand op met de naam “ideas.rmd” waarin je ideeën kunt uitwisselen, inclusief onderwerpen en gegevens die je misschien zou willen analyseren. 155 leerlingen moeten de enquête over het groepsproject op de Moodle-pagina invullen.\r\nBron\r\nHappy git with R door Jenny Bryan. Leer meer over Distill via https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-19-website-met-distill/",
    "title": "Website met distill",
    "description": "Website met blog maken",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\r\nDistill\r\nMijn eigen Harrie’s Hoekje blog, over een aantal ontwikkelingen in de dataanalyse, maak ik ook met het gebruik van het pakket Distill. Met Distill kun je wetenschappelijke websites maken, een blog en artikelen schrijven. Over hoe je dat doet schreef Lisa Lendway een kort en krachtige blog. Dat kan ik niet beter. Dank je, Lisa, hiervoor. Haar blog staat hier\r\nWaarom een website?\r\nNou, eindelijk heb ik het gedaan!, zo begint zij haar blog dat ik (Harrie) hier verder volg.\r\nIk (Lisa) heb een website gemaakt. En om dat te vieren, ga ik met jullie delen hoe ik het gedaan heb. En waarom heb ik dat gedaan? Twee belangrijke redenen zijn er: 1. om materiaal te delen dat nuttig kan zijn voor anderen, 2. om wat dingen voor mezelf te documenteren, allemaal op één plek.\r\nIk koos voor een {distill} site omdat het me genoeg vrijheid leek te geven om mijn site aan te passen en ook weer niet zo veel vrijheid dat ik zou verzanden in details (bv. kleuren kiezen … oeps, daar heb ik toch nog veel tijd aan besteed).\r\nBronnen\r\nVoordat ik begin, zal ik wat bronnen delen die ik heb gebruikt.\r\nAlison Hill en Desirée De Leon’s webinar over Sharing on Short Notice. KIJK HIERNAAR voordat je verder gaat. Hier werd ik voor het eerst geïntroduceerd op netlify en toen zag ik pas hoe makkelijk het is om html-files tot een website om te vormen website. Je zou daar zelfs eerst mee kunnen beginnen voordat jij je op een website springt. Misschien spreken sommige van de andere opties die ze bespreken jou meer aan dan {distill}.\r\nDe distill documentatie, ook al in de vorm van een … distill website!\r\nDistill websites van anderen: Ijeamaka Anyene, Shannon Pileggi(aka Piping Hot Data), Miles McBain, Tom Mock, en meer!\r\nAlison Hill’s website voor hoog niveau inhoud en design inspiratie. Iedere keer vind ik wel een nieuw bron als ik haar website bezoek. Bijvoorbeeld, bekijk eens haar praatje op ‘Recent updates in the R markdown family’.\r\nEn meer! Ik zal proberen in dit blog op enkele bronnen terug te komen.\r\nKijk ook naar de video die Lisa maakte en die je hier vindt.\r\nBouwen van de site\r\nLaten we nu verder gaan met het maken van de site. Onderweg kom ik terug op de YouTube-video. Ik kom steeds terug op dezelfde YouTube-video, maar ik zal ze daar zetten waar ik het op dat moment over heb. Zo is het makkelijker voor je om delen over te slaan als je dat wilt.\r\nEen GitHub repo opzetten & het project starten\r\nKijk naar Tom Mock’s post hier. Ik denk dat zijn manier om dit te doen logischer is dan de mijne. Helaas, zag ik het toen ik mijn ding had gedaan :(\r\nIk probeer er een gewoonte van te maken om al mijn projecten met een GitHub repo te beginnen. Dus, dat is wat ik hier ook heb gedaan. Hier zijn alle stappen:\r\nMaak een repo\r\nCreëer project in R Studio door de repo te klonen * Laad de {distill} bibliotheek\r\nMaak een “starter” site met de create_website() functie. Ik heb dit gebruikt in plaats van create_blog() omdat ik van mijn hoofdpagina een About pagina wilde maken in plaats van een blog. Ik zal het blog gedeelte later toevoegen. Lees de {distill} documentatie om je te helpen bij het beslissingsproces. Omdat ik eerst mijn GitHub repo heb gemaakt, moest ik wat rare dingen doen om de mappenstructuur te fixen. Het werkt, maar het is een beetje lelijk.\r\nVerplaats alle bestanden behalve het .Rproj bestand van de zojuist gemaakte map naar de hoofdmap van het archief.\r\nVerwijder de website map (zou leeg moeten zijn, behalve het .Rproj bestand).\r\nVerwijder het README.md bestand in de hoofdmap van het archief (als ik dat niet deed, bouwde de site later niet).\r\n\r\nOf kijk naar dit deel van de video (tot minuut 8:04):\r\n\r\n\r\nDe site voor de eerste keer bouwen\r\nVervolgens willen we de site bouwen. Om dit op een eenvoudige manier te doen, sla je jouw bestanden op, sluit je RStudio en open je het opnieuw, waarbij jij ervoor zorgt dat je je in het project van jouw distill-site bevindt. Wanneer je dit doet, zou er een Build tab moeten verschijnen in jouw paneel aan de rechterbovenhoek (of waar u gewoonlijk jouw Environment, History, enz. hebt). Klik op het Build Website-icoon en je zou je site moeten zien! (8:25 in de video, als je het mij wilt zien doen).\r\nOp dit punt zijn er veel verschillende richtingen die je op kunt gaan. Ik zal je vertellen wat ik gedaan heb. Als je niet veel meer wilt aanpassen, kun je naar ?? gaan om een eenvoudige manier te vinden om je website te publiceren.\r\nAanpassen van de home page\r\nIk wilde dat mijn “Home” pagina mijn “About” pagina zou worden. Om dit te doen, heb ik eerst wat veranderingen aangebracht in het _site.yml bestand, het “About” gedeelte van de navigatiebalk verwijderd en de tekst voor de homepage hernoemd naar “About”.\r\nDan, om te beginnen met het aanpassen van mijn “About” pagina, voeg ik een foto van mezelf toe aan het index.Rmd bestand en plaats ik wat plaatshouders voor plaatsen waar ik wat informatie zal schrijven.\r\nBekijk dit in de video (tot minuut 17:35):\r\n\r\n\r\nVoeg het blog toe en maak jouw eerste post\r\nAls je de blog route vanaf het begin hebt gevolgd, hoef je dit deel niet te doen. Merk op dat ik in de video de dingen in de verkeerde volgorde deed\r\nVoeg een post toe met create_post(\"mijnpost\"). Dit genereert een R Markdown bestand met de naam mypost.Rmd (tenzij je de slug verandert), een _posts map, en een map die de datum en de naam van het bericht heeft. Door te beginnen met de datum, houdt het je berichten in een mooie volgorde :)\r\nBewerk jouuw blog post-Rmarkdownbestand naar believen. Zorg ervoor dat je dit bestand knit zodat het op de blog verschijnt. Deze bestanden worden niet automatisch gebreid. Dat is met opzet.\r\nMaak een nieuw R Markdown bestand met ALLEEN een yaml kop met een titel en listing. Sla het op in de hoofd repository.\r\nWijzig het _site.yml bestand om de listing pagina te linken. De tekst kan zijn wat je maar wilt - dit is wat er op de navigatiebalk komt te staan. De href waarde is de .html van het listing .Rmd bestand.\r\nVoeg een aangepaste blog preview afbeelding toe. Zet de afbeelding die je hiervoor wilt gebruiken in de map voor de blog post. In de yaml kop van het R Markdown bestand van uw blog, voeg je preview: image.png toe, waar image.png de naam van jouw afbeelding is. Standaard zal de preview de eerste plot zijn die gegenereerd wordt in uw R code.\r\nBekijk dit in de video (tot minuut 33:27):\r\n\r\n\r\nPas_site.yml aan\r\nIn dit deel voeg ik enkele aangepaste iconen toe aan de bovenste navigatiebalk van de site. Deze bevatten een persoonlijke favicon aan de linkerkant (die ik uiteindelijk toch weer weghaal) en links naar mijn GitHub, LinkedIn en Twitter pagina’s (en later voeg ik er een toe aan mijn YouTube kanaal).\r\nVoeg het volgende toe aan het _site.yml bestand na de navbar koptekst. Wees voorzichtig met inspringen. Je kunt mijn bestand hier bekijken (ik heb meer bewerkt sinds het maken van de video, dat wel).\r\n- icon: fa fa-github\r\n  href: https://github.com/YOUR_USERNAME\r\n- icon: fa fa-linkedin\r\n  href: https://www.linkedin.com/in/YOUR_LINKEDIN/\r\n- icon: fa fa-twitter\r\n  href: https://twitter.com/YOUR_TWITTER\r\nOm een gepersonaliseerde favicon toe te voegen, voeg het volgende toe na navbar:, waar ll.png de persoonlijke favicon is. Je kunt ook een link naar een website toevoegen waar hij naartoe gaat als je er op klikt. Nogmaals, wees voorzichtig met inspringen.\r\n  logo:\r\n    image: ll.png\r\nVolg de video hieronder (tot minuut 44:22). Toen ik dit de eerste keer deed, maakte ik wat fouten, dus ik laat je dat deel van de video overslaan.\r\n\r\n\r\nPubliseer de site via netlify\r\nNu je een website hebt, kun je die gemakkelijk publiceren via netlify. Ik zal je laten zien hoe je deze aan je GitHub repo kunt koppelen, zodat iedere keer dat je wijzigingen naar GitHub stuurt, je website die wijzigingen zal weergeven. Ik raad aan om eerst een account op netlify aan te maken.\r\nBekijk de video om te zien hoe ik het doe (tot minuut 48:22):\r\n\r\n\r\nMaak het je eigen!\r\nHet laatste stuk is om wat aanpassingen te doen. Dankzij de geweldige {distill} auteurs, kunnen we de create_theme() functie gebruiken om ons door het aanpassen van wat css te leiden. Ik ben een echte beginner als het op css aankomt, dus er is van een makkelijkere manier. Ik raad ten zeerste aan om de documentatie over theming en de recente updates door te lezen. En lees grondig de tekst van de website (misschien heb ik dat de eerste keer niet gedaan)!\r\nVoegtheme: \"my_theme.css\" aan de bodem van de _site.yml file toe.\r\nJe kunt de video tot het einde bekijken:\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-17-testen-met-bayes/",
    "title": "Testen met Bayes",
    "description": "Resultaten testen met Bayesiaanse onderzoekstechnieken.",
    "author": [
      {
        "name": "Makowski en anderen, vertaling Harrie Jonkman",
        "url": "https://easystats.github.io/bayestestR/"
      }
    ],
    "date": "2020-11-17",
    "categories": [],
    "contents": "\r\nKorte inleiding\r\nDe laatste weken lees ik weer regelmatig over de achtergronden, de principes en de voordelen van bayesiaanse onderzoekstechnieken. De update van Statistical Rethinking. A Bayesian Course with Examples in R and Stan (McElreath, 2020) en het nieuwe boek Regression and other stories (Gelman, Hill & Vehtari, 2020) geven veel inspiratie. Daarover later meer. Ondertussen verscheen vorig jaar het R-pakket bayestestR met een hele duidelijke bijbehorende website waarin een aantal uitgangspunten heel duidelijk worden uitgelegd en de voordelen van deze manier van onderzoek doen worden vergeleken met de klassieke onderzoekstechniek. Ik kon het niet laten om een aantal lessen te vertalen. Mogelijk dat ik hier later nog een keer aandacht aan besteed. De website is gebaseerd op twee artikelen waar de wetenschappers naar refereren. Natuurlijk moet ik deze artikelen hier aan het begin noemen.\r\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. 10.21105/joss.01541\r\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Frontiers in Psychology 2019;10:2767. 10.3389/fpsyg.2019.02767\r\nWaarom zou je het Bayesiaanse kader gebruiken?\r\nHet Bayesiaanse statistische raamwerk wint snel aan populariteit onder wetenschappers, wat samenhangt met de algemene verschuiving naar open en eerlijke wetenschap. Redenen om de voorkeur te geven aan deze aanpak zijn betrouwbaarheid, nauwkeurigheid (in rommelige data en kleine steekproeven), de mogelijkheid om prior kennis in de analyse te introduceren en, kritisch gezien, de intuïtiviteit van de resultaten en hun rechtstreekse interpretatie (Andrews & Baguley, 2013; Etz & Vandekerckhove, 2016; Kruschke, 2010; Kruschke, Aguinis, & Joo, 2012; Wagenmakers et al., 2018).\r\nIn het algemeen wordt de frequentistische aanpak geassocieerd met de focus op null hypothesetests en het misbruik van p-waarden blijkt kritisch bij te dragen aan de reproduceerbaarheidscrisis van psychologische wetenschap (Chambers, Feredoes, Muthukumaraswamy, & Etchells, 2014; Szucs & Ioannidis, 2016). Men is het er algemeen over eens dat de veralgemening van de Bayesiaanse aanpak een manier is om deze problemen te overwinnen (Benjamin et al., 2018; Etz & Vandekerckhove, 2016).\r\nAls we het er eenmaal over eens zijn dat het Bayesiaanse raamwerk de juiste weg is, kun je je vervolgens afvragen wat het Bayesiaanse raamwerk is.\r\nWaar gaat al dat gedoe over?\r\nWat is het Bayesiaanse kader?\r\nHet aannemen van het Bayesiaanse raamwerk is meer een verschuiving in paradigma dan een verandering in methodologie. Inderdaad, alle gemeenschappelijke statistische procedures (t-tests, correlaties, ANOVA’s, regressies, …) kunnen nog steeds worden uitgevoerd met behulp van het Bayesiaanse raamwerk. Een van de kernverschillen is dat in het frequentische perspectief (de “klassieke” statistiek, met p- en t-waarden, evenals met die rare vrijheidsgraden), de effecten vastliggen (maar onbekend zijn) en data random zijn. Aan de andere kant wordt in het Bayesiaanse inferentieproces, in plaats van schattingen van het “ware effect”, de waarschijnlijkheid van verschillende effecten berekend gegeven de waargenomen gegevens. Dat resulteert in een verdeling van mogelijke waarden voor de parameters, de zogenaamde posterior-distributie.\r\nDe onzekerheid in de Bayesiaanse inferentie kan bijvoorbeeld worden samengevat door de mediaan van de verdeling, evenals een reeks waarden van de posterior distributie die de 95% meest waarschijnlijke waarden omvat (het 95% waarschijnlijke interval). Deze kunnen worden beschouwd als de tegenhangers van de punt-schatting en het betrouwbaarheidsinterval in een frequentistisch kader. Om het verschil in interpretatie te illustreren, laat het Bayesiaanse raamwerk toe om te zeggen “gezien de geobserveerde gegevens, heeft het effect een 95% kans om binnen dit bereik te vallen”. Het minder eenvoudige alternatief voor de frequentist zou zijn “wanneer herhaaldelijk betrouwbaarheidsintervallen uit deze reeks gegevens worden berekend, is er een 95% kans dat het effect binnen een bepaald bereik valt”. In wezen geven de Bayesiaanse samplingsalgoritmen (met MCMC-technieken) een waarschijnlijkheidsverdeling (de posterior) van een effect dat compatibel is met de waargenomen gegevens. Zo kan een effect worden beschreven door de posterior verdeling te karakteriseren in relatie tot de centraliteit (punt-schattingen), en gaat het over onzekerheid en het bestaan en de betekenis ervan.\r\nMet andere woorden, als we de ingewikkelde wiskunde achterwege laten, kunnen we zeggen dat:\r\nDe frequentist probeert “het reële effect” in te schatten, bijvoorbeeld, de “echte” waarde van de correlatie tussen x en y. Vandaar dat de modellen van frequentisten een “punt-schatting” opleveren. (d.w.z. één enkele waarde) van de “echte” correlatie (bv. r = 0,42) die wordt geschat op basis van een aantal onduidelijke veronderstellingen (minimaal, aangezien de gegevens willekeurig worden onttrokken van een “ouder”, meestal een normale verdeling).\r\nDe Bayesiaan gaat niet van zoiets uit. De gegevens zijn wat ze zijn. Op basis van deze geobserveerde gegevens (en een eerdere overtuiging over het resultaat) geeft het Bayesiaanse samplingsalgoritme (soms ook wel MCMC sampling genoemd) een waarschijnlijkheidsverdeling (de zogenaamde posterior) van het effect dat compatibel is met de geobserveerde gegevens. Voor de correlatie tussen x en y geeft het een verdeling, die bijvoorbeeld zegt: “het meest waarschijnlijke effect is 0,42, maar deze gegevens zijn ook compatibel met correlaties tussen 0,12 en 0,74”.\r\nOm onze effecten te karakteriseren is geen behoefte aan p-waarden of andere cryptische indices. We beschrijven gewoon de posterior verdeling van het effect. We kunnen bijvoorbeeld de mediaan, de 89% Credible Interval of andere indices rapporteren.\r\nMet andere woorden, als we de wiskunde even achterwege laten, kunnen we zeggen dat:\r\n\r\nHoewel het doel van dit pakket is het gebruik van Bayesiaanse statistieken te verdedigen, zijn er serieuze argumenten die de frequentie-indexen ondersteunen (zie bijvoorbeeld hier). Zoals altijd is de wereld niet zwart-wit (p < .001).\r\n\r\nNou… hoe werkt het?\r\nEen eenvoudig voorbeeld\r\nInstallatie van BayestestR\r\nU kunt bayestestR samen met de hele easystats suite installeren (of alleen bayestestR, omdat de suite installeren bij mij niet werkte) door het volgende uit te voeren: ## A simple example\r\n\r\n\r\n\r\nLaten we ook het pakket rstanarm installeren en laden, die het mogelijk maakt om de Bayesiaanse modellen, evenals de bayestestR, te werken.\r\n\r\n\r\n\r\nTraditionele lineaire regressie\r\nLaten we beginnen met een eenvoudige frequentistische lineaire regressie (de lm() functie staat voor lineair model) tussen twee numerieke variabelen, Sepal.Length en Petal.Length uit de beroemde iris-dataset, standaard opgenomen in R.\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nDeze analyse laat een significante (wat dat ook moge betekenen) en een positieve (met een coëfficiënt van 0,41) lineaire relatie zien tussen de twee variabelen.\r\nHet aanpassen en interpreteren van frequentiemodellen is zo eenvoudig dat het duidelijk is dat mensen het gebruiken in plaats van het Bayesiaanse kader… toch?\r\nNiet meer.\r\nBayesiaanse lineaire regressie\r\n\r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 8.1e-05 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.81 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 0.026836 seconds (Warm-up)\r\nChain 1:                0.0399 seconds (Sampling)\r\nChain 1:                0.066736 seconds (Total)\r\nChain 1: \r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\r\nChain 2: \r\nChain 2: Gradient evaluation took 1.3e-05 seconds\r\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\r\nChain 2: Adjust your expectations accordingly!\r\nChain 2: \r\nChain 2: \r\nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 2: \r\nChain 2:  Elapsed Time: 0.028608 seconds (Warm-up)\r\nChain 2:                0.040535 seconds (Sampling)\r\nChain 2:                0.069143 seconds (Total)\r\nChain 2: \r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\r\nChain 3: \r\nChain 3: Gradient evaluation took 1.2e-05 seconds\r\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\r\nChain 3: Adjust your expectations accordingly!\r\nChain 3: \r\nChain 3: \r\nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 3: \r\nChain 3:  Elapsed Time: 0.028328 seconds (Warm-up)\r\nChain 3:                0.040522 seconds (Sampling)\r\nChain 3:                0.06885 seconds (Total)\r\nChain 3: \r\n\r\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\r\nChain 4: \r\nChain 4: Gradient evaluation took 1.2e-05 seconds\r\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\r\nChain 4: Adjust your expectations accordingly!\r\nChain 4: \r\nChain 4: \r\nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 4: \r\nChain 4:  Elapsed Time: 0.028263 seconds (Warm-up)\r\nChain 4:                0.039091 seconds (Sampling)\r\nChain 4:                0.067354 seconds (Total)\r\nChain 4: \r\n# Description of Posterior Distributions\r\n\r\nParameter    | Median |         89% CI |      pd |        89% ROPE | % in ROPE |  Rhat |      ESS\r\n-------------------------------------------------------------------------------------------------\r\n(Intercept)  |  4.306 | [4.176, 4.433] | 100.00% | [-0.083, 0.083] |         0 | 1.000 | 3981.310\r\nPetal.Length |  0.409 | [0.380, 0.441] | 100.00% | [-0.083, 0.083] |         0 | 1.000 | 3967.091\r\n\r\nDat is het! Je hebt een Bayesiaanse versie van het model gedraaid door eenvoudigweg stan_glm() te gebruiken in plaats van lm() en hebt de posterior distributie van de parameters beschreven. De conclusie die we kunnen trekken, voor dit voorbeeld, zijn zeer vergelijkbaar. Het effect (de mediaan van de posterior verdeling van het effect) is ongeveer 0,41, en het kan ook als significant worden beschouwd in de Bayesiaanse zin (meer daarover later).\r\nDus, klaar om meer te leren?\r\n1. Initiatie tot Bayesiaanse modellen\r\nNu je de beginsectie hebt gelezen, laten we een duik nemen in de subtiliteiten van Bayesiaanse modellering met behulp van R.\r\nLaden van pakketten\r\nAls je de benodigde pakketten hebt geïnstalleerd, kun je rstanarm laden (om de modellen te draaien) en ook bayestestR (om bruikbare indices te berekenen) en insight (om toegang te krijgen tot de parameters).\r\n\r\n\r\n\r\nEenvoudig lineair model (ook wel regressie genoemd)\r\nWe beginnen met het uitvoeren van een eenvoudige lineaire regressie om het verband tussen Petal.Length (onze voorspeller, of onafhankelijke, variabele) en Sepal.Length (onze respons-, of afhankelijke-variabele) te testen vanuit de irisdataset die standaard is opgenomen in R.\r\nPassend bij het model\r\nLaten we beginnen met het draaien van de frequentistische versie van het model, gewoon om een referentiepunt te hebben:\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nIn dit model is de lineaire relatie tussen Petal.Length en Sepal.Length positief en significant (beta = 0,41, t(148) = 21,6, p < .001). Dit betekent dat je voor elke toename van Petal.Length (de voorspeller) met één eenheid kunt verwachten dat de Sepal.Length (het antwoord) met 0,41 zal toenemen. Dit effect kan worden gevisualiseerd door de voorspellingswaarden op de x-as en de responswaarden als y te plotten met behulp van het ggplot2 pakket:\r\n\r\n\r\n\r\nLaten we nu een Bayesiaanse versie van het model draaien door gebruik te maken van de stan_glm-functie dat in het rstanarmpakket zit:\r\n\r\n\r\n\r\nJe ziet dat het samplingsalgoritme draait.\r\nDe posterior eruit halen\r\nLaten we, als het bovenstaande eenmaal gedaan is, de parameters (d.w.z. de coëfficiënten) van het model extraheren.\r\n\r\n  (Intercept) Petal.Length\r\n1    4.473657    0.3846090\r\n2    4.312237    0.4120467\r\n3    4.282567    0.4154728\r\n4    4.336305    0.4070268\r\n5    4.364582    0.4019826\r\n6    4.246177    0.4172826\r\n\r\nZoals we kunnen zien, hebben de parameters de vorm van een lange dataframe met twee kolommen, die overeenkomen met de intercept en het effect van Petal.Length. Deze kolommen bevatten de posterior distributies van deze twee parameters. Eenvoudig gezegd is de posterior distributie een set van verschillende plausibele waarden voor elke parameter.\r\nOver de posterior trekkingen\r\nLaten we eerst eens kijken naar de lengtes van de posteriors.\r\n\r\n[1] 4000\r\n\r\n\r\nWaarom zijn dit er 4000, en niet meer of minder?\r\n\r\nIn de eerste plaats worden deze waarnemingen (de rijen) meestal aangeduid als posterior ‘draws’ (trekkingen). De achterliggende gedachte is dat het Bayesiaanse samplingsalgoritme (b.v. Monte Carlo Markov Chains - MCMC) zal putten uit de verborgen ware posterior distributie Het is dus door middel van deze ‘posterior draws’ dat we de onderliggende ware posterior distribution kunnen inschatten. Hoe meer trekkingen je hebt, hoe beter je de posterior distriubtion kunt inschatten. Meer trekkingen betekent echter ook een langere rekentijd.\r\nAls we kijken naar de documentatie (?sampling) voor het rstanarm“sampling”-algoritme dat standaard in het bovenstaande model wordt gebruikt, kunnen we verschillende parameters zien die het aantal posterior draws beïnvloeden. Standaard zijn er 4 ketens (je kunt het zien als aparte sampling runs), die elk 2000 iter (trekkingen, iteraties) aanmaken. Echter, slechts de helft van deze iteraties wordt behouden, aangezien de helft wordt gebruikt voor de opwarming (het convergeren van het algoritme). Het totaal is dus 4 ketens * (2000 iteraties - 1000 warming-up) = 4000 posterior trekkingen. Dat kunnen we aanpassen naar 2 ketens, bijvoorbeeld:\r\n\r\n\r\n\r\nIn dit geval hebben we, zoals verwacht, 2 ketens * (1000 iteraties - 250 warming-up) = 1500 posterior trekkingen. Maar laten we ons eerste model de standaard instelling aanhouden (omdat het meer trekkingen heeft).\r\nHet visualiseren van de posterieure verdeling\r\nNu we hebben begrepen waar deze waarden vandaan komen, laten we er eens naar kijken. We zullen beginnen met het visualiseren van de posterieure distributie van de parameter waarin we geïnteresseerd zijn, het effect van Petal.Length.\r\n\r\n\r\n\r\nDeze verdeling vertegenwoordigt de waarschijnlijkheid (de y-as) van verschillende effecten (de x-as). De centrale waarden zijn waarschijnlijker dan de extreme waarden. Zoals u ziet varieert deze verdeling van ongeveer 0,35 tot 0,50, waarbij het grootste deel rond 0,41 ligt.\r\n\r\nGefeliciteerd! Je hebt zojuist je posterior distribution beschreven.\r\n\r\nEn dit is het hart van de Bayesiaanse analyse. We hebben geen p-waarden, t-waarden of vrijheidsgraden nodig: Alles is aanwezig, binnen deze posterior verdeling.\r\nOnze beschrijving hierboven is consistent met de waarden verkregen uit de frequentistische regressie (die resulteerde in een bèta van 0,41). Dit is geruststellend! Inderdaad, in de meeste gevallen verandert een Bayesiaanse analyse de resultaten niet drastisch of hun interpretatie. Het maakt de resultaten wel beter interpreteerbaar en intuïtief en uiteindelijk gemakkelijker te begrijpen en te beschrijven.\r\nWe kunnen nu doorgaan en deze posterior verdeling nauwkeurig karakteriseren.\r\nDe Posterior beschrijven\r\nHelaas, het is vaak niet praktisch om de hele posterior verdelingen als grafiek te rapporteren. We moeten een beknopte manier vinden om het samen te vatten. We raden aan om de posterior verdeling te beschrijven op basis van 3 elementen:\r\nEen puntschatting die een samenvatting is van één waarde (vergelijkbaar met de bèta in frequente regressies).\r\nEen credible interval die de bijbehorende onzekerheid weergeeft.\r\nSommige indices van betekenis, die informatie geven over het relatieve belang van dit effect.\r\nPuntschatting\r\nWelke ene waarde kan het beste mijn posterior distributie representeren?\r\nCentrale indices, zoals het gemiddelde, de mediaan of de modus worden meestal gebruikt als puntschatting - maar wat is het verschil tussen het frequentische en Bayesiaanse raamwerk? Laten we dit beantwoorden door eerst het gemiddelde te inspecteren:\r\n\r\n[1] 0.4089701\r\n\r\nDit ligt dicht bij de frequentistische beta. Maar zoals we weten, is het gemiddelde vrij gevoelig voor uitschieters of extremen. Misschien is de mediaan robuuster?\r\n\r\n[1] 0.4087528\r\n\r\nNou, dit ligt zeer dicht bij het gemiddelde (en identiek als de waarden worden afgerond). Misschien kunnen we de modus nemen, dat wil zeggen, de piek van de posterior verdeling? In het Bayesiaanse kader wordt deze waarde de Maximum A Posteriori (MAP) genoemd. Laten we daar eens kijken:\r\n\r\nMAP = 0.41\r\n\r\nZe zitten allemaal heel dichtbij elkaar! Laten we deze drie waarden visualiseren op de posterior distributie:\r\n\r\n\r\n\r\nNou, al deze waarden geven zeer gelijkaardige resultaten. We zullen de mediaan kiezen, omdat deze waarde een directe betekenis heeft vanuit een probabilistisch perspectief: er is 50% kans dat het werkelijke effect hoger is en 50% kans dat het effect lager is (omdat het de verdeling in twee gelijke delen verdeelt).\r\nOnzekerheid\r\nNu we een puntschatting hebben, moeten we de onzekerheid beschrijven. We zouden het bereik kunnen berekenen:\r\n\r\n[1] 0.3302504 0.4806783\r\n\r\nMaar heeft het zin om al deze extreme waarden op te nemen? Waarschijnlijk niet. Dus, we zullen een credible interval berekenen. Lang verhaal kort, het lijkt een beetje op een frequentistische confidence interval, maar is makkelijker te interpreteren en gemakkelijker te berekenen - en het is logischer.\r\nWe zullen dit credible interval berekenen op basis van het Highest Density Interval (HDI). Het geeft ons het bereik dat de 89% meest waarschijnlijke effectwaarden bevat. We zullen 89% CIs gebruiken in plaats van 95% CIs (zoals in het frequentistische kader), omdat het 89%-niveau stabielere resultaten geeft (Kruschke, 2014) en ons herinnert aan de willekeur van dergelijke conventies (McElreath, 2020).\r\n\r\n# Highest Density Interval\r\n\r\n89% HDI     \r\n------------\r\n[0.38, 0.44]\r\n\r\nMooi, dus we kunnen concluderen dat het effect 89% kans heeft om binnen het [0,38, 0,44] bereik te vallen. We hebben zojuist de twee belangrijkste stukken informatie berekend om onze effecten te beschrijven.\r\nEffect significantie\r\nOp veel wetenschappelijke gebieden is het echter niet voldoende om alleen de effecten te beschrijven. Wetenschappers willen ook weten of dit effect betekenis heeft in praktische of statistische termen. Of, om het met andere woorden te zeggen, of het effect belangrijk is. Wijkt het effect af van 0? Dus hoe berekenen we de significantie van een effect. Hoe kunnen we dit doen?\r\nWel, in dit specifieke geval is het zeer welsprekend: Alle mogelijke effectwaarden (d.w.z. de hele posterior distributie) zijn positief en meer dan 0,35, wat al een substantieel bewijs is dat het effect niet nul is.\r\nMaar toch willen we een objectief beslissingscriterium, om te zeggen of het effect ja of nee ‘significant’ is. Een benadering, vergelijkbaar met het frequentistisch kader, zou zijn om te kijken of het Credible Interval een 0 bevat. Als dat niet het geval is, zou dat betekenen dat ons effect ‘significant’ is.\r\nMaar deze index is toch niet erg fijnmazig? Kunnen we het beter doen? Ja.\r\nEen lineair model met een categorische voorspeller\r\nStel je voor dat je geïnteresseerd bent in hoe het gewicht van de kippen varieert, afhankelijk van twee verschillende voedersoorten. Voor dit examen zullen we beginnen met het selecteren van twee voor ons interessante voersoorten uit de chickwts-dataset (zit ook in basis R) (we hebben wel bijzondere interesses): vleesmaaltijden (‘meat meals’) en zonnebloemen (‘sunflowers’).\r\nData voorbereiden en model draaien\r\n\r\n\r\n\r\nLaten we nog een Bayesiaanse regressie uitvoeren om het gewicht te voorspellen met de twee voertypesoorten.\r\n\r\n\r\n\r\nPosterior beschrijving\r\n\r\n\r\n\r\nDit representeert de posterior distributie van het verschil tussen ‘meatmeal’ (‘0’) en ‘sunflowers’(‘1’). Het lijkt erop dat het verschil eerder positief is (de waarden lijken geconcentreerd aan de rechterkant van 0). Het eten van zonnebloemen maakt je dikker (tenminste, als je een kip bent). Maar, door hoeveel?  Laten we de mediaan en de CI berekenen:\r\n\r\n[1] 51.4211\r\n\r\n\r\n# Highest Density Interval\r\n\r\n89% HDI       \r\n--------------\r\n[12.36, 93.22]\r\n\r\nHet maakt je met ongeveer 51 gram (de mediaan) dikker. De onzekerheid is echter vrij groot: er is 89% kans dat het verschil tussen de twee voersoorten tussen 14 en 91 ligt.\r\n\r\nVerschilt dit effect van 0?\r\n\r\nROPE Percentage\r\nTesten of deze verdeling anders is dan 0 heeft geen zin, omdat 0 een enkele waarde is (en de kans dat een verdeling anders is dan een enkele waarde is oneindig).\r\nEen manier om significantie te beoordelen kan echter zijn om een gebied rond 0 te definiëren, wat als praktisch equivalent van nul zal worden beschouwd (d.w.z. afwezigheid van, of verwaarloosbaar, effect). Dit wordt de ‘Region of Practical Equivalence’ (ROPE) genoemd en is een manier om de betekenis van de parameters te testen.\r\nHoe definiëren we dit gebied?\r\n\r\nTringgg Tringgg\r\n\r\n– U spreekt met het easystatsteam. Hoe kunnen we u helpen?\r\n– Ja met Prof. Sanders. Ik ben kippenexpert. Ik bel u vanwege mijn expertkennis. Een effect tussen -20 en 20 is verwaarloosbaar. Tot ziens.\r\nNou, dat komt goed uit. Nu weten we dat we de ROPE kunnen definiëren als het [-20, 20] bereik. Alle effecten binnen dit bereik worden als nihil (te verwaarlozen) beschouwd. We kunnen nu het aandeel van de 89% meest waarschijnlijke waarden (de 89% CI) berekenen die niet nul zijn, d.w.z., die buiten dit bereik liggen.\r\n\r\n# Proportion of samples inside the ROPE [-20.00, 20.00]:\r\n\r\ninside ROPE\r\n-----------\r\n4.38 %     \r\n\r\n5% van de 89% CI kan als nihil worden beschouwd. Is dat veel? Gebaseerd op onze richtlijnen, ja, het is te veel. Op basis van deze specifieke definitie van ROPE concluderen we dat dit effect niet significant is (de kans dat het verwaarloosbaar is, is te groot).\r\nHoewel, om eerlijk te zijn, heb ik een aantal twijfels over deze Prof. Sanders. Ik vertrouw zijn definitie van ROPE niet echt. Is er een meer objectieve manier om het te definiëren?\r\nJa. Een betrouwbare manier is bijvoorbeeld het gebruik van een tiende (1/10 = 0,1) van de standaardafwijking (SD) van de responsvariabele, die als een “verwaarloosbare” effectomvang kan worden beschouwd (Cohen, 1988).\r\n\r\n[1] -6.17469  6.17469\r\n\r\nLaten we onze ROPE opnieuw definiëren als de regio binnen het [-6.2, 6.2] bereik. Merk op dat dit direct kan worden verkregen met de rope_range functie :)\r\n\r\n[1] -6.17469  6.17469\r\n\r\nLaten we nu het percentage in ROPE opnieuw berekenen:\r\n\r\n# Proportion of samples inside the ROPE [-6.17, 6.17]:\r\n\r\ninside ROPE\r\n-----------\r\n0.00 %     \r\n\r\nMet deze redelijke definitie van ROPE stellen we vast dat de 89% van de posterior distributie van het effect niet overlapt met de ROPE. We kunnen dus concluderen dat het effect significant is (in de zin van belangrijk genoeg om op te merken).\r\nWaarschijnlijkheid van Richting (Probability of Direction (pd))\r\nMisschien zijn we niet geïnteresseerd in de vraag of het effect niet te verwaarlozen is. Misschien willen we alleen weten of dit effect positief of negatief is. In dit geval kunnen we eenvoudigweg berekenen welk deel van de posterior distributie positief is, ongeacht de “grootte” van het effect.\r\n\r\n[1] 97.575\r\n\r\nWe kunnen concluderen dat het effect positief is met een waarschijnlijkheid van 98%. We noemen deze index de Waarschijnlijkheid van Richting (pd). Het kan in feite gemakkelijker worden berekend met het volgende:\r\n\r\npd = 97.58%\r\n\r\nInteressant is dat deze index meestal sterk gecorreleerd is met de meest frequente p-waarde. We kunnen de overeenkomstige p-waarde bijna ruwweg afleiden met een eenvoudige transformatie:\r\n\r\n[1] 0.0436\r\n\r\nAls we ons model in het frequentistisch kader hebben uitgevoerd, zouden we ongeveer een effect moeten waarnemen met een p-waarde van 0.04. Is dat waar?\r\nVergelijking met frequentisten\r\n\r\n\r\nCall:\r\nlm(formula = weight ~ feed, data = data)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-123.909  -25.913   -6.917   32.091  103.091 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)     276.91      17.20  16.097 2.74e-13 ***\r\nfeedsunflower    52.01      23.82   2.184   0.0405 *  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 57.05 on 21 degrees of freedom\r\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1463 \r\nF-statistic: 4.769 on 1 and 21 DF,  p-value: 0.04047\r\n\r\nHet frequentistische model vertelt ons dat het verschil positief en significant (beta = 52, p = 0.04) is.\r\nAlhoewel we tot een gelijkaardige conclusie kwamen, liet het Bayesiaanse kader ons toe om een meer diepgaand en intuïtief begrip te ontwikkelen van ons effect en van de onzekerheid van de inschatting ervan.\r\nAlles met één functie\r\nEn toch, ik ben het ermee eens, het was een beetje omslachtig om alle indices eruit te halen en te berekenen. Maar wat als ik je vertel dat we dit allemaal kunnen doen, en meer, met slechts één functie?\r\n\r\nZie, beschrijf_posterior!\r\n\r\nDeze functie berekent alle genoemde indexen, en kan direct op het model worden uitgevoerd:\r\n\r\n# Description of Posterior Distributions\r\n\r\nParameter     |  Median |             89% CI |      pd |        89% ROPE | % in ROPE |        BF |  Rhat |      ESS\r\n-------------------------------------------------------------------------------------------------------------------\r\n(Intercept)   | 277.412 | [248.675, 305.459] | 100.00% | [-6.175, 6.175] |         0 | 1.228e+11 | 1.001 | 3273.839\r\nfeedsunflower |  51.421 | [ 12.363,  93.221] |  97.58% | [-6.175, 6.175] |         0 |     0.761 | 1.001 | 3144.829\r\n\r\nTada! Daar hebben we het! De mediaan, de CI, de pd en het ROPE percentage!\r\nHet begrijpen en beschrijven van posterior distributies is slechts één aspect van Bayesiaanse modellering… Ben je klaar voor meer? \r\nBevestiging van Bayesiaanse vaardigheden\r\nNu het beschrijven en begrijpen van posterior distributies van lineaire regressies voor jou geen geheimen meer heeft, zullen we een stap terug doen en wat eenvoudigere modellen bestuderen: correlaties en t-testen.\r\nMaar laten we eerst even stilstaan bij het feit dat alle statistische basisprocedures zoals correlaties, t-testen, ANOVA’s of Chisquare-testen ** lineaire regressies** zijn (we raden deze uitstekende demonstratie ten zeerste aan). Op basis van deze eenvoudige modellen introduceren we een complexere index, zoals de Bayes-factor.\r\nCorrelaties\r\nFrequentistische versie\r\nLaten we opnieuw beginnen met een frequentistische correlatie tussen twee continue variabelen, de breedte en de lengte van de kelkbladen van sommige bloemen (‘sepals’). De gegevens zijn beschikbaar in R als de iris dataset (dezelfde die we hierboven hebben gebruikt).\r\nWe zullen een Pearson’s correlatietest berekenen, de resultaten opslaan in een object met de naam resultaat en vervolgens deze resultaten weergeven:\r\n\r\n\r\n    Pearson's product-moment correlation\r\n\r\ndata:  iris$Sepal.Width and iris$Sepal.Length\r\nt = -1.4403, df = 148, p-value = 0.1519\r\nalternative hypothesis: true correlation is not equal to 0\r\n95 percent confidence interval:\r\n -0.27269325  0.04351158\r\nsample estimates:\r\n       cor \r\n-0.1175698 \r\n\r\nZoals je in de output kunt zien, heeft de test die we hebben gedaan eigenlijk twee hypothesen vergeleken: de nul-hypothese (h0; geen correlatie) met de alternatieve hypothese (h1; een niet-nul-correlatie). Op basis van de p-waarde kan de nulhypothese niet worden verworpen: de correlatie tussen de twee variabelen is negatief maar niet significant (r = -.12, p > .05).\r\nBayesiaanse correlatie\r\nOm een Bayesiaanse correlatietest te berekenen, hebben we het BayesFactor-pakket nodig (u kunt het installeren door install.packages (“BayesFactor”) uit te voeren). We kunnen dan dit pakket laden, de correlatie berekenen met behulp van de correlatieBF() functie en de resultaten op een vergelijkbare manier opslaan.\r\n\r\n\r\n\r\nLaten we nu eens onze describe_posterior()-functie hierop los:\r\n\r\n  Parameter     Median CI     CI_low    CI_high     pd ROPE_CI\r\n1       rho -0.1138341 89 -0.2369139 0.01535852 0.9165      89\r\n  ROPE_low ROPE_high ROPE_Percentage        BF Prior_Distribution\r\n1     -0.1       0.1        0.424319 0.5090175             cauchy\r\n  Prior_Location Prior_Scale\r\n1              0   0.3333333\r\n\r\nWe zien hier weer veel dingen, maar de belangrijke indices voor nu zijn de mediaan van de posterior distributie, -.11. Dit komt (weer) dicht in de buurt van de frequentistische correlatie. We zouden, zoals eerder, het credible interval, de pd of het ROPE-percentage kunnen beschrijven, maar we zullen ons hier richten op een andere index die door het Bayesiaanse kader wordt geboden, de Bayes-factor (BF).\r\nBayes-factor (BF)\r\nWe zeiden eerder dat een correlatietest eigenlijk twee hypothesen vergelijkt, een nul (afwezigheid van effect) met een alarmerende (aanwezigheid van een effect). De Bayes-factor (BF) laat dezelfde vergelijking toe en bepaalt onder welke van twee modellen de geobserveerde gegevens waarschijnlijker zijn: een model met het effect waarin we geinteresseerd zijn, en een nulmodel zonder het effect daarvan. We kunnen de bayes-factor() gebruiken om de Bayes-factor specifiek te berekenen bij het vergelijken van die modellen:\r\n\r\n# Bayes Factors for Model Comparison\r\n\r\n  Model             BF\r\n  [2] (rho != 0) 0.509\r\n\r\n* Against Denominator: [1] (rho = 0)\r\n*   Bayes Factor Type: JZS (BayesFactor)\r\n\r\nWe hebben een BF van 0,51. Wat betekent dat?\r\nBayes-factoren zijn continue metingen van het relatieve bewijs, waarbij een Bayes-factor groter dan 1 bewijs geeft ten gunste van één van de modellen (vaak de teller genoemd), en een Bayes-factor kleiner dan 1 die bewijs geeft ten gunste van het andere model (de noemer).\r\n\r\nJa, je hebt het goed gehoord, bewijs ten gunste van de nul!\r\n\r\nDat is een van de redenen waarom het Bayesiaanse kader soms als superieur wordt beschouwd aan het frequentistische kader. Onthoud uit je statistiekenlessen, dat de p waarde alleen gebruikt kan worden om h0 af te wijzen, maar niet om het te accepteren. Met de Bayes-factor kunt je -evidentie meten tegen - en ook ten gunste van - de nul.\r\nBF’s die het bewijs voor het alternatief tegen de null vertegenwoordigen kunnen worden teruggedraaid met 𝐵𝐹01=1/𝐵𝐹10 (de 01 en 10 komen respectievelijk overeen met h0 tegen h1 en h1 tegen h0) om het bewijs voor de null weer te geven. Dit verbetert de leesbaarheid in gevallen waarin het BF van het alternatief tegen de nul kleiner is dan 1 (d.w.z. ter ondersteuning van de nul).\r\nIn ons geval, BF = 1/0,51 = 2, geeft aan dat de gegevens 2 keer meer waarschijnlijk zijn onder de null in vergelijking met de alternatieve hypothese. Die weliswaar de voorkeur geeft aan de nul-hypothese, maar slechts als anekdotisch bewijs moet wordt beschouwd.\r\nWe kunnen dus concluderen dat er anecdotisch bewijs is ten gunste van de hypothese ‘gebrek aan correlatie tussen de twee variabelen’ (mediaan = 0,11, BF = 0,51), wat veel meer informatie geeft dan wat we kunnen doen met de frequentistische statistiek.\r\nEn dat is nog niet alles!\r\nVisualiseren van de Bayes-factor\r\nIn het algemeen zijn taartgrafieken een absolute ‘no-go’ in datavisualisatie, omdat het waarnemingssysteem van onze hersenen de gepresenteerde informatie op deze manier sterk vervormt. Toch is er één uitzondering: pizzagrafieken.\r\nHet is een intuïtieve manier om de bewijskracht van BFs te interpreteren als een soort verrassing\r\nDergelijke “pizzapercelen” kunnen direct worden aangemaakt via het zie visualisatiepakket voor easystats (u kunt het installeren door het uitvoeren van\r\nDergelijke ‘pizzagrafieken’ kunnen direct worden aangemaakt met het visualisatiepakket voor easystats (u kunt het installeren door install.packages(\"see\")) uit te voeren):\r\n\r\n\r\n\r\nDus, na het zien van deze pizza, ben je dan nog verrast door de uitkomst?\r\nt-testen\r\n\r\n“Ik weet dat ik niets weet, en vooral niet als versicolor en virginica verschillen in termen van Sepal.Width”, zei de beroemde Socrates.\r\n\r\nTijd om eindelijk een antwoord te geven op deze cruciale vraag!\r\nVersicolor vs. virginica\r\nBayesiaanse t-testen kunnen worden uitgevoerd op een zeer vergelijkbare manier als correlaties. We zijn met name geïnteresseerd in twee niveaus van de Specie factor, versicolor en virginica. We zullen beginnen met het uit iris uitfilteren van de niet-relevante waarnemingen die overeenkomen met de setosa specie, en we zullen dan de waarnemingen en de distributie van de Sepal.Width variabele visualiseren.\r\n\r\n\r\n\r\nBereken de Bayesiaanse t-test\r\nHet lijkt er (visueel) op dat virgnica bloemen gemiddeld een iets grotere kelkbladbreedte hebben. Laten we dit verschil statistisch beoordelen met behulp van de ttestBF in het BayesFactor pakket.\r\n\r\n   Parameter    Median CI     CI_low   CI_high      pd ROPE_CI\r\n1 Difference 0.1854753 89 0.08450306 0.2857605 0.99825      89\r\n    ROPE_low ROPE_high ROPE_Percentage       BF Prior_Distribution\r\n1 -0.0332751 0.0332751               0 17.71872             cauchy\r\n  Prior_Location Prior_Scale\r\n1              0   0.7071068\r\n\r\nOp basis van de indexen kunnen we zeggen dat het verschil tussen virginica en versicolor (van Sepal.Width) een kans heeft van 100% om negatief te zijn [van de pd en het teken van de mediaan] (mediaan = -0,19, 89% CI [-0,29, -0,092]). De gegevens leveren een sterk bewijs tegen de nulhypothese (BF = 18).\r\nHoud dat in gedachten, want we zullen een andere manier zien om deze vraag te onderzoeken.\r\nLogistisch Model\r\nEen hypothese waarvoor men een t-test gebruikt, kan ook getest worden met een binomiaal model (bv. een logistisch model). Het is inderdaad mogelijk om de volgende hypothese te herformuleren, “er is een belangrijk verschil in deze variabele tussen de twee groepen” door “deze variabele in staat te stellen om te discrimineren tussen (of te classificeren in) de twee groepen”. Deze modellen zijn echter veel krachtiger dan een gewone t-test.\r\nIn het geval van het verschil van Sepal.Width tussen virginica en versicolor wordt de vraag, hoe goed kunnen we de twee soorten classificeren met alleen Sepal.Width.\r\nHet model fitten\r\n\r\n\r\n\r\nPrestatie en parameters\r\nEerst prestatie van het model in kaart brengen.\r\n\r\nCan't calculate log-loss.\r\n# Indices of model performance\r\n\r\nELPD   | ELPD_SE |  LOOIC | LOOIC_SE |   WAIC |   R2 | RMSE | Sigma | Score_log | Score_spherical\r\n-------------------------------------------------------------------------------------------------\r\n-66.34 |    3.07 | 132.69 |     6.14 | 132.68 | 0.10 | 1.11 |  1.00 |   -104.44 |            0.01\r\n\r\nVervolgens de resultaten van enkele indices presenteren.\r\n\r\n# Description of Posterior Distributions\r\n\r\nParameter   | Median |           89% CI |     pd |        89% ROPE | % in ROPE |    BF |  Rhat |      ESS\r\n---------------------------------------------------------------------------------------------------------\r\n(Intercept) | -6.135 | [-9.619, -2.807] | 99.98% | [-0.181, 0.181] |         0 | 9.566 | 1.001 | 2640.283\r\nSepal.Width |  2.131 | [ 0.979,  3.341] | 99.95% | [-0.181, 0.181] |         0 | 8.933 | 1.001 | 2653.992\r\n\r\nReferenties\r\nAndrews, M., & Baguley, T. (2013). Prior approval: The growth of bayesian methods in psychology. British Journal of Mathematical and Statistical Psychology, 66(1), 1–7.\r\nBenjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., … others. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6.\r\nChambers, C. D., Feredoes, E., Muthukumaraswamy, S. D., & Etchells, P. (2014). Instead of ’playing the game’ it is time to change the rules: Registered reports at aims neuroscience and beyond. AIMS Neuroscience, 1(1), 4–17.\r\nEtz, A., & Vandekerckhove, J. (2016). A bayesian perspective on the reproducibility project: Psychology. PloS One, 11(2), e0149794.\r\nKruschke, J. K. (2010). What to believe: Bayesian methods for data analysis. Trends in Cognitive Sciences, 14(7), 293–300.\r\nKruschke, J. K., Aguinis, H., & Joo, H. (2012). The time has come: Bayesian methods for data analysis in the organizational sciences. Organizational Research Methods, 15(4), 722–752.\r\nSzucs, D., & Ioannidis, J. P. (2016). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. BioRxiv, 071530.\r\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., … others. (2018). Bayesian inference for psychology. Part i: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-17-testen-met-bayes/testen-met-bayes_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-01-nogmaals-grafieken/",
    "title": "Het Goede, het Slechte en het Lelijke: Data effectief visualiseren en communiceren",
    "description": "Shirin Elsinghorst schreef deze blog onlangs op [Codecentric](https://blog.codecentric.de/2020/10/goodbadugly/). Omdat ze op mijn werk de vormgeving van de uitgaven hebben aangepast, wilde ik het maken van figuren aan de nieuwe kleursetting van mijn werk aanpassen. Shirin's blog was een mooie oefenplaats voor mij. Tegelijk is het een mooie introductie op datavisualisatie en daarom de moeite waard het in het Nederlands te bewerken.    [Hier](https://docs.google.com/presentation/d/e/2PACX-1vR4pD2EmW9Gzxr1Q3qwgjEYkU64o2-ThlX1mXqfNQ2EKteVUVt6Qg2ImEKKi9XLv-Iutb3lD8esLyU7/pub?start=false&loop=false&delayms=3000&slide=id.g58b36409ef_0_0) vind je de presentatie die zij zelf hierover op 20 oktober 2020 in Duitsland gaf.",
    "author": [
      {
        "name": "Shirin Elsinghorst, vertaling Harrie Jonkman",
        "url": "https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/"
      }
    ],
    "date": "2020-11-01",
    "categories": [],
    "contents": "\r\nEnkele handelingen vooraf\r\nEerst maar eens de pakketten laden die gebruikt worden:\r\n\r\n\r\n\r\nVervolgens de kleuren instellen.\r\nDe dataset\r\n\r\n# A tibble: 6 x 8\r\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\r\n  <fct>   <fct>              <dbl>         <dbl>             <int>\r\n1 Adelie  Torgersen           39.1          18.7               181\r\n2 Adelie  Torgersen           39.5          17.4               186\r\n3 Adelie  Torgersen           40.3          18                 195\r\n4 Adelie  Torgersen           NA            NA                  NA\r\n5 Adelie  Torgersen           36.7          19.3               193\r\n6 Adelie  Torgersen           39.3          20.6               190\r\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\r\n\r\n\r\n\r\n[1] 0.3926991 1.1780972 1.9634954 2.7488936 3.5342917 4.3196899\r\n[7] 5.1050881 5.8904862\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n0. Datavisualisatie, cruciaal voor begrip en communicatie\r\nDatavisualisatie is een cruciaal onderdeel van elke analyse. Of het nu “voor jezelf” is om verbanden en resultaten beter te begrijpen of om resultaten te presenteren en te “verkopen” aan anderen. Omdat goede grafieken de gegevens intuïtief toegankelijk maken, vertellen ze een verhaal en laten ze duidelijk patronen, trends of uitschieters zien. Daarom is Explorative Data Analyse (EDA) meestal de eerste stap van elke gegevensanalyse en -modellering. Alleen als we onze gegevens begrijpen, kunnen we de juiste voorbewerkingsstappen en analysemethoden, statistieken of ’deep learning’technieken toepassen. En aangezien mensen veel beter zijn in het visueel begrijpen van getallen in een grafiek dan in tabellen, moeten we de kracht van datavisualisatie gebruiken! Vooral bij het maken van visualisaties voor rapporten of publicaties is het cruciaal dat deze zowel feitelijk correct als visueel aantrekkelijk zijn.\r\n\r\nDatavisualisatie is deels kunst en deels wetenschap. De uitdaging is om de kunst goed te krijgen zonder dat de wetenschap het bij het verkeerde eind heeft en vice versa. (Wilke 2019)\r\n\r\nHet doel van een goede illustratie is dat het in één oogopslag begrijpelijk is en een duidelijke uitspraak doet. Slechte grafieken variëren van eenvoudigweg lelijk tot (opzettelijk?) misleidend of zelfs verkeerd. In dit artikel leg ik uit wat goede grafieken zijn (en wat niet) en hoe we ze kunnen maken met behulp van de Grammatica van Grafieken. En ik introduceer enkele van de meest gebruikte soorten grafieken, samen met negatieve voorbeelden “uit de vrije natuur”.\r\nWat maakt een goede grafiek?\r\n1. Data\r\nHet belangrijkste aspect en de basis van elke grafiek zijn de gebruikte gegevens! Ze moeten correct zijn en we moeten altijd controleren op mogelijke (meet)fouten. Zoals bijvoorbeeld in deze afbeelding:\r\n\r\nHet is duidelijk dat er een fout in de voorspelling zit, want temperaturen van -100°C, zelfs op een hele speciale dag, zijn echt heel onwaarschijnlijk! En niet alleen toont deze grafiek duidelijk verkeerde gegevens; deze uitschietwaarde comprimeert de rest van de gegevens in de grafiek zodanig dat de curven niet meer duidelijk zichtbaar zijn en de waarden van verschillende dagen moeilijk te vergelijken zijn.\r\nTot goede praktijk van datavisualisatie behoort tevens het specificeren van de gegevensbron.\r\n2. Overzichtelijkheid & kleuren\r\nEen goede grafiek is juist zo complex omdat ze haar boodschap in één oogopslag moet overbrengen; ze moet duidelijk zijn en geen “chart junk” bevatten. Edward Tufte beschouwde alle visuele elementen in een grafiek die ofwel niet nodig zijn om de grafiek te begrijpen, ofwel zelfs afleiden van de centrale informatie als grafiektroep (Tufte 1983). Drie voorbeelden daarvan zijn in deze figuur te zien:\r\n\r\nIedereen die regelmatig kranten leest zal merken dat “chart junk” extreem vaak voorkomt en vooral populair is bij populaire media om een chart te kruiden. In veel gevallen kan een onschuldige chartjunk worden afgedaan als “artistieke vrijheid”, maar hoe ernstiger jouw grafiek wilt maken, hoe meer je die moet vermijden.\r\nDuidelijkheid omvat ook de keuze van geschikte kleuren en duidelijke contrasten. Te veel, felle of grillige kleuren maken een plot visueel weerzinwekkend en zorgen ervoor dat het er overbeladen en verwarrend uitziet. Daarnaast moet je er bij de keuze van de kleuren ook op letten dat het voor kleurenblinden mogelijk is om de grafiek te lezen. Dit moet worden gedaan met behulp van een geschikt kleurenpalet, evenals redundante functies zoals verschillende vormen, patronen of lijntypes die het mogelijk maken om de grafiek te lezen, zelfs zonder kleurinformatie.\r\nKleuren\r\n3. etikettering & assen\r\nEen goede grafiek is in één oogopslag te begrijpen, wat betekent dat ze voor zichzelf spreken. Essentieel hiervoor is de juiste etikettering met titel, aslabels (met eenheden!), legenda’s en bijschriften! Ook het aantal assentikken moet op de juiste manier worden gekozen. En vooral: de afstanden tussen de assen moeten voor de numerieke waarden regelmatig zijn, d.w.z. dat de afstand tussen de teken in geen geval mag variëren, zoals te zien is op de y-as in deze vreselijke illustratie:\r\n\r\nDe grammatica van de grafiek\r\nWe kennen nu de belangrijkste aspecten van goede grafieken. Maar wat is de beste manier om goede grafieken te maken? Er zijn vele manieren om grafieken te genereren: met de hand tekenen, met Excel of met verschillende programmeertalen zoals R, Python, Java, enz. De beste manier, hoewel met een hogere instapdrempel, is het gebruik van programmeertalen. Dit is de eenvoudigste manier om ervoor te zorgen dat de gegevens schoon zijn en op een traceerbare manier kunnen worden verwerkt. Excel kan ook gebruikt worden om grafieken te maken, maar het programma heeft een paar valkuilen: Excel-format kan makkelijk leiden tot fouten in de gegevens. En het maakt het erg moeilijk om grafieken te reproduceren, omdat het niet documenteert welke stappen handmatig werden uitgevoerd en in welke volgorde.\r\nR en Python zijn bijzonder geschikt omdat ze de meest gebruikte programmeertalen zijn voor het genereren van grafieken en omdat ze pakketten aanbieden die het analyseren van gegevens en het maken van grafieken zeer efficiënt maken. Hier presenteer ik de (naar mijn mening) beste manier om op een gestructureerde manier grafieken te genereren: met de pakketten ggplot2 voor R of plotnine voor Python (gebaseerd op ggplot2).\r\nMet ggplot2 heeft Hadley Wickham een implementatie gemaakt van de 1999 Grammatica van Graphics voor de door Leland Wilkinson beschreven R-programmeertaal, die ik hieronder introduceer met codevoorbeelden (Wilkinson et al. 1999; Wickham 2010). Deze Grammatica van Graphics beschrijft een raamwerk voor het gestructureerd maken van grafieken die bestaan uit lagen die op elkaar voortbouwen (Wickham en Grolemund 2017). Hieronder laat ik een paar voorbeelden zien. Een overzicht van alle mogelijke opties is te vinden in het ggplot2-cheatsheet.\r\n1. Data\r\nOok voor de Grammatica van Graphics zijn data het belangrijkste en meest fundamentele element. Hier gebruik ik een voorbeelddataset met verschillende groottes van drie pinguïnsoorten (Gorman, 2014). De centrale functie van het ggplot2-pakket wordt ggplot() genoemd en neemt een dataset als invoer. Deze functie creëert eerst een leeg coördinatensysteem waarop we met de volgende lagen kunnen bouwen en zo onze grafiek stap voor stap kunnen creëren, aanpassen en uitbreiden.\r\n2. Esthetiek\r\nHet tweede argument dat we definiëren in de ggplot() functie is de esthetiek (aes()). Esthetiek beschrijft grafische elementen zoals X- en Y-waarden, grootte, kleuren, vormen, enz. Voor een eenvoudige scatterplot moeten we ten minste de X- en Y-posities specificeren. Hiervoor moeten we eerst beslissen welke gegevens (variabelen) we in kaart willen brengen. Bijvoorbeeld hier de snavellengte van de pinguïn op de X-as tegen de vinlengte op de Y-as:\r\n\r\n\r\n\r\nZelfs met een bepaalde esthetiek krijgen we nog steeds geen echte grafiek te zien, maar we hebben de volgende laag nodig, de zogenaamde geometrie. Omdat esthetiek en geometrie zeer nauw met elkaar verbonden zijn en deels van elkaar afhankelijk zijn, laat ik hieronder extra esthetiek zien.\r\n3. Geometriek\r\nGeometrische objecten of geometrieën beschrijven hoe de gegevens die we in de esthetiek hebben gedefinieerd, moeten worden weergegeven. Dit kan bijvoorbeeld een puntgrafiek (geom_point()) of een lijngrafiek (geom_line()) zijn, die nu ook als grafiek in deze laag wordt weergegeven:\r\n\r\n\r\n\r\nEen lijngrafiek is echter niet nuttig voor de gegevens hier; meer hierover in de sectie Grafiektypen - Lijngrafieken. Andere geometrieën zijn staafdiagrammen (geom_bar()) of boxplots (geom_boxplot()). Geometrie en esthetiek zijn onderling afhankelijk in die zin dat het gegevenstype van de esthetische variabelen alleen bepaalde geometrieën toelaat of zinvol is. Zo zijn strooi- en lijndiagrammen geschikt voor doorlopende X- en Y-assen (rationele getallen, tijden of datum). Voor staafdiagrammen moeten de X-as gegevens categorisch zijn. Voordat ik in de loop van latere lagen meer in detail zal ingaan op geometrieën en esthetiek, zal ik eerst facetten introduceren.\r\n4. Facetten\r\nFacetten betekent het splitsen van een grafiek in verschillende subplots. In onze voorbeelddataset worden de meetwaarden van drie verschillende pinguïnsoorten verzameld. Het bovenstaande strooiplot laat echter niet toe om een onderscheid te maken tussen de drie soorten, wat natuurlijk een belangrijke bijkomende informatie is in de gegevens. Daarom moeten we dit in onze grafiek weergeven. Een manier om dit te doen is door gebruik te maken van facetten:\r\n\r\n\r\n\r\nNu zien we de punten voor elke soort pinguïn in een aparte subplot. Facetten kunnen worden gecreëerd voor één of meer categorische variabelen, maar meer dan twee facetten zullen in het algemeen verwarrend zijn. Standaard gebruikt ggplot2 dezelfde X- en Y-asafmetingen om de subplots vergelijkbaar te maken. Met facetten is het in dit geval echter niet zo eenvoudig om de drie typen te vergelijken. Als alternatief kunnen we de drie pinguïnsoorten zichtbaar maken met behulp van verschillende kleuren. Deze mogelijkheid valt onder schaalvergroting.\r\n5. Schaalvergroting\r\nMet schaalvergroting kunnen we naast de twee X- en Y-dimensies nog extra dimensies tonen, vergelijkbaar met wat we al gedaan hebben voor de pinguïnsoorten met facetten. Zo kunnen we bijvoorbeeld een kleurenschaal kiezen. In ggplot2 worden schalen gegeven door extra variabelen in de esthetiek:\r\n\r\n\r\n\r\nDe bijbehorende legende wordt automatisch aangemaakt. Andere schalen zijn maatschalen, punt- of lijntypes. Niet alle schalen zijn geschikt voor elk datatype. Terwijl kleuren ook continue getallen kunnen vertegenwoordigen, zijn punt- en lijntypes slechts voor een beperkt aantal categorieën beschikbaar.\r\n\r\n\r\n\r\nIn principe kan elk aantal dimensies van de gegevens in een grafiek worden weergegeven, ook al kunnen meer dan vier dimensies de grafiek meestal te chaotisch en verwarrend maken.\r\nEen ander type schaling is de asschaalverdeling. Zo kunnen we bijvoorbeeld de assen omdraaien zodat de waarden niet van links/naar beneden = laag naar rechts/boven = hoog worden weergegeven, maar de hoge waarden wel links/naar beneden worden weergegeven:\r\n\r\n\r\n\r\n6. Statistische Transformaties\r\nStats, afkorting voor statistische transformaties, worden gebruikt om statistische waarden of berekeningen toe te voegen aan een plot of om deze te definiëren. Dit kunnen bijvoorbeeld gemiddelde waarden zijn, mediaan, betrouwbaarheidsintervallen, standaardafwijkingen, enz.\r\nIn deze figuur is een staafdiagram weergegeven met een numerieke waarde:\r\n\r\n\r\n\r\nOmdat de standaardstatistiek voor staafdiagrammen in ggplot2 „Aantal (count)“ ist, kunnen deze verhoudingen mit de stat „identity“ worden veranderd.\r\nEen van de meest gebruikte statistieken zijn ‘Smoothed Conditional Means’, om bijvoorbeeld de samenhang van de X- en Y-Variabelen met gladde lijnen en bijbehorende foutmarges aan te tonen:\r\n\r\n\r\n\r\n7. Coördinatensystemen\r\nDe laatste laag in de Grammatica van Graphics zijn coördinatensystemen. Coördinatensystemen bepalen hoe de assen van onze grafiek moeten worden gerangschikt. Meestal is de X-as horizontaal en de Y-as verticaal (cartesiaans coördinatenstelsel); maar er zijn ook gevallen waarin we radiale of gebogen assen hebben, bijvoorbeeld in een taartdiagram of kaartweergave. Een taartdiagram is dus niets meer dan een staafdiagram waarin we het coördinatensysteem hebben veranderd:\r\n\r\n\r\n\r\nDiagramtypen Met deze Grammatica van Graphics kunnen nu alle gangbare diagramtypen eenvoudig en flexibel worden gegenereerd en uitgebreid. De meest gebruikte diagramtypen zijn:\r\nPuntdiagrammen\r\n\r\n\r\n\r\nPuntdiagrammen worden vaak gebruikt wanneer we numerieke X-waarden tegen numerieke Y-waarden willen weergeven en dus hun correlatie willen laten zien. Plots kunnen verschillende kleuren, vormen en maten hebben. Meestal zijn puntdiagrammen gemakkelijk te begrijpen, maar ze kunnen ook verwarrend worden als er te veel overlappende punten zijn.\r\nLijndiagrammen\r\n\r\n\r\n\r\nLijndiagrammen zijn meestal vergelijkbaar met puntdiagramma, met dat verschil dat de (denkbeeldige) punten met elkaar verbonden zijn door lijnen. Deze verbonden lijnen vertegenwoordigen de denkbeeldige tussenliggende waarden tussen twee meetpunten; punten moeten dus alleen verbonden worden als deze veronderstelling wordt gemaakt! Om deze reden is een lijngrafiek niet bruikbaar voor het bovenstaande voorbeeld, omdat we onafhankelijke metingen van individuele personen laten zien. Lijndiagrammen zijn vooral nuttig voor tijdreeksen.\r\nStaafdiagrammen\r\n\r\n\r\n\r\nStaafdiagrammen tonen ofwel het aantal gebeurtenissen of ze tonen een numerieke waarde Y ter vergelijking tussen verschillende categorieën. Vooral bij staafdiagrammen vinden we veel negatieve voorbeelden met misleidende voorstellingen (waarschijnlijk omdat ze zo gemakkelijk te maken zijn met de eenvoudigste tekenprogramma’s zonder enige gegevensbasis). Hier zijn twee zeer opvallende negatieve voorbeelden van de coronavirus situatie: in beide voorbeelden komen de staafhoogtes niet overeen met de waarden op de (niet getoonde) Y-as!\r\n\r\n\r\nEen verzameling van andere veelgebruikte diagramtypen met illustraties en negatieve voorbeelden is te vinden in de dia’s bij deze lezing.\r\nReferenties\r\nGorman, Tony D. AND Fraser, Kristen B. AND Williams. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\r\nTufte, Edward R. 1983. The Visual Display of Quantitative Information. Graphics Press.\r\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. O’Reilly Media, Inc. https://r4ds.had.co.nz/.\r\nWilke, C. O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. https://books.google.de/books?id=L3ajtgEACAAJ.\r\nWilkinson, L., D. Wills, J. Chambers, R. Dubbs, W. Eddy, A. Norton, and W. Haerdie. 1999. The Grammar of Graphics. Statistics and Computing. Springer New York. https://books.google.de/books?id=5boZAQAAIAAJ.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-01-nogmaals-grafieken/nogmaals-grafieken_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-17-een-eenvoudige-introductie-op-machine-learning/",
    "title": "Een eenvoudige introductie op `tidymodels`",
    "description": "Edgar Ruiz' eenvoudige introductie op machine learning met de inzet van het pakket `tidymodels`.",
    "author": [
      {
        "name": "Edgar Ruiz, vertaling Harrie Jonkman",
        "url": "https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/"
      }
    ],
    "date": "2020-10-17",
    "categories": [],
    "contents": "\r\nIntroductie Harrie\r\nIk heb me voorgenomen om wat machine learning te leren. Het komt mij allemaal nog wat onbekend voor. Door soms tutorials van anderen over te zetten en te kijken wat er gebeurt, wil ik hierin verder komen. Edgar Ruiz schreef een korte inleiding en zijn tutorial heb ik, brutaal als ik ben, naar het Nederlands overgezet. Edgar Ruiz, ik hoop dat je dit goed vindt. Onderaan vermeld ik waar de lezer jouw oorspronkelijke tutorial kan vinden.\r\nEen rustige introductie op tidymodels\r\nThe WorkflowOnlangs had Edgar Ruiz de gelegenheid om tidymodels te laten zien in workshops en gesprekken. Omdat hij zichzelf meer ziet als gebruiker dan ontwikkelaar, zou het wel eens waardevol en interessant kunnen zijn, zo dacht hij, om te delen wat hij tot nu toe had geleerd. Laten we eerst eens bekijken wat tidymodels in onze analyseprojecten kan betekenen, dat was het doel van zijn korte en duidelijke introductie tidymodels.\r\nHet figuur hierboven is gebaseerd op een figuur uit R voor Data Science boek, het boek van Wickham en Grolemund en wordt heel vaak gebruikt. Alleen wordt hier het onderdeel modeleren met tidymodels uitgevoerd en dat is nieuw. In de introductie laat hij zien welke stappen hier gezet moeten worden. Modeleren kan baat hebben bij een ‘nette’ interface, dat is waar tidymodels een rol speelt.\r\nHet is belangrijk om te verduidelijken dat de groep van pakketten die deel uitmaken van tidymodels niet zelf statistische modellen implementeren. In plaats daarvan richten ze zich vooral op het makkelijker maken van alle taken die te maken hebben met modeleren. Deze taken omvatten het voorbewerken van gegevens tot en met het valideren van resultaten.\r\nIn zekere zin kent het modeleren enkele substappen. Voor deze substappen levert tidymodels één of meerdere pakketten. Dit artikel toont functies uit vier tidymodels pakketten, die alle vier in de suite tidymodels zijn opgesloten:\r\nrsample - Verschillende types van re-samples recipes - Transformaties om data voor te bewerken voor modeleren parnip - Een algemene interface voor modelcreatie yardstick - Meten hoe het model het doet\r\nHet volgend figuur illustrates elkw modelleerstapt, en laat de tidymodels pakketten zien die we in dit artikel zullen gebruiken:\r\nThe WorkflowIn een bepaalde analyse kan al dan niet het tidyverse pakket worden gebruikt. Niet alle projecten hoeven te werken met tijdsvariabelen, dus het is niet altijd nodig om functies uit het hms pakket te gebruiken. Hetzelfde idee geldt voor tidymodels. Afhankelijk van wat voor soort modelering er gedaan gaat worden, zullen alleen functies uit sommige van de pakketten gebruikt worden.\r\nEen voorbeeld\r\nWe zullen de iris dataset hiervoor gebruiken. De data zijn al binnen gehaald en voldoende opgeschoond om direct te modeleren.\r\nLaad alleen de tidymodels bibliotheek\r\nDit is mogelijk het eerste artikel dat hij heeft geschreven waarbij hij slechts één pakket heeft aangeroepen via de bibliotheek(). Naast het laden van de kernpakketten voor het modelleren, laadt tidymodels, ook handig, een aantal tidyverse pakketten, waaronder dplyr en ggplot2. Gedurende deze oefening zullen we enkele functies uit die pakketten gebruiken, maar we hoeven ze dus niet expliciet in onze R-sessie te laden.\r\n\r\n\r\n\r\nHet voorwerk\r\nDeze eerstestap richt zich op het geschikt maken van data voor modellering. Daarbij wordt gebruik gemaakt van datatransformaties. Alle transformaties kunnen worden uitgevoerd met dplyr, of andere tidyverse pakketten Overweeg het gebruik van tidymodels pakketten wanneer dit deel zwaarder en complexer is.\r\nData Sampling\r\nDe initial_split() functie is vooral gebouwd om de dataset op te splitsen in een trainings en test set. Standaard wordt 3/4 van de data voor training en de rest voor testen gebruikt. Dat kan aangepast worden door het prop functie te gebruiken. Dit genereert een rplit object, geen dataframe. De geprinte output laat het aantal rijen voor testen, trainen en het totaal zien.\r\n\r\n<Analysis/Assess/Total>\r\n<90/60/150>\r\n\r\nOm toegang te krijgen tot de observaties van de trainingsset, gebruik je de training() functie. Hetzelfde voor testset waar je toegang toe krijgt via testing().\r\n\r\nRows: 90\r\nColumns: 5\r\n$ Sepal.Length <dbl> 4.7, 5.4, 5.0, 4.4, 4.9, 4.8, 4.3, 5.8, 5.1, 5…\r\n$ Sepal.Width  <dbl> 3.2, 3.9, 3.4, 2.9, 3.1, 3.0, 3.0, 4.0, 3.8, 3…\r\n$ Petal.Length <dbl> 1.3, 1.7, 1.5, 1.4, 1.5, 1.4, 1.1, 1.2, 1.5, 1…\r\n$ Petal.Width  <dbl> 0.2, 0.4, 0.2, 0.2, 0.1, 0.1, 0.1, 0.2, 0.3, 0…\r\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\r\n\r\nDeze samplingfuncties zijn mogelijk met behulp van het rsample pakket, dat deel uitmaakt van tidymodels.\r\nPre-proces interface\r\nIn tidymodels biedt het recipes pakket een interface dat gespecialiseerd is in het voorbewerken van gegevens. Binnen het pakket worden de functies die de gegevenstransformaties starten, of uitvoeren, vernoemd naar kookacties. Dat maakt de interface gebruiksvriendelijker. Bijvoorbeeld:\r\nrecipe() - start een nieuwe set van toe te passen transformaties, vergelijkbaar met het ggplot() commando. Het belangrijkste argument is de formule van het model.\r\nprep() - Voert de transformaties uit bovenop de geleverde gegevens (meestal de trainingsgegevens).\r\nElke datatransformatie is een stap. Functies komen overeen met specifieke soorten stappen, die elk een voorvoegsel van step_ hebben. Er zijn verschillende step_ functies; in dit voorbeeld gebruiken we er drie:\r\nstep_corr() - Verwijdert variabelen die sterk correleren met andere variabelen\r\nstep_center() - Normaliseert numerieke data die een gemiddelde van nul krijgen\r\nstep_scale() - Normaliseert numerieke data die een standard deviatie van één krijgen\r\nEen ander aardig kenmerk van deze step is dat deze kan worden toegepast op een specifieke variabele, groepen variabelen of alle variabelen. De all_outocomes() en all_predictors() functie bieden een hele prettige manier om specifieke groepen variabelen te specificeren. Bijvoorbeeld, als we de step_corr() willen gebruiken om alleen de predictorvariabelen te analyseren, gebruiken we step_corr(all_predictors()). Zo hoeven we niet elke variabele op te sommen.\r\nIn het volgende voorbeeld, brengen we de recipe(), prep() en step-functies om een recipe object te creëren. De training() functie wordt gebruikt om de dataset uit de eerder aangemaakte gesplitste dataset te halen.\r\n\r\n\r\n\r\nAls we het iris_recipe object oproepen, zal het details hierover afdrukken. De Operations sectie beschrijft wat er met de gegevens is gedaan. Een van de bewerkingen in het voorbeeld legt uit dat de correlatiestap de Petal.Length variabele heeft verwijderd.\r\n\r\nData Recipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor          4\r\n\r\nTraining data contained 90 data points and no missing data.\r\n\r\nOperations:\r\n\r\nCorrelation filter removed Petal.Length [trained]\r\nCentering for Sepal.Length, ... [trained]\r\nScaling for Sepal.Length, ... [trained]\r\n\r\nUitvoeren van het pre-proces\r\nDe testgegevens kunnen nu worden getransformeerd met behulp van precies dezelfde stappen, gewichten en categorisatie als bij de voorbewerking van de trainingsgegevens. Hiervoor wordt een andere functie met een kookterm gebruikt: bake(). Merk op dat de functie testing() wordt gebruikt om de juiste dataset te extraheren.\r\nThe testing data can now be transformed using the exact same steps, weights, and categorization used to pre-process the training data. To do this, another function with a cooking term is used: bake(). Notice that the testing() function is used in order to extract the appropriate data set.\r\n\r\nRows: 60\r\nColumns: 4\r\n$ Sepal.Length <dbl> -0.9090229, -1.1427716, -1.4933947, -1.0258972…\r\n$ Sepal.Width  <dbl> 1.0457854, -0.1076544, 0.1230336, 1.2764734, 0…\r\n$ Petal.Width  <dbl> -1.3566929, -1.3566929, -1.3566929, -1.3566929…\r\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\r\n\r\nHet uitvoeren van dezelfde operatie over de trainingsgegevens is overbodig, omdat die gegevens al zijn voorgeprogrammeerd. Om de voorbereide trainingsgegevens in een variabele te laden, gebruiken we juice(). Het zal de gegevens uit het iris_recipe object halen.\r\n\r\nRows: 90\r\nColumns: 4\r\n$ Sepal.Length <dbl> -1.37652034, -0.55839976, -1.02589723, -1.7271…\r\n$ Sepal.Width  <dbl> 0.3537215, 1.9685372, 0.8150974, -0.3383423, 0…\r\n$ Petal.Width  <dbl> -1.3566929, -1.0918288, -1.3566929, -1.3566929…\r\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\r\n\r\nModel training\r\nIn R zijn er meerdere pakketten die op hetzelfde type model passen. Het is gebruikelijk dat elk pakket een unieke interface biedt. Met andere woorden, zaken als een argument voor hetzelfde modelattribuut wordt voor elk pakket anders gedefinieerd. Bijvoorbeeld, de ranger en randomForest-pakketten passen bij Random Forest-modellen. In de ranger() functie gebruiken we num.trees om het aantal bomen te definiëren. In randomForest wordt dat argument dan weer ntree genoemd. Het is niet gemakkelijk om te wisselen tussen pakketten om hetzelfde model te draaien.\r\nIn plaats van het modelleerpakket te vervangen, vervangt tidymodels de interface. Beter gezegd, tidymodels biedt een enkele set functies en argumenten om een model te definiëren. Een specifiek pakket wordt dan aangepast in het model en algemeen gemaakt.\r\nIn het onderstaande voorbeeld wordt de rand_forest() functie gebruikt om een Random Forest model te initialiseren. Om het aantal bomen te definiëren wordt het treesargument gebruikt. Om de ranger versie van Random Forest te gebruiken, wordt de set_engine() functie gebruikt. Tenslotte wordt de fit() functie gebruikt om het model uit te voeren. De verwachte argumenten zijn de formule en de gegevens. Merk op dat het model boven op de gesausde getrainde gegevens draait.\r\n\r\n\r\n\r\nAls we nu hetzelfde model niet met ranger maar met randomForest willen draaien, hoeven we alleen maar de waarde in set_engine() te veranderen in randomForest.\r\n\r\n\r\n\r\nHet is ook het vermelden waard dat het model niet in een enkele, grote functie met veel argumenten is gedefinieerd. De definitie van het model is gescheiden in kleinere functies zoals fit() en set_engine(). zo krijgen we een flexibelere - en gemakkelijker te leren - interface.\r\nVoorspellingen\r\nIn plaats van een vector geeft de predictfunctie tibble terug. Standaard wordt de voorspellingsvariabele .pred_class genoemd. Merk op dat in het voorbeeld de ’kook’testgegevens worden gebruikt.\r\n\r\n# A tibble: 60 x 1\r\n   .pred_class\r\n   <fct>      \r\n 1 setosa     \r\n 2 setosa     \r\n 3 setosa     \r\n 4 setosa     \r\n 5 setosa     \r\n 6 setosa     \r\n 7 setosa     \r\n 8 setosa     \r\n 9 setosa     \r\n10 setosa     \r\n# … with 50 more rows\r\n\r\nHet is eenvoudig om de voorspellingen toe te voegen aan de ’kook’testgegevens door gebruik te maken van dplyr’s bind_cols() functie.\r\n\r\nRows: 60\r\nColumns: 5\r\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa…\r\n$ Sepal.Length <dbl> -0.9090229, -1.1427716, -1.4933947, -1.0258972…\r\n$ Sepal.Width  <dbl> 1.0457854, -0.1076544, 0.1230336, 1.2764734, 0…\r\n$ Petal.Width  <dbl> -1.3566929, -1.3566929, -1.3566929, -1.3566929…\r\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\r\n\r\nModel validatie\r\nGebruik de metrics() functie om de prestaties van het model te meten. Het zal automatisch metrieken kiezen die geschikt zijn voor een bepaald type model. De functie verwacht een tibble dat de werkelijke resultaten bevat (waarheid) en wat het model heeft voorspeld (schatting).\r\n\r\n# A tibble: 2 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy multiclass     0.983\r\n2 kap      multiclass     0.975\r\n\r\nDoor de consistentie van de nieuwe interface is het meten van de resultaten voor het randomForest-model eenvoudig omdat je alleen maar de modelvariabele aan de bovenkant van de code hoeft te vervangen (nu iris_rf ipv iris_ranger.\r\n\r\n# A tibble: 2 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy multiclass     0.983\r\n2 kap      multiclass     0.975\r\n\r\nPer classificator metriek\r\nHet is eenvoudig om de waarschijnlijkheid voor elke mogelijke voorspelde waarde te verkrijgen door het type-argument op prob te zetten. Dat levert een tibble op met zoveel mogelijk variabelen als er mogelijke voorspelde waarden zijn. Hun naam zal standaard op de oorspronkelijke waarde worden gezet, voorafgegaan door .pred_.\r\n\r\nRows: 60\r\nColumns: 7\r\n$ .pred_setosa     <dbl> 0.99800000, 0.90737302, 0.97193651, 0.9780…\r\n$ .pred_versicolor <dbl> 0.000000000, 0.062146825, 0.006666667, 0.0…\r\n$ .pred_virginica  <dbl> 0.002000000, 0.030480159, 0.021396825, 0.0…\r\n$ Sepal.Length     <dbl> -0.9090229, -1.1427716, -1.4933947, -1.025…\r\n$ Sepal.Width      <dbl> 1.0457854, -0.1076544, 0.1230336, 1.276473…\r\n$ Petal.Width      <dbl> -1.3566929, -1.3566929, -1.3566929, -1.356…\r\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, se…\r\n\r\nOok hier, gebruik bind_cols() om de voorspellingen toe te voegen aan de test dataset die je hiervoor hebt voorbereid.\r\n\r\nRows: 148\r\nColumns: 5\r\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"se…\r\n$ .n              <dbl> 0, 2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1…\r\n$ .n_events       <dbl> 0, 2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1…\r\n$ .percent_tested <dbl> 0.000000, 3.333333, 5.000000, 10.000000, 11…\r\n$ .percent_found  <dbl> 0.00000, 9.52381, 14.28571, 28.57143, 33.33…\r\n\r\nNu alles in een tibble zit, is het eenvoudig om curve-methoden te berekenen. In dit geval gebruiken we de gain_curve().\r\n\r\n\r\n\r\nIn de curve-methoden zit ook een autoplot() functie dat makkelijk kan worden omgezet naar een ggplot2 visualizatie.\r\n\r\n\r\n\r\nDit is een voorbeeld van een roc_curve(). Nogmaals, vanwege de consistentie van de interface, hoeft maar een functienaam te worden omgezet; zelfs de argument waarden blijven hetzelfde.\r\n\r\n\r\n\r\nOm de gecombineerde enkelvoudige voorspelde waarde en de waarschijnlijkheid van elke mogelijke waarde te meten, combineer je de twee voorspellingsmodi (met en zonder prop type). In dit voorbeeld is met het gebruik van dplyr’s select() de resulterende tibble makkelijker af te lezen.\r\n\r\nRows: 60\r\nColumns: 5\r\n$ .pred_setosa     <dbl> 0.99800000, 0.90737302, 0.97193651, 0.9780…\r\n$ .pred_versicolor <dbl> 0.000000000, 0.062146825, 0.006666667, 0.0…\r\n$ .pred_virginica  <dbl> 0.002000000, 0.030480159, 0.021396825, 0.0…\r\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, se…\r\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, se…\r\n\r\nPipe de resultatentabel in metrics(). In dit geval, specificeer de .pred_class als de schatting.\r\n\r\n# A tibble: 4 x 3\r\n  .metric     .estimator .estimate\r\n  <chr>       <chr>          <dbl>\r\n1 accuracy    multiclass     0.983\r\n2 kap         multiclass     0.975\r\n3 mn_log_loss multiclass     0.167\r\n4 roc_auc     hand_till      0.990\r\n\r\nSlotopmerkingen van Edgar Ruiz\r\nDit voorbeeld is bedoeld als een hele voorzichtige kennismaking met tidymodels. Het aantal functies en de mogelijkheden van dergelijke functies zijn voor deze demonstratie tot een minimum beperkt, maar er kan nog veel meer worden gedaan met deze prachtige suite van pakketten. Hopelijk helpt dit artikel jou op weg en moedigt het u misschien zelfs aan om uw kennis verder uit te breiden.\r\nDank je wel!\r\nEdgar Ruiz wil graag Max Kuhn en Davis Vaughan, de ontwikkelaars van tidymodels, bedanken. Ze waren hem genadig in het geven van instructie, feedback en begeleiding tijdens zijn reis om tidymodels te leren.\r\nSlotopmerkingen van Harrie Jonkman\r\nVoor mij was dit inderdaad een van de eerste kennismakingen met tidymodels. Ik kende het pakket caret en ook het mlr. tidymodels is een modernisering van deze eerste interfaces en moet net zo’n RStudio succes worden als tidyverse. Ondertussen het ik de website van tidymodels gelezen, de interactieve cursus van Julia Silge en de introducties van Alison Hill. Ook het nieuwe boek van Max Kuhn en Julia Silge ben ik op dit moment aan het lezen. Hieronder vind je die literatuur die ik op dit moment wat bij elkaar aan het zoeken ben. Voor mij is er nog een lange weg te gaan maar het artikel van Edgar Ruiz was voor mij een eerste uitstapje. Ik wil hem hartelijk dank voor zijn uitnodiging.\r\nLiteratuur en verwijzingen\r\nEdgar Ruiz (A Gentle introduction to tidymodels)[https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/]\r\nTidymodels (website)[https://www.tidymodels.org/]\r\nHefin Rhys (Machine learning with R)[https://education.rstudio.com/blog/2020/02/conf20-intro-ml/]\r\nJulia Silge (Supervised learning course)[https://juliasilge.com/blog/supervised-ml-course/]\r\nMax Kuhn and Julia Silge (Tidy modeling with R)[https://www.tmwr.org/]\r\nAlison Hill (Introduction to machine learning)[https://education.rstudio.com/blog/2020/02/conf20-intro-ml/]\r\nRebecca Barter (Tidymodels: tidy machine learning in R)[http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/]\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-17-een-eenvoudige-introductie-op-machine-learning/een-eenvoudige-introductie-op-machine-learning_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-21-survival-analyse-met-r/",
    "title": "Survival analyse met R",
    "description": "Dit is een deel van een tutorial die Emily Zabore schreef over survival analyse met R.",
    "author": [
      {
        "name": "Emily Zabore, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-08-21",
    "categories": [],
    "contents": "\r\nVoorwoord\r\nOnlangs volgde ik een webinar bij STATA over analyse van survival data (https://www.stata.com/training/webinar_series/survival-analysis/). Vervolgens ben ik op zoek gegaan naar een tutorial in R over survival analyse en kwam terecht by deze interessante tutorial van Emily Zabore. Deze had ik al eens eerder zien staan maar dit was het moment om er eens goed naar te kijken. Om mij de techniek goed eigen te maken heb ik deze voor het grootste deel naar het Nederlands toe omgezet en waar nodig bewerkt. Alle credits gaan naar Emily Zabore die dit zo helder en duidelijk op rij heeft gezet (hartelijke dank, Emily). Deze tutorial is dus een introductie op survival analyse en hoe deze techniek in R uit te voeren. Emily Zabore presenteerde dit voor op R-Presentatie serie van het Memorial Sloan Kettering Cancer Center in New York op 30 augustus 2018. Daarna heeft ze deze aangepast voor een meer intensievere training op hetzelfde centrum in Maart 2019. De informatie kun je vinden op haar Github repository waar je deze tutorial en de bron-files kunt vinden.\r\nDeel 1: Introductie op Survival Analyse\r\nDeze tutorial bevat enige basisinformatie over survival analyse en de volgende uitgaven kunnen jou hierbij verder helpen:\r\n\r\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part I: Basic concepts and first analyses. 232-238. ISSN 0007-0920.\r\n\r\n\r\nM J Bradburn, T G Clark, S B Love, & D G Altman. (2003). Survival Analysis Part II: Multivariate data analysis – an introduction to concepts and methods. British Journal of Cancer, 89(3), 431-436.\r\n\r\n\r\nBradburn, M., Clark, T., Love, S., & Altman, D. (2003). Survival analysis Part III: Multivariate data analysis – choosing a model and assessing its adequacy and fit. 89(4), 605-11.\r\n\r\n\r\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part IV: Further concepts and methods in survival analysis. 781-786. ISSN 0007-0920.\r\n\r\nPakketten\r\nEnkele R-pakketten zullen in ieder geval gebruikt worden:\r\nlubridate\r\nsurvival\r\nsurvminer\r\nZorg ervoor dat ze geïnstalleerd zijn (wel doen, anders werkt het niet) en open ze vervolgens zo:\r\n\r\n\r\n\r\nWat zijn survival data?\r\nHet gaat om Tijd-tot-gebeurtenis (Time-to-event) data die bestaan uit een aparte starttijd en eindtijd.\r\nVoorbeelden uit kankeronderzoek zijn:\r\nTijd van operatie tot dood;\r\nTijd van start van de behandeling tot progressie;\r\nTijd van antwoord op herhaling.\r\nVoorbeelden uit andere velden:\r\nTijd-tot-gebeurtenis data zijn in verschillende velden algemeen, waaronder:\r\nTijd van HIV infectie tot ontwikkeling van AIDS;\r\nTijd tot hardaanval;\r\nTijd tot begin van alcoholmisbruik;\r\nTijd tot initiatie van sexuele activiteit;\r\nTijd tot machine niet meer goed functioneert.\r\nHoe survival analyse ook wel wordt genoemd\r\nOmdat survival analyse in allerlei velden wordt gebruikt, kent het ook verschillende namen:\r\nBetrouwbaarheids-analyse;\r\nDuur-analyse;\r\nGebeurtenisgeschiedenis-analyse;\r\nTijd-tot-gebeurtenis-analyse.\r\nDe longdataset\r\nDe long dataset is beschikbaar via het survival pakket in R. De data bevatten subjecten met gevorderde longkanker van de North Central Cancer Treatment Group. Enkele variabelen (waarvoor ik de Engelse namen gebruik zoals ze in de dataset voorkomen) die we zullen gebruiken om methodes aan te tonen zijn:\r\ntime: Overlevingstijd (Survival tijd) in dagen;\r\nstatus: censoring status 1=censored, 2=dood;\r\nsex: Man=1 Vrouw=2.\r\nWat is censoring?\r\n\r\n\r\n\r\n\r\nRICH JT, NEELY JG, PANIELLO RC, VOELKER CCJ, NUSSENBAUM B, WANG EW. A PRACTICAL GUIDE TO UNDERSTANDING KAPLAN-MEIER CURVES. Otolaryngology head and neck surgery: official journal of American Academy of Otolaryngology Head and Neck Surgery. 2010;143(3):331-336. doi:10.1016/j.otohns.2010.05.007.\r\n\r\nTypen van censoring\r\nEen subject kan gecensored zijn vanwege:\r\nOmdat we ze niet meer te volgen zijn;\r\nZe uit de studie zijn gestapt;\r\nEr geen gebeurtenis kon worden vastgesteld bij het einde van de vaststaande studieperiode.\r\nDit zijn voorbeelden van rechts censoring.\r\nLinks censoring en interval censoring zijn ook mogelijk en er zijn methodes om dit soort data te analyseren, maar hier beperken we ons tot rechts censoring.\r\nGecensorde survival data\r\n\r\n\r\n\r\nHoe zouden we in dit voorbeeld de proportie vaststellen van hen die gebeurtenisvrij zijn bij 10 jaar?\r\nSubjecten 6 en 7 zijn gebeurtenisvrij bij 10 jaar. Subjecten 2, 9 en 10 hadden de gebeurtenis voor 10 jaar. Subjecten 1, 3, 4, 5 en 8 zijn gecensord voor 10 jaar, dus we weten niet of ze de gebeurtenis hebben of niet bij 10 jaar - hoe kunnen we deze subjecten incorpereren in onze schatting?\r\nDistributie van follow-up tijd\r\nGecensorde subjecten geven nog steeds informatie dus zijn geschikt om in de analyse mee te nemen;\r\nDistributie van follow-up tijden is scheef en kunnen verschillen tussen gecensorde patiënten en hen met gebeurtenissen;\r\nFollow-up tijden zijn altijd positief.\r\n\r\n\r\n\r\nComponenten van survival data\r\nVoor subject \\(i\\):\r\nGebeurtenistijd \\(T_i\\)\r\nCensortijd \\(C_i\\)\r\nGebeurtenis indicator \\(\\delta_i\\):\r\n1 als gebeurtenis geobserveerd (bv. \\(T_i \\leq C_i\\))\r\n0 als gecensord (bv. \\(T_i > C_i\\))\r\n\r\nGeobserveerde tijd: \\(Y_i = \\min(T_i, C_i)\\)\r\nDe geobserveerde tijden en een gebeurtenis indicator zitten in de long data\r\ntime: Survival tijd in dagen (\\(Y_i\\))\r\nstatus: censor status 1=gecensord, 2=dood (\\(\\delta_i\\))\r\n\r\ninst\r\ntime\r\nstatus\r\nage\r\nsex\r\nph.ecog\r\nph.karno\r\npat.karno\r\nmeal.cal\r\nwt.loss\r\n3\r\n306\r\n2\r\n74\r\n1\r\n1\r\n90\r\n100\r\n1175\r\nNA\r\n3\r\n455\r\n2\r\n68\r\n1\r\n0\r\n90\r\n90\r\n1225\r\n15\r\n3\r\n1010\r\n1\r\n56\r\n1\r\n0\r\n90\r\n90\r\nNA\r\n15\r\n5\r\n210\r\n2\r\n57\r\n1\r\n1\r\n90\r\n60\r\n1150\r\n11\r\n1\r\n883\r\n2\r\n60\r\n1\r\n0\r\n100\r\n90\r\nNA\r\n0\r\n12\r\n1022\r\n1\r\n74\r\n1\r\n1\r\n50\r\n80\r\n513\r\n0\r\n\r\nOmgaan met data (van datum) in R\r\nData laten vaak de start en einddata zien eerder dan de voorberekende survival tijden. De eerste stap is om er zeker van te zijn dat deze als data in R zijn geformatteerd.\r\nOm het duidelijk te maken, laten we eens een kleine dataset als voorbeeld maken met de variabelen sx_date voor operatiedatum en last_fup_date voor de laatste follow-up datum.\r\n\r\n\r\n# A tibble: 3 x 2\r\n  sx_date    last_fup_date\r\n  <chr>      <chr>        \r\n1 2007-06-22 2017-04-15   \r\n2 2004-02-13 2018-07-04   \r\n3 2010-10-27 2016-10-31   \r\n\r\nWe zien dat het beide chr-variabelen betreft, wat vaker het geval is. Maar het is nodig ze naar data-variabelen te formatteren.\r\nFormatteren van data - basis R\r\n\r\n\r\n# A tibble: 3 x 2\r\n  sx_date    last_fup_date\r\n  <date>     <date>       \r\n1 2007-06-22 2017-04-15   \r\n2 2004-02-13 2018-07-04   \r\n3 2010-10-27 2016-10-31   \r\n\r\nMerk op dat in basis R het format zowel de onderscheider als het symbool moet omvatten. Dus als jouw data in format m/d/Y staan dan heb je het format = \"%m/%d/%Y\"nodig.\r\nVoor een hele lijst van dataformat symbolen kun je kijken naar https://www.statmethods.net/input/dates.html\r\nFormatteren van data-lubridate pakket\r\nWe kunnen ook het lubridate pakket gebruien om data te formatteren. In in dit geval, gebruik de ymd functie\r\n\r\n\r\n# A tibble: 3 x 2\r\n  sx_date    last_fup_date\r\n  <date>     <date>       \r\n1 2007-06-22 2017-04-15   \r\n2 2004-02-13 2018-07-04   \r\n3 2010-10-27 2016-10-31   \r\n\r\nMerk op dat hier de onderscheiders in de R optie niet hoeven worden toegepast.\r\nDe help pagina voor ?dmy zullen je alle format opties kunnen geven.\r\nCalculeren van survival tijden - basis R\r\nNu de data zijn geformatteerd, moeten we het verschil tussen start en eindtijd in een bepaalde eenheid uitdrukken,meestal maanden of jaren. In basis R, wordtdifftime gebruikt om het aantal dagen te berekenen tussen onze twee data en dit te converteren naar een numerieke waarde met as.numeric. Dan zetten we het over naar dagen door het te delen door 365.25, het gemiddelde aantal dagen in een jaar.\r\n\r\n\r\n\r\n\r\n\r\n# A tibble: 3 x 3\r\n  sx_date    last_fup_date os_yrs\r\n  <date>     <date>         <dbl>\r\n1 2007-06-22 2017-04-15      9.82\r\n2 2004-02-13 2018-07-04     14.4 \r\n3 2010-10-27 2016-10-31      6.01\r\n\r\nBerekenen van survival tijden - lubridate\r\nAls we het lubridate pakket gebruiken, dan drukt %--% een tijd interval uit, dat dan geconverteerd wordt naar het aantal seconden door as.duration te gebruiken en tenslotte naar jaren door het te delen door dyears(1), wat het aantal seconden in een jaar geeft.\r\n\r\n\r\n# A tibble: 3 x 3\r\n  sx_date    last_fup_date os_yrs\r\n  <date>     <date>         <dbl>\r\n1 2007-06-22 2017-04-15      9.82\r\n2 2004-02-13 2018-07-04     14.4 \r\n3 2010-10-27 2016-10-31      6.01\r\n\r\nNotitie: we moeten het lubridate pakket laden door library te gebruiken en zo toegang te krijgen tot specifieke functies.\r\nGebeurtenis indicator\r\nVoor de componenten van survivaldata noemde ik eerder de gebeurtenisindicator:\r\nGebeurtenisindicator \\(\\delta_i\\):\r\n1 als de gebeurtenis heeft plaatsgevonden (bv. \\(T_i \\leq C_i\\))\r\n0 als het gecensord is (i.e. \\(T_i > C_i\\))\r\nEchter, in R accepteert de Surv-function ook ‘TRUE/FALSE’ (’TRUE = gebeurtenis) of 1/2 (2 = gebeurtenis).\r\nIn de lung data, hebben we:\r\nstatus: censoring status 1=gecensord, 2=dood\r\nSurvivalfunctie\r\nDe waarschijnlijkheid dat een subject zal overleven voorbij een bepaalde gespecificeerde tijd\r\n\\[S(t) = Pr(T>t) = 1 - F(t)\\]\r\n\\(S(t)\\): survivalfunctie \\(F(t) = Pr(T \\leq t)\\): cumulatieve distributiefunctie\r\nIn theorie is de survivalfunctie gelijdelijk; in de praktijk observeren we gebeurtenissen op een discrete tijdschaal.\r\nSurvivalwaarschijnlijkheid\r\nSurvivalwaarschijnlijkheid op een bepaalde tijd, \\(S(t)\\), is een conditionele waarschijnlijkheid van overleven voorbij die tijd gegeven dat een individu overleefd heeft juist voorafgaand aan die tijd.\r\nHet kan worden geschat als het aantal patiënten die leven zonder verlies van follow-up gegevens op die tijd, gedeeld door het aantal patiënten die leven voorafgaand aan dat moment.\r\nDe Kaplan-Meier schatting van survivalwaarschijnlijkheid is het product van deze conditionele waarschijnlijkheden tot op dat moment.\r\nOp tijd 0, de survivalwaarschijnlijkheid is 1, bv. \\(S(t_0) = 1\\)\r\nCreëren van survivalobjecten\r\nDe Kaplan-Meier methode is de meest algemene manier om survivaltijden en -waarschijnlijkheden te schatten. Het is een niet-parametrische benadering die resulteert in een stapsgewijze-functie, met steeds een stap naar beneden iedere keer wanneer de gebeurtenis plaatsvindt.\r\nDe Surv functie van het survival pakket creëert een survival object voor gebruik als antwoord in een modelformule. Er is een ingang voor elk subject dat de survivaltijd is, dat wordt gevolgd door een + als het subject gecensord is. Laten we eens naar de eerste tien observaties kijken, dat maakt meer duidelijk:\r\n\r\n\r\n [1]  306   455  1010+  210   883  1022+  310   361   218   166 \r\n\r\nSchatten van de survivalcurves met de Kaplan-Meier methode\r\nDe survfit functie creëert survivalcurves die gebaseerd zijn op een formule. Laten we de overallsurvivalcurve eens genereren voor de hele cohort, benoem het als het object f1, en kijk naar de namen(names) van dat object:\r\n\r\n\r\n [1] \"n\"         \"time\"      \"n.risk\"    \"n.event\"   \"n.censor\" \r\n [6] \"surv\"      \"std.err\"   \"cumhaz\"    \"std.chaz\"  \"type\"     \r\n[11] \"logse\"     \"conf.int\"  \"conf.type\" \"lower\"     \"upper\"    \r\n[16] \"call\"     \r\n\r\nEnkele sleutelcomponenten van dit survfit object dat wordt gebruikt om survivalcurves te maken omvatten:\r\ntime, die de start en eindpunten van elk tijdinterval inhouden\r\nsurv, die de survivalwaarschijnlijkheid inhouden die corresponderen met elke tijd (time)\r\nKaplan-Meier grafiek - basis R\r\nNu plotten we het survfit object met basis R om de Kaplan-Meier grafiek te krijgen.\r\n\r\n\r\n\r\nDe standaard grafiek in basis R toont de stapfunctie (doorlopende lijn) met de betrouwbaarheidsintervallen die ermee samenhangen (stippellijnen);\r\nHorizontale lijnen representeren de survivalduur voor het interval;\r\nDe interval beëindigd door een gebeurtenis;\r\nDe hoogte van de verticale lijnen laat de verandering in cumulatievewaarschijnlijkheid zien;\r\nGecensorde observaties, aangegeven met vinkjes, reduceren de cumulatieve survival tussen intervallen. (Opmerking de vinkjes voor gecensorde patiënten worden niet in deze standaardgrafiek getoond, maar worden toegevoegd door de optie mark.time = TRUE).\r\n\r\n\r\n\r\nKaplan-Meier grafiek - ggsurvplot\r\nAls alternatief kun je de ggsurvplot functie gebruiken van het survminer pakket dat op ggplot2 is gebouwd, en dat kan worden gebruikt om de Kaplan-Meier grafieken te maken. Bekijk de cheatsheet maar eens voor het survminer pakket.\r\n\r\n\r\n\r\nDe standaardgrafiek van ggsurvplot laat de stapfunctie (doorlopende lijn) zien met bijbehorende betrouwbaarheidsbanden (donkere gebied);\r\nDe vinkjes voor gecensorde patienten worden hier wel standaard getoond, enigszins onduidelijk aangegeven in de lijn in dit voorbeeld en kan worden weggehaald met de optie censor = FALSE.\r\nSchatten van \\(x\\)-jaar overleven\r\nEen kwantiteit waar we in de survivalanalyse vaak in geïnteresseerd zijn is de waarschijnlijkheid van overleven voorbij een bepaald aantal (\\(x\\)) jaren.\r\nBijvoorbeeld, om de waarschijnlijkheid te schatten van overleven bij \\(1\\) jaar, gebruik je summary met het times argument (Opgelet de time variabele in de lung data is eigenlijk in dagen, dus we moeten times = 365.25 gebruiken).\r\n\r\n\r\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\r\n\r\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\r\n  365     65     121    0.409  0.0358        0.345        0.486\r\n\r\nWe vinden dat de \\(1\\)-jaar waarschijnlijkheid van overleven in deze study 41% is.\r\nDe boven- en ondergrens van het 95% betrouwbaarheidsinterval wordt ook getoond.\r\n\\(x\\)-jaar survival en de survivalcurve\r\nDe \\(1\\)-jaar survivalwaarschijnlijkheid is het punt op de y-as dat correspondeert met \\(1\\) jaar op de x-as voor de survivalcurve.\r\n\r\n\r\n\r\n\\(x\\)-jaar survival wordt vaak niet correct geschat\r\nWat gebeurt als je een “naïeve” schatting gebruikt?\r\n121 van de 228 patiënten stierven bij \\(1\\) jaar dus:\r\n\\[\\Big(1 - \\frac{121}{228}\\Big) \\times 100 = 47\\%\\] - Je krijgt een incorrecte schatting van de \\(1\\)-jaar survivalwaarschijnlijheid als het feit over het hoofd ziet dat 42 patiënen waren gecensord voor \\(1\\) jaar.\r\nHerinner dat de correcte schatting van de \\(1\\)-jaar survivalwaarschijnlijkheid 41% was.\r\nImpact op \\(x\\)-jaar survival door censoring over het hoofd te zien\r\nStel twee studies, met elk 228 subjecten. In iedere studie zijn er 165 doden. Geen censoring bij de een (oranje lijn), 63 patiënten gecensord in de andere (blauwe lijn);\r\nOntkennen van censoring leidt tot een overschatting van de overall survivalwaarschijnlijkheid, omdat de gecensorde subjecten alleen informatie bijdragen voor het deel van de follow-up tijd en dan vallen ze uit de risk set, dus drukken ze de cumulatieve waarschijnlijkheid van de survival naar beneden.\r\n\r\n\r\n\r\nSchatten van de mediaan survivaltijd\r\nEen andere kwantiteit die vaak interessant is voor overlevingsanalyse is de gemiddelde overlevingstijd, die we kwantificeren met behulp van de mediaan. Er wordt niet verwacht dat de overlevingstijd normaal wordt verdeeld, dus het gemiddelde is geen passende samenvatting.\r\nWe kunnen dit rechtstreeks verkrijgen uit ons survfit object\r\n\r\n\r\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\r\n\r\n      n  events  median 0.95LCL 0.95UCL \r\n    228     165     310     285     363 \r\n\r\nWe zien dat de mediaan survivaltijd 310 dagen is. De onder- en bovengrens van het 95% betrouwbaarheidsinterval worden ook gegeven.\r\nMediaan overlevingstijd and de overlevingscurve\r\nMediaan survival is de tijd die correspondeert met de overlevingswaarschijnlijkheid van \\(0.5\\):\r\n\r\n\r\n\r\nDe mediaan van overleving wordt vaak verkeerd geschat\r\nWat gebeurt als je een “naïeve” schatting gebruikt?\r\nVat de mediaan overlevingstijd samen onder de 165 patiënten die stierven\r\n\r\n\r\n  median_surv\r\n1         226\r\n\r\nJe krijgt een verkeerde schatting van de mediaan overlevingstijd van 226 dagen als je het feit ontkent dat gecensorde patiënten ook bijdragen aan de follow-up tijd.\r\nOnthoud dat de correcte schatting van de mediaan overlevingstijd 310 dagen is.\r\nImpact op de overlevingsmediaan wanneer je censoring negeert\r\nNegeren van censoring creëert een kunstmatig lagere overlevinscurve omdat de follow-up tijd die gecensorde patiënten bijdragen er buiten wordt gehouden (paarse lijn).\r\nDe ware overlevingscurve voor de lung data zie je voor de vergelijking in blauw staan.\r\n\r\n\r\n\r\nVergelijken van overlevingstijd tussen groepen\r\nWe kunnen significantie testen tussen-groepen uitvoeren met een ‘log-rank test’.\r\nDe ‘log-rank test’ weegt observaties gelijk over de hele follow-up tijd en is de meest algemene manier om overlevingstijden tussen groepen te vergelijken.\r\nEr zijn ook versie die meer gewicht geven aan de eerdere of latere follow-up die meer geschikt zijn afhankelijk van de onderzoeksvraag (zie ?survdiff voor verschillende test opties).\r\nWe krijgen de log-rank p-waarde wanneer we de survdiff functie gebruiken. Bijvoorbeeld, we kunnen testen of er een verschil was in overlevingstijd wat sexe betreft in de lung data.\r\n\r\n\r\nCall:\r\nsurvdiff(formula = Surv(time, status) ~ sex, data = lung)\r\n\r\n        N Observed Expected (O-E)^2/E (O-E)^2/V\r\nsex=1 138      112     91.6      4.55      10.3\r\nsex=2  90       53     73.4      5.68      10.3\r\n\r\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \r\n\r\nExtraheren van informatie van een survdiff object\r\nHet is lastig om een p-waarde te extraheren van de resultaten van survdiff. Hier is een coderegel om dat te doen.\r\n\r\n\r\n[1] 0.001311165\r\n\r\nOf er is de sdp functie in het ezfun pakket, dat je kunt installeren via devtools::install_github(\"zabore/ezfun\"). Het geeft een opgemaakt p-value terug.\r\n\r\n\r\n[1] 0.001\r\n\r\nHet Cox regressiemodel\r\nWe kunnen een effectgrootte voor een enkele variabele kwantificeren of meer dan één variabele opnemen in een regressiemodel wanneer we rekening willen houden met de effecten van meerdere variabelen.\r\nHet Cox-regressiemodel is een semi-parametrisch model dat kan worden gebruikt voor univariabele en multivariabele regressiemodellen met overlevingsuitkomsten.\r\n\\[h(t|X_i) = h_0(t) \\exp(\\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip})\\]\r\n\\(h(t)\\): gevaar (‘hazard’), of de mate waarin de gebeurtenis plaatsvindt \\(h_0(t)\\): onderliggende baseline van het gevaar (‘hazard’).\r\nEnkele aannames van het model:\r\nnon-informatieve censoring\r\nproportionele ‘hazards’\r\nOpgelet: parametrische regressiemodels voor overlevingsuitkomsten zijn ook beschikbaar, daar zal hier niet op worden ingegaan.\r\nWe kunnen regressiemodellen voor survivaldata draaien door de coxph functie te gebruiken, die een Surv object aan de linkerkant neemt en een standaard regressie formule in R aan de rechterkant hanteert.\r\n\r\n\r\nCall:\r\ncoxph(formula = Surv(time, status) ~ sex, data = lung)\r\n\r\n       coef exp(coef) se(coef)      z       p\r\nsex -0.5310    0.5880   0.1672 -3.176 0.00149\r\n\r\nLikelihood ratio test=10.63  on 1 df, p=0.001111\r\nn= 228, number of events= 165 \r\n\r\nFormatteren van Cox regressieresultaten\r\nWe kunnen een opgeschoonde versie van de output zien door de tidy functie van het broom pakket te gebruiken:\r\n\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\nsex\r\n0.5880028\r\n0.1671786\r\n-3.176385\r\n0.0014912\r\n\r\nOf door tbl_regression te gebruiken van het gtsummary pakket.\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#bthodvqsjj .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#bthodvqsjj .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#bthodvqsjj .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#bthodvqsjj .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#bthodvqsjj .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#bthodvqsjj .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#bthodvqsjj .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#bthodvqsjj .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#bthodvqsjj .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#bthodvqsjj .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#bthodvqsjj .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#bthodvqsjj .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#bthodvqsjj .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#bthodvqsjj .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#bthodvqsjj .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#bthodvqsjj .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#bthodvqsjj .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#bthodvqsjj .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#bthodvqsjj .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#bthodvqsjj .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#bthodvqsjj .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#bthodvqsjj .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#bthodvqsjj .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#bthodvqsjj .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#bthodvqsjj .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#bthodvqsjj .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#bthodvqsjj .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nCharacteristic\r\n      HR1\r\n      95% CI1\r\n      p-value\r\n    sex\r\n      0.59\r\n      0.42, 0.82\r\n      0.001\r\n    \r\n        \r\n          1\r\n          \r\n           \r\n          HR = Hazard Ratio, CI = Confidence Interval\r\n          \r\n      \r\n    \r\n\r\nHazard ratio’s\r\nDe interesse bij een Cox-regressiemodel gaat uit naar een hazard ratio (HR). De HR staat voor de verhouding van de gevaren tussen twee groepen op een bepaald moment;\r\nDe HR wordt geïnterpreteerd als het momentane tempo waarin de gebeurtenis zich voordoet bij degenen die nog steeds risico lopen voor de gebeurtenis. Het is geen risico, maar wordt wel als zodanig geïnterpreteerd;\r\nAls je een regressieparameter hebt van \\(\\beta\\) (van kolom estimate in ons coxph) dan is HR = \\(\\exp(\\beta)\\);\r\nEen HR < 1 duidt op een verminderd doodsgevaar, terwijl een HR > 1 duidt op een verhoogd doodsgevaar;\r\nDus onze HR = 0.59 betekent dat er ongeveer 0.6 keer zoveel vrouwen sterven als mannen, op een bepaald moment.\r\n\r\n\r\n\r\nDeel 2: Oriëntatiepuntanalyse en Tijd Afhankelijke Covariaten\r\nIn deel 1 hebben we met behulp van log-rank tests en Cox regressie de associaties tussen covariaten die van belang zijn en overlevingsresultaten onderzocht.\r\nMaar deze analyses zijn gebaseerd op het meten van het covariaat op baseline, dat wil zeggen, voordat de vervolgtijd voor het evenement begint.\r\nWat gebeurt er als u geïnteresseerd bent in een covariaat dat wordt gemeten nadat de vervolgtijd begint?\r\nVoorbeeld: Tumorreactie\r\nIn het algemeen wordt de totale overleving gemeten vanaf het begin van de behandeling en de interesse gaat daarbij uit naar de associatie tussen de volledige respons op de behandeling en de overleving.\r\nAnderson et al. (JCO, 1983) beschreven waarom tradionele methoden zoals log-rank tests of Cox regressie in dit scenario bevooroordeeld zijn ten gunste van de responders en stelden de oriëntatiepuntaanpak voor. De nul-hypothese in de landmarkbenadering is dat het overleven van een oriëntatiepunt niet afhankelijk is van de status van de respons bij dat punt.\r\n\r\nAnderson, J., Cain, K., & Gelber, R. (1983). Analysis of survival by tumor response. Journal of Clinical Oncology : Official Journal of the American Society of Clinical Oncology, 1(11), 710-9.\r\n\r\nAndere voorbeelden\r\nEnkele andere mogelijke covariaten die van belang zijn bij kankeronderzoek en die niet op de basislijn kunnen worden gemeten, zijn onder andere:\r\ntransplantatiefout\r\nent versus gastheer ziekte\r\ntweede resectie\r\nadjuvante therapie\r\nnaleving\r\nongunstige gebeurtenissen\r\nVoorbeeld data - de BMT dataset van het SemiCompRisks pakket\r\nGegevens over 137 beenmergpatiënten. Variabelen van belang zijn onder meer:\r\nT1 tijd(in dagen) tot dood or laatste follow-up\r\ndelta1 dood indicator; 1-Dood, 0-Levend\r\nTA tijd (in dagen) tot ent versus gastheer ziekte\r\ndeltaA acute ent versus gastheer ziekte indicator; 1-Ontwikkelt ent versus gastheer ziekte, 0-Nooit een ent versus gastheer ziekte ontwikkelt.\r\nLaten we de gegevens laden voor gebruik in de voorbeelden.\r\n\r\n\r\n\r\nOriëntatiepuntmethode\r\nSelecteer een vaste tijd na de basislijn als uw oriëntatiepunt. Opgelet: dit moet worden gedaan op basis van klinische informatie, voorafgaand aan de inspectie van de gegevens.\r\nSubset populatie voor degenen die volgen ten minste tot aan de mijlpaal tijd. Opgelet: rapporteer altijd het nummer dat is uitgesloten vanwege het evenement van belang of de censuur vóór het tijdstip van de mijlpaal.\r\nBereken de follow-up van de landmarktijd en voer de traditionele log-ranktests of Cox-regressie toe.\r\nIn de BMT data interesseert men zich voor de associatie tussen acute ent versus gastheer ziekte (op zijn Engels ‘aGVHD’) en overleving. Maar dit wordt beoordeeld na de transplantatie, wat onze basislijntijd is of bij het begin van de follow-up-tijd.\r\nStap 1 Selecteer een mijlpaal tijd\r\nMeestal treedt de ziekte op binnen de eerste 90 dagen na de transplantatie, dus gebruiken we een 90-dagen oriëntatiepunt.\r\nDe interesse gaat uit naar de associatie tussen acute enting versus gastheerziekte (‘aGVHD’) en overleving. Maar ‘aGVHD’ wordt beoordeeld na de transplantatie, wat onze basislijn is, of het begin van de follow-up, tijd.\r\nStep 2 Subset populatie voor degenen die gevolgd zijn ten minste tot mijlpaal tijd\r\n\r\n\r\n\r\nDit reduceert onze sampleomvang van 137 tot 122.\r\nAlle 15 data zijn exclusief patiënten die voor het 90-daags oriëntatiepunt zijn overleden.\r\nDe belangstelling gaat uit naar het verband tussen acute enting versus gastheerziekte (‘aGVHD’) en overleving. Maar ‘aGVHD’ wordt vastgesteld na de transplantatie, wat onze basislijn is of het begin van de follow-up-tijd.\r\nStep 3 Berekenen van follow-up tijd vanuit het oriëntatiepunt en toepassen van traditionele methodes.\r\n\r\n\r\n\r\n\r\n\r\n\r\nCox-regressie voor oriëntatievoorbeeld bij gebruik van BMT data\r\nIn de Cox-regressie kun je de subset option in coxph gebruiken om die patiënten uit te sluiten die niet zijn gevolgd gedurende de oriëntatietijd.\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#dmpqwnvorn .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#dmpqwnvorn .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#dmpqwnvorn .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#dmpqwnvorn .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#dmpqwnvorn .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#dmpqwnvorn .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#dmpqwnvorn .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#dmpqwnvorn .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#dmpqwnvorn .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#dmpqwnvorn .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#dmpqwnvorn .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#dmpqwnvorn .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#dmpqwnvorn .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#dmpqwnvorn .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#dmpqwnvorn .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#dmpqwnvorn .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#dmpqwnvorn .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#dmpqwnvorn .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#dmpqwnvorn .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#dmpqwnvorn .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#dmpqwnvorn .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#dmpqwnvorn .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#dmpqwnvorn .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#dmpqwnvorn .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#dmpqwnvorn .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#dmpqwnvorn .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#dmpqwnvorn .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nCharacteristic\r\n      HR1\r\n      95% CI1\r\n      p-value\r\n    deltaA\r\n      1.08\r\n      0.57, 2.07\r\n      0.8\r\n    \r\n        \r\n          1\r\n          \r\n           \r\n          HR = Hazard Ratio, CI = Confidence Interval\r\n          \r\n      \r\n    \r\n\r\nTijd-afhankelijke covariaat\r\nEen alternatief voor de oriëntatieanalyse is de incorporatie van een tijd-afhankelijke covariaat. Dit is beter geschikt wanneer\r\nde waarde van covariaat over tijd verandert;\r\nals er geen duidelijke oriëntatietijd is;\r\nwanneer de oriëntatie tot heel veel uitsluitingen leidt.\r\nData setup van een tijd-afhankelijke covariaat\r\nAnalyse van tijdafhankelijke covariaten in R veronderstelt de setup van een speciale dataset. Informatie hierover vind je in een gedetailleerd artikel door de auteur van het survival pakket Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model.\r\nEr was geen ID variable in het BMT data, dat nodig is om die special dataset te maken. Dus maak er een die heet my_id.\r\n\r\n\r\n\r\nGebruik de tmerge functie met de event en tdc functieopties om die speciale dataset te creëren.\r\ntmerge creëert een lange dataset met verschillende tijdintervallen voor de verschillende covariaatwaarden voor elke patiënt;\r\nevent creëert de nieuwe tijdindicator die samengaat met de gecreëerde tijdintervallen;\r\ntdc creëert de tijd-afhankelijke covariaatindicator die samengaat met de gecreëerde tijdintervallen.\r\n\r\n\r\n\r\nTime-afhankelijke covariaat - voorbeeld van enkele patiënten\r\nOm te zien wat dit doet, laten we eens kijken naar de eerste vijf individuele patiënten.\r\nDe variabelen waar onze interesse naar uitgaan zien er in de originele data als volgt uit:\r\n\r\n\r\n  my_id   T1 delta1   TA deltaA\r\n1     1 2081      0   67      1\r\n2     2 1602      0 1602      0\r\n3     3 1496      0 1496      0\r\n4     4 1462      0   70      1\r\n5     5 1433      0 1433      0\r\n\r\nDe nieuwe dataset voor dezelfde patiënten ziet er als volgt uit:\r\n\r\n\r\n  my_id   T1 delta1 id tstart tstop death agvhd\r\n1     1 2081      0  1      0    67     0     0\r\n2     1 2081      0  1     67  2081     0     1\r\n3     2 1602      0  2      0  1602     0     0\r\n4     3 1496      0  3      0  1496     0     0\r\n5     4 1462      0  4      0    70     0     0\r\n6     4 1462      0  4     70  1462     0     1\r\n7     5 1433      0  5      0  1433     0     0\r\n\r\nTijd-afhankelijke covariaat - Cox regressie\r\nNu kunnen we de tijd-afhankelijke covariaat analyseren zoals we gewend zijn met Coxregressie met coxph en een aanpassing in het gebruik van Surv door zowel de argumenten time en time2 op te nemen.\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#vcipwnwpuw .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#vcipwnwpuw .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#vcipwnwpuw .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#vcipwnwpuw .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#vcipwnwpuw .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#vcipwnwpuw .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#vcipwnwpuw .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#vcipwnwpuw .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#vcipwnwpuw .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#vcipwnwpuw .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#vcipwnwpuw .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#vcipwnwpuw .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#vcipwnwpuw .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#vcipwnwpuw .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#vcipwnwpuw .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#vcipwnwpuw .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#vcipwnwpuw .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#vcipwnwpuw .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#vcipwnwpuw .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#vcipwnwpuw .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#vcipwnwpuw .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#vcipwnwpuw .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#vcipwnwpuw .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#vcipwnwpuw .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#vcipwnwpuw .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#vcipwnwpuw .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#vcipwnwpuw .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nCharacteristic\r\n      HR1\r\n      95% CI1\r\n      p-value\r\n    agvhd\r\n      1.40\r\n      0.81, 2.43\r\n      0.2\r\n    \r\n        \r\n          1\r\n          \r\n           \r\n          HR = Hazard Ratio, CI = Confidence Interval\r\n          \r\n      \r\n    \r\n\r\nSamenvatting\r\nWe vinden dat acute ent versus gastheer ziekte niet significant geassocieerd is met de dood met behulp van hetzij oriëntatieanalyse of met inzet van een tijdafhankelijke covariate.\r\nVaak zal men oriëntatieanalyse willen gebruiken voor de visualisatie van een enkel covariaat en Cox regressie met een tijdsafhankelijk covariaat voor univariabele en multivariabele modellering.\r\nPart 3: Concurrerende Risico’s\r\nPakketten\r\nHet belangrijkste pakket om te gebruiken in concurrerende risico analyse is:\r\ncmprsk\r\n\r\n\r\n\r\nWat zijn concurrerende risico’s?\r\nHier is sprake van wanneer subjecten meerdere mogelijke gebeurtenissen kennen in een tijd tot gebeurtenissetting.\r\nVoorbeelden:\r\nherhaling\r\nsterfte door ziekte\r\ndood door andere oorzaken\r\nbehandelingsreactie\r\nAl deze of sommige van deze (onder andere) gebeurtenissen kunnen in een bepaalde studie mogelijk zijn.\r\nDus wat is het probleem?\r\nOnopgemerkte afhankelijkheid tussen de momenten van het evenement is het fundamentele probleem dat leidt tot de noodzaak om speciale aandacht hieraan te besteden.\r\nMen kan zich bijvoorbeeld voorstellen dat patiënten die terugkomen meer kans hebben om te sterven en daarom zouden tijden om terug te keren en tijden om te sterven geen onafhankelijke gebeurtenissen zijn.\r\nAchtergronden van concurrerende risico’s\r\nTwee benaderingen van de analyse in de aanwezigheid van meerdere potentiële uitkomsten:\r\nOorzaak-specifiek gevaar van een bepaalde gebeurtenis: dit is het percentage per tijdseenheid van de gebeurtenis onder degenen die niet hebben gefaald op andere gebeurtenissen.\r\nCumulatieve incidentie van een bepaald evenement: dit is het deel per tijdseenheid van het evenement en de invloed van concurrerende evenementen.\r\nElk van deze benaderingen kan slechts één belangrijk aspect van de gegevens belichten, terwijl andere mogelijk worden verdoezeld, en de gekozen aanpak moet afhangen van de kwestie van het belang.\r\nEen aantal aanvullende aantekeningen en referenties\r\nWanneer de gebeurtenissen onafhankelijk zijn (bijna nooit waar), zijn de oorzaak-specifieke gevaren onbevooroordeeld.\r\nWanneer de gebeurtenissen afhankelijk zijn, kunnen afhankelijk van de instelling verschillende resultaten worden verkregen\r\nCumulatieve incidentie met Kaplan-Meier is altijd >= cumulatieve incidentie met behulp van concurrerende risicomethoden, dus kan alleen leiden tot een overschatting van de cumulatieve incidentie, de hoogte van de overschatting hangt af van het aantal gebeurtenissen en de afhankelijkheid van de gebeurtenissen.\r\nOm vast te stellen dat een covariaat inderdaad reageert op de gebeurtenis dat van belang is, kan de voorkeur worden gegeven aan oorzaken-specifieke gevaren voor de behandeling of voor het testen van het pronostiek-markeereffect.\r\nOm het algemene voordeel vast te stellen, kan de voorkeur worden gegeven aan subdistributierisico’s voor het bouwen van prognostische nomogrammen of het overwegen van gezondheidseconomische effecten om een beter gevoel te krijgen van de invloed van de behandeling en andere covariaten op een absolute schaal.\r\n\r\nDignam JJ, Zhang Q, Kocherginsky M. The use and interpretation of competing risks regression models. Clin Cancer Res. 2012;18(8):2301-8.\r\n\r\n\r\nKim HT. Cumulative incidence in competing risks data and competing risks regression analysis. Clin Cancer Res. 2007 Jan 15;13(2 Pt 1):559-65.\r\n\r\n\r\nSatagopan JM, Ben-Porat L, Berwick M, Robson M, Kutler D, Auerbach AD. A note on competing risks in survival data analysis. Br J Cancer. 2004;91(7):1229-35.\r\n\r\n\r\nAustin, P., & Fine, J. (2017). Practical recommendations for reporting Fine‐Gray model analyses for competing risk data. Statistics in Medicine, 36(27), 4391-4400.\r\n\r\nCumulatieve incidentie voor concurrerende risico’s\r\n– Niet-parametrische schatting van de cumulatieve incidentie;\r\n- Schat het cumulatieve effect van het evenement van de rente in;\r\n- Op elk moment is de som van de cumulatieve incidentie van elke gebeurtenis gelijk aan de totale cumulatieve incidentie van elke gebeurtenis (niet waar in de oorzaak-specifieke setting);\r\n- Gray’s test is een aangepaste Chi-kwadraat test die wordt gebruikt om 2 of meer groepen te vergelijken.\r\nVoorbeeld Melanoma data van het MASS pakket\r\nWe gebruiken de Melanoma data van het MASS pakket om deze concepten duidelijk te maken. Deze omvatten de volgende variabelen:\r\ntime overlevingstijd in dagen, mogelijk gecensord.\r\nstatus 1 dood vanwege melanoma, 2 levend, 3 dood vanwege andere oorzaken.\r\nsex 1 = man, 0 = vrouw.\r\nage leeftijd in jaren.\r\nyear van de operatie.\r\nthickness tumor dikte in mm.\r\nulcer 1 = aanwezig, 0 = afwezig.\r\n\r\n\r\n\r\nCumulatieve incidentie in de Melanoma data\r\nSchat de cumulatieve incidentie in de context van concurrerende risico’s met gebruik van de cuminc functie.\r\nOpgelet: in de Melanoma data, gecensorde patiënten zijn gecodeerd als \\(2\\) voor status, dus we kunnen niet de cencode standaardoptie gebruiken van \\(0\\)\r\n\r\n\r\nEstimates and Variances:\r\n$est\r\n          1000       2000       3000      4000      5000\r\n1 1 0.12745714 0.23013963 0.30962017 0.3387175 0.3387175\r\n1 3 0.03426709 0.05045644 0.05811143 0.1059471 0.1059471\r\n\r\n$var\r\n            1000         2000         3000        4000        5000\r\n1 1 0.0005481186 0.0009001172 0.0013789328 0.001690760 0.001690760\r\n1 3 0.0001628354 0.0002451319 0.0002998642 0.001040155 0.001040155\r\n\r\nPlotten van de cumulatieve incidentie - basis R\r\nGenereer een basis R plot met al de standaards.\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn de legende:\r\nHet eerste getal geeft de groep aan, in dit geval is er slechts een globale schatting, dus het is \\(1\\) voor beide…\r\nHet tweede getal geeft het type gebeurtenis aan, in dit geval is het de ononderbroken lijn \\(1\\) voor de dood door melanoom en de stippellijn \\(3\\) voor de dood door andere oorzaken.\r\nZet de cumulatieve incidentie uit - ggcompetingrisks\r\nWe kunnen ook de cumulatieve incidentie berekenen met behulp van de ggs-competingrisks functie van het survminer pakket.\r\nIn dit geval krijgen we een panel gelabeld volgens de groep en een legenda gelabeld evenement, met vermelding van het type evenement voor elke regel.\r\nIn the legenda:\r\nHet eerste getal geeft de groep aan, in dit geval is er alleen een overall schatte en is \\(1\\) voor beiden;\r\nHet tweede getal geeft de gebeurtenis type aan, in dit geval is de doorlopende lijn \\(1\\) voor overleden aan melanoma en de stippellijn is \\(3\\) voor overleden vanwege overleden vanwege andere oorzaken.\r\nPlot de cumulatieve incidentie - ‘ggcompetingrisks’\r\nWe kunnen de cumulatieve incidentie ook plotten met de ggscompetingrisks functie van het survminer pakket.\r\nIn dit geval krijgen we een panel gelabeld volgens de groep en een legenda gelabeld evenement, met vermelding van het type evenement voor elke regel.\r\nOpgelet\r\nJe kunt de optie multiple_panels = FALSE gebruiken om alle groepen op een enkel panel afgedrukt te krijgen;\r\nanders dan bij R basis gaat de y-as niet standaard naar 1, dus die moet je handmatig aanpassen\r\n\r\n\r\n\r\nVergelijken van cumulatieve incidentie tussen groepen\r\nIn cuminc wordt de Gray’s test gebruikt voor tussen-groepen tests.\r\nAls voorbeeld vergelijken we de Melanoma uitkomsten volgens ulcer, de aan- of afwezigheid van ulceratie (zweervorming). De resultaten hiervan vind je in Tests.\r\n\r\n\r\n       stat           pv df\r\n1 26.120719 3.207240e-07  1\r\n3  0.158662 6.903913e-01  1\r\n\r\nPlotten van cumulatieve incidentie volgens groep - ‘ggcompetingrisks’\r\n\r\n\r\n\r\nPlotten van cumulatieve incidentie per groep - handmatig\r\nOpgelet Persoonlijk vind ik ggcompetingrisks functie lastig, vooral vergeleken met ggsurvplot. Het plotten doe ik veelal door eerst te zorgen voor een schone dataset van de cuminc fit resultaten en pas daarna de resultaten te plotten. Kijk maar eens naar de broncode hieronder.\r\n\r\n\r\n\r\nPlotten van een enkele gebeurtenis type - handmatig\r\nVaak is slechts een van de soorten evenementen interessant, hoewel we nog steeds rekenschap willen afleggen van de concurrerende gebeurtenis. In dat geval kan de gebeurtenis van belang alleen worden uitgezet. Nogmaals, ik doe dit handmatig door eerst een nette dataset te maken van de cuminc fit resultaten en dan de resultaten te plotten. Zie hieronder:\r\n\r\n\r\n\r\nGetallen toevoegen aan de risicotabel\r\nMisschien wil je de getallen van de risicotabel toevoegen aan een cumulatieve incidentiegrafiek. Hier weet ik geen eenvoudige manier voor.\r\nMaak een grafiek met basis R, ggcompetingrisks of ggplot\r\nHaal het getal van de risico tabel vanggsurvplot door survfit te gebruiken waar alle gebeurtenissen tellen als een enkel eindpunt\r\nDwing de assen zodat ze dezelfde limieten, breekpunten en titels hebben;\r\nWees er zeker van dat de kleuren/lijntypen matchen met de labels;\r\nZorg ervoor dat de lettergrootte steeds hetzelfde is;\r\n\r\nCombineer dan de grafiek en de risicotabel. Ik gebruik hiervoor de plot_grid functie van het cowplot pakket;\r\nIk weet niet hoe ik de tekstgrootte van “Number at risk” moet veranderen…\r\n\r\n\r\n\r\n\r\nConcurrerende risico regressie\r\nTwee benaderingen:\r\nOorzaak-specifieke gevaren (’hazards)\r\nsnelheid van het optreden van het gegeven type van gebeurtenis die op dit moment gebeurtenis-vrij zijn\r\ngeschat met behulp van Cox regressie (coxph functie)\r\n\r\nSubdistributierisico’s\r\nsnelheid van het optreden van het gegeven type gebeurtenis bij proefpersonen die nog geen gebeurtenis van dat type hebben meegemaakt\r\ngeschat met behulp van Fine-Gray regressie (crr functie)\r\n\r\nConcurrerende risico regressie in Melanoma data - subdistributie hazard benadering\r\nLaten we zeggen dat we geïnteresseerd zijn in het effect van leeftijd en geslacht op de dood door melanoom, met de dood door andere oorzaken als een concurrerende gebeurtenis.\r\nOpmerkingen:\r\ncrr vereist specificatie van covariaten als een matrix\r\nAls er meer dan één gebeurtenis van belang is, kunt u de resultaten voor een andere gebeurtenis opvragen met behulp van de failcode optie, standaard worden de resultaten geretourneerd voor failcode = 1.\r\nLet’s say we’re interested in looking at the effect of age and sex on death from melanoma, with death from other causes as a competing event.\r\n\r\n\r\nconvergence:  TRUE \r\ncoefficients:\r\n    sex     age \r\n0.58840 0.01259 \r\nstandard errors:\r\n[1] 0.271800 0.009301\r\ntwo-sided p-values:\r\n sex  age \r\n0.03 0.18 \r\n\r\nIn het vorige voorbeeld werden zowel sex en agegecodeerd als numerieke variabelen. De crr functie kan niet op een natuurlijke manier omgaan met karaktervariabelen en je krijgt een fout. Dus als er karaktervariabelen aanwezig zijn moeten we dummy-variabelen maken met behulp van model.matrix.\r\n\r\n\r\n\r\nFormatteren van de resultaten van van crr\r\nOutput van crr wordt niet ondersteund door broom::tidy() of noch door gtsummary::tbl_regression() op dit moment. Als alternatief, gebruik de (niet flexibele, maar beter dan niks?) mvcrrres van mijn ezfun pakket\r\n\r\n\r\nHR (95% CI)\r\np-value\r\nsex\r\n1.8 (1.06, 3.07)\r\n0.03\r\nage\r\n1.01 (0.99, 1.03)\r\n0.18\r\n\r\nConcurrerende risico’s regressie in de gegevens van Melanoom - oorzaak-specifieke gevarenbenadering\r\nCensor alle onderwerpen die niet voor de gebeurtenis van belang zijn, in dit geval de dood door melanoom, en gebruik coxph zoals voorheen. Dus patiënten die zijn overleden aan andere oorzaken worden nu gecensord voor de oorzaak-specifieke gevarenbenadering van concurrerende risico’s.\r\nDe resultaten kunnen worden geformatteerd met broom::tidy() of gtsummary::tbl_regression().\r\n\r\nterm\r\nestimate\r\nstd.error\r\nstatistic\r\np.value\r\nsex\r\n1.818949\r\n0.2676386\r\n2.235323\r\n0.0253961\r\nage\r\n1.016679\r\n0.0086628\r\n1.909514\r\n0.0561958\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#tjxokapgnw .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#tjxokapgnw .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 4px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#tjxokapgnw .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#tjxokapgnw .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#tjxokapgnw .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#tjxokapgnw .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#tjxokapgnw .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#tjxokapgnw .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#tjxokapgnw .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#tjxokapgnw .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#tjxokapgnw .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#tjxokapgnw .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#tjxokapgnw .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#tjxokapgnw .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#tjxokapgnw .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#tjxokapgnw .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#tjxokapgnw .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#tjxokapgnw .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#tjxokapgnw .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#tjxokapgnw .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#tjxokapgnw .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#tjxokapgnw .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#tjxokapgnw .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#tjxokapgnw .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#tjxokapgnw .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#tjxokapgnw .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#tjxokapgnw .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-size: 65%;\r\n}\r\nCharacteristic\r\n      HR1\r\n      95% CI1\r\n      p-value\r\n    sex\r\n      1.82\r\n      1.08, 3.07\r\n      0.025\r\n    age\r\n      1.02\r\n      1.00, 1.03\r\n      0.056\r\n    \r\n        \r\n          1\r\n          \r\n           \r\n          HR = Hazard Ratio, CI = Confidence Interval\r\n          \r\n      \r\n    \r\n\r\nWat hebben we gedaan?\r\nDe basis van de overlevingsanalyse met inbegrip van de Kaplan-Meier overlevingsfunctie en Cox-regressie\r\nOriëntatieanalyse en tijdsafhankelijke covariaten\r\nCumulatieve incidentie en regressie voor concurrerende risicoanalyses\r\n",
    "preview": "posts/2020-08-21-survival-analyse-met-r/img/trial_anatomy.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-31-grafieken/",
    "title": "Grafieken",
    "description": "Hier een serie grafieken die je kunt maken. Dit is gebaseerd op blog van het Urban Institute in de Verenigde Staten. Zij maken hun grafieken altijd op eenzelfde manier. Hoe ze dat doen kan je hierlezen",
    "author": [
      {
        "name": "Urban Institute, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-05-31",
    "categories": [],
    "contents": "\r\nR is een krachtige, open-source programmeertaal en -omgeving. R blinkt uit in databeheer en bewerking, traditionele statistische analyse, machine learning en reproduceerbaar onderzoek. Maar is waarschijnlijk nog het meest bekend om zijn grafieken. In deze blog staan voorbeelden en instructies voor populaire en minder bekende plottechnieken in R. Het bevat ook instructies voor het gebruik van urbnthemes, het R-pakket van het Urban Institute voor het maken van bijna-publicatie-klare plots met ggplot2. Als u vragen heeft, zo staat op hun site, aarzel dan niet om contact op te nemen met Aaron Williams of Kyle Ueyama.\r\nAchtergrond\r\nHet R-pakket (library(urbnthemes)) maakt ggplot2 output dat verbonden is met Urban Institute’s Data Visualisatie stijl gids. Tzijn pakket produceert ** geen publicatieklare grafieken**. Visuele stijlen moeten nog steeds worden bewerkt met behulp van de normale bewerkingsworkflow van het project. Door grafieken te exporteren als pdf kunnen ze gemakkelijker worden bewerkt. Zie de sectie Plots opslaan voor meer informatie.\r\nHet vaste thema dat hier gebruikt wordt is getest met ggplot2 versie 3.0.0. Het zal niet goed functioneren met oudere versies van ggplot2.\r\nGebruik library(urbnthemes)\r\nJe moet in ieder geval de volgende code gebruiken om urbnthemes te installeren of te updaten:\r\n# install.packages(\"devtools\")\r\n# devtools::install_github(\"UrbanInstitute/urbnthemes\")\r\nVoer de volgende code bovenaan elk script uit. Als je dit hebt gedaan, kun je aan de slag:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(urbnthemes)\r\nlibrary(ggrepel)\r\nlibrary(extrafont)\r\n\r\nset_urbn_defaults(style = \"print\")\r\n\r\n\r\n\r\nIAls het nog niet is geïnstalleerd, installeer dan het gratis Lato-lettertype van Google-lettertypen. Als je op een Mac werkt sla je Lato op in je font-book. Als je op Windows werkt, moet je eerst Ghostscript installeren. Vertel dan in R waar uw ghostscript-bestand zich bevindt. Bewerk het bestandspad als het uwe zich op een andere plaats bevindt.\r\nSys.setenv(R_GSCMD=\"C:/Program Files/gs/gs9.05/bin/gswin32c.exe\")\r\nVoer dit script één keer uit om Lato te importeren en te registreren:\r\n# install.packages(c(\"ggplot2\", \"ggrepel\", \"extrafont\"))\r\n# urbnthemes::lato_install()\r\nHet laden en importeren van dit lettertype kan enkele minuten duren.\r\nGrammar of Graphics en de conventies\r\nHadley Wickham’s ggplot2 is gebaseerd op Leland Wilkinsons The Grammar of Graphics en Wickhams A Layered Grammar of Graphics. De gelaagde Grammer of Graphics is een gestructureerde manier van denken over de componenten van een plot, die zich vervolgens lenen voor de eenvoudige structuur van ggplot2.\r\nData zijn wat in een plot wordt gevisualiseerd en mappings zijn aanwijzingen voor hoe gegevens in een plot in kaart worden gebracht op een manier die door de mens kan worden waargenomen.\r\nGegevens zijn weergaven van de werkelijke gegevens zoals punten, lijnen en balken.\r\nStatistieken zijn statistische transformaties die samenvattingen van de gegevens weergeven, zoals histogrammen.\r\nScales kaartwaarden in de dataruimte naar waarden in de esthetische ruimte. Schalen tekenen legendes en assen.\r\nCoördinatensystemen beschrijven hoe geomen in het vlak van de grafiek in kaart worden gebracht.\r\nFacetten splitsen de gegevens op in betekenisvolle deelverzamelingen zoals kleine veelvouden. *Thema’s** controleren de fijnere punten van een plot zoals lettertypes, lettergroottes en achtergrondkleuren.\r\nMeer informatie vind je hier: ggplot2: Elegant Graphics for Data Analysis\r\nTips en trucs\r\nggplot2 verwacht dat de gegevens in dataframes of tibbles zitten. Het heeft de voorkeur dat de dataframes “netjes” zijn met elke variabele als een kolom, elke obseravtion als een rij, en elke observatie-eenheid als een aparte tabel. De dplyr en tidyr bevatten beknopte en effectieve hulpmiddelen voor het “opruimen” van gegevens.\r\nR staat toe dat functie-argumenten expliciet bij naam en impliciet bij positie worden aangeroepen. De codeervoorbeelden in deze handleiding bevatten alleen benoemde argumenten voor de duidelijkheid.\r\nGrafieken zullen soms verschillend worden weergeven op verschillende besturingssystemen. Dit zal geen probleem zijn als de afbeeldingen eenmaal zijn opgeslagen.\r\nDoorlopende x-assen hebben tikken. Discrete x-assen hebben geen teken. Gebruik remove_ticks() om teken te verwijderen.\r\nStaaf grafieken\r\nEen kleur\r\n\r\n\r\nmtcars %>%\r\n  count(cyl) %>%\r\n  ggplot(mapping = aes(x = factor(cyl), y = n)) +\r\n  geom_col() +\r\n  geom_text(mapping = aes(label = n), vjust = -1) +    \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\r\n  labs(x = \"Cilinders\",\r\n       y = NULL) +\r\n  remove_ticks() +\r\n  remove_axis() \r\n\r\n\r\n\r\n\r\nEen kleur (Geroteerd)\r\nDit introduceert coord_flip() en remove_axis(axis = \"x\", flip = TRUE). remove_axis() komt van library(urbnthemes) en creëert een aangepast thema voor geroteerde staafgrafieken.\r\n\r\n\r\nmtcars %>%\r\n  count(cyl) %>%\r\n  ggplot(mapping = aes(x = factor(cyl), y = n)) +\r\n  geom_col() +\r\n  geom_text(mapping = aes(label = n), hjust = -1) +  \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\r\n  labs(x = \"Cilinders\",\r\n       y = NULL) +  \r\n  coord_flip() +\r\n  remove_axis(axis = \"x\", flip = TRUE)\r\n\r\n\r\n\r\n\r\nDrie kleuren\r\nDit is identiek aan de vorige grafiek, behalve dat kleuren en een legenda zijn toegevoegd met fill = cyl. Door x om te zetten in een factor met factor(cyl) worden 5 en 7 op de x-as overgeslagen. Het toevoegen van fill = cyl zonder factor() zou een doorlopend kleurenschema en een legenda hebben gecreëerd.\r\n\r\n\r\nmtcars %>%\r\n  mutate(cyl = factor(cyl)) %>%\r\n  count(cyl) %>%\r\n  ggplot(mapping = aes(x = cyl, y = n, fill = cyl)) +\r\n  geom_col() +\r\n  geom_text(mapping = aes(label = n), vjust = -1) +    \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\r\n  labs(x = \"Cylinders\",\r\n       y = NULL) +\r\n  remove_ticks() +\r\n  remove_axis()\r\n\r\n\r\n\r\n\r\nGestapelde staafgrafiek\r\nEen extra esthetiek kan eenvoudig worden toegevoegd aan de staafgrafiek door fill = categorical variable toe te voegen aan de mapping. Hier toont elk onderdeel een subset van een aantal auto’s met verschillende aantallen cilinders.\r\n\r\n\r\nmtcars %>%\r\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\r\n         cyl = factor(cyl)) %>%  \r\n  group_by(am) %>%\r\n  count(cyl) %>%\r\n  group_by(cyl) %>%\r\n  arrange(desc(am)) %>%\r\n  mutate(label_height = cumsum(n)) %>%\r\n  ggplot() +\r\n  geom_col(mapping = aes(x = cyl, y = n, fill = am)) +\r\n  geom_text(aes(x = cyl, y = label_height - 0.5, label = n, color = am)) +\r\n  scale_color_manual(values = c(\"white\", \"black\")) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\r\n  labs(x = \"Cylinders\",\r\n       y = NULL) +  \r\n  remove_ticks() +\r\n  remove_axis() +\r\n  guides(color = FALSE)\r\n\r\n\r\n\r\n\r\nGestapelde staafgrafiek met Position = Fill\r\nDe vorige voorbeelden gebruiken geom_col(), die een y-waarde voor de staafhoogte neemt. Dit voorbeeld gebruikt geom_bar() die de waarden opsomt en een waarde voor de staafhoogte genereert. In dit voorbeeld verandert position = \"fill\" in geom_bar() de y-as van de telling naar de verhouding van elke staaf.\r\n\r\n\r\nmtcars %>%\r\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\r\n         cyl = factor(cyl)) %>%  \r\n  ggplot() +\r\n  geom_bar(mapping = aes(x = cyl, fill = am), position = \"fill\") +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1)), labels = scales::percent) +\r\n  labs(x = \"Cylinders\",\r\n       y = NULL) +  \r\n  remove_ticks() +\r\n  guides(color = FALSE)\r\n\r\n\r\n\r\n\r\nOpgedeelde staafgrafiek\r\nDelen van de staafgrafiek in ggplot2 worden standaard opgestapeld. position = \"dodge\" in geom_col() breidt het staafdiagram uit zodat de subsets naast elkaar verschijnen.\r\n\r\n\r\nmtcars %>%\r\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\r\n         cyl = factor(cyl)) %>%\r\n  group_by(am) %>%\r\n  count(cyl) %>%\r\n  ggplot(mapping = aes(cyl, y = n, fill = factor(am))) +\r\n  geom_col(position = \"dodge\") +\r\n  geom_text(aes(label = n), position = position_dodge(width = 0.7), vjust = -1) +    \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\r\n  labs(x = \"Cylinders\",\r\n       y = NULL) +  \r\n  remove_ticks() +\r\n  remove_axis()\r\n\r\n\r\n\r\n\r\nLolly grafiek/Cleveland puntgrafiek\r\nLolly en Cleveland puntgrafiek zijn minimalistische alternatieven voor staafgrafieken. De sleutel tot beide grafieken is om de gegevens te ordenen op basis van de continue variabele met behulp van arrange() en dan de discrete variabele om te zetten in een factor met de geordende niveaus van de continue variabele met behulp van mutate(). Deze stap “slaat” de volgorde van de gegevens op.\r\nLollygrafiek\r\n\r\n\r\nmtcars %>%\r\n  rownames_to_column(\"model\") %>%\r\n  arrange(mpg) %>%\r\n  mutate(model = factor(model, levels = .$model)) %>%\r\n  ggplot(aes(mpg, model)) +\r\n    geom_segment(aes(x = 0, xend = mpg, y = model, yend = model)) +  \r\n    geom_point() +\r\n    scale_x_continuous(expand = expand_scale(mult = c(0, 0)), limits = c(0, 40)) +\r\n    labs(x = NULL, \r\n         y = \"Miles Per Gallon\")\r\n\r\n\r\n\r\n\r\nCleveland puntgrafiek\r\n\r\n\r\nmtcars %>%\r\n  rownames_to_column(\"model\") %>%\r\n  arrange(mpg) %>%\r\n  mutate(model = factor(model, levels = .$model)) %>%\r\n  ggplot(aes(mpg, model)) +\r\n    geom_point() +\r\n    scale_x_continuous(expand = expand_scale(mult = c(0, 0)), limits = c(0, 40)) +\r\n    labs(x = NULL, \r\n         y = \"Miles Per Gallon\")\r\n\r\n\r\n\r\n\r\nDumbellgrafieken\r\nPuntengrafieken\r\nEen kleur puntengrafiek\r\nPuntengrafieken zijn nuttig voor het tonen van relaties tussen twee of meer variabelen. Gebruik scatter_grid() van library(urbnthemes) om eenvoudig verticale rasterlijnen toe te voegen aan de puntengrafieken.\r\n\r\n\r\nmtcars %>%\r\n  ggplot(mapping = aes(x = wt, y = mpg)) +\r\n  geom_point() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 6),\r\n                     breaks = 0:6) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 40),\r\n                     breaks = 0:8 * 5) +\r\n  labs(x = \"Gewicht (duizenden ponden)\",\r\n       y = \"MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nPuntengrafiek met hoge dichtheid met transparantie\r\nGrote aantallen waarnemingen maken soms strooiplekken soms moeilijk om te interpreteren omdat punten elkaar overlappen. Het toevoegen van alpha = met een getal tussen 0 en 1 voegt transparantie toe aan punten en helderheid aan grafieken.\r\n\r\n\r\ndiamonds %>%\r\n  ggplot(mapping = aes(x = carat, y = price)) +\r\n  geom_point(alpha = 0.05) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 6),\r\n                     breaks = 0:6) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 20000),\r\n                     breaks = 0:4 * 5000,\r\n                     labels = scales::dollar) +\r\n  labs(x = \"Carat\",\r\n       y = \"Price\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nHexus puntengrafiek\r\nSoms is transparantie niet genoeg om duidelijkheid te brengen in een verstrooide grafiek met veel waarnemingen. Als n toeneemt in de honderdduizenden en zelfs miljoenen, kan geom_hex een van de beste manieren zijn om relaties tussen twee variabelen weer te geven.\r\n\r\n\r\ndiamonds %>%\r\n  ggplot(mapping = aes(x = carat, y = price)) +\r\n  geom_hex(mapping = aes(fill = ..count..)) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 6),\r\n                     breaks = 0:6) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 20000),\r\n                     breaks = 0:4 * 5000,\r\n                     labels = scales::dollar) +\r\n  scale_fill_gradientn(labels = scales::comma) +  \r\n  labs(x = \"Carat\",\r\n       y = \"Price\") +\r\n  scatter_grid() +\r\n  theme(legend.position = \"right\",\r\n        legend.direction = \"vertical\")\r\n\r\n\r\n\r\n\r\nPuntengrafiek met random vervuiling\r\nSoms hebben puntengrafieken veel overlappende punten, maar een redelijk aantal waarnemingen. Geom_jitter voegt een kleine hoeveelheid willekeurige ruis toe zodat punten minder snel overlappen. De breedte en hoogte bepalen de hoeveelheid ruis die wordt toegevoegd. Merk in het volgende voor- en naschrijven op hoeveel punten er nog zichtbaar zijn na het toevoegen van jitter.\r\nVoor\r\n\r\n\r\nmpg %>%\r\n  ggplot(mapping = aes(x = displ, y = cty)) +\r\n  geom_point() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 8),\r\n                     breaks = 0:8) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 40),\r\n                     breaks = 0:4 * 10) +\r\n  labs(x = \"Uitval\",\r\n       y = \"MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nNa\r\n\r\n\r\nset.seed(2017)\r\nmpg %>%\r\n  ggplot(mapping = aes(x = displ, y = cty)) +\r\n  geom_jitter() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 8),\r\n                     breaks = 0:8) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 40),\r\n                     breaks = 0:4 * 10) +\r\n  labs(x = \"Uitval\",\r\n       y = \"MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nPuntengrafiek met gevarieerde puntenomvang\r\nGewichten en populaties kunnen in puntengrafieken met de grootte van de punten in kaart worden gebracht. Hier wordt het aantal huishoudens in elke staat in kaart gebracht op de grootte van elk punt met behulp van aes(size = hhpop). Opmerking: ggplot2::geom_point() wordt gebruikt in plaats van geom_point().\r\nWel eerst dit pakket laden (wel installeren als je dat nog niet hebt gedaan).\r\n\r\n\r\nlibrary(urbnmapr)\r\n\r\n\r\n\r\n\r\n\r\nurbnmapr::statedata %>%\r\n  ggplot(mapping = aes(x = medhhincome, y = horate)) +\r\n  ggplot2::geom_point(mapping = aes(size = hhpop), alpha = 0.3) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(30000, 80000),\r\n                     breaks = 3:8 * 10000,\r\n                     labels = scales::dollar) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 0.8),\r\n                     breaks = 0:4 * 0.2) +\r\n  scale_radius(range = c(3, 15),\r\n               breaks = c(2500000, 7500000, 12500000), \r\n               labels = scales::comma) +\r\n  labs(x = \"Huishoud inkomen\",\r\n       y = \"Ratio huizenbezit\") +\r\n  scatter_grid() +\r\n  theme(plot.margin = margin(r = 20))\r\n\r\n\r\n\r\n\r\nPuntengrafieken met vulling\r\nEen derde esthetiek kan worden toegevoegd aan puntengrafieken. Hier betekent kleur het aantal cilinders in elke auto. Voordat ggplot() wordt aangeroepen, worden de cilinders aangemaakt met behulp van library(dplyr) en de functie %>%.\r\n\r\n\r\nmtcars %>%\r\n  mutate(cyl = paste(cyl, \"cylinders\")) %>%\r\n  ggplot(aes(x = wt, y = mpg, color = cyl)) +\r\n  geom_point() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 6),\r\n                     breaks = 0:6) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 40),\r\n                     breaks = 0:8 * 5) +\r\n  labs(x = \"Gewicht (duizenden ponden)\",\r\n       y = \"MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nLijngrafieken\r\n\r\n\r\neconomics %>%\r\n  ggplot(mapping = aes(x = date, y = unemploy)) +\r\n  geom_line() +\r\n  scale_x_date(expand = expand_scale(mult = c(0.002, 0)), \r\n               breaks = \"10 years\",\r\n               limits = c(as.Date(\"1961-01-01\"), as.Date(\"2020-01-01\")),\r\n               date_labels = \"%Y\") +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     breaks = 0:4 * 4000,\r\n                     limits = c(0, 16000),\r\n                     labels = scales::comma) +\r\n  labs(x = \"Jaar\", \r\n       y = \"Aantal werklozen (1,000den)\")\r\n\r\n\r\n\r\n\r\nLijngrafieken met meerdere lijnen\r\n\r\n\r\nlibrary(gapminder)\r\n\r\ngapminder %>%\r\n  filter(country %in% c(\"Australia\", \"Netherlands\", \"New Zealand\")) %>%\r\n  mutate(country = factor(country, levels = c(\"Netherlands\", \"Australia\", \"New Zealand\"))) %>%\r\n  ggplot(aes(year, gdpPercap, color = country)) +\r\n  geom_line() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     breaks = c(1952 + 0:12 * 5), \r\n                     limits = c(1952, 2007)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     breaks = 0:8 * 5000,\r\n                     labels = scales::dollar, \r\n                     limits = c(0, 40000)) +\r\n  labs(x = \"Jaap\",\r\n       y = \"BNP per hoofd van de bevolking (US dollars)\")\r\n\r\n\r\n\r\n\r\nHet plotten van meer dan één variabele kan nuttig zijn om de relatie van variabelen in de tijd te zien, maar het vergt een kleine hoeveelheid databewerking.\r\nDit komt omdat ggplot2 gegevens in een “lang” formaat wil hebben in plaats van een “breed” formaat voor lijnplots met meerdere lijnen. gather() en spread() uit het tidyr pakket maakt het wisselen tussen “lang” en “breed” pijnloos. In wezen gaan variabele titels naar “key” en variabele waarden naar “value”. Dan verandert ggplot2, de verschillende niveaus van de sleutelvariabele (bevolking, werkloosheid) in kleuren.\r\n\r\n\r\nas_tibble(EuStockMarkets) %>%\r\n  mutate(date = time(EuStockMarkets)) %>%\r\n  gather(key = \"key\", value = \"value\", -date) %>%\r\n  ggplot(mapping = aes(x = date, y = value, color = key)) +\r\n  geom_line() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(1991, 1999), \r\n                     breaks = c(1991, 1993, 1995, 1997, 1999)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     breaks = 0:4 * 2500,\r\n                     labels = scales::dollar, \r\n                     limits = c(0, 10000)) +  \r\n  labs(x = \"Tijd\",\r\n       y = \"Waarde\")\r\n\r\n\r\n\r\n\r\nTrapgrafiek\r\ngeom_line() verbindt coördinaten met de kortst mogelijke rechte lijn. Soms zijn trapgrafieken nodig omdat de y-waarden niet veranderen tussen de coördinaten. Zo wordt bijvoorbeeld de bovengrens van de Federal Funds Rate met regelmatige tussenpozen ingesteld en blijft deze constant totdat deze wordt gewijzigd.\r\n\r\n\r\n# downloaded from FRED on 2018-12-06\r\n\r\n# https://fred.stlouisfed.org/series/DFEDTARU\r\n\r\nfed_fund_rate <- read_csv(\r\n  \"date, fed_funds_rate\r\n   2014-01-01,0.0025\r\n   2015-12-16,0.0050\r\n   2016-12-14,0.0075\r\n   2017-03-16,0.0100\r\n   2017-06-15,0.0125\r\n   2017-12-14,0.0150\r\n   2018-03-22,0.0175\r\n   2018-06-14,0.0200\r\n   2018-09-27,0.0225\r\n   2018-12-06,0.0225\")\r\n\r\nfed_fund_rate %>%\r\n  ggplot(mapping = aes(x = date, y = fed_funds_rate)) + \r\n  geom_step() +\r\n  scale_x_date(expand = expand_scale(mult = c(0.002, 0)), \r\n               breaks = \"1 year\",\r\n               limits = c(as.Date(\"2014-01-01\"), as.Date(\"2019-01-01\")),\r\n               date_labels = \"%Y\") +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     breaks = c(0, 0.01, 0.02, 0.03),\r\n                     limits = c(0, 0.03),\r\n                     labels = scales::percent) +  \r\n  labs(x = \"Tijd\",\r\n       y = \"Bovengrens van de Federal Funds Rate\")\r\n\r\n\r\n\r\n\r\nPadgrafiek\r\nDe Beveridge-curve is een macro-economisch plot dat een verband toont tussen de werkloosheidsgraad en de vacaturegraad. Bewegingen op de curve duiden op veranderingen in de bedrijfscultuur en horizontale verschuivingen van de curve duiden op structurele veranderingen op de arbeidsmarkt.\r\nLijnen in de Beveridge-curve bewegen niet monotoon van links naar rechts. Daarom is het noodzakelijk om geom_pad() te gebruiken.\r\n\r\n\r\nlibrary(ggrepel)\r\n\r\nbeveridge <- read_csv(\r\n  [1336 chars quoted with '\"'])\r\n\r\nlabels <- beveridge %>%\r\n  filter(lubridate::month(quarter) == 1)\r\n\r\nbeveridge %>%\r\n  ggplot() +\r\n  geom_path(mapping = aes(x = unempoyment_rate, y = vacanacy_rate), alpha = 0.5) +\r\n  geom_point(data = labels, mapping = aes(x = unempoyment_rate, y = vacanacy_rate)) +\r\n  geom_text_repel(data = labels, mapping = aes(x = unempoyment_rate, y = vacanacy_rate, label = lubridate::year(quarter))) +  \r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0.04, 0.1),\r\n                     labels = scales::percent) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     breaks = c(0, 0.01, 0.02, 0.03, 0.04, 0.05),\r\n                     limits = c(0, 0.05),\r\n                     labels = scales::percent) +  \r\n  labs(x = \"Seizoen gecontroleerd werkloosheidspercentage\",\r\n       y = \"Seizoen gecontroleerd beschikbare banenpercentage\") +  \r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nHellingsgrafiek\r\n\r\n\r\n# https://www.bls.gov/lau/\r\nlibrary(ggrepel)\r\n\r\nunemployment <- tibble(\r\n  time = c(\"October 2009\", \"October 2009\", \"October 2009\", \"August 2017\", \"August 2017\", \"August 2017\"),\r\n  rate = c(7.4, 7.1, 10.0, 3.9, 3.8, 6.4),\r\n  state = c(\"Maryland\", \"Virginia\", \"Washington, D.C.\", \"Maryland\", \"Virginia\", \"Washington, D.C.\")\r\n)\r\n\r\nlabel <- tibble(label = c(\"October 2009\", \"August 2017\"))\r\noctober <- filter(unemployment, time == \"October 2009\")\r\naugust <- filter(unemployment, time == \"August 2017\")\r\n\r\nunemployment %>%\r\n  mutate(time = factor(time, levels = c(\"October 2009\", \"August 2017\")),\r\n         state = factor(state, levels = c(\"Washington, D.C.\", \"Maryland\", \"Virginia\"))) %>%\r\n  ggplot() + \r\n  geom_line(aes(time, rate, group = state, color = state), show.legend = FALSE) +\r\n  geom_point(aes(x = time, y = rate, color = state)) +\r\n  labs(subtitle = \"Werkloosheidspercentage\") +\r\n  theme(axis.ticks.x = element_blank(),\r\n        axis.title.x = element_blank(),\r\n        axis.ticks.y = element_blank(),\r\n        axis.title.y = element_blank(), \r\n        axis.text.y = element_blank(),\r\n        panel.grid.major.y = element_blank(),\r\n        panel.grid.minor.y = element_blank(),\r\n        panel.grid.major.x = element_blank(),\r\n        axis.line = element_blank()) +\r\n  geom_text_repel(data = october, mapping = aes(x = time, y = rate, label = as.character(rate)), nudge_x = -0.06) + \r\n  geom_text_repel(data = august, mapping = aes(x = time, y = rate, label = as.character(rate)), nudge_x = 0.06)\r\n\r\n\r\n\r\n\r\nUnivariaat\r\nEr zijn een aantal manieren om de verdeling van univariate data in R te onderzoeken. Sommige methoden, zoals strookdiagrammen, laten alle datapunten zien. Andere methoden, zoals de box- en whiskerplot, tonen geselecteerde datapunten die belangrijke waarden communiceren zoals de mediaan en het 25e percentiel. Tenslotte tonen sommige methoden geen van de onderliggende data, maar berekenen ze dichtheidsschattingen. Elke methode heeft voor- en nadelen, dus het is de moeite waard om de verschillende vormen te begrijpen. Lees voor meer informatie 40 jaar boxplottem van Hadley Wickham en Lisa Stryjewski.\r\nStripdiagram\r\nStripdiagrammen, de eenvoudigste univariate plot, tonen de verdeling van de waarden langs één as. Stripdiagrammen werken het beste met variabelen die veel variatie hebben. Zo niet, dan hebben de punten de neiging om op elkaar te clusteren. Zelfs als de variabele veel variatie heeft, is het vaak belangrijk om transparantie toe te voegen aan de punten met alpha = zodat overlappende waarden zichtbaar zijn.\r\n\r\n\r\nmsleep %>%\r\n  ggplot(aes(x = sleep_total, y = factor(1))) +\r\n  geom_point(alpha = 0.2, size = 5) +\r\n  labs(y = NULL) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 25), \r\n                     breaks = 0:5 * 5) +\r\n  scale_y_discrete(labels = NULL) +\r\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren\",\r\n       x = \"Totale slaaptijd (uren)\",\r\n       y = NULL) +\r\n  theme(axis.ticks.y = element_blank())\r\n\r\n\r\n\r\n\r\nStripdiagram met Highlighting\r\nOmdat strookdiagrammen alle waarden weergeven, zijn ze nuttig om aan te geven waar de geselecteerde punten in de verdeling van een variabele liggen. De duidelijkste manier om dit te doen is door geom_point() tweemaal toe te voegen met filter() in het gegevensargument. Op deze manier worden de geaccentueerde waarden boven op de niet geaccentueerde waarden getoond.\r\n\r\n\r\nggplot() +\r\n  geom_point(data = filter(msleep, name != \"Red fox\"), \r\n                    aes(x = sleep_total, \r\n                        y = factor(1)),\r\n             alpha = 0.2, \r\n             size = 5,\r\n             color = \"grey50\") +\r\n  geom_point(data = filter(msleep, name == \"Red fox\"),\r\n             aes(x = sleep_total, \r\n                 y = factor(1), \r\n                 color = name),\r\n             alpha = 0.8,\r\n             size = 5) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 25), \r\n                     breaks = 0:5 * 5) +  \r\n  scale_y_discrete(labels = NULL) +\r\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren\",\r\n       x = \"Totale slaaptijd (uren)\",\r\n       y = NULL,\r\n       legend) +\r\n  guides(color = guide_legend(title = NULL)) +\r\n  theme(axis.ticks.y = element_blank())\r\n\r\n\r\n\r\n\r\nOndergeschikte Strip Chart\r\nVoeg een y-variabele toe om de verdelingen van de continue variabele in deelverzamelingen van een categorische variabele te zien.\r\n\r\n\r\nlibrary(forcats)\r\n\r\nmsleep %>%\r\n  filter(!is.na(vore)) %>%\r\n  mutate(vore = fct_recode(vore, \r\n                            \"Insectivore\" = \"insecti\",\r\n                            \"Omnivore\" = \"omni\", \r\n                            \"Herbivore\" = \"herbi\", \r\n                            \"Carnivore\" = \"carni\"\r\n                            )) %>%\r\n  ggplot(aes(x = sleep_total, y = vore)) +\r\n  geom_point(alpha = 0.2, size = 5) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 25), \r\n                     breaks = 0:5 * 5) +  \r\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren volgens dieet\",\r\n       x = \"Totale slaaptijd (uren)\",\r\n       y = NULL) +\r\n  theme(axis.ticks.y = element_blank())\r\n\r\n\r\n\r\n\r\nHistogrammen\r\nHistogrammen verdelen de verdeling van een variabele in n staven van gelijke grootte en tellen en tonen vervolgens het aantal waarnemingen in elke staaf. Histogrammen zijn gevoelig voor de breedte van de staven. Zoals ?geom_histogram merkt op, “U moet altijd de waarde van [de standaard staafbreedte] overschrijven, door meerdere breedtes te onderzoeken om het beste te vinden om de verhalen in uw gegevens te illustreren”.\r\n\r\n\r\nggplot(data = diamonds, mapping = aes(x = depth)) + \r\n  geom_histogram(bins = 100) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 100)) +  \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), labels = scales::comma) +\r\n  labs(x = \"Depth\",\r\n       y = \"Count\")\r\n\r\n\r\n\r\n\r\nBoxplots\r\nBoxplots zijn in de jaren zeventig uitgevonden door John Tukey. In plaats van de onderliggende gegevens te tonen of het aantal blikken van de onderliggende gegevens, richten ze zich op belangrijke waarden zoals het 25e percentiel, de mediaan en het 75e percentiel.\r\n\r\n\r\nInsectSprays %>%\r\n  ggplot(mapping =  aes(x = spray, y = count)) +\r\n  geom_boxplot() +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\r\n  labs(x = \"Type of insect spray\",\r\n       y = \"Number of dead insects\") +\r\n  remove_ticks()\r\n\r\n\r\n\r\n\r\nGladde kernel verdelingsgrafieken\r\nDoorlopende variabelen met vloeiende verdelingen worden soms beter weergegeven met afgevlakte kerneldichtheidsschattingen dan histogrammen of boxplots. geom_density() berekent en plot een kerneldichtheidsschatting. Let op de klontjes rond gehele en halve getallen in de volgende verdeling vanwege afrondingen.\r\n\r\n\r\ndiamonds %>%\r\n  ggplot(mapping = aes(carat)) +\r\n  geom_density(color = NA) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, NA)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\r\n  labs(x = \"Carat\",\r\n       y = \"Density\")\r\n\r\n\r\n\r\n\r\n\r\n\r\ndiamonds %>%\r\n  mutate(cost = ifelse(price > 5500, \"More than $5,500 +\", \"$0 to $5,500\")) %>%\r\n  ggplot(mapping = aes(carat, fill = cost)) +\r\n  geom_density(alpha = 0.25, color = NA) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, NA)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\r\n  labs(x = \"Carat\",\r\n       y = \"Density\")\r\n\r\n\r\n\r\n\r\n‘Ridgeline’ grafieken\r\nRidgeline plots zijn gedeeltelijk overlappende afgevlakte kerneldichtheid plots gefacetteerd door een categorische variabele die veel informatie verpakken in één elegante plot. Onderstaande maakt duidelijk wat we hiermee bedoelen.\r\n\r\n\r\nlibrary(ggridges)\r\n\r\nggplot(diamonds, mapping = aes(x = price, y = cut)) +\r\n  geom_density_ridges(fill = \"#1696d2\") +\r\n  labs(x = \"Price\",\r\n       y = \"Cut\")\r\n\r\n\r\n\r\n\r\nViool grafieken\r\nVioolplots zijn symmetrische weergaven van gladde kerneldichtheidplots.\r\n\r\n\r\nInsectSprays %>%\r\n  ggplot(mapping = aes(x = spray, y = count, fill = spray)) +\r\n  geom_violin(color = NA) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\r\n  labs(x = \"Type of insect spray\",\r\n       y = \"Number of dead insects\") +\r\n  remove_ticks()\r\n\r\n\r\n\r\n\r\nBonenplot\r\nIndividuele uitschieters en belangrijke samenvattende waarden zijn niet zichtbaar in vioolplots of afgevlakte kerneldichtheidsplots. Bonenplots, gemaakt door Peter Kampstra in 2008, zijn vioolplots met gegevens weergegeven als kleine lijnen in een eendimensionale stripplot en grotere lijnen voor het gemiddelde.\r\n\r\n\r\nmsleep %>%\r\n  filter(!is.na(vore)) %>%\r\n  mutate(vore = fct_recode(vore, \r\n                            \"Insectivore\" = \"insecti\",\r\n                            \"Omnivore\" = \"omni\", \r\n                            \"Herbivore\" = \"herbi\", \r\n                            \"Carnivore\" = \"carni\"\r\n                            )) %>%\r\n  ggplot(aes(x = vore, y = sleep_total, fill = vore)) +\r\n  stat_summary(fun.y = \"mean\",\r\n               colour = \"black\", \r\n               size = 30,\r\n               shape = 95,\r\n               geom = \"point\") +\r\n  geom_violin(color = NA) +\r\n  geom_jitter(width = 0,\r\n              height = 0.05,\r\n              alpha = 0.4,\r\n              shape = \"-\",\r\n              size = 10,\r\n              color = \"grey50\") +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +  \r\n    labs(x = NULL,\r\n         y = \"Total sleep time (hours)\") +\r\n  theme(legend.position = \"none\") +\r\n  remove_ticks()\r\n\r\n\r\n\r\n\r\nGebiedsplot\r\nGestapeld gebied\r\n\r\n\r\ntxhousing %>%\r\n  filter(city %in% c(\"Austin\",\"Houston\",\"Dallas\",\"San Antonio\",\"Fort Worth\")) %>%\r\n  group_by(city, year) %>%\r\n  summarize(sales = sum(sales)) %>%\r\n  ggplot(aes(x = year, y = sales, fill = city)) +\r\n  geom_area(position = \"stack\") +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\r\n                     limits = c(2000, 2015),\r\n                     breaks = 2000 + 0:15) +  \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \r\n                     labels = scales::comma) +\r\n  labs(x = \"Year\",\r\n       y = \"Home sales\")\r\n\r\n\r\n\r\n\r\nGevuld gebied\r\n\r\n\r\ntxhousing %>%\r\n  filter(city %in% c(\"Austin\",\"Houston\",\"Dallas\",\"San Antonio\",\"Fort Worth\")) %>%\r\n  group_by(city, year) %>%\r\n  summarize(sales = sum(sales)) %>%\r\n  ggplot(aes(x = year, y = sales, fill = city)) +\r\n  geom_area(position = \"fill\") +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\r\n                     limits = c(2000, 2015),\r\n                     breaks = 2000 + 0:15) +  \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.02)),\r\n                     breaks = c(0, 0.25, 0.5, 0.75, 1),\r\n                     labels = scales::percent) +\r\n  labs(x = \"Year\",\r\n       y = \"Home sales\")\r\n\r\n\r\n\r\n\r\nWafelkaart / Vierkante taartkaart\r\nHet wafelpakket {CRAN en Github} maakt vierkante taartkaarten. Het kan ook gecombineerd worden met glyphs voor meer elegantere vormen dan vierkanten. Data hiervoor komen hier vandaan: A Vision for an Equitable DC.\r\nWafelkaarten vereisen een beetje extra knutselen omdat ze worden genoemd vanuit library(waffle) in plaats van library(ggplot2). Het belangrijkste is dat voor wafeldiagrammen theme_urban(text = element_text(family = \"Lato\")) nodig is voor het lettertype Lato.\r\nEnkele wafelkaart\r\n\r\n\r\nlibrary(waffle)\r\n\r\nparts <- c(`Virginia\\nClinics` = (1000 - 208 - 105), `Maryland\\nClinics` = 208, `D.C.\\nClinics` = 105)\r\nwaffle(parts, rows = 25, size = 1, colors = c(\"#1696d2\", \"#fdbf11\", \"#000000\"), legend_pos = \"bottom\") +\r\n  labs(title = \"Free Clinics in the D.C.-Maryland-Virginia Area\",\r\n       subtitle = \"1 Square == 1 Clinic\") +\r\n  theme(text = element_text(family = \"Lato\"))\r\n\r\n\r\n\r\n\r\nMeervoudige wafelkaarten\r\nlibrary(waffle) allows multiple waffle charts to be ironed together using iron(). maakt het mogelijk om meerdere wafelkaarten in elkaar te strijken met behulp van iron(). Het samen strijken van meerdere wafeldiagrammen vereist wat trial-and-error om de maten en resolutie goed te krijgen, maar de resultaten kunnen de moeite waard zijn. Vergeet niet theme(text = element_text(family = \"Lato\"))!\r\n\r\n\r\nlibrary(waffle)\r\n\r\nwhite <- c(`With Degree` = 169300, `Without Degree` = 800)\r\nblack <- c(`With Degree` = 174900, `Without Degree` = 34700)\r\nhispanic <- c(`With Degree` = 27700, `Without Degree` = 12400)\r\n\r\niron(\r\n  waffle(white / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"White\", keep = FALSE, pad = 10) + \r\n  theme(text = element_text(family = \"Lato\")),\r\n  waffle(black / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"Black\", keep = FALSE) + \r\n  theme(text = element_text(family = \"Lato\")),\r\n  waffle(hispanic / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"Hispanic\", keep = FALSE, pad = 59, xlab = \"1 Square == 83 People\") + \r\n  theme(text = element_text(family = \"Lato\"))\r\n) \r\n\r\n\r\n\r\n\r\nWarmtekaart\r\n\r\n\r\nlibrary(fivethirtyeight)\r\n\r\nbad_drivers %>%\r\n  filter(state %in% c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Connecticut\", \"New York\")) %>%\r\n  mutate(`Number of\\nDrivers` = scale(num_drivers),\r\n         `Percent\\nSpeeding` = scale(perc_speeding),\r\n         `Percent\\nAlcohol` = scale(perc_alcohol),\r\n         `Percent Not\\nDistracted` = scale(perc_not_distracted),\r\n         `Percent No\\nPrevious` = scale(perc_no_previous),\r\n         state = factor(state, levels = rev(state))\r\n         ) %>%\r\n  select(-insurance_premiums, -losses, -(num_drivers:losses)) %>%\r\n  gather(`Number of\\nDrivers`:`Percent No\\nPrevious`, key = \"variable\", value = \"SD's from Mean\") %>%\r\n  ggplot(aes(variable, state)) +\r\n    geom_tile(aes(fill = `SD's from Mean`)) +\r\n    labs(x = NULL,\r\n         y = NULL) + \r\n    scale_fill_gradientn() +\r\n    theme(legend.position = \"right\",\r\n          legend.direction = \"vertical\",\r\n          axis.line.x = element_blank(),\r\n          panel.grid.major.y = element_blank()) +\r\n  remove_ticks()\r\n\r\n\r\n\r\n#https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/\r\n\r\n\r\n\r\nFaceteren en kleine kaartjes\r\nfacet_wrap()\r\nR’s faceteersysteem is een krachtige manier om kleinere kaarten te maken.\r\nSommige bewerkingen aan het thema kunnen nodig zijn, afhankelijk van het aantal rijen en kolommen in de plot.\r\n\r\n\r\ndiamonds %>%\r\n  ggplot(mapping = aes(x = carat, y = price)) +\r\n  geom_point(alpha = 0.05) +\r\n  facet_wrap(~cut, ncol = 5) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\r\n                     limits = c(0, 6)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)),\r\n                     limits = c(0, 20000), \r\n                     labels = scales::dollar) +\r\n  labs(x = \"Karaat\",\r\n       y = \"Prijs\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nfacet_grid()\r\n\r\n\r\ndiamonds %>%\r\n  filter(color %in% c(\"D\", \"E\", \"F\", \"G\")) %>%\r\n  ggplot(mapping = aes(x = carat, y = price)) +\r\n  geom_point(alpha = 0.05) +\r\n  facet_grid(color ~ cut) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\r\n                     limits = c(0, 4)) +  \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)),\r\n                     limits = c(0, 20000), \r\n                     labels = scales::dollar) +\r\n  labs(x = \"Carat\",\r\n       y = \"Price\") +\r\n  theme(panel.spacing = unit(20L, \"pt\")) +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\n‘Smoothers’\r\ngeom_smooth() past en modelleert op gegevens met twee of meer dimensies.\r\nHet begrijpen en manipuleren van defaults is belangrijker voor geom_smooth() dan andere ‘geoms’ omdat het een aantal aannames bevat. geom_smooth() gebruikt automatisch loess voor datasets met minder dan 1.000 waarnemingen en een algemeen model met formula = y ~ s(x, bs = \"cs\") voor datasets met meer dan 1.000 waarnemingen. Beide zijn standaard ingesteld op een betrouwbaarheidsinterval van 95%.\r\nModellen worden gekozen met method = en kunnen worden ingesteld op lm(), glm(), gam(), loess(), rlm(), en meer. Formules kunnen worden opgegeven met formule = en y ~ x syntaxis. Het plotten van de standaardfout wordt omgeschakeld met se = TRUE en se = FALSE, en het niveau wordt gespecificeerd met level =. Zoals altijd is er meer informatie te zien in RStudio met ?geom_smooth().\r\ngeom_point() voegt een scatterplot toe aan geom_smooth(). De volgorde van de functie-aanroepen is belangrijk. De functie die als tweede wordt aangeroepen wordt bovenop de functie die als eerste wordt aangeroepen gelegd.\r\n\r\n\r\ndiamonds %>%\r\n  ggplot(mapping = aes(x = carat, y = price)) +\r\n  geom_point(alpha = 0.05) +\r\n  geom_smooth(color =  \"#ec008b\") +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 5),\r\n                     breaks = 0:5) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 20000), \r\n                     labels = scales::dollar) +  \r\n  labs(x = \"Carat\",\r\n       y = \"Price\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\ngeom_smooth kan worden onderverdeeld in categorische en factorvariabelen. Dit vereist subgroepen met een behoorlijk aantal waarnemingen en een behoorlijke mate van variabiliteit over de x-as. De betrouwbaarheidsintervallen worden vaak groter aan de uiteinden, zodat speciale zorg nodig is om de grafiek zinvol en leesbaar te maken.\r\nDit voorbeeld gebruikt Loess met MPG = verplaatsing.\r\n\r\n\r\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = factor(cyl))) +\r\n  geom_point(alpha = 0.2) +\r\n  geom_smooth() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 7),\r\n                     breaks = 0:7) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 60)) +  \r\n  labs(x = \"Engine displacement\",\r\n       y = \"Highway MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nDit voorbeeld gebruikt liniaire regressie met MPG = displacement.\r\n\r\n\r\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = factor(cyl))) +\r\n  geom_point(alpha = 0.2) +\r\n  geom_smooth(method = \"lm\") +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \r\n                     limits = c(0, 7),\r\n                     breaks = 0:7) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 60)) +  \r\n  labs(x = \"Engine displacement\",\r\n       y = \"Highway MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nBenadrukken\r\nlibrary(gghighlight) maakt intuitief benadrukken van ggplot2 plots mogelijk. gghighlight wijzigt bestaande ggplot2-objecten, dus geen enkele andere code mag veranderen. Alle markering wordt afgehandeld door de functie gghighlight(), die alle soorten geomen kan verwerken.\r\nWaarschuwing:  R zal een fout maken als er te veel kleuren worden gemarkeerd vanwege het ontwerp van urbnthemes. Verlaag gewoon het aantal gemarkeerde geomen om dit probleem op te lossen.\r\nEr zijn twee belangrijke manieren om de aandacht te vestigen.\r\nDrempelwaarde\r\nDe eerste manier om te markeren is met een drempel. Voeg een logische test toe aan gghighlight() om te beschrijven welke lijnen gemarkeerd moeten worden. Hier worden regels met een maximale verandering in het bruto binnenlands product per hoofd van de bevolking van meer dan $35.000 gemarkeerd met gghighlight(max(pcgpd_change) > 35000, use_direct_label = FALSE).\r\n\r\n\r\nlibrary(gghighlight)\r\nlibrary(gapminder)\r\n\r\ndata <- gapminder %>%\r\n  filter(continent %in% c(\"Europe\")) %>%\r\n  group_by(country) %>%\r\n  mutate(pcgpd_change = ifelse(year == 1952, 0, gdpPercap - lag(gdpPercap))) %>%\r\n  mutate(pcgpd_change = cumsum(pcgpd_change))\r\n  \r\ndata %>%\r\n  ggplot(aes(year, pcgpd_change, group = country, color = country)) +\r\n  geom_line() +\r\n  gghighlight(max(pcgpd_change) > 35000, use_direct_label = FALSE) +  \r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\r\n                     breaks = c(seq(1950, 2010, 10)),\r\n                     limits = c(1950, 2010)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     breaks = 0:8 * 5000,\r\n                     labels = scales::dollar,\r\n                     limits = c(0, 40000)) +\r\n  labs(x = \"Year\",\r\n       y = \"Change in per-capita GDP (US dollars)\")\r\n\r\n\r\n\r\n\r\nRang\r\nDe tweede manier om te markeren is door middel van een rangorde. Hier worden de landen met de eerste hoogste waarden voor verandering in het bruto binnenlands product per hoofd van de bevolking benadrukt met gghighlight(max(pcgpd_change), max_highlight = 5, use_direct_label = FALSE).\r\n\r\n\r\ndata %>%\r\n  ggplot(aes(year, pcgpd_change, group = country, color = country)) +\r\n  geom_line() +\r\n  gghighlight(max(pcgpd_change), max_highlight = 5, use_direct_label = FALSE) +  \r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\r\n                     breaks = c(seq(1950, 2010, 10)),\r\n                     limits = c(1950, 2010)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     breaks = 0:8 * 5000,\r\n                     labels = scales::dollar,\r\n                     limits = c(0, 40000)) +\r\n  labs(x = \"Year\",\r\n       y = \"Change in per-capita GDP (US dollars)\")\r\n\r\n\r\n\r\n\r\nFaceteren\r\ngghighlight() werkt goed met ggplot2’s facetsysteem.\r\n\r\n\r\ndata %>%\r\n  ggplot(aes(year, pcgpd_change, group = country)) +\r\n  geom_line() +\r\n  gghighlight(max(pcgpd_change), max_highlight = 4, use_direct_label = FALSE) +  \r\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\r\n                     breaks = c(seq(1950, 2010, 10)),\r\n                     limits = c(1950, 2010)) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     breaks = 0:8 * 5000,\r\n                     labels = scales::dollar,\r\n                     limits = c(0, 40000)) +\r\n  labs(x = \"Year\",\r\n       y = \"Change in per-capita GDP (US dollars)\") +\r\n  facet_wrap(~ country) +\r\n  theme(panel.spacing = unit(20L, \"pt\"))\r\n\r\n\r\n\r\n\r\nTekst en annotatie\r\nVerschillende functies kunnen worden gebruikt om verschillende delen van percelen te annoteren, te labelen en te markeren. geom_text() en geom_text_repel() geven beide variabelen uit dataframes weer. annotate(), die verschillende toepassingen heeft, geeft variabelen en waarden weer die zijn opgenomen in de functie-aanroep.\r\ngeom_text()\r\ngeom_text() zet tekstvariabelen in datasets om in geometrische objecten. Dit is nuttig voor het labelen van gegevens in plots. Beide functies hebben x waarden en y waarden nodig om de plaatsing op het coördinatenvlak te bepalen en een tekstvector van labels.\r\nDit kan gebruikt worden voor het labelen van geom_bar().\r\n\r\n\r\ndiamonds %>%\r\n  group_by(cut) %>%\r\n  summarize(price = mean(price)) %>%\r\n  ggplot(aes(cut, price)) +\r\n  geom_bar(stat = \"identity\") +\r\n  geom_text(aes(label = scales::dollar(price)), vjust = -1) +\r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)),\r\n                     labels = scales::dollar) +\r\n  labs(title = \"Average Diamond Price by Diamond Cut\",\r\n       x = \"Cut\",\r\n       y = \"Price\") +\r\n  remove_ticks()\r\n\r\n\r\n\r\n\r\nHet kan ook gebruikt worden om punten in een scatterplot te labelen.\r\nHet is zelden nuttig om elk punt in een scatter plot te labelen. Gebruik filter() om een tweede dataset te maken die wordt onderverdeeld en deze door te geven aan de labelfunctie.\r\n\r\n\r\nlabels <- mtcars %>%\r\n  rownames_to_column(\"model\") %>%\r\n  filter(model %in% c(\"Toyota Corolla\", \"Merc 240D\", \"Datsun 710\"))\r\n\r\nmtcars %>%\r\n  ggplot() +\r\n  geom_point(mapping = aes(x = wt, y = mpg)) +\r\n  geom_text(data = labels, mapping = aes(x = wt, y = mpg, label = model), nudge_x = 0.38) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 6)) + \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 40)) +  \r\n  labs(x = \"Weight (Tons)\",\r\n       y = \"Miles per gallon (MPG)\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nTekst overlapt te vaak met andere tekst of geomen bij gebruik van geom_text(). library(ggrepel) is een library(ggplot2) add-on die automatisch tekst positioneert zodat deze niet overlapt met geomen of andere tekst. Om deze functionaliteit toe te voegen, installeer en laad je library(ggrepel) en gebruikt je vervolgens geom_text_repel() met dezelfde syntaxis als geom_text().\r\ngeom_text_repel()\r\n\r\n\r\nlibrary(ggrepel)\r\n\r\nlabels <- mtcars %>%\r\n  rownames_to_column(\"model\") %>%\r\n  top_n(5, mpg)\r\n\r\nmtcars %>%\r\n  ggplot(mapping = aes(x = wt, y = mpg)) +\r\n  geom_point() +\r\n  geom_text_repel(data = labels, \r\n                  mapping = aes(label = model), \r\n                  nudge_x = 0.38) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 6)) + \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 40)) +  \r\n  labs(x = \"Weight (Tons)\",\r\n       y = \"Miles per gallon (MPG)\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nannotate()\r\nannotate() gebruikt geen dataframes. In plaats daarvan zijn er waarden nodig voor x = en y =. Het kan tekst, rechthoeken, segmenten en puntenreeksen toevoegen.\r\n\r\n\r\nmsleep %>%\r\n  filter(bodywt <= 1000) %>%\r\n  ggplot(aes(bodywt, sleep_total)) +\r\n  geom_point() +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(-10, 1000),\r\n                     labels = scales::comma) + \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 25)) +  \r\n  annotate(\"text\", x = 500, y = 12, label = \"These data suggest that heavy \\n animals sleep less than light animals\") +\r\n  labs(x = \"Body weight (pounds)\",\r\n       y = \"Sleep time (hours)\") +\r\n  scatter_grid()  \r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(AmesHousing)\r\n\r\names <- make_ames()\r\n\r\names %>%\r\n  mutate(square_footage = Total_Bsmt_SF - Bsmt_Unf_SF + First_Flr_SF + Second_Flr_SF) %>%\r\n  mutate(Sale_Price = Sale_Price / 1000) %>%  \r\n  ggplot(aes(square_footage, Sale_Price)) +\r\n  geom_point(alpha = 0.2) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(-10, 12000),\r\n                     labels = scales::comma) + \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\r\n                     limits = c(0, 800),\r\n                     labels = scales::dollar) +  \r\n  annotate(\"rect\", xmin = 6800, xmax = 11500, ymin = 145, ymax = 210, alpha = 0.1) +\r\n  annotate(\"text\", x = 8750, y = 230, label = \"Unfinished homes\") +\r\n  labs(x = \"Square footage\", \r\n       y = \"Sale price (thousands)\") +\r\n  scatter_grid()   \r\n\r\n\r\n\r\n\r\nGelaagde geoms\r\nGeomen kunnen worden gelaagd in ggplot2. Dit is nuttig voor het ontwerp en de analyse.\r\nHet is vaak nuttig om punten toe te voegen aan lijngrafieken met een klein aantal waarden over de x-as. Dit voorbeeld uit R voor Data Science laat zien hoe het veranderen van de lijn naar grijs aantrekkelijk kan zijn.\r\nDesign\r\nVoor\r\n\r\n\r\ntable1 %>%\r\n  ggplot(aes(x = year, y = cases)) +\r\n    geom_line(aes(color = country)) +\r\n    geom_point(aes(color = country)) +\r\n    scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \r\n                       labels = scales::comma) +\r\n    scale_x_continuous(breaks = c(1999, 2000)) +\r\n    labs(title = \"Changes in Tuberculosis Cases in Three Countries\")\r\n\r\n\r\n\r\n\r\nNa\r\n\r\n\r\ntable1 %>%\r\n  ggplot(aes(year, cases)) +\r\n    geom_line(aes(group = country), color = \"grey50\") +\r\n    geom_point(aes(color = country)) +\r\n    scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \r\n                       labels = scales::comma) +\r\n    scale_x_continuous(breaks = c(1999, 2000)) +\r\n    labs(title = \"Changes in Tuberculosis Cases in Three Countries\")\r\n\r\n\r\n\r\n\r\nGelaagde geomen zijn ook nuttig voor het toevoegen van trendlijnen en centroïden aan scatterplots.\r\n\r\n\r\n# Simpele lijn\r\n# Regressie model\r\n# Centroiden\r\n\r\n\r\n\r\nCentroiden\r\n\r\n\r\nmpg_summary <- mpg %>%\r\n  group_by(cyl) %>%\r\n  summarize(displ = mean(displ), cty = mean(cty))\r\n\r\nmpg %>%\r\n  ggplot() +\r\n  geom_point(aes(x = displ, y = cty, color = factor(cyl)), alpha = 0.5) +\r\n  geom_point(data = mpg_summary, aes(x = displ, y = cty), size = 5, color = \"#ec008b\") +\r\n  geom_text(data = mpg_summary, aes(x = displ, y = cty, label = cyl)) +\r\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)), \r\n                     limits = c(0, 8)) +  \r\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)), \r\n                     limits = c(0, 40)) +\r\n  labs(x = \"Displacement\",\r\n       y = \"City MPG\") +\r\n  scatter_grid()\r\n\r\n\r\n\r\n\r\nGrafieken opslaan\r\nggsave() exporteert ggplot2 percelen. De functie kan op twee manieren worden gebruikt. Als plot = niet is gespecificeerd in de functie-aanroep, dan slaat ggsave() automatisch de plot op die het laatst werd weergegeven in het Viewer-venster. Ten tweede, als plot = is gespecificeerd, dan slaat ggsave() het gespecificeerde plot op. ggsave() raadt het type grafische soort dat gebruikt moet worden bij het exporteren (.png, .pdf, .svg, etc.) van de bestandsextensie in de bestandsnaam.\r\nmtcars %>%\r\n  ggplot(aes(x = wt, y = mpg)) +\r\n  geom_point()\r\n\r\nggsave(filename = \"cars.png\")\r\n\r\nplot2 <- mtcars %>%\r\n  ggplot(aes(x = wt, y = mpg)) +\r\n  geom_point()\r\n\r\nggsave(filename = \"cars.png\", plot = plot2)\r\nGeëxporteerde plots zien er zelden identiek uit als de plots die in het Viewervenster in RStudio verschijnen omdat de totale grootte en de beeldverhouding van de Viewer vaak anders is dan de standaardinstellingen voor ggsave(). Specifieke afmetingen, beeldverhoudingen en resoluties kunnen worden gecontroleerd met argumenten in ggsave(). RStudio heeft een nuttig cheatsheet genaamd “How Big is Your Graph?” dat zou moeten helpen bij het kiezen van de beste grootte, beeldverhouding en resolutie.\r\nLettertypen zijn niet standaard in PDF’s opgenomen. Om lettertypes in te sluiten in PDF’s, neem device = cairo_pdf op in ggsave().\r\nplot <- mtcars %>%\r\n  ggplot(aes(x = wt, y = mpg)) +\r\n  geom_point()\r\n\r\nggsave(filename = \"cars.pdf\", plot = plot2, width = 6.5, height = 4, device = cairo_pdf)\r\nBibliography and Session Information\r\nNote: Examples present in this document by Aaron Williams were created during personal time.\r\nBob Rudis and Dave Gandy (2017). waffle: Create Waffle Chart Visualizations in R. R package version 0.7.0. https://CRAN.R-project.org/package=waffle\r\nChester Ismay and Jennifer Chunn (2017). fivethirtyeight: Data and Code Behind the Stories and Interactives at ‘FiveThirtyEight’. R package version 0.3.0. https://CRAN.R-project.org/package=fivethirtyeight\r\nHadley Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.\r\nHadley Wickham (2017). tidyverse: Easily Install and Load the ‘Tidyverse’. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\r\nHadley Wickham (2017). forcats: Tools for Working with Categorical Variables (Factors). R package version 0.2.0. https://CRAN.R-project.org/package=forcats\r\nJennifer Bryan (2017). gapminder: Data from Gapminder. R package version 0.3.0. https://CRAN.R-project.org/package=gapminder\r\nKamil Slowikowski (2017). ggrepel: Repulsive Text and Label Geoms for ‘ggplot2’. R package version 0.7.0. https://CRAN.R-project.org/package=ggrepel\r\nMax Kuhn (2017). AmesHousing: The Ames Iowa Housing Data. R package version 0.0.3. https://CRAN.R-project.org/package=AmesHousing\r\nPeter Kampstra (2008). Beanplot: A Boxplot Alternative for Visual Comparison of Distributions, Journal of Statistical Software, 2008. https://www.jstatsoft.org/article/view/v028c01\r\nR Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\r\nWinston Chang, (2014). extrafont: Tools for using fonts. R package version 0.17. https://CRAN.R-project.org/package=extrafont\r\nYihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.19.\r\n\r\n\r\nsessionInfo()\r\n\r\n\r\nR version 4.0.3 (2020-10-10)\r\nPlatform: x86_64-apple-darwin17.0 (64-bit)\r\nRunning under: macOS High Sierra 10.13.6\r\n\r\nMatrix products: default\r\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\r\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\r\n\r\nlocale:\r\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n [1] AmesHousing_0.0.4     gghighlight_0.3.0     fivethirtyeight_0.6.1\r\n [4] waffle_0.7.0          ggridges_0.5.2        gapminder_0.3.0      \r\n [7] urbnmapr_0.0.0.9002   extrafont_0.17        ggrepel_0.8.2        \r\n[10] urbnthemes_0.0.1      forcats_0.5.0         stringr_1.4.0        \r\n[13] dplyr_1.0.2           purrr_0.3.4           readr_1.4.0          \r\n[16] tidyr_1.1.2           tibble_3.0.4          ggplot2_3.3.2        \r\n[19] tidyverse_1.3.0       knitr_1.30           \r\n\r\nloaded via a namespace (and not attached):\r\n [1] httr_1.4.2         jsonlite_1.7.1     splines_4.0.3     \r\n [4] modelr_0.1.8       assertthat_0.2.1   blob_1.2.1        \r\n [7] cellranger_1.1.0   yaml_2.2.1         Rttf2pt1_1.3.8    \r\n[10] pillar_1.4.6       backports_1.1.10   lattice_0.20-41   \r\n[13] glue_1.4.2         extrafontdb_1.0    digest_0.6.27     \r\n[16] RColorBrewer_1.1-2 rvest_0.3.6        colorspace_1.4-1  \r\n[19] Matrix_1.2-18      htmltools_0.5.0    plyr_1.8.6        \r\n[22] pkgconfig_2.0.3    broom_0.7.2        haven_2.3.1       \r\n[25] scales_1.1.1       distill_1.0        downlit_0.2.0     \r\n[28] mgcv_1.8-33        generics_0.0.2     farver_2.0.3      \r\n[31] ellipsis_0.3.1     withr_2.3.0        hexbin_1.28.1     \r\n[34] cli_2.1.0          magrittr_1.5       crayon_1.3.4      \r\n[37] readxl_1.3.1       evaluate_0.14      fs_1.5.0          \r\n[40] fansi_0.4.1        nlme_3.1-149       xml2_1.3.2        \r\n[43] tools_4.0.3        hms_0.5.3          lifecycle_0.2.0   \r\n[46] munsell_0.5.0      reprex_0.3.0       compiler_4.0.3    \r\n[49] rlang_0.4.8        grid_4.0.3         rstudioapi_0.11   \r\n[52] labeling_0.4.2     rmarkdown_2.5      gtable_0.3.0      \r\n[55] DBI_1.1.0          R6_2.4.1           gridExtra_2.3     \r\n[58] lubridate_1.7.9    stringi_1.5.3      Rcpp_1.0.5        \r\n[61] vctrs_0.3.4        dbplyr_1.4.4       tidyselect_1.1.0  \r\n[64] xfun_0.18         \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-05-31-grafieken/grafieken_files/figure-html5/staafgrafiek-1.png",
    "last_modified": "2022-05-05T14:08:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-31-dslabs/",
    "title": "dslabs",
    "description": "Een aantal mooie grafieken uit het goede data-analyseboek van Rafa Irizarri (Harvard University)",
    "author": [
      {
        "name": "Rafa Irizarri en Amy Hill, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-04-22",
    "categories": [],
    "contents": "\r\ndslabs-pakket om van de werkelijkheid te leren\r\nRafael Irizarri schreef een prachtig boek over moderne data analyse Introduction to Data Science. Data Analysis and Prediction Algorithms with R dat je gratis in boekdown formaat kunt inzien hier. Daar heb ik het al eens over gehad en daar zal ik vast op een later tijdstip nog wel weer eens op terugkomen. Dat boek komt voort uit diverse colleges die hij over dit onderwerp op de Harvard University heeft gegeven. Irizarri wil je van echte data laten leren en leert je zo hele verschillende technieken. Voor het gebruik van die data heeft hij een speciaal pakket gemaakt dat dslabs heet. Daarover heeft hijzelf eerder en Amy Hill later een post geschreven op Simply Statistics. Ik heb deze posts bewerkt om te laten zien hoe levensechte datasets eruit zien en hoe je deze data aantrekkelijk zichtbaar kunt maken. Misschien vooral ook wel om op zijn goede boek te wijzen.\r\nEen veelbesproken onderwerp in het statistiekonderwijs is dat informatica-computatie een prominentere rol zou moeten spelen in het curriculum. Izigarri (en in alle bescheidenheid ik ook) is het daar volledig mee eens, maar hij denkt dat de belangrijkste verbetering zal komen van het op de voorgrond brengen van toepassingen en het zo goed mogelijk nabootsen van de uitdagingen waarmee de toegepaste statistici in het echte leven worden geconfronteerd. Izigarri probeert daarom het gebruik van veelgebruikte voorbeelden, zoals de -mtcarsdataset die in R zo vaak worden gebruikt, te vermijden wanneer hij les geeft in datawetenschap. Volgens hem is het niet zo eenvoudig om voorbeelden te vinden die zowel realistisch en interessant zijn als geschikt voor beginners. Na een paar jaar lesgeven heeft hij een aantal datasets verzameld die volgens hem wel aan deze criteria voldoen. Om het gebruik ervan in introductielessen te vergemakkelijken, heeft hij ze in het dslabs-pakket opgenomen. Dat pakket heb ikzelf al geinstalleerd (en daarom staat er een # voor). Als jij het wilt gebruiken, moet je het #-teken weghalen.\r\n\r\n\r\n# install.packages(\"dslabs\")\r\n\r\nHieronder laat hij wat voorbeelden zien. Je kunt in ieder geval zien welke datasets in het pakket zitten:\r\n\r\n\r\nlibrary(\"dslabs\")\r\ndata(package=\"dslabs\")\r\n\r\nMerk op dat het pakket ook enkele van de scripts bevat die worden gebruikt om de gegevens uit hun oorspronkelijke bron te halen:\r\n\r\n\r\nlist.files(system.file(\"script\", package = \"dslabs\"))\r\n\r\n [1] \"make-admissions.R\"                   \r\n [2] \"make-brca.R\"                         \r\n [3] \"make-brexit_polls.R\"                 \r\n [4] \"make-death_prob.R\"                   \r\n [5] \"make-divorce_margarine.R\"            \r\n [6] \"make-gapminder-rdas.R\"               \r\n [7] \"make-greenhouse_gases.R\"             \r\n [8] \"make-historic_co2.R\"                 \r\n [9] \"make-mnist_27.R\"                     \r\n[10] \"make-movielens.R\"                    \r\n[11] \"make-murders-rda.R\"                  \r\n[12] \"make-na_example-rda.R\"               \r\n[13] \"make-nyc_regents_scores.R\"           \r\n[14] \"make-olive.R\"                        \r\n[15] \"make-outlier_example.R\"              \r\n[16] \"make-polls_2008.R\"                   \r\n[17] \"make-polls_us_election_2016.R\"       \r\n[18] \"make-reported_heights-rda.R\"         \r\n[19] \"make-research_funding_rates.R\"       \r\n[20] \"make-stars.R\"                        \r\n[21] \"make-temp_carbon.R\"                  \r\n[22] \"make-tissue-gene-expression.R\"       \r\n[23] \"make-trump_tweets.R\"                 \r\n[24] \"make-weekly_us_contagious_diseases.R\"\r\n[25] \"save-gapminder-example-csv.R\"        \r\n\r\nIn het boek Introduction to Data Science kun je zien hoe de datasets worden gebruikt. Hier volgt een kort inkijkje op het geheel.\r\nUS murders\r\nDeze dataset bevat gegevens over moorden met wapens in de Verenigde Staten in 2012. Hij gebruikt deze dataset om de basis van het R-programma te introduceren.\r\n\r\n\r\ndata(\"murders\")\r\nlibrary(tidyverse)\r\nlibrary(ggthemes)\r\nlibrary(ggrepel)\r\n\r\nr <- murders %>%\r\n  summarize(pop=sum(population), tot=sum(total)) %>%\r\n  mutate(rate = tot/pop*10^6) %>% .$rate\r\n\r\nds_theme_set()\r\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\r\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\r\n  geom_point(aes(color=region), size = 3) +\r\n  geom_text_repel() +\r\n  scale_x_log10() +\r\n  scale_y_log10() +\r\n  xlab(\"Populations in millions (log scale)\") +\r\n  ylab(\"Total number of murders (log scale)\") +\r\n  ggtitle(\"US Gun Murders in 2010\") +\r\n  scale_color_discrete(name=\"Region\") \r\n\r\n\r\nGapminder\r\nOver deze dataset heb ikzelf ook vaker geschreven. Deze dataset omvat de gezondheids- en inkomensresultaten van 184 landen van 1960 tot 2016. Het bevat ook twee karaktervectoren, de OESO en de OPEC, met de namen van de OESO- en OPEC-landen vanaf 2016. Hij gebruikt deze dataset om data visualisatie en ggplot2 te onderwijzen.\r\n\r\n\r\ndata(\"gapminder\")\r\n\r\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\r\n          \"Northern America\",\"Australia and New Zealand\")\r\n\r\ngapminder <- gapminder %>%\r\n  mutate(group = case_when(\r\n    region %in% west ~ \"The West\",\r\n    region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\r\n    region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\r\n    continent == \"Africa\" & region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\r\n    TRUE ~ \"Others\"))\r\ngapminder <- gapminder %>%\r\n  mutate(group = factor(group, levels = rev(c(\"Others\", \"Latin America\", \"East Asia\",\"Sub-Saharan Africa\", \"The West\"))))\r\n\r\nfilter(gapminder, year%in%c(1962, 2013) & !is.na(group) &\r\n         !is.na(fertility) & !is.na(life_expectancy)) %>%\r\n  mutate(population_in_millions = population/10^6) %>%\r\n  ggplot( aes(fertility, y=life_expectancy, col = group, size = population_in_millions)) +\r\n  geom_point(alpha = 0.8) +\r\n  guides(size=FALSE) +\r\n  theme(plot.title = element_blank(), legend.title = element_blank()) +\r\n  coord_cartesian(ylim = c(30, 85)) +\r\n  xlab(\"Fertility rate (births per woman)\") +\r\n  ylab(\"Life Expectancy\") +\r\n  geom_text(aes(x=7, y=82, label=year), cex=12, color=\"grey\") +\r\n  facet_grid(. ~ year) +\r\n  theme(strip.background = element_blank(),\r\n        strip.text.x = element_blank(),\r\n        strip.text.y = element_blank(),\r\n   legend.position = \"top\")\r\n\r\n\r\nGegevens over besmettelijke ziekten in de Verenigde Staten\r\nDeze dataset bevat jaarlijkse tellingen voor Hepatitis A, mazelen, bof, pertussis, polio, rodehond en pokken voor de Amerikaanse staten. Originele gegevens met dank aan Tycho Project. Hij gebruikt het om te laten zien hoe men meer dan 2 dimensies kan plotten.\r\n\r\n\r\nlibrary(RColorBrewer)\r\ndata(\"us_contagious_diseases\")\r\nthe_disease <- \"Measles\"\r\nus_contagious_diseases %>%\r\n  filter(!state%in%c(\"Hawaii\",\"Alaska\") & disease ==  the_disease) %>%\r\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %>%\r\n  mutate(state = reorder(state, rate)) %>%\r\n  ggplot(aes(year, state,  fill = rate)) +\r\n  geom_tile(color = \"grey50\") +\r\n  scale_x_continuous(expand=c(0,0)) +\r\n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\r\n  geom_vline(xintercept=1963, col = \"blue\") +\r\n  theme_minimal() +  theme(panel.grid = element_blank()) +\r\n  ggtitle(the_disease) +\r\n  ylab(\"\") +\r\n  xlab(\"\")\r\n\r\n\r\nFivethirtyeight Data van de verkiezingen van 2016\r\nDeze gegevens omvatten de resultaten van de Amerikaanse presidentsverkiezingen van 2016, geaggregeerd door HuffPost Pollster, RealClearPolitics, stembureaus en nieuwsberichten. De dataset bevat ook de verkiezingsresultaten (volksstemming) en de stemmen van de kiescolleges in results_us_election_2016. Hij gebruikt deze dataset om les te geven over inferenties.\r\n\r\n\r\ndata(polls_us_election_2016)\r\npolls_us_election_2016 %>%\r\n  filter(state == \"U.S.\" & enddate>=\"2016-07-01\") %>%\r\n  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%\r\n  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%\r\n  gather(candidate, percentage, -enddate, -pollster) %>% \r\n  mutate(candidate = factor(candidate, levels = c(\"Trump\",\"Clinton\")))%>%\r\n  group_by(pollster) %>%\r\n  filter(n()>=10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(enddate, percentage, color = candidate)) +  \r\n  geom_point(show.legend = FALSE, alpha=0.4)  + \r\n  geom_smooth(method = \"loess\", span = 0.15) +\r\n  scale_y_continuous(limits = c(30,50))\r\n\r\n\r\nStudenten rapporteren lengte\r\nDit zijn zelfgerapporteerde lengtes in inches voor mannen en vrouwen die de afgelopen jaren aan de cursus dataanalyse hebben meegedaan. Hij gebruikt ze voor het onderwijzen van distributies en samenvattende statistieken.\r\n\r\n\r\ndata(\"heights\")\r\nheights %>% \r\n  ggplot(aes(height, fill=sex)) + \r\n  geom_density(alpha = 0.2)\r\n\r\n\r\nDeze data zijn behoorlijk aangepast omdat studenten lengte vaak in andere waarden dan inches rapporteren. De originele vormen staan hier:\r\n\r\n\r\ndata(\"reported_heights\")\r\nreported_heights %>% filter(is.na(as.numeric(height))) %>% select(height) %>% .$height\r\n\r\n [1] \"5' 4\\\"\"                 \"165cm\"                 \r\n [3] \"5'7\"                    \">9000\"                 \r\n [5] \"5'7\\\"\"                  \"5'3\\\"\"                 \r\n [7] \"5 feet and 8.11 inches\" \"5'11\"                  \r\n [9] \"5'9''\"                  \"5'10''\"                \r\n[11] \"5,3\"                    \"6'\"                    \r\n[13] \"6,8\"                    \"5' 10\"                 \r\n[15] \"Five foot eight inches\" \"5'5\\\"\"                 \r\n[17] \"5'2\\\"\"                  \"5,4\"                   \r\n[19] \"5'3\"                    \"5'10''\"                \r\n[21] \"5'3''\"                  \"5'7''\"                 \r\n[23] \"5'12\"                   \"2'33\"                  \r\n[25] \"5'11\"                   \"5'3\\\"\"                 \r\n[27] \"5,8\"                    \"5'6''\"                 \r\n[29] \"5'4\"                    \"1,70\"                  \r\n[31] \"5'7.5''\"                \"5'7.5''\"               \r\n[33] \"5'2\\\"\"                  \"5' 7.78\\\"\"             \r\n[35] \"yyy\"                    \"5'5\"                   \r\n[37] \"5'8\"                    \"5'6\"                   \r\n[39] \"5 feet 7inches\"         \"6*12\"                  \r\n[41] \"5 .11\"                  \"5 11\"                  \r\n[43] \"5'4\"                    \"5'8\\\"\"                 \r\n[45] \"5'5\"                    \"5'7\"                   \r\n[47] \"5'6\"                    \"5'11\\\"\"                \r\n[49] \"5'7\\\"\"                  \"5'7\"                   \r\n[51] \"5'8\"                    \"5' 11\\\"\"               \r\n[53] \"6'1\\\"\"                  \"69\\\"\"                  \r\n[55] \"5' 7\\\"\"                 \"5'10''\"                \r\n[57] \"5'10\"                   \"5'10\"                  \r\n[59] \"5ft 9 inches\"           \"5 ft 9 inches\"         \r\n[61] \"5'2\"                    \"5'11\"                  \r\n[63] \"5'11''\"                 \"5'8\\\"\"                 \r\n[65] \"708,661\"                \"5 feet 6 inches\"       \r\n[67] \"5'10''\"                 \"5'8\"                   \r\n[69] \"6'3\\\"\"                  \"649,606\"               \r\n[71] \"728,346\"                \"6 04\"                  \r\n[73] \"5'9\"                    \"5'5''\"                 \r\n[75] \"5'7\\\"\"                  \"6'4\\\"\"                 \r\n[77] \"5'4\"                    \"170 cm\"                \r\n[79] \"7,283,465\"              \"5'6\"                   \r\n[81] \"5'6\"                   \r\n\r\nZe gebruiken het vaak om het string proces en regex uit te leggen.\r\nMargarine en het niveau van scheiden\r\nTot slot is hier een gek voorbeeld van de website Spurious Correlations dat hij gebruikt als hij wil uitleggen dat correlatie niet te verwarren is met oorzaak.\r\n\r\n\r\nthe_title <- paste(\"Correlation =\",\r\n                round(with(divorce_margarine,\r\n                           cor(margarine_consumption_per_capita, divorce_rate_maine)),2))\r\ndata(divorce_margarine)\r\ndivorce_margarine %>%\r\n  ggplot(aes(margarine_consumption_per_capita, divorce_rate_maine)) +\r\n  geom_point(cex=3) +\r\n  geom_smooth(method = \"lm\") +\r\n  ggtitle(the_title) +\r\n  xlab(\"Margarine Consumption per Capita (lbs)\") +\r\n  ylab(\"Divorce rate in Maine (per 1000)\")\r\n\r\n\r\nUitbreiding in 2019\r\nZe hebben het dslabs-pakket, dat ze eerder introduceerden als een pakket met realistische, interessante en toegankelijke datasets die gebruikt kunnen worden in inleidende datawetenschappelijke cursussen, in 2019 uitgebreid. Deze nieuwe uitgave heeft nog eens zeven nieuwe datasets toegevoegd, met data over klimaatverandering, astronomie, levensverwachting en borstkankerdiagnose. Ze worden gebruikt in verbeterde probleemsets en nieuwe projecten binnen het HarvardX Data Science Professional Certificate Program, dat beginners R-programmering aanleert, maar ook laat werken met datavisualisatie, dataverwerking, statistiek en machine learning zonder dat ze een coderings- of programmeringsachtergrond hebben.\r\nHet dslabs-pakket is al geinstalleerd. Om verder te gaan is het ook nodig om de volgende pakketten en opties te installeren.\r\n\r\n\r\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\r\nif(!require(\"ggrepel\")) install.packages(\"ggrepel\")\r\nif(!require(\"matrixStats\")) install.packages(\"matrixStats\")\r\n\r\nEn daarna actief te maken:\r\n\r\n\r\n# load libraries\r\nlibrary(tidyverse)\r\nlibrary(ggrepel)\r\nlibrary(matrixStats)\r\n\r\nHaal ook een kleurenpakket binnen als je wilt:\r\n\r\n\r\n# set colorblind-friendly color palette\r\ncolorblind_palette <- c(\"black\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\r\n                        \"#CC79A7\", \"#F0E442\", \"#0072B2\", \"#D55E00\")\r\n\r\nKlimaatverandering\r\nDrie datasets met betrekking tot klimaatverandering worden gebruikt om datavisualisatie en dataverwerking te leren. Deze gegevens leveren duidelijke plots op die een toename van de temperatuur, het broeikasgasniveau en de koolstofuitstoot van 800.000 jaar geleden tot de moderne tijd laten zien. Studenten kunnen hun eigen impactvolle visualisaties maken met echte atmosferische en ijskernmetingen.\r\nModerne temperatuur anomalie en koolstofdioxide data: temp_carbon\r\nDe temp_carbon dataset bevat jaarlijkse globale temperatuurafwijkingen in graden Celsius ten opzichte van de 20e eeuwse gemiddelde temperatuur van 1880-2018. De temperatuurafwijkingen boven land en boven de oceaan worden ook gerapporteerd. Daarnaast zijn de jaarlijkse koolstofemissies (in miljoenen tonnen) van 1751-2014 opgenomen. De temperatuurafwijkingen zijn afkomstig van NOAA en de koolstofuitstoot van Boden et al., 2017 via CDIAC.\r\n\r\n\r\ndata(temp_carbon)\r\n\r\n\r\n\r\n# line plot of annual global, land and ocean temperature anomalies since 1880\r\ntemp_carbon %>%\r\n    select(Year = year, Global = temp_anomaly, Land = land_anomaly, Ocean = ocean_anomaly) %>%\r\n    gather(Region, Temp_anomaly, Global:Ocean) %>%\r\n    ggplot(aes(Year, Temp_anomaly, col = Region)) +\r\n    geom_line(size = 1) +\r\n    geom_hline(aes(yintercept = 0), col = colorblind_palette[8], lty = 2) +\r\n    geom_label(aes(x = 2005, y = -.08), col = colorblind_palette[8], \r\n               label = \"20th century mean\", size = 4) +\r\n    ylab(\"Temperature anomaly (degrees C)\") +\r\n    xlim(c(1880, 2018)) +\r\n    scale_color_manual(values = colorblind_palette) +\r\n    ggtitle(\"Temperature anomaly relative to 20th century mean, 1880-2018\")\r\n\r\n\r\nBroeikasgasconcentraties over 2000 jaar: broeikasgassen\r\nHet gegevensframe voor broeikasgassen bevat vanaf 0-2000 CE elke 20 jaar kooldioxide (CO2, ppm), methaan (CO2, ppb) en lachgas (N2O, ppb) concentraties. De gegevens zijn een subset van ijskernmetingen van MacFarling Meure et al., 2006 via NOAA. Er is een duidelijke toename van alle 3 de gassen vanaf het Industriële Revolutietijdperk.\r\n\r\n\r\ndata(greenhouse_gases)\r\n\r\n# line plots of atmospheric concentrations of the three major greenhouse gases since 0 CE\r\ngreenhouse_gases %>%\r\n    ggplot(aes(year, concentration)) +\r\n    geom_line() +\r\n    facet_grid(gas ~ ., scales = \"free\") +\r\n    xlab(\"Year\") +\r\n    ylab(\"Concentration (CH4/N2O ppb, CO2 ppm)\") +\r\n    ggtitle(\"Atmospheric greenhouse gas concentration by year, 0-2000 CE\")\r\n\r\n\r\nVergelijk dit patroon met de door de mens veroorzaakte koolstofuitstoot sinds 1751 uit temp_carbon, die op vergelijkbare wijze is gestegen:\r\n\r\n\r\n# line plot of anthropogenic carbon emissions over 250+ years\r\ntemp_carbon %>%\r\n    ggplot(aes(year, carbon_emissions)) +\r\n    geom_line() +\r\n    xlab(\"Year\") +\r\n    ylab(\"Carbon emissions (metric tons)\") +\r\n    ggtitle(\"Annual global carbon emissions, 1751-2014\")\r\n\r\n\r\nCarbon dioxide niveaus over de laatste 800,000 jaren, historic_co2\r\nEen veelvoorkomend argument tegen het bestaan van antropogene klimaatveranderingen is dat de aarde van nature cycli van opwarming en afkoeling ondergaat die worden beheerst door natuurlijke veranderingen die buiten de macht van de mens liggen. CO2-niveaus van ijskernen en moderne atmosferische metingen in het Mauna Loa-observatorium tonen aan dat de snelheid en de omvang van natuurlijke variaties in broeikasgassen verbleken in vergelijking met de snelle veranderingen in de moderne industriële tijd. Terwijl de planeet in het verre verleden warmer was en hogere CO2-niveaus had (gegevens niet getoond), laat de huidige ongekende snelheid van verandering weinig tijd voor planetaire systemen om zich aan te passen.\r\n\r\n\r\ndata(historic_co2)\r\n\r\n# line plot of atmospheric CO2 concentration over 800K years, colored by data source\r\nhistoric_co2 %>%\r\n    ggplot(aes(year, co2, col = source)) +\r\n    geom_line() +\r\n    ylab(\"CO2 (ppm)\") +\r\n    scale_color_manual(values = colorblind_palette[7:8]) +\r\n    ggtitle(\"Atmospheric CO2 concentration, -800,000 BCE to today\")\r\n\r\n\r\nEigenschappen van sterren voor het maken van een H-R-diagram: sterren\r\nIn de sterrenkunde worden sterren ingedeeld naar verschillende belangrijke kenmerken, waaronder temperatuur, spectrale klasse (kleur) en lichtkracht (helderheid). Een gemeenschappelijke plot voor het demonstreren van de verschillende groepen sterren en hun interpretaties is het Hertzsprung-Russell-diagram, of H-R-diagram. Het gegevensframe van de sterren verzamelt informatie voor het maken van een H-R-diagram met ongeveer 100 genoemde sterren, inclusief hun temperatuur, spectrale klasse en magnitude (die omgekeerd evenredig is met de lichtkracht).\r\nHet H-R-diagram heeft de heetste, helderste sterren linksboven en de koudste, zwakste sterren rechtsonder. Hoofdreekssterren staan langs de hoofddiagonaal, terwijl reuzen rechtsboven staan en dwergen linksonder. Met deze gegevens kunnen verschillende aspecten van de datavisualisatie geoefend worden.\r\n\r\n\r\ndata(stars)\r\n\r\n# H-R diagram color-coded by spectral class\r\nstars %>%\r\n    mutate(type = factor(type, levels = c(\"O\", \"B\", \"DB\", \"A\", \"DA\", \"DF\", \"F\", \"G\", \"K\", \"M\")),\r\n           star = ifelse(star %in% c(\"Sun\", \"Polaris\", \"Betelgeuse\", \"Deneb\",\r\n                                     \"Regulus\", \"*SiriusB\", \"Alnitak\", \"*ProximaCentauri\"),\r\n                         as.character(star), NA)) %>%\r\n    ggplot(aes(log10(temp), magnitude, col = type)) +\r\n    geom_point() +\r\n    geom_label_repel(aes(label = star)) +\r\n    scale_x_reverse() +\r\n    scale_y_reverse() +\r\n    xlab(\"Temperature (log10 degrees K)\") +\r\n    ylab(\"Magnitude\") +\r\n    labs(color = \"Spectral class\") +\r\n    ggtitle(\"H-R diagram of selected stars\")\r\n\r\n\r\nLevenstabellen van de Verenigde Staten: death_prob\r\nDe levenstabel voor de periode 2015, die is verkregen van het Amerikaanse Ministerie Sociale zekerheid, vermeldt de kans op overlijden binnen een jaar op elke leeftijd en voor beide geslachten. Deze waarden worden vaak gebruikt om levensverzekeringspremies te berekenen. Ze kunnen worden gebruikt voor oefeningen over waarschijnlijkheid en willekeurige variabelen. De premies kunnen bijvoorbeeld worden berekend met een soortgelijke benadering als die welke wordt gebruikt voor de rentevoeten in de casestudie over The Big Short in Rafael Irizarry’s Introduction to Data Science-boek.\r\nBrexit stemdata: brexit_polls\r\nbrexit_polls bevat stempercentages en verdelingen van de zes maanden voorafgaand aan het Brexit EU-lidmaatschapsreferendum in 2016 samengesteld uit Wikipedia. Deze kunnen worden gebruikt om een verscheidenheid aan inferentie- en modelleringsconcepten te oefenen, waaronder betrouwbaarheidsintervallen, p-waarden, hiërarchische modellen en voorspellingen.\r\n\r\n\r\ndata(brexit_polls)\r\n\r\n# plot of Brexit referendum polling spread between \"Remain\" and \"Leave\" over time\r\nbrexit_polls %>%\r\n    ggplot(aes(enddate, spread, color = poll_type)) +\r\n    geom_hline(aes(yintercept = -.038, color = \"Actual spread\")) +\r\n    geom_smooth(method = \"loess\", span = 0.4) +\r\n    geom_point() +\r\n    scale_color_manual(values = colorblind_palette[1:3]) +\r\n    xlab(\"Poll end date (2016)\") +\r\n    ylab(\"Spread (Proportion Remain - Proportion Leave)\") +\r\n    labs(color = \"Poll type\") +\r\n    ggtitle(\"Spread of Brexit referendum online and telephone polls\")\r\n\r\n\r\n# Borstkanker diagnose voorspelling: brca\r\nDit is de Breast Cancer Wisconsin (Diagnostic) Dataset, een klassieke dataset voor machine learning die classificatie mogelijk maakt van borstlaesie biopsies als kwaadaardig of goedaardig op basis van celkernkenmerken geëxtraheerd uit gedigitaliseerde beelden van fijne naald aspiratie cytologie dia’s. De gegevens zijn geschikt voor de analyse van de belangrijkste componenten en een verscheidenheid aan algoritmen voor machinaal leren. De modellen kunnen worden getraind tot een voorspellende nauwkeurigheid van meer dan 95%.\r\n\r\n\r\n# scale x values\r\nx_centered <- sweep(brca$x, 2, colMeans(brca$x))\r\nx_scaled <- sweep(x_centered, 2, colSds(brca$x), FUN = \"/\")\r\n\r\n# principal component analysis\r\npca <- prcomp(x_scaled) \r\n\r\n# scatterplot of PC2 versus PC1 with an ellipse to show the cluster regions\r\ndata.frame(pca$x[,1:2], type = ifelse(brca$y == \"B\", \"Benign\", \"Malignant\")) %>%\r\n    ggplot(aes(PC1, PC2, color = type)) +\r\n    geom_point() +\r\n    stat_ellipse() +\r\n    ggtitle(\"PCA separates breast biospies into benign and malignant clusters\")\r\n\r\n\r\nTot slot\r\nDe datasets in het dslabs-pkket maken data science onderwijs bruikbaarder door echte en wereldse casestudies en met motiverende voorbeelden.\r\nIs programmeren in R nieuw voor jou en wil je dit leren? Check dan het Data Science Professional Certificaat Programma van Harvard University, onder leiding van Rafael Irizarry!\r\n\r\n\r\n",
    "preview": "posts/2020-05-31-dslabs/dslabs_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-15-beste-boeken-2019/",
    "title": "Beste Boeken 2019",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-15",
    "categories": [],
    "contents": "\r\nBeste boeken van 2019\r\nDecember is de maand van de lijstjes. Net als vele anderen maak ik dan jaarlijstje met, in mijn geval, de beste films, de beste cd’s en de beste boeken. Hier nu ook zo’n lijstje met beste boeken van 2019, maar dan statistische boeken. Op dit terrein verschijnt bijzonder veel en ik wil niet zeggen dat ik op dit terrein alles overzie. Maar hieronder de vijf boeken die mij afgelopen jaar het meeste aanspraken en die ik er nog steeds regelmatig op na sla.\r\nKieran Healey, Data visualization: a practical introduction\r\nDit is een boek over data visualisatie en met name het R-pakket ggplot. Hier (https://kieranhealy.org/publications/dataviz/,) vind je er informatie over. Het is bijzonder duidelijk geschreven. Aan de hand van het maken van figuren, grafieken en kaarten leer je heel goed met R en RStudio om te gaan. Lees dit boek aandachtig en maak de kaarten zelf die Kieran Healey in dit boek maakt. Het boek is met plezier en gemak geschreven. Ik schreef er eerder een kort stukje over ().\r\nRobin Lovelace, Jakob Novosad en Jannes Muenchow, Geocomputation with R\r\nDat boek vind je hier (https://kieranhealy.org/publications/dataviz/). Deze drie wetenschappers hebben in twee, drie jaar dit boek geschreven over het maken van geografische kaarten. De theorie en de praktijk worden heel goed door deze drie jonge wetenschappers besproken. Op internet kun je hun syntaxen vinden en verschillende presentaties die ze hierover hebben gegeven. R is een gratis programma dat tegenwoordig de concurrentie aan kan met dure GIS-programma’s. Net als het boek van Healey is ook dit boek met plezier en gemak geschreven en het boek duwt je in de tijd vooruit. Ook over dit boek schreef ik eerder een blog ().\r\nRafael Irizarry, Introduction to Data Science. Data Analysis and Prediction Algorithms with R\r\nRafael Irizarry is epidemioloog van de Harvard University en geeft uitgebreide collegereeksen over moderne data-analyse. Als ik begin 20 zou zijn en waar ook een cursus mocht uitzoeken zou ik misschien wel zijn cursus uit hebben gekozen. Op basis van de introductiecursus gaf hij dit open-source boek uit (https://kieranhealy.org/publications/dataviz/). Hij laat zien hoe je data analyse uitvoert en wat er bij komt kijken aan voorbereiden, opschonen, visualiseren, modelleren en communiceren. Hij heeft aandacht voor standaard statistiek maar ook voor waarschijnlijkheidsleer en machinelearning. Daarnaast besteedt hij aandacht aan allerlei aanvullende zaken die hierbij om de hoek komen kijken als het gebruik van GitHUB, basis computerkennis en reproduceerbaar onderzoek. Een heel goed studieboek over moderne data-analyse in deze tijd.\r\nDanielle Navarro, Learning Statistics with R\r\nDanielle Navarro geeft al jarenlang statistiek aan studenten psychologie van de Universiteit van Adeleide (Australië). Zij heeft haar boek dat ze hierbij gebruikt ook toegankelijk gemaakt voor anderen en je vindt het hier (https://learningstatisticswithr.com/). Dat las ik tegen het einde van het jaar. Verschillende aspecten van de statistiek worden uitgelegd en als je dit boek hebt doorgewerkt, kun je goed artikelen begrijpen en ook analyses uitvoeren. Daarnaast besteedt zij aandacht aan Bayesiaanse statistiek. Deze vorm van statistiek ligt haar beter en aan het einde legt zij heel goed uit hoe dit werkt. Dat hoofdstuk heb ik hier vertaald ()\r\nRussel Poldrock, Statistical Thinking for the 21st Century\r\nDit is het boek dat ik recent las en dat mij bijzonder aanspreekt. Het is een heerlijk tegendraads boek van iemand die les geeft over statistiek en niet tevreden was met het studieboek dat hij gebruikte. Dat kon hij beter. Hij is een Bayesiaan en bouwt zijn boek vanuit dit raamwerk op. Voor hem is deze vorm van statistiek de statistiek van deze eeuw. Verschillende aspecten die je in gewone statistiek boeken tegenkomt, bespreek hij ook maar dan vanuit dat 21eeuwse perspectief. Net als de andere boeken laat hij ook zien hoe je dit het allemaal uitvoert (zowel het schrijven met wiskundige tekens, tabellen maken als de modellen maken). Bijzonder leerzaam en dit boek zal ik er nog vaak op naslaan. Dit boek vind je hier (http://statsthinking21.org/\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-15-r-als-een-gis/",
    "title": "R als een Gis",
    "description": "Over ruimtelijke data en het gebruik van R als een GIS",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-15",
    "categories": [],
    "contents": "\r\nIntroduction to Spatial Data & Using R as a GIS\r\nDit is een bewerking en verkorte versie van de tutorial die Nick Bearman eerder schreef (Introduction to Spatial Data & Using R as a GIS) en die vrij toegankelijk is hier. De data die in deze tutorial worden gebruikt zijn eigen data of open data ook om de techniek goed onder de knie te krijgen. Dank je wel Nick Bearman.\r\nEindtermen: R Functies & Bibliotheken:\r\nR gebruiken om CSV data in te lezen: read.csv()\r\nR gebruiken om ruimtelijke gegevens in te lezen: st_read()\r\nWeten hoe ruimtelijke gegevens te plotten met behulp van R: qtm() & tm_shape()\r\nWeten hoe je kleuren en classificaties moet aanpassen: style\r\nBegrijpen hoe je loops moet gebruiken om meerdere kaarten te maken: for(){}\r\nWeten hoe je ruimtelijke gegevens opnieuw geprojecteerd krijgt: st_transform()\r\nIn staat zijn om punten te gebruiken in veelhoekanalyse: poly.counts()\r\nWeten hoe je shapefiles moet opslaan: st_write()\r\nIntro op R & GIS\r\nR Basis\r\nR begon als een statistisch programma en wordt nog steeds door veel gebruikers als een programma gebruikt. We gaan een programma gebruiken dat RStudio heet, dat bovenop R werkt en een goede gebruikersinterface biedt. Ik zal het in de presentatie even hebben over RStudio, en de belangrijkste gebieden van het venster zijn op de achterzijde gemarkeerd.\r\nOpen RStudio (klik op Start en typ RStudio in of dubbelklik op het icoontje op het bureaublad). R kan in eerste instantie als rekenmachine worden gebruikt - voer het volgende in de linkerkant van het venster in - het gedeelte met de titel Console:\r\n\r\n\r\n[1] 14\r\n\r\nMaak je voorlopig geen zorgen over de [1] - let wel dat R 14 heeft afgedrukt, want dit is het antwoord op de som die je hebt ingetikt. In deze werkbladen laat ik soms de resultaten zien van wat je hebt ingetypt, zoals hieronder:\r\n\r\n\r\n[1] 20\r\n\r\nMerk ook op dat * hier het symbool voor vermenigvuldiging is - in het laatste commando vroeg R om de berekening 5 maal 4 uit te voeren. Andere symbolen zijn - voor aftrekken en / voor delen:\r\n\r\n\r\n[1] -2\r\n\r\n\r\n\r\n[1] 0.3529412\r\n\r\nJe kunt de antwoorden van de berekeningen ook toewijzen aan variabelen en gebruiken in berekeningen.\r\n\r\n\r\n\r\nHier wordt de waarde 300 opgeslagen in de variabele prijs. Het <- symbool betekent dat de waarde rechts in de variabele links in de variabele wordt gezet, deze wordt getypt met een << gevolgd door een -. De variabelen worden getoond in het venster met de naam Environment, rechtsboven in het venster. Variabelen kunnen gebruikt worden in volgende berekeningen. Om bijvoorbeeld een korting van 20% op deze prijs toe te passen, kunt je het volgende invoeren:\r\n#\r\n\r\n\r\n[1] 240\r\n\r\nof gebruik tussenvariabelen:\r\n\r\n\r\n[1] 240\r\n\r\nR kan ook werken met lijsten met nummers, maar ook met individuele nummers. Lijsten worden gespecificeerd met behulp van de c-functie. Stel dat je een lijst hebt met huizenprijzen in duizenden euro’s. Je zou ze kunnen opslaan in een variabele die house.prices genoemd wordt, zoals hieronder:\r\n\r\n\r\n[1] 120 150 212  99 199 299 159\r\n\r\nMerk op dat er geen probleem is met punten in het midden van variabelenamen. U kunt dan functies toepassen op deze lijsten.\r\n\r\n\r\n[1] 176.8571\r\n\r\nAls de huizenprijzen in duizenden euro’s zijn, dan zegt dit ons dat de gemiddelde huizenprijs 176.900 EURO bedraagt. Merk op dat het antwoord op jouw scherm meer cijfers kan weergegeven. Dus je kunt iets als 176.8571429 voor gemiddelde waarde hebben.\r\nHet Dataframe\r\nR heeft een manier om gegevens op te slaan in een object dat een dataframe wordt genoemd. Dit lijkt op een interne spreadsheet.\r\n\r\n\r\n[1] 240\r\n\r\nWaar alle relevante gegevenselementen samen als een set kolommen worden opgeslagen.\r\nWe hebben een CSV-bestand van huizenprijzen en inbraakcijfers, dat we in R kunnen laden. We kunnen gebruik maken van een functie genaamd read.csv die, zoals je misschien wel kunt bedenken, CSV-bestanden leest. Voer de onderstaande coderegel uit, die het CSV-bestand in een variabele met de naam hp.data laadt.\r\nRotterdam\r\n\r\n\r\n\r\nAls we de gegevens inlezen, is het altijd een goed idee om te controleren of ze goed zijn binnengekomen. Om dit te doen, kunnen we een voorbeeld van de dataset bekijken. Het head-commando toont de eerste 6 rijen van de data.\r\nHieronder lezen we dan ons databestand in.\r\n\r\n\r\n# A tibble: 6 x 2\r\n  buurtenrotterdam            NNGB\r\n  <chr>                      <dbl>\r\n1 Afrikaanderwijk             0.86\r\n2 Agniesebuurt               NA   \r\n3 Bedrijvenpark Noord_West   NA   \r\n4 Bedrijventerrein Schieveen NA   \r\n5 Bergpolder                 NA   \r\n6 Beverwaard                  0.78\r\n\r\nJe kunt ook op de variabele in het venster Environment klikken, die de gegevens in een nieuw tabblad zal tonen. Je kunt ook zelf invoeren en een tabblad openen met de gegevens:\r\n\r\n#Probeer onderstaande, hier niet afgedrukt want dit wordt te lang\r\nView(RotterdamStaat)\r\nJe kunt ook elke kolom in de dataset beschrijven met behulp van de summary-functie:\r\nItem Beschrijving\r\nMin. De kleinste waarde in de kolom 1st. Qu. Het eerste kwartiel (de waarde 1/4 van de variabele) Median De mediaan (de waarde 1/2 van de variabele) Mean Het gemiddelde van de kolom 3rd. Qu. Het derde kwartiel (de waarde 3/4 van de variabele) Max. De hoogste waarde in de kolom\r\n\r\n\r\n buurtenrotterdam        NNGB       \r\n Length:93          Min.   :0.6500  \r\n Class :character   1st Qu.:0.7650  \r\n Mode  :character   Median :0.8000  \r\n                    Mean   :0.8006  \r\n                    3rd Qu.:0.8400  \r\n                    Max.   :0.9000  \r\n                    NA's   :30      \r\n\r\nVoor elke kolom wordt een aantal waarden genoemd:\r\nOp basis van deze getallen kan een indruk worden verkregen van de spreiding van de waarden van elke variabele. Met name kan worden vastgesteld dat de mediaan van de huizenprijs in St. Helens per wijk varieert van 65.000 EURO tot 260.000 EURO en dat de helft van de prijzen tussen 152.500 EURO en 210.000 EURO ligt. Ook kan worden vastgesteld dat, aangezien de mediaan van het gemeten inbraakpercentage nul is, ten minste de helft van de gebieden geen inbraken had in de maand waarin de tellingen werden samengesteld..\r\nWe kunnen vierkante haken gebruiken om specifieke delen van het dataframe te bekijken, bijvoorbeeld hp.data[1,] of hp.data[,1]. We kunnen ook kolommen verwijderen en nieuwe kolommen aanmaken met behulp van de onderstaande code. Vergeet niet om het head() commando te gebruiken zoals we eerder deden om naar het dataframe te kijken.\r\n\r\n\r\n# A tibble: 6 x 3\r\n  buurtenrotterdam            NNGB counciltax\r\n  <chr>                      <dbl> <lgl>     \r\n1 Afrikaanderwijk             0.86 NA        \r\n2 Agniesebuurt               NA    NA        \r\n3 Bedrijvenpark Noord_West   NA    NA        \r\n4 Bedrijventerrein Schieveen NA    NA        \r\n5 Bergpolder                 NA    NA        \r\n6 Beverwaard                  0.78 NA        \r\n\r\n\r\n\r\n# A tibble: 6 x 3\r\n  buurtenrotterdam            NNGB `Price-thousands`\r\n  <chr>                      <dbl> <lgl>            \r\n1 Afrikaanderwijk             0.86 NA               \r\n2 Agniesebuurt               NA    NA               \r\n3 Bedrijvenpark Noord_West   NA    NA               \r\n4 Bedrijventerrein Schieveen NA    NA               \r\n5 Bergpolder                 NA    NA               \r\n6 Beverwaard                  0.78 NA               \r\n\r\nGeograpfische Informatie\r\nR heeft zich ontwikkeld tot een GIS waar gebruikers aan hebben bijgedragen met pakketten, of ‘libraries’ zoals R ze noemt. We zullen in de tutorial verschillende van dit soort ‘libraries’ gebruiken en zullen ze laden als dat nodig is.\r\nAls u uw computer gebruikt, moet u de R-libraries installeren en ze ook laden. Om dit te doen, start u install.packages (“library_name”).\r\nOm met ruimtelijke gegevens te kunnen werken, moeten we na dat installeren een aantal ‘libraries’ laden>\r\n\r\n\r\n\r\nOm met ruimtelijke gegevens te werken, moeten we enkele libraries laden. Daarmee is R echter alleen maar in staat om geografische data te verwerken. Het laadt nog geen specifieke data sets. Om dit te doen, moeten we enkele gegevens inlezen. Hiervoor gaan we shapefiles gebruiken - een bekend GIS-dataformat. We gaan LSOA(Lower layer Super Output Areas)-data gebruiken voor St. Helens in Merseyside.\r\nR gebruikt werkmappen om informatie op te slaan die relevant is voor het huidige project waaraan je werkt. Ik stel voor dat je een map een bepaalde naam geeft die het R-werk ergens zinvol maakt. Dan moeten we R vertellen waar deze map staat, dus klik op Session > Set Working Directory > Choose Directory. . en selecteer de map die je hebt aangemaakt.\r\nZoals met de meeste programma’s, zijn er meerdere manieren om dingen te doen. Om bijvoorbeeld de werkmap in te stellen, kunnen we het volgende typen: setwd(“M:/R_werk”). Jouw versie kan een langere titel hebben, afhankelijk van hoe je de map noemt. Merk ook op dat schuine streepjes worden aangegeven met een ‘/’ en niet ’'.\r\nEr is een set van shapefiles voor de St. Helens-wijken op dezelfde locatie als de dataset die je eerder hebt gelezen. Omdat er meerdere bestanden nodig zijn, heb ik deze in één zip-bestand gebundeld. Deze download je naar jouw lokale map en pakt deze vervolgens uit. Dit doe je met de volgende R-functies:\r\n\r\n\r\n\r\nDe eerste functie downloadt het zip-bestand daadwerkelijk in uw werkmap. De tweede functie pakt het zip-bestand uit. Nu kunnen we het bestand in R lezen.\r\n\r\n\r\nReading layer `wijkindeling' from data source `C:\\HARRIE\\Tijdelijk\\Git\\HarriesHoekje\\_posts\\2019-12-15-r-als-een-gis\\wijkindeling.shp' using driver `ESRI Shapefile'\r\nSimple feature collection with 85 features and 14 fields\r\ngeometry type:  POLYGON\r\ndimension:      XY\r\nbbox:           xmin: 55500 ymin: 428647.4 xmax: 101032.6 ymax: 447000\r\nepsg (SRID):    NA\r\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs\r\n\r\nDe st_read functie doet dit en slaat ze op als een Simple Features (of sf) object. Je kunt de qtm-functie gebruiken om de polygonen (d.w.z. de kaart van de LSOA) te tekenen.\r\n\r\n\r\n\r\nWe kunnen ook het head()-commando gebruiken om de eerste zes rijen te tonen, precies hetzelfde als bij een data frame.\r\n\r\n\r\nSimple feature collection with 6 features and 14 fields\r\ngeometry type:  POLYGON\r\ndimension:      XY\r\nbbox:           xmin: 87699.71 ymin: 433848.1 xmax: 96594.48 ymax: 440401.8\r\nepsg (SRID):    NA\r\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs\r\n  GROEP CODE GEMEENTE GEBIED BUURT SUBBUURT SBTDEEL BLOK TEKST\r\n1   TIR   15      599     10    79       -1      -1   -1  1079\r\n2   TIR   15      599      3    27       -1      -1   -1  0327\r\n3   TIR   15      599     19    26       -1      -1   -1  1926\r\n4   TIR   15      599      3    22       -1      -1   -1  0322\r\n5   TIR   15      599      8    42       -1      -1   -1  0842\r\n6   TIR   15      599     10    82       -1      -1   -1  1082\r\n             GEBDNAAM                    BUURTNAAM wijknr Shape_Leng\r\n1          Feijenoord      Kop van Zuid - Entrepot     79   4134.193\r\n2          Delfshaven    Oud Mathenesse/Witte Dorp   3004   4828.106\r\n3    Nieuw Mathenesse             Nieuw Mathenesse      0   6494.563\r\n4          Delfshaven                 Tussendijken     22   2853.695\r\n5 Kralingen-Crooswijk Kralingen Oost/Kralingse Bos   3005  12400.652\r\n6          Feijenoord                   Hillesluis     82   4786.269\r\n  Shape_Area                       geometry\r\n1   671847.0 POLYGON ((93530.4 436049.7,...\r\n2   880203.0 POLYGON ((89592.66 437446.3...\r\n3  2071912.9 POLYGON ((87705.53 436406.7...\r\n4   399761.8 POLYGON ((89484.61 436419.5...\r\n5  6521950.1 POLYGON ((95980.45 440392, ...\r\n6   902617.3 POLYGON ((94062.15 434700.2...\r\n\r\nVoor degene die met GIS werkt: Dit is hetzelfde als de attribuutentententabel in programma’s als ArcGIS, QGIS of MapInfo. Als u het shapefile in QGIS of ArcGIS wilt openen om vast te stellen hoe het er zo’n beetje uit ziet, kunt u dat doen.\r\nJe kunt zien dat er veel informatie beschikbaar is, inclusief de geometrie. Voor ons is het ID-veld belangrijk, en zien dat dit overeenkomt met het ID-veld in het hp.data bestand. We kunnen dit gebruiken om de twee datasets samen te voegen om de inbraakgegevens op de kaart te tonen.\r\nHet idee is dat er in elke dataset een veld is dat we kunnen gebruiken om de twee samen te voegen; in dit geval hebben we het ID-veld in sthelens en het ID-veld in hp.data.\r\n\r\n\r\n\r\n\r\n\r\n\r\nGebruik de head-functie om te controleren of de gegevens correct zijn samengevoegd.\r\n\r\n\r\nSimple feature collection with 6 features and 16 fields\r\ngeometry type:  POLYGON\r\ndimension:      XY\r\nbbox:           xmin: 88174.91 ymin: 432495.7 xmax: 99128.64 ymax: 442409.5\r\nepsg (SRID):    NA\r\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs\r\n                   BUURTNAAM GROEP CODE GEMEENTE GEBIED BUURT\r\n1            Afrikaanderwijk   TIR   15      599     10    86\r\n2               Agniesebuurt   TIR   15      599      5    15\r\n3 Bedrijventerrein Schieveen   TIR   15      599     26    50\r\n4                 Bergpolder   TIR   15      599      5    31\r\n5                 Beverwaard   TIR   15      599     12    90\r\n6                   Bloemhof   TIR   15      599     10    81\r\n  SUBBUURT SBTDEEL BLOK TEKST                   GEBDNAAM wijknr\r\n1       -1      -1   -1  1086                 Feijenoord     86\r\n2       -1      -1   -1  0515                      Noord     15\r\n3       -1      -1   -1  2650 Bedrijventerrein Schieveen      0\r\n4       -1      -1   -1  0531                      Noord     31\r\n5       -1      -1   -1  1290                IJsselmonde     90\r\n6       -1      -1   -1  1081                 Feijenoord     81\r\n  Shape_Leng Shape_Area NNGB Price-thousands\r\n1   3461.527   621148.9 0.86              NA\r\n2   2809.173   384570.5   NA              NA\r\n3   5036.165  1518931.1   NA              NA\r\n4   3253.672   454195.3   NA              NA\r\n5   6592.857  1514886.2 0.78              NA\r\n6   3613.866   788457.4 0.81              NA\r\n                        geometry\r\n1 POLYGON ((93595.07 434773.5...\r\n2 POLYGON ((91881.82 438449.9...\r\n3 POLYGON ((88174.91 441742.2...\r\n4 POLYGON ((91216.07 439018.2...\r\n5 POLYGON ((97775.88 434798.3...\r\n6 POLYGON ((93506.9 434355.2,...\r\n\r\nNu we de gegevens hebben samengevoegd, kunnen we een kaart maken van deze huizen-prijzen.\r\n\r\n\r\n\r\nDit is een zeer snelle manier om een kaart met R te maken. Om de kaart te gebruiken, klikt u op de Export-knop en kiest u vervolgens voor Copy naar Clipboard. . . . Kies vervolgens Copy Plot. Als je ook Word hebt, kun je de kaart in je document plakken. Je kunt de kaart ook opslaan als Afbeelding of PDF.\r\nEen Kaart maken Census Data\r\nWerken met R vereist vaak meerdere coderegels code om een output te krijgen. In plaats van de code in de Console in te typen, kunnen we in plaats daarvan een script gebruiken. Daar kunnen we altijd naar teruggaan en de code zeer eenvoudig te bewerken, om fouten te corrigeren!\r\nMaak een nieuw script aan (File > New File > R-script) en voer de code daar in. Vervolgens kunt je de regels die je wilt uitvoeren selecteren door ze te markeren en vervolgens op Ctrl+Enter te drukken, of door de Run knop bovenaan te gebruiken.\r\nNu gaan we hetzelfde principe gebruiken als voorheen om een kaart te maken van enkele gegevens uit 2018. We moeten de gegevens eerst downloaden.\r\nGa naar https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/wijk-en-buurtstatistieken/kerncijfers-wijken-en-buurten-2004-2019.\r\nOpen vervolgens https://www.cbs.nl/nl-nl/maatwerk/2018/30/kerncijfers-wijken-en-buurten-2018.\r\nsla het bestand kwb-2018.xls op in map Buurtexcel.\r\nOmdat de bestanden te zwaar zijn, vind je de informatie hierover niet hier. In het pdf bestand kun je hier wel over lezen (zie in deze map WerkdocumentNEDoriginal.pdf).\r\nReferenties\r\nNick Bearman. Introduction to Spatial Data & Using R as a GIS. https://github.com/nickbearman/intro-r-spatial-analysis/blob/master/workbook.pdf\r\nLovelace, R., Nowosad, J. and Muenchow, J. Geocomputation with R. https://geocompr.github.io/\r\nDeze ‘practical’ is geschreven met R 3.5.1 (2018-07-02) en RStudio 1.1.463 door Dr. Nick Bearman (nick@ geospatialtrainingsolutions.co.uk).\r\nHet werk is gelicenseerd onder Creative Commons Attribution-ShareAlike 4.0 International License. Om een kopie van deze licentie te zien, ga dan naar http://creativecommons.org/licenses/by-sa/4.0/deed.en. De laatste PDF-versie kun je hier https://github.com/nickbearman/intro-r-spatial-analysis vinden. Deze versie is op 18 May 2019 gemaakt.\r\n\r\n\r\n",
    "preview": "posts/2019-12-15-r-als-een-gis/r-als-een-gis_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-04-bayesbasis/",
    "title": "Bayes'basis",
    "description": "Over statistiek en waarschijnlijkheid op de eenvoudige manier.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-04",
    "categories": [],
    "contents": "\r\nBayes’ basis\r\nOns leven zit vol onzekerheid, schrijft Will Kurt in zijn nieuwe boek Bayesian statistics the fun away. Understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. Veel gebeurt zoals we ons van te voren hebben bedacht, andere zaken lopen toch weer net een beetje anders. Om met de werkelijkheid om te gaan moeten we wel een gevoel hebben bij het omgaan met onzekerheid in bepaalde situaties. Als we dat systematische willen doen, kan de Bayesiaanse statistiek ons een handje helpen. Het leert ons logisch denken en we kunnen het ook nog mathematisch uitdrukken. Will Kurt helpt mensen bij het omgaan met onzekerheid. De boeken die over Bayesiaanse statistiek zijn geschreven zijn vaak moeilijk, zeker als het een nieuw onderwerp is voor je. De standaardboeken over dit onderwerp, van Gelman, McElreath of Spiegelhalter bijvoorbeeld, zijn vooral weggelegd voor een kleine groep statistisch onderlegde lezers. Kurt heeft een boek voor iedereen voor ogen. Bayesiaanse statistiek zet hij tegenover frequentistische statistiek. Bij frequentische statistiek heeft waarschijnlijkheid met frequentie te maken, bij Bayesiaanse statistiek gaat het om de vraag hoe onzeker we zijn over de informatie waarover we beschikken. Bij Bayesiaanse statistiek beschrijven we zo nauwkeurig mogelijk hoe onzeker we over zaken zijn en gebruiken daarbij specifieke gereedschappen. Hier kijken naar een probleem, lossen we het logisch op, gebruiken bepaalde regels, drukken het mathematisch uit en kijken we vervolgens weer opnieuw naar het probleem. Het boek deelt hij in vier stukken op.\r\nIn het eerste deel laat Kurt ons Bayesiaans denken en alledaags redeneren. Hier introduceert hij Bayesiaans denken en geeft er een overzicht van. Hij laat zien dat Bayesiaans denken heel dicht bij gewoon denken en de dagelijkse problemen ligt. Het is niet anders dan het observeren van data, het formuleren van een hypothese en dan weer het updaten van wat je denkt op basis van de data. Om de wereld te begrijpen, kun je vertrouwen op wat je ziet en de hypothesen die je erover hebt. Met nieuwe gegevens kun je de hypothese aanpassen, waarna je vervolgens beter met de data om kunt gaan. Onzekerheid is te meten en in getallen om te zetten. Iets is weinig of zeer waarschijnlijk en dat kunnen we in mathematische modellen omzetten. Iets wat dichter bij 0 ligt, is waarschijnlijk niet waar en iets dat dicht bij 1 ligt is dat wel. Iets kan niet waar en onwaar tegelijk zijn. \\(P(X) + P_n(X)=1\\) Om te rekenen met waarschijnlijkheid waarin we geloven, moeten we kunnen zeggen hoeveel keer meer we in een bepaalde hypothese geloven dan in een andere hypothese. Daarvoor moeten we met logica kunnen werken en dan zijn drie operators belangrijk: EN, OF, NIET, zoals in de volgende stelling: Ik heb een paraplu nodig als het regent EN ik naar buiten ga of Als het niet regent OF als ik NIET naar buiten ga, heb ik een paraplu NIET nodig. Wat ook belangrijk is, hoe je verschillende waarschijnlijkheden combineert. Als de uitkomsten niet met elkaar te maken hebben (bv. de kans op een kop van een munt en een zes van de dobbelstaan) is \\(P(A,B)=P(A)*P(B)\\). In dat voorbeeld gaat het wel om zaken die elkaar uitsluiten en dat is niet altijd het geval. Je kunt geen kop en een munt gooien EN ook niet een zes en een twee met de dobbelsteen. Maar het wordt anders bij een kop van de munt gooien OF een zes met de dobbelsteen. Dan moeten we alle waarschijnlijkheden optellen en dan de waarschijnlijkheid dat beide tegelijk gebeuren, eraf trekken of te wel: \\(P(A) OR P(B)=P(A)+P(B)-P(A,B)\\) of, zoals in dit voorbeeld: \\[P(kop) OR P(zes)=P(kop,zes)=1/2+1/6-1/12=7/12\\] Het eerste voorbeeld van een mathematisch model dat Kurt geeft, is een binomiale verdeling, waarmee je bepaalde successen in de uitkomst kunt onderzoeken. Bij deze uitkomstmaat heeft iets succes of niet (twee uitkomsten, vandaar: bi).k Is het aantal uitkomsten waar het ons om gaat (bv. k=2, 2 keer kop van de munt)n is het totale aantal (bv. n=3, het aantal keren dat een munt wordt opgegeooid)p is de waarschijnlijkheid dat iets gebeurt.(p=1/2, de waarschijnlijkheid op het gooien van een munt)\r\nUitgedrukt als \\(B(2; 3;1/2)\\), waarbij de B de binomiale verdeling is.\r\nDe tweede verdeling die hij ons geeft is de Beta verdeling, een continue verdeling. Deze gebruik je als je een aantal keren iets hebt geobserveerd en het aantal succesvolle uitkomsten weet, bv. je hebt 41 keer gegooid en je hebt 14x kop gegooid en 27 keer munt. Hier gaat het om \\(alpha,beta,p\\). Hierbij isp de waarschijnlijkheid dat iets gebeurta de keren dat het ook echt gebeurtb de keren dat het niet gebeurt\r\nBayes is vooral interessant bij conditionele waarschijnlijkheid, het ene heeft wel degelijk invloed op het andere, daar gaat het om in het tweede deel. Juist hier is Bayes interessant. Conditionele waarschijnlijkheid drukken we uit als \\(P(A|B)\\), de waarschijnlijkheid van A gegeven B. Dit is belangrijk omdat hier de aanvullende informatie ons geloof in iets beinvloedt. Hij geeft het voorbeeld van kleurenblindheid:\r\nP(kleurenblindheid)=0.00425\r\nP(kleurenblindheid|vrouw)=0.0005\r\nP(kleurenblindheid|man)=0.08\r\nAls je de kans wilt weten op kleurenblindheid van een man, dan is dat: P(man| kleurenblindheid)=P(man) x P(kleurenblindheid|man) 0.5X0.08=0.04.Dit kunnen we ook omdraaien. Ingewikkelder is de vraag: wat is de kans dat de persoon man is gegeven dat je weet dat er sprake is van kleurenblindheid (P(man|kleurenblindheid)=?)\r\n\\[\r\nP(man|kleurenblind)={\\frac{P(man)P(kleurenblind|man)}{P(kleurenblind)}}=0.5X0.08/0.0425=0.941\r\n\\]\r\nHiervoor is de Bayesiaanse wet geintroduceerd die zo goed en makkelijk te gebruiken is:\r\n\\[\r\nP(A,B)={\\frac{P(A)P(B|A)}{P(B)}}\r\n\\] Met deze Bayesiaanse wet nemen we onze kijk op de werkelijkheid en ons geloof daarin als uitgangspunt. Hier combineren data en transformeren dit in een schatting van onze nieuwe kijk gegeven hetgeen we hebben geobserveerd. Met Bayes nemen we deze overtuiging en kwantificeren exact hoe sterk dit bewijs ons denken verandert. Met lego stukje laat Kurt heel concreet zien hoe dit werkt, zoals hij steeds in het boek eenvoudige voorbeelden neemt om duidelijk te maken wat hij bedoelt. Met Bayes heb je drie delen: de prior P(H), de hypothes oftewel hoe de manier waarop we naar de werkelijkheid kijken; de likelihood P(D|H), de data gegeven onze hypothese en de posterior, wat we eigenlijk willen weten: P(H|D), de hypothese gegeven de data die we hebben, de theorie die we op basis van de gegevens opstellen. Deze delen maken tezamen vormen het theorema van Bayes. De prior is natuurlijk het meest controversiele deel van deze wet. Hier gebruiken we de informatie die we al hebben in het schatten van een onzekere situatie. Ook bij de prior gebruik je niet één bepaalde waarde maar vaak ook een verdeling van waarden. Juist om verschillende mogelijkheden mee te kunnen nemen in jouw berekening.\r\nIn het derde deel van het boek gaat hij in op het schatten van parameters. Hij begint met het gemiddelde en dat we dat meten door alle observaties te wegen door de waarschijnlijkheid dat deze observaties voorkomen. Hier is niet veel nieuws onder de zon, behalve dat je wel goed moet weten waar je het over hebt. Datzelfde geldt voor het meten van de spreiding van de data waar hij ook over schrijft. Hij laat zien dat er drie manieren zijn om de spreiding in kaart te brengen: MAD (Mean Absolute Deviation), Variantie en de Standaard Deviatie waar het meest mee gewerkt wordt. Het is goed om niet alleen gemiddelden te kennen maar ook uitspraken te doen over de spreiding omdat deze meestal aangeven hoe zeker of onzeker we over iets zijn. Hoe groter de spreiding, hoe onzekerder we zijn en andersom. In de normale verdeling is de spreiding ook uitgedrukt in standaard deviatie. Hieronder zien we een normale verdeling met een standaard deviatie van 15, relatief veel zekerheid.\r\n\r\n\r\n\r\nEn hier onder een normaal verdeling met standaard deviatie 40, relatief veel onzekerheid.\r\n\r\n\r\n\r\n65 procent van de mogelijke waarden vallen binnen ongeveer een standaardeviatie, 95 procent binnen twee standaarddeviaties en 99.7 procent binnen drie standaard deviatie. Interessanter hier zijn de gereedschappen om de parameters te schatten op een Bayesiaanse manier. Kurt presenteert drie functies en ik hou even de Engelse termen aan: de Probability Density Function (PDF), die ons laat zien hoeveel iets voorkomt. Kurt geeft een voorbeeld. Hij start een email lijst waar je je voor kunt inschrijven. Hij stuurt 40.000 mails uit en 300 mensen geven zich op. Dit kan in een beta-functie worden uitgedrukt\r\n\r\n\r\n\r\nDe Cumulative Distribution Function (CDF) die ons helpt om de waarschijnlijkheid van bepaalde waardes vast te stellen. R geeft ons daar ook het instrumentarium voor. Als we in R de waarschijnlijkheid van Beta(300, 39700) vast willen stellen, uitgaande van kleiner dan 0.0065, kunnen we dat met de volgende formule uitdrukken.\r\n\r\n\r\n[1] 0.007978686\r\n\r\nEn om vast te stellen dat de bevestigingsgraad meer is dan 0.0085 is, schrijven we het volgende:\r\n\r\n\r\n[1] 0.01248151\r\n\r\nHet maakt niet uit of het continu is zoals hierboven of discreet zoals hieronder:\r\n\r\n\r\n[1] 0.8125\r\n\r\nOm bepaalde waarden vast te stellen kunnen we de Kwantielen Functie gebruiken, bv. de mediaan en het interval dat er bij hoort. Of als we de waarde van 99,9 procent en minder willen hebben kunnen we de volgende functie gebruiken. We weten hier voor 99,9 procent zeker dat minder dan 0.0089 zich opgeven voor onze lijst.\r\n\r\n\r\n[1] 0.008903462\r\n\r\nEn ook, weten we voor 95 procent zeker dat tussen de 0.67 procent en 0.84 procent van de mensen zich opgeeft voor de lijst. Dat onderzoeken we op de volgende wijze:\r\n\r\n\r\n[1] 0.006678074\r\n\r\n[1] 0.008368562\r\n\r\nOp deze manier kunnen we parameters en intervallen berekenen die met onze waarden samenhangen. Die schattingen kunnen we ook maken door informatie toe te voegen. We hebben dan niet alleen de data (300, 39700) maar gebruiken ook kennis die we hebben. Dan kunnen we verschillende prior gebruiken die onze kennis uitdrukken. Bijvoorbeeld als we pessimistisch zijn kunnen we de Beta(1,41) gebruiken en die veronderstelt vooral lage waarden of Beta(5,200) als we iets meer geloof in verandering hebben. Of als we niks weten de Beta(1,1). Maar ook als is jouw prior verkeerd, deze wordt door data overruled en zeker als je veel data hebt. Het is jammer dat Kurt niet eenvoudig laat zien hoe je dat doet. Kun je er bij optellen.\r\nHet laatste, vierde deel gaat over de kern van statistiek: het testen van hypothesen. Hij begint met een simpele A/B test, een test om vast te stellen of het ene beter werkt dan het andere. Ook hier laat hij zien hoe je dat kunt doen en hoe je daar simulaties bij kunt gebruiken. De ene groep van 150 ontvangt een email met een grote illustratie erop en de andere niet. Hij wil weten welke groep zich op basis van de email vaker opgeeft\r\nVariant\r\nGeklikt\r\nNiet geklikt\r\nOpgegeven\r\nVariantA\r\n36\r\n114\r\n0.24\r\nVariant B\r\n50\r\n100\r\n0.33\r\nWe weten uit eerder onderzoek dat 3 van 10 mensen zich opgeven en die kennis verwerken we in de prior.\r\n\r\n\r\n[1] 0.95954\r\n\r\nBij 100.000 pogingen was variant B 96 procent beter.\r\nIn dit hoofdstuk presenteert hij ook twee maten: de Bayes Factor and Posterior Odds. Als we twee hypothesen met elkaar willen vergelijken dan vergelijken we in elk geval de prior maal de likelihood en dan krijgen we de ratio van de posteriors als volgt:\r\n\\[\r\n{\\frac{P(H_1)*(P(D|H_1)}{P(H_2)*(P(D|H_2)}}\r\n\\] Dit vergelijkt hoe elke hypothese de data verklaart die we observeren. Stel dat we uitgaan van dezelfde prior houden we het volgende over: \\[\r\n{\\frac{(P(D|H_1)}{(P(D|H_2)}}\r\n\\] Dit is de Bayes factor die we dan overhouden, de ratio tussen de likelihoods van twee hypothesen. Is het meer dan 1, dan is hypothese_1 (H_1) beter, is het kleiner dan 1 dan is H_2 beter. De Prior Odds is de ratio van de waarschijnlijkheid voordat we naar de data hebben gekeken \\({\\frac{P(H_1)}{P(H_2)}}\\). Als we de Bayes Factor en de Prior Odds samen nemen krijven we de posterior odds: \\[posterior odds=O(H_1){\\frac{P(D|H_1)}{P(D|H_2)}}\\]\r\nEn dan is het handig om de volgende regels in het achterhoofd te houden:\r\nPosterior odds\r\nSterkte van het bewijs\r\n1-3\r\nInteressant, geen conclusies\r\n3-20\r\nHet echt ergens op lijken\r\n20-150\r\nSterk bewijs ten faveure H1\r\n>150\r\nOverweldigend bewijs\r\nHij geeft enkele voorbeelden waaronder deze: Stel je wordt op een ochtend wakker en je hebt problemen met horen en een ringtoon (tinnitus) in je oor. Er zijn twee mogelijkheden, twee hypothesen: - Vestibular swannoma en je hebt een tumor die tot oorproblemen leidt (H_1). Hoorproblemen zijn hier 94% en tinnitus is hier 89%: \\(P(D|H1)=0.94*0.89=0.78\\) - Oorsmeer en je oor moet worden uitgespoten (H_2). Hier geldt voor de complicaties: \\(P(D|H1)=0.63*0.55=0.35\\) en de Bayes Factor wordt dan: \\[\r\n{\\frac{(P(D|H_1)}{(P(D|H_2)}}={\\frac{0.78}{0.35}}=2.23\r\n\\] Het lijkt de kans op te gaan van vestibular swannoma maar dat kan niet echt geconcludeerd worden. Maar hier kunnen we er niet vanuit gaan dat de prior in beide geval even groot is. De kans op de tumor is \\[P(H_1)={\\frac{11}{1,000,000}}\\]\r\nEn de kans op gewoon oorsmeer is \\[P(H_2)={\\frac{37,000}{1,000,000}}\\]\r\nDe Odds Priors is dan \\[O(H_1)={\\frac{P(H_1)}{P(H_2)}}={\\frac{11}{37,000}}\\]\r\nAls we nu rekening houden met deze voorkennis en dit met de Bayes Factor vermenigvuldigen krijgen we het volgende resultaat:\r\n\\[\r\nO(H_1*{\\frac{(P(D|H_1)}{(P(D|H_2)}}={\\frac{11}{37,000}}*2.23={\\frac{223}{370,000}}\r\n\\]\r\nHet is 1,659 meer waarschijnlijk dat het met jouw oorsmeer te maken heeft en dat je het best naar de huisarts kunt gaan. En zo werkt hij dit concept in dit hoofdstuk verder uit en krijg je een goed idee wat die Bayes’ basis jou op kan leveren. Soms is het goed om weer terug te grijpen op de basis van een heel goed instrumentarium en Kurt kan jou als lezer heel goed vertellen hoe die basis eruit ziet.\r\nKurt, W. (2019). Bayesian statistics the fun way. Understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. San Francisco: No Starch Press.\r\n\r\n\r\n",
    "preview": "posts/2019-12-04-bayesbasis/bayesbasis_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-09-25-sf-ggplot-en-tmap/",
    "title": "Sf, ggplot en tmap",
    "description": "Over het maken van kaarten van Nederland met nieuwe pakketten en mogelijkheden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-09-25",
    "categories": [],
    "contents": "\r\nDoel\r\nIn deze blog wil ik laten zien hoe je tegenwoordig geografische kaarten in R kunt maken. Het pakket sf (Simple Features) is de moderne standaard daarvoor en is de natuurlijke opvolger van sp. Dit sf pakket kan goed met het databewerkingspakket tidyverse werken en met onderdelen daarvan zoals dplyr en het visualisatiepakket ggplot. Een pakket waar je, zeker in combinatie met sf, goed kaarten mee kunt maken is tmap (statische en interactieve kaarten). Ik had mij bij het maken van deze blog de taak gesteld om te laten zien hoe je Nederland geografisch af kunt beelden. Dat laat ik hieronder zien en daar gebruik ik die moderne R-pakketten voor. Ik kwam er bij het maken ook achter dat je van de open data van het CBS ook makkelijk kaarten van Nederland kunt maken. Dat laat ik je hieronder ook zien. In een latere fase kom ik hier nog eens op terug.\r\nPakketten\r\nEerst maar eens enkele moderne R-pakketten binnenhalen.\r\n\r\n\r\n\r\nGeographic Data Science (GDS) en Geographic Information Science (GISc)\r\nIn een mooie blog laat Katie Jolly het verschil zien tussen Geografische Datawetenschap en Geografische Informatiewetenschap Blog van Katie Jolly. Bij R en Python praat je meer over geografische datawetenschap of geocomputation, zoals Lovelace e.a. dat noemen Geocomputatie. Daarnaast heb je Geografische Informatie Systemen zoals QGIS en ArcMap as GISc (vaak GIS-programma’s genoemd). Een van de grote verschillen, en daar heeft ze gelijk in, is de de reproduceerbaarheid van Geografische Datawetenschap ten opzichte van GIS-programma’s. Met het volgende overzicht, dat zijn ontleent aan Lovelace e.a., maakt zij dat duidelijk:  Deze tabel komt uit Robin Lovelace’s blog-post Can Geographic Data Save the World?.\r\nMet sf in combinatie met tidyverse en enkele specifieke pakketten kom je een heel eind tegenwoordig en kun je dat doen wat dure GIS-programma’s ook kunnen en soms nog meer.\r\nR’s ruimtelijke ecosysteem\r\nEr is tegenwoordig een hele variëteit aan ruimtelijke data-analyse pakketten beschikbaar.\r\nsp: Classes and Methods for Spatial Data\r\nsf: Simple Features for R (bouwt voort op sp)\r\nspdep: Spatial Dependence: Weighting Schemes, Statistics, and Models\r\nlwgeom: Binding to the liblwgeom library\r\n…\r\nDe nieuwste en de beste op dit moment is sf. Over ruimtelijk analyseren is een hele serie tutorials beschikbaar op internet, ook al lopen ze in de tijd wat achter, is te vinden op de r-spatial site. Daarnaast is er een hele serie specieke pakketten voor data-visualisatie waar je mee kunt werken en die de gebruiker veel flexibiliteit bieden. Terwijl sf zelf heel veel kan, werkt het heel goed in combinatie met deze twee pakketten.\r\nggplot2: De Grammar of Graphics in R.\r\ntmap: R pakket voor thematic maps.\r\nDe laatste maand heb ik wat gewerkt met sf in combinatie met ggplot en tmap. Ik wilde mij dit wat beter eigen maken en het gebruiken voor het maken van kaarten van Nederland en delen van Nederland. Hieronder vind je achtereenvolgens wat informatie over sf en ggplot en daarna iets over tamp.\r\nStructure of sf data\r\n\r\nEen figuur uit Geocomputation with R, zie ook mijn vorig blog Geocomputation with R.\r\nSimple Features is een hierarchisch data model dat een behoorlijk breed palet aan geometrische types representeert. Het komt erop neer dat simple features een dataframe is met in elke rij ruimtelijke gegevens (een bevolkingsgegeven, een punt, een stad, …) met een list-kolom met coördinaten waarmee de geografische vorm gemaakt kan worden. Hieronder leer je hoe het werkt.\r\nProjecties\r\nHet is goed om te weten dat projecties een 3D oppervlakte naar een 2D-oppervlakte omvormen. Verschillende projecties laten de geografische vorm er heel anders uit zien.\r\n\r\nOpen data file binnen tmap\r\nLaten we eerst eens een databestand van Nederland binnenhalen dat in het pakket tmap opgeslagen zit. Dit is een databestand met de steden van Nederland en enkele gegevens van deze steden. Onderstaande werkt dus alleen als je de library(tmap) hebt geopend.\r\n\r\n\r\n\r\nLaten we met sf eens kijken wat we in huis hebben gehaald. We tonen enkele plot mogelijkheden van sf. Eerst maar eens de grove kaart van ons land.\r\n\r\n\r\n\r\nWelke data zitten er in het bestand, in combinatie met de geografische kaders? In het databestand zitten de codes, de namen, de provincies, de populatie, populatie_man, populatie_vrouw, populaties over verschillende leeftijdsgroepen etc.\r\n\r\n\r\n\r\nStel dat we maximaal 15 variabelen willen afdrukken.\r\n\r\n\r\n\r\nOf alleen maar een, de eerste.\r\n\r\n\r\n\r\nWe kunnen de gegevens nu ook goed vanuit sf met ggplot afdrukken. Stel bijvoorbeeld dat we de populatie van 0-14 jaar (een variabele die in het databestand zit) willen laten zien en hoe dat percentage in de verschillende steden in Nederland eruit ziet. We breken de percentages op in van 14 tot 40 procent (je kunt zien dat de percentages daar tussen liggen).\r\n\r\n\r\n\r\nOmdat het met tidyverse, en dus met dplyr werkt, kunnen we ook de standaard databewerkingscodes gebruiken. Dat maakt het werken met geografische data een stuk makkelijker. Als we alleen Zuid-Holland willen laten zien, maken we het databestand Zuid-Holland.\r\n\r\n\r\n\r\nDan kunnen we hier de eerste variabele laten zien en dan zien we de gemeenten van deze provincie afgebeeld.\r\n\r\n\r\n\r\nEn hetzelfde als hierboven. Hoe zit het met de jonge bevolking in de ZuidHollandse steden? We zien het hieronder met inzet van het pakket ggplot.\r\n\r\n\r\n\r\nOverstappen naar tmap\r\nDaar waar sf goed is voor het binnenhalen en bewerken van de data op allerlei manieren, daar is tmap heel goed in het maken van de kaarten op een eenvoudige manier. tmapis gemaakt om met grote flexibiliteit kaarten te kunnen maken. Het heeft dezelfde gelaagde structuur als ggplot. Je vindt op de website een document om er makkelijk mee te kunnen beginnentmap: get started. En de ontwikkelaar (Martijn Tennekes van het CBS ) heeft er een inzichtelijk artikel over geschreven in Journal of Statistical Software artike.\r\nStel dat je er een percentage bevolking aan wilt toevoegen en dit percentage per provincie wilt afbeelden. Met de volgende code doe je dat.\r\n\r\n\r\n\r\nStel dat we twee grafieken naast elkaar willen zetten. Dat doe je zo.\r\n\r\n\r\n\r\nMet tmap kun je ook en net zo makkelijk interactieve kaarten maken.\r\nOpen data via CBS\r\nIk kwam er ook achter dat je kaarten ook kunt maken via de open-data mogelijkheden van het CBS (Het kan, maar het vraagt nog wel wat oefening de komnende tijd). Open hiervoor het pakket cbsodataR\r\n\r\n\r\n\r\nZoek vervolgens op welke data beschikbaar zijn:\r\n\r\n\r\n  [1] \"WijkenEnBuurten\"                         \r\n  [2] \"\"                                        \r\n  [3] \"Gemeentenaam_1\"                          \r\n  [4] \"SoortRegio_2\"                            \r\n  [5] \"Codering_3\"                              \r\n  [6] \"IndelingswijzigingWijkenEnBuurten_4\"     \r\n  [7] \"\"                                        \r\n  [8] \"AantalInwoners_5\"                        \r\n  [9] \"\"                                        \r\n [10] \"Mannen_6\"                                \r\n [11] \"Vrouwen_7\"                               \r\n [12] \"\"                                        \r\n [13] \"k_0Tot15Jaar_8\"                          \r\n [14] \"k_15Tot25Jaar_9\"                         \r\n [15] \"k_25Tot45Jaar_10\"                        \r\n [16] \"k_45Tot65Jaar_11\"                        \r\n [17] \"k_65JaarOfOuder_12\"                      \r\n [18] \"\"                                        \r\n [19] \"Ongehuwd_13\"                             \r\n [20] \"Gehuwd_14\"                               \r\n [21] \"Gescheiden_15\"                           \r\n [22] \"Verweduwd_16\"                            \r\n [23] \"\"                                        \r\n [24] \"WestersTotaal_17\"                        \r\n [25] \"\"                                        \r\n [26] \"NietWestersTotaal_18\"                    \r\n [27] \"Marokko_19\"                              \r\n [28] \"NederlandseAntillenEnAruba_20\"           \r\n [29] \"Suriname_21\"                             \r\n [30] \"Turkije_22\"                              \r\n [31] \"OverigNietWesters_23\"                    \r\n [32] \"\"                                        \r\n [33] \"GeboorteTotaal_24\"                       \r\n [34] \"GeboorteRelatief_25\"                     \r\n [35] \"SterfteTotaal_26\"                        \r\n [36] \"SterfteRelatief_27\"                      \r\n [37] \"\"                                        \r\n [38] \"HuishoudensTotaal_28\"                    \r\n [39] \"Eenpersoonshuishoudens_29\"               \r\n [40] \"HuishoudensZonderKinderen_30\"            \r\n [41] \"HuishoudensMetKinderen_31\"               \r\n [42] \"GemiddeldeHuishoudensgrootte_32\"         \r\n [43] \"Bevolkingsdichtheid_33\"                  \r\n [44] \"\"                                        \r\n [45] \"Woningvoorraad_34\"                       \r\n [46] \"GemiddeldeWoningwaarde_35\"               \r\n [47] \"\"                                        \r\n [48] \"PercentageEengezinswoning_36\"            \r\n [49] \"PercentageMeergezinswoning_37\"           \r\n [50] \"\"                                        \r\n [51] \"PercentageBewoond_38\"                    \r\n [52] \"PercentageOnbewoond_39\"                  \r\n [53] \"\"                                        \r\n [54] \"Koopwoningen_40\"                         \r\n [55] \"\"                                        \r\n [56] \"HuurwoningenTotaal_41\"                   \r\n [57] \"InBezitWoningcorporatie_42\"              \r\n [58] \"InBezitOverigeVerhuurders_43\"            \r\n [59] \"EigendomOnbekend_44\"                     \r\n [60] \"\"                                        \r\n [61] \"BouwjaarVoor2000_45\"                     \r\n [62] \"BouwjaarVanaf2000_46\"                    \r\n [63] \"\"                                        \r\n [64] \"\"                                        \r\n [65] \"GemiddeldElektriciteitsverbruikTotaal_47\"\r\n [66] \"\"                                        \r\n [67] \"Appartement_48\"                          \r\n [68] \"Tussenwoning_49\"                         \r\n [69] \"Hoekwoning_50\"                           \r\n [70] \"TweeOnderEenKapWoning_51\"                \r\n [71] \"VrijstaandeWoning_52\"                    \r\n [72] \"\"                                        \r\n [73] \"Huurwoning_53\"                           \r\n [74] \"EigenWoning_54\"                          \r\n [75] \"\"                                        \r\n [76] \"GemiddeldAardgasverbruikTotaal_55\"       \r\n [77] \"\"                                        \r\n [78] \"Appartement_56\"                          \r\n [79] \"Tussenwoning_57\"                         \r\n [80] \"Hoekwoning_58\"                           \r\n [81] \"TweeOnderEenKapWoning_59\"                \r\n [82] \"VrijstaandeWoning_60\"                    \r\n [83] \"\"                                        \r\n [84] \"Huurwoning_61\"                           \r\n [85] \"EigenWoning_62\"                          \r\n [86] \"PercentageWoningenMetStadsverwarming_63\" \r\n [87] \"\"                                        \r\n [88] \"\"                                        \r\n [89] \"AantalInkomensontvangers_64\"             \r\n [90] \"GemiddeldInkomenPerInkomensontvanger_65\" \r\n [91] \"GemiddeldInkomenPerInwoner_66\"           \r\n [92] \"k_40PersonenMetLaagsteInkomen_67\"        \r\n [93] \"k_20PersonenMetHoogsteInkomen_68\"        \r\n [94] \"Actieven1575Jaar_69\"                     \r\n [95] \"\"                                        \r\n [96] \"k_40HuishoudensMetLaagsteInkomen_70\"     \r\n [97] \"k_20HuishoudensMetHoogsteInkomen_71\"     \r\n [98] \"HuishoudensMetEenLaagInkomen_72\"         \r\n [99] \"HuishOnderOfRondSociaalMinimum_73\"       \r\n[100] \"\"                                        \r\n[101] \"PersonenPerSoortUitkeringBijstand_74\"    \r\n[102] \"PersonenPerSoortUitkeringAO_75\"          \r\n[103] \"PersonenPerSoortUitkeringWW_76\"          \r\n[104] \"PersonenPerSoortUitkeringAOW_77\"         \r\n[105] \"\"                                        \r\n[106] \"BedrijfsvestigingenTotaal_78\"            \r\n[107] \"\"                                        \r\n[108] \"ALandbouwBosbouwEnVisserij_79\"           \r\n[109] \"BFNijverheidEnEnergie_80\"                \r\n[110] \"GIHandelEnHoreca_81\"                     \r\n[111] \"HJVervoerInformatieEnCommunicatie_82\"    \r\n[112] \"KLFinancieleDienstenOnroerendGoed_83\"    \r\n[113] \"MNZakelijkeDienstverlening_84\"           \r\n[114] \"RUCultuurRecreatieOverigeDiensten_85\"    \r\n[115] \"\"                                        \r\n[116] \"\"                                        \r\n[117] \"PersonenautoSTotaal_86\"                  \r\n[118] \"\"                                        \r\n[119] \"PersonenautoSJongerDan6Jaar_87\"          \r\n[120] \"PersonenautoS6JaarEnOuder_88\"            \r\n[121] \"\"                                        \r\n[122] \"PersonenautoSBrandstofBenzine_89\"        \r\n[123] \"PersonenautoSOverigeBrandstof_90\"        \r\n[124] \"PersonenautoSPerHuishouden_91\"           \r\n[125] \"PersonenautoSNaarOppervlakte_92\"         \r\n[126] \"Motorfietsen_93\"                         \r\n[127] \"\"                                        \r\n[128] \"AfstandTotHuisartsenpraktijk_94\"         \r\n[129] \"AfstandTotGroteSupermarkt_95\"            \r\n[130] \"AfstandTotKinderdagverblijf_96\"          \r\n[131] \"\"                                        \r\n[132] \"AfstandTotSchool_97\"                     \r\n[133] \"ScholenBinnen3Km_98\"                     \r\n[134] \"\"                                        \r\n[135] \"OppervlakteTotaal_99\"                    \r\n[136] \"OppervlakteLand_100\"                     \r\n[137] \"OppervlakteWater_101\"                    \r\n[138] \"\"                                        \r\n[139] \"MeestVoorkomendePostcode_102\"            \r\n[140] \"Dekkingspercentage_103\"                  \r\n[141] \"\"                                        \r\n[142] \"MateVanStedelijkheid_104\"                \r\n[143] \"Omgevingsadressendichtheid_105\"          \r\n[144] \"\"                                        \r\n[145] \"TotaalDiefstalUitWoningSchuurED_106\"     \r\n[146] \"VernielingMisdrijfTegenOpenbareOrde_107\" \r\n[147] \"GeweldsEnSeksueleMisdrijven_108\"         \r\n\r\nGebruik de WijkenenBuurten-data en de GeboorteRelatief_25 en verwijder spaties uit regiocodes.\r\n\r\n\r\n\r\nHaal de kaart met gemeentegrenzen op van PDOK\r\n\r\n\r\nReading layer `OGRGeoJSON' from data source `https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetFeature&service=WFS&version=2.0.0&typeName=cbs_gemeente_2017_gegeneraliseerd&outputFormat=json' using driver `GeoJSON'\r\nSimple feature collection with 388 features and 5 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: 13565.4 ymin: 306846.9 xmax: 277992.8 ymax: 619291\r\nepsg (SRID):    28992\r\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +towgs84=565.2369,50.0087,465.658,-0.406857,0.350733,-1.87035,4.0812 +units=m +no_defs\r\n\r\nKoppel CBS-data aan geodata met regiocodes\r\n\r\n\r\n\r\nMaak een thematische kaart\r\n\r\n\r\n\r\nIk zal kijken of ik de komende maanden een eenvoudige tutorial over dit onderwerp kan maken. Als ik deze klaar heb, kom ik terug op dit onderwerp.\r\n\r\n\r\n",
    "preview": "posts/2019-09-25-sf-ggplot-en-tmap/sf-ggplot-en-tmap_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-31-geocomputation/",
    "title": "Geocomputation",
    "description": "Bespreking van het fantastische boek Geocomputation with R",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-08-31",
    "categories": [],
    "contents": "\r\n\r\nEverything is related to everything else, but near things are more related than distant things (blz. 259)\r\n\r\nIntroductie\r\n\r\nBegin dit jaar verscheen Geocomputation with R, een boek van Robin Lovelace, Jakub Nowosad en Jannes Muenchow. Het is een standaarwerk over wat vandaag de dag mogelijk is met geografische data analyse, visualisatie en modellering. Aan het boek hebben ze met z’n drieën meer dan twee jaar gewerkt en dat proces konden we gedurende die periode volgen. Ze hebben het boek open-source gemaakt en mensen uitgenodigd om te reageren op eerdere versie. Alles kon je binnenhalen en je kon als het ware meedoen aan het maken ervan. Ze hebben het met Bookdown gemaakt, ze hebben het op GitHub geplaatst en je kunt het nu nog als Git Hub-boek lezen link en alle syntaxen gebruiken. Daar wordt het ook up-to-date gehouden. Het boek en onderdelen ervan zijn reproduceerbaar, openbaar en wereldwijd toegankelijk. Recent verscheen ook een uitgave bij CRC link. Daarnaast is er ook een volle map met aanvullend materiaal link; je moet je best doen om niet om te komen in al dit materiaal.\r\nGeocomputation with R wil iedereen helpen bij het analysere, visualiseren en modelleren van open-source geografische data. Met R heb je gereedschap in handen dat jou daartoe goed in staat stelt. Dat is eigenlijk iets van de laatste jaren. Voorheen werd dit geografische of ruimtelijke data-analyse werk met dure GIS-apparatuur uitgevoerd. Dat was enkel weggelegd voor enkele geografen die specialistische cursussen hadden gevolgd en met dure kliksytemen konden werken en voor anderen was het zeker niet te reproduceren. Maar tegenwoordig kan dit specialistische werk met een standaard laptop-computer thuis gedaan worden en Lovelace en companen laten jou heel goed zien hoe je dat moet doen. R is heel goed in moderne data-analyse en wordt door steeds meer mensen wereldwijd gebruikt. Er zijn ook andere talen waar veel mee gewerkt wordt zoals Python, Java en C++ maar de statistische capaciteiten van R kunnen hier tegenwoordig heel goed mee concurreren. R is niet alleen open-source en kosteloos, het is ook geschikt voor andere disciplines, het werkt met commando’s en syntaxen en het werk kan juist heel goed door anderen worden gereproduceerd. Anderen kunnen de resultaten van de analyses zelf genereren door gebruik te maken van toegankelijke codes. Met de term Geocomputatie onderscheiden zij zich van GIS-programma’s omdat het werk echt computerwerk is geworden, de focus ligt op het schrijven en gebruiken van codes en reproduceerbaarheid staat centraal.De laatste jaren is er niet alleen veel ontwikkeling geweest op het gebied van reproduceerbaarheid maar zijn er ook een groot aantal programma’s gekomen die het werken met ruimtelijke data mogelijk maken. R is een waar data-ecosysteem geworden met een hele sterke community van gebruikers. Lovelace et al. gaan in op de geschiedenis van die spatial pakketten waarin rgdal en sp lang de agenda hebben bepaald.Er zijn programma’s gekomen om data beter te bewerken (zoals dplyr en tidyverse) maar ook specifieke programma’s die daarop afgestemd zijn zaols sf en via dat kun je weer goed met ggplot2, plotly, raster, leaflet, sp en tmap werken. In het eerste deel van het boek leggen ze de basis, die bereiden ze in het tweede deel verder uit en in het derde deel laten ze drie concrete toepassing zien en kijken ze vooruit.\r\nDe basis\r\nIn het eerste deel van dit boek gaat in op twee fundamentele manieren om met geografische data om te gaan: vector en raster modellen. Dit zijn de twee modellen die het vak beheersen. Vector data modellen representeren de wereld door punten, lijnen en polygonen te gebruiken. Het raster model deelt de werkelijkheid in cellen van gelijke omvang in. Vector modellen worden vooral in sociale wetenschappen gebruikt en raster modellen komen we vooral in omgevingswetenschappen tegen. Tegenwoordig is sf het pakket waar verctor modellen in bewerkt worden. Het pakket is betrekkelijk nieuw en incorporeert eerdere pakketten die hiervoor gebruikelijk waren (sp, rgeos en rgdal).\r\n\r\n\r\n\r\nsf leest en schrijft data makkelijk, het kan goede figuren maken, het behandelt de hele dataset (inclusief geografische data) als een data frame. Daarom werkt het goed met het databewerkingspakket tidyverse en de functies waarmee gewerkt worden zijn consistent en intuïtief om mee te werken. Op eenvoudige manier kun je eruit krijgen wat je wil.\r\n\r\n\r\n\r\nRaster modellen is de andere manier om kaarten te maken.\r\n\r\n\r\n\r\nVoor geografische data-analyse zijn Coördinaten Referentie Systemen belangrijk waarmee 3D in 2D wordt omgezet. Kaarten kunnen er, afhankelijk van het systeem dat wordt gebruikt, anders uitzien. Lovelace en companen leggen verschillende systemen uit en ook hoe deze om te zetten zijn. Vervolgens gaan ze in op Attribute data operaties, waarbij het gaat om niet-geografische informatie die wel wel met de geografische data te maken hebben. Een bepaalde hoogte hoort bij een bepaalde ruimte, bv.. In sf zijn deze gegevens goed te verwerken en je kunt er makkelijk een subset van een dataset mee maken (uit de gegevens van de wereld kun je bijvoorbeeld de gegevens voor Europa halen of je wil alleen maar de landen afbeelden waar de gemiddelde levensverwachting groter dan 82 jaar is). Je kunt ook gegevens uit andere datasets aan een geografische dataset koppelen met sf. Een voorbeeld: de wereldgegevens gekoppeld aan koffie data:\r\n\r\n\r\n[1] \"sf\"         \"data.frame\"\r\n\r\n\r\nUitbreiding\r\nHalverwege het boek laten Lovelace et al. zien hoe je vandaag de dag goed kaarten kunt maken. Ze openen een aantal standaardpakketten\r\n\r\n\r\n\r\nen enkele pakketten om verder te visualiseren:\r\n\r\n\r\n\r\nZe werken vooral met tmap omdat dat een goed visualisatiepakket is (gemaakt door de Nederlander M. Tennekes), het werkt net als sf goed met tidyverse en je kunt er statische en interactieve kaarten mee maken, in een handomdraai. Het werkt laag voor laag. Het zo opgebouwd, als we bijvoorbeeld de kaart van Nieuw Zeeland willen laten zien:\r\n\r\n# Eerst vul je de vorm aan\r\ntm_shape(nz) +\r\n  tm_fill() \r\n# Dan de grenzen\r\ntm_shape(nz) +\r\n  tm_borders() \r\n# Dan vorm, vulling en grenzen\r\ntm_shape(nz) +\r\n  tm_fill() +\r\n  tm_borders() \r\n\r\nZe laten heel veel verschillende toepassingen zien.\r\n\r\nma1 = tm_shape(nz) + tm_fill(col = \"red\")\r\nma2 = tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3)\r\nma3 = tm_shape(nz) + tm_borders(col = \"blue\")\r\nma4 = tm_shape(nz) + tm_borders(lwd = 3)\r\nma5 = tm_shape(nz) + tm_borders(lty = 2)\r\nma6 = tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3) +\r\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\r\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\r\n\r\nZe gaan in dit hoofdstuk in op allerlei aspecten van het kaarten maken: de objecten zelf, de esthetica, de kleur, de lay-out en ook om verschillende kaarten naast elkaar te kunnen plaatsen (bv van 1970, 1990, 2010 en 2030).\r\n\r\nurb_1970_2030 = urban_agglomerations %>% \r\n  filter(year %in% c(1970, 1990, 2010, 2030))\r\n\r\ntm_shape(world) +\r\n  tm_polygons() +\r\n  tm_shape(urb_1970_2030) +\r\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\r\n  tm_facets(by = \"year\", nrow = 2, free.coords = FALSE)\r\n\r\nZe laten ook zien hoe je animaties kunt maken, interactieve kaarten en hoe je met andere pakketten kaarten kunt maken (mapview, leaflet, plotly, ggplot). Dit is een belangrijk hoofdstuk om zelf door te nemen met jouw computer op jouw schoot zoals ik dat nu heb. Een apart hoofdstuk besteden ze aan de relaties met andere GIS-pakketten. Omdat ik daar niet mee werk, laat ik dit voor hier liggen. Je hoeft niet alleen anderen te volgen in wat zij hebben gedaan maar je kunt ook jouw eigen scripts en algoritmes schrijven en de Lovelace et al. nodigen je daarvoor uit. Hoofstuk 11 gaat in op statistisch leren en machine learning. Ze geven een voorbeeld van supervised en unsupervised leren, verdelen de dataset in een training en een testset en volgen met het pakket mlr een aantal standaard stappen om te kunnen voorspellen. Het boek laat je de toekomst proeven.\r\nTot slot\r\nIn de laatste hoofdstukken hebben de drie schrijvers elk een hoofdstuk genomen om te laten zien hoe geocomputation op hun vakgebied is te gebruiken. Lovelace heeft het hoofdstuk over Transport geschreven en werkt een case study van Bristol uit.\r\n\r\n\r\n\r\nNovosad werkt het onderwerp Geomarketing uit aan de hand van een studie over fietswinkels in Duitsland.\r\n\r\n\r\n\r\nMuenchov werkt tot slot een ecologisch studie onderwerp in Peru uit\r\n\r\n\r\n\r\nHet laatste hoofdstuk is een concluderend hoofdstuk waarin ze vaststellen dat de manier waarop ze met sf en tidyverse in dit boek de ruimtelijke data benaderen een manier is om met de werkelijkheid om te gaan. Veel van wat ze hier presenteren kan ook met sp en rdal bereikt worden zoals de afgelopen jaren gebruikelijk. Deze nieuwe manier, hun manier, zal zich de komende jaren verder ontwikkelen. De keus van de pakketten is aan de gebruiker. Dit boek is de basis van geocomputation en veel wordt ook niet in het boek besproken. Daarvan zijn ze zich bewust, of het nu om big data gaat of andere analysetechnieken. Hierover is elders meer te vinden. Maar met hun kennis in de achterzak kom je wel verder in het eigen maken van deze andere technieken, kun je meer ontdekken en ook meer daarvan leren. Hun boek, en daarom vind ik het ook zo goed, toont vooral hoe een open-source benadering kan werken. Het leert je creatief met data omgaan, je leert echte problemen op te lossen, gebruik te maken van goede wetenschappelijke tools en het leert je kennis te delen en te reproduceren. Het daagt je uit verder te kijken, voort te bouwen op wat anderen hebben gedaan en alles praktisch toe te passen. Het daagt je uit hier zelf mee aan de slag te gaan en daarin samen te werken met anderen.\r\nEerder had ik de html-versie gelezen en afgelopen weken heb ik met veel plezier dit boek gelezen, een groot aantal presentaties van Lovelace op YouTube bekeken en enkele powerpoint presentaties bekeken. Ik heb verschillende syntaxen geprobeerd en ik kan je zeggen: deze schrijvers helpen je echt om bij de tijd te blijven en achgterstand in te halen. Het plezier en het gemak straalt er vanaf. Het is stimulerend en het zijn geen nerds. Ze hebben wat te vertellen en willen dat deze ontwikkelingen ons verder helpen. Geocomputation with R; het is een fantastisch boek.\r\nBoek: Lovelace,R., Nowosad, J.& Muenchow, J. (2019). Geocomputation with R. London: Chapman & Hall. 353 p.\r\n\r\n\r\n",
    "preview": "posts/2019-08-31-geocomputation/geocomputation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-17-xaringan/",
    "title": "Xaringan",
    "description": "Een mooie presentatie geven met het pakket Xaringan",
    "author": [
      {
        "name": "Yuhui Xie, overgezet Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\r\nPRACHTIGE HTML-SLIDES MAKEN MET XARINGAN\r\nOm mij dit goed eigen te maken heb ik de post van Fabio Votta (7 augustus 2018) en de presentatie van Yihui Xie vertaald (zie: Prachtigepresentaties.rmd). Het lijkt simpel, maar voor mij is dit een goede manier om het eigen te maken. Nadat ik dit heb gedaan, heb ik er zelf enkele zaken aan toegevoegd die ik handig vind of die nieuw zijn.\r\nIntroductie\r\nEr zijn vele manieren om presentaties te maken met R Markdown. Het pakket xaringan, gebouwd door Rstudio medewerker Yihui Xie, is een van de mogelijkheden om attractractief vormgegeven HTML5 slides te maken die echt opvallen. De mooie layout en de beknopte en nette codeerstructuur maakt het aangenaam om mee te werken. Het pakket is gebaseerd op de remark.js bibliotheek en als je er meer over wilt weten kun je dit hier (https://remark.js.org/) doen. Hier zijn enkele voorbeelden van dia’s van de maker die ook als introductie dienen:\r\nhttps://slides.yihui.name/xaringan/\r\nBen je klaar om een presentatie ninja te worden?\r\nInstallatie\r\nEr zijn verschillende manieren om pakketten in R te installeren. Een veilige manier is het om het xaringan pakket van CRAn te installeren, en wel op de volgende manier\r\ninstall.packages(\"xaringan\")\r\nSoms zijn de versies op CRAN iets ouder. Als je geïnteresseerd bent in de nieuwste versie, is het altijd een optie om te zoeken naar de gerelateerde (GitHub) repository. Om het vanaf hier te installeren, moet je eerst het devtools- pakket installeren voor het geval je het niet hebt en vervolgens install_github(\"yihui/xaringan\") gebruiken om het pakket vanuit GitHub te installeren. ## installeer alleen devtools indien nodig ## devtools::installeer_github(‘yihui/xaringan’)\r\nOm een xaringan-presentatie te maken, maak je een RMarkdown document met het xaringan::moon_reader outoutformat. Het is makkelijk om binnen RStudio te beginnen. Klik eert op het New File icoon en dan op RMarkdown zoals hieronder\r\n\r\nSelecteer dan vervolgens “From Template” de Ninja Presentation en je zult een RMarkdown voorbeeld zien:\r\n\r\nDruk op de Knit-knop en voeg de boel samen.\r\nBasis\r\nZoals bij elk R Markdown document, is er een yaml header aan het begin die enkele meta data specificeert. Hier kun je de titel van de presentatie, auteurs en meer opgeven. Dit configureert de titeldia. Hier kan nog veel meer gedaan worden om je dia’s aan te passen en vast te stellen hoe ze eruit zullen zien, maar voorlopig houden we ons aan de basis. Hier is hoe uw yaml header eruit zou kunnen zien:\r\n\r\nDat geeft het volgende resultaat:\r\n\r\nZo worden slides met xaringan gemaakt. Makkelijk!\r\nOver het algemeen gelden de gewone R Markdown-regels, zodat je deze cursief kunt gebruiken en twee sterren voor bijvoorbeeld vetgedrukte of zelfs je meest ingewikkelde LaTeX-vergelijkingen.\r\nHoe meer hashtags je toevoegt, hoe kleiner de header.\r\nDe dia’s worden dan gescheiden door drie streepje— Voor de eerste dia hoeft u dit niet te doen omdat die na de yaml header verschijnt.\r\nAls we elementen op de dia’s met een klik willen laten verschijnen, scheiden we ze met twee streepje, – zoals deze.\r\nTenslotte, als we meer ruimte tussen de elementen op een dia willen hebben, kunnen we de html-tag  gebruiken.\r\nJe zou met zoiets kunnen beginnen:\r\n\r\n# Slide 1\r\n\r\nDit is slide 1\r\n\r\n* Item 1\r\n* Item 2\r\n    + Item 2a\r\n    + Item 2b\r\n  \r\n\r\n---\r\n\r\n# Slide 2\r\n\r\nDit is slide 2\r\n\r\nHier een moeilijke vergelijking:\r\n\r\n$$S (ω)=1.466\\, H_s^2 \\,  \\frac{ω_0^5}{ω^6 }  \\, e^[-3^ { ω/(ω_0  )]^2}$$\r\n\r\n---\r\n\r\n# Slide 3\r\n\r\nDit is slide 3\r\n\r\n--\r\n\r\nDit verschijnt met een klik\r\n\r\n--\r\n\r\n<br>\r\n<br>\r\n\r\nDit verschijnt ook met een klik maar wat later\r\n\r\n\r\n\r\nDit ziet er al goed uit.\r\nHet opmaken van dia’s\r\nLaten we zeggen dat we een beetje met het formaat van onze dia’s willen spelen. Er zijn enkele ingebouwde functies waarmee we precies dat kunnen doen. We definiëren de volgende code aan het begin van een dia:\r\n\r\n---\r\nclass: inverse, center, middle\r\n# Statistische Analyse\r\n\r\nHierdoor wordt de kleur (hier: zwart) omgekeerd door de elementen horizontaal (midden) en verticaal (midden) op die dia te centreren. Dit zorgt voor een aantal mooie koele overgangsplaten.\r\nDat geeft de volgende slide:\r\nEen GIF of een afbeelding toevoegen\r\nU kunt ook GIF’s of afbeeldingen aan uw dia’s toevoegen. Dit gebeurt op precies dezelfde manier als bij een normaal R Markdown document. Hier is een voorbeeld:\r\n\r\n---\r\n\r\nclass: inverse, center, middle\r\n\r\n![](https://www.ukcophumour.co.uk/wp-content/uploads/f2w/1526816_674334732588821_1244473478_n.jpg)\r\n\r\n ## Inzet van thema’s en xaringanthemer\r\nWe hebben al een lange weg afgelegd om mooie xaringan dia’s te maken. Als we het thema van de presentatie wilden aanpassen, dan wordt xaringan geleverd met een aantal ingebouwde kleurenschema’s die je kunt uitproberen. Hier is er een van. Voeg gewoon de volgende regel toe in je YAML header en je bent klaar om te gaan:\r\n\r\noutput:\r\n  xaringan::moon_reader:\r\n    css: [metropolis]\r\nHier is een rijtje thema’s: - metropolis - hygge - rladies Als u uw eigen thema’s wilt creëren, kan ik u het xaringanthemer pakket aanbevelen: https://github.com/gadenbuie/xaringanthemer\r\nDe presentatie exporteren naar .pdf\r\nDe output van xaringan is html-format. Als u echter een .pdf-bestand wilt, kunt u de dia’s gewoon openen in uw favoriete webbrowser en ze afdrukken naar .pdf. Dit lijkt de gemakkelijkste versie om dit te doen.\r\nWat nu?\r\nIk hoop dat je genoten hebt van deze kleine tutorial!\r\nHier zijn nog enkele voorbeelden van mooie xaringan dia’s die als inspiratiebron kunnen dienen: https://github.com/favstats/xaringan_slides/\r\nWilt u animatieovergangen toevoegen? Nou, hier is een implementatie daarvan: https://www.garrickadenbuie.com/blog/2018/12/03/animate-xaringan-slide-transitions/\r\nTot slot helpt een wiki om meer geavanceerde opties te implementeren. Zorg ervoor dat je het op een gegeven moment bekijkt: https://github.com/yihui/xaringan/wiki\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-06-24-bookdown/",
    "title": "Bookdown",
    "description": "Hoe maak je een boek. Bookdown is het pakket van R waar dat mee kan. Hier enkele tips om dat te doen",
    "author": [
      {
        "name": "Jack Dougherty en Ilya Ilyankou, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-06-24",
    "categories": [],
    "contents": "\r\nIntroductie\r\nHier zie je een klein open-source boekje dat met R/RStudio, het pakket Bookdown en het programma GitHub is gemaakt. Voor deze leidraad heb ik gebruik gemaakt van een stukje stukje tekst dat Jack Dougherty and Ilya Ilyankou hebben gemaakt voor hun open-source boek link.\r\nOok hun boek is gemaakt met open-source-gereedschappen, het pakket Bookdown en de programma’s RStudio en GitHub. Als je in Bookdown werkt ben je als auteur in staat om met Markdown te werken (een makkelijke computer syntaxtaal om op verschillende platforms mee te lezen en te schrijven). Met die taal kun je ook publiceren in verschillende formats (HTML web-taal, PDF, ePUB en Microsoft Word documenten). Als je het boek host in een openbaar toegankelijke GitHub repository en het met GitHub Pages publiceert, wordt de tekst van het boek en het product toegankelijk op het web. Over Markdown en RMarkdown heb ik in eerdere blogs van Harrie’s hoekje geschreven. Voor de technische basisgids om met Bookdown te publiceren kunnen we verwijzen naar Yihui Xie, Bookdown: Authoring Books and Technical Documents with R Markdown, 2018, link.\r\nEen alternatief platform voor het uitgeven van boeken is Pressbooks. Deze open-source bron van WordPress ondersteunt ook meerdere publicatie formaten (HMTL, PDF, ePUB). Auteurs kunnen gebruik maken van de betaalde hosting service http://Pressbooks.com. Gebruikers met geavanceerde WordPress-expertise en wat systeembeheervaardigheden kunnen de code ook downloaden van [link] (http://github.com/pressbooks) en hun eigen zelf gehoste boekenuitgeverij runnen.\r\nSetup RStudio, Bookdown en TinyTeX instellen\r\nHieronder volgen enkele stappen die je moet zetten om een boek op te zetten zoals hierboven geformuleerd. Het zijn algemene principes die van toepassing zijn op verschillende computersytemen. Je hebt geen speciale kennis nog, maar het is allemaal makkelijker als je avontuurlijk bent of al bekend bent met R Studio, GitHub en/of computercodes.\r\nInstalleer het programma R link om jouw boek te bouwen met Bookdown. (Ja, boeken kun je ook met een statistisch pakket maken en publiceren, en het werkt!). Zie link\r\nIs R de motor, RStudio is het dashboard voor jou. Installeer daarom de gratis RStudio Desktop versie om R gemakkelijker te gebruiken met een visuele editor link. Zie link\r\nSelecteer in RStudio het tabblad Pakketten en selecteer Installeren. Zie link\r\nIn RStudio installeer je het ‘bookdown’-pakket om jouw boek te bouwen en gebruik je het pakket (library(bookdown). Zie link\r\nBookdown zou nu met succes geïnstalleerd moeten zijn in RStudio. Zie link\r\nIn RStudio installeer je ook het ‘tinytex’-pakket voor Bookdown. Daarmee kun je ook een PDF-editie van het boek te maken. Zie link\r\nVergeet niet: in RStudio console tinytex::install_tinytex() te typen en druk op return om de installatie te voltooien. Zie link\r\nIk hoop dat dit met tinytext lukt. Ikzelf werk niet met tinytex maar heb een latex-programma op mijn computer staan die de pdf-versie maakt.\r\nDownload en bouw een voorbeeldboekje\r\nMaak een gratis GitHub-account link aan om de codes te delen en online boekuitgaven te publiceren.\r\nIn jouw webbrowser, log je in op jouw GitHub account, vervolgens ga je naar de minimale repo link van de software ontwikkelaar en maak je een kopie voor jouw GitHub account. Hoe je dat doet daarover je in dit hoofdstuk link van het dataviz-boek.\r\nInstalleer GitHub Desktop link om bestanden over te brengen tussen jouw online GitHub repo en uw lokale computer.\r\nGa in je webbrowser naar het exemplaar van bookdown-minimal en klik op de groene knop Clone of Download het en selecteer Open in Desktop. Dit opent automatisch de GitHub Desktop applicatie. Zoek de code en kopieer deze naar een map op uw lokale computer.\r\nIn RStudio in de rechterbovenhoek selecteert u Project > Open Project om de bookdown-minimal map op uw lokale computer te openen. Zie link\r\nOpen in RStudio het bestand index.rmd en maak enkele eenvoudige bewerkingen op de tekst van dit minimale boek. Verwijder bijvoorbeeld het hashtag # commentaarsymbool in regel 8 om commentaar te verwijderen en activeer de optie PDF-boek. Bewaar uw bewerkingen. Zie link\r\nOptioneel: Gebruik jouw favoriete teksteditor, bv. Atom editor link, om de tekst te wijzigen maar het kan ook goed met RStudio.\r\nIn RStudio, rechtsboven in de hoek, selecteer je de Build Book tab, selecteer je Build Book, en kies je All Formats om zowel de gitbook-stijl statische webeditie als de PDF-editie te bouwen.\r\nAls RStudio met succes beide edities van je minimale boek bouwt, zal de output worden opgeslagen in je bookdown-minimal map. Dit wordt in een submap met de naam: _book geplaatst, omdat dit voorbeeld zo is geconfigureerd. Open ook de submap en bekijk de PDF-editie. Als RStudio fouten heeft gevonden, zullen deze in de Build viewer verschijnen. Zie link\r\nTip: In toekomstige sessies met RStudio moet u mogelijk het tabblad Pakketten selecteren en op Update klikken om uw softwarepakketten up-to-date te houden. Zie link\r\nSluit RStudio.\r\nPubliceer je boek met GitHub Pages…\r\nOpen het GitHub-bureaublad en navigeer naar de map bookdown-minimal op uw lokale computer. Schrijf een samenvatting om de wijzigingen die u hierboven gemaakt hebt vast te leggen (op te slaan) aan uw master branch, en duw (pull) deze versie naar uw online GitHub repo. Over GitHub zal ik binnenkort nog eens een blog schrijven.\r\nGa in jouw webbrowser naar jouw online GitHub repo, met een webadres gelijkaardig aan https://github.com/USERNAME/bookdown-minimal (vul je GitHub gebruikersnaam in).\r\nIn je GitHub repo, selecteer Settings, scroll naar beneden naar de GitHub Pages sectie (dat is een gratis web hosting service om je code en boekuitgaven op het publieke web te publiceren). Selecteer Master Branch als jouw bron, en sla het op.\r\nScroll weer naar beneden naar deze sectie, en het webadres van uw gepubliceerde site zou moeten verschijnen. Kopieer dit adres.\r\nPlak het webadres van bovenaf in een nieuw browsertabblad en voeg uiteindelijk _book/index.html toe, omdat dit voorbeeld is geconfigureerd om de webeditie van uw boek in deze submap op te slaan. Uw webadres moet vergelijkbaar zijn met: https://USERNAME.github.io/bookdown-minimal/_book/index.html.\r\nUw Bookdown en GitHub instellingen aanpassen\r\nOm de aangepaste instellingen voor dit boek te bekijken, ga naar de online repository https://github.com/datavizforall/dataviz-bookdown\r\nIn het _bookdown.yml bestand is de uitvoermap ingesteld om alle boekformaten in de docs map te bouwen.\r\nDe GitHub Pages Settings voor deze repo (die je niet kunt bekijken) is ingesteld om te publiceren vanuit de master/docs map, zodat ze overeenkomen met de uitvoer map hierboven. Dit vereenvoudigt het gepubliceerde webadres naar dit formaat: https://USERNAME.github.com/REPONAME\r\nDe meeste van de Bookdown configuratie-instellingen verschijnen in het bestand index.Rmd. Lees meer over deze opties in de technische handleiding van de software ontwikkelaar, (https://bookdown.org/yihui/bookdown/).\r\nDaarnaast is deze GitHub Pages repo gepubliceerd met een aangepaste domeinnaam https://DataVizForAll.org. Meer informatie over de aangepaste domeinnamen vind je op https://help.github.com/articles/using-a-custom-domain-with-github-pages/, waarvoor je een domeinnaam moet kopen bij een webhostingdienst (zoals http://ReclaimHosting.com). Door het toevoegen van een GitHub Pages aangepaste domeinnaam creëer je een extra CNAME-bestand in de submap docs. Wees voorzichtig om het niet te verwijderen (of plaats een kopie in een submap voor de bewaring).\r\nDit boek bevat ook een aangepast 404.html-bestand dat handmatig werd overgezet naar de submap van de documenten, aangezien het niet automatisch wordt gebouwd door Bookdown.\r\nDit boek bevat ook een aangepaste google-analytics-datavizforall.html bestand in het root-niveau van repo (waar bookdown naar zoekt) en wordt ook handmatig overgebracht naar de docs submap (aangezien bookdown het niet lijkt te kopiëren naar daar bij elke build). Deze volgt het webverkeer met Google Analytics.\r\nNog enkele tips\r\nLinken met gehoekte en ronde haakjes.\r\nGebruik gehoekte én ronde haakjes voor link in de tekst.\r\nGebruik alleen ronde haakjes voor een niet-ingesloten link (http://example.com).\r\nOok, geef URL met ronde haakjes weer: (http://example.com)\r\nGebruik indien nodig HTML om een link te maken die in een nieuwe pagina wordt geopend.\r\nDit Den Haag-boekje, bestaat uit acht hoofdstukken. Elk hoofdstuk is een apart Rmd-bestand. De numerieke volgorde van de bestandsnamen (index.Rmd, 0.1-AntilliaanseNederlandersinDenHaag.Rmd, 0.2-opleidingsniveauouders.Rmd, 03-werkeninkomen.Rmd, 04-zorggebruikalgemeen.Rmd, 05-zorggebruijeugd.Rmd, 06-gezondheid.Rmd, 07-criminaliteit.Rmd, 08-conclusies.Rmd) bepaalt de volgorde waarin ze in het gebouwde boek verschijnen.\r\nIn het index.Rmd bestand staat in de instellingen voor het webboek dat elk hoofdstuk en sectie wordt opgesplitst in een eigen HTML pagina, zonder automatische nummering. split_by: section number_sections: false\r\nHoofdstukken beginnen met een eerste niveau koptekst (één hashtag: #) en secties beginnen met een tweede niveau koptekst (twee hashtags: ##). Zowel de hoofdstuk- als sectietitels worden onmiddellijk gevolgd door een korte ID tussen gehoekte haakjes om kruisverwijzingen in het boek mogelijk te maken. De korte ID’s MOETEN uniek zijn, en moeten idealiter overeenkomen met de .Rmd-bestandsnaam. Hoewel elke hashtag hoofdstuk/sectie titel de standaard ID is (zoals #Inleiding in index.Rmd), is het veiliger om voor elk hoofdstuk/sectie een unieke korte ID te maken om verwarring te voorkomen.\r\nDe cursief gedrukte lijnen staan bovenaan elk hoofdstuk of sectie, met een kruisverwijzing naar de auteurspagina, gevolgd door de laatste bijgewerkte datum. Voorbeelden van hoofdstukkop, met korte ID’s, bylines en data: # Hoofdstuk Titel {#Hoofdstuk} *by[Esther Horrevorts, Hans Bellaarts](authors), last updated: June 18, 2019*\r\nSubrubrieken binnen de hoofdstukken en secties beginnen met drie hashtags en eindigen met een symbool zonder nummering {-}.\r\nOm een interne kruisverwijzing naar een hoofdstuk of sectie in het boek in te voegen, kun je een link invoegen met haakjes [voor de tekst] en haakjes (met de korte ID). Als verwijzing in de tekst verschijnt, voeg dan de Furfase “in dit boek” toe aan het einde van de zin, zodat we in de toekomst alle kruisverwijzingen kunnen zoeken/vinden als dat nodig is:\r\nZie de [GitHub tutorial](github) in het Dataviz-boek\r\nOPMERKING: de bovenstaande verwijzingen zijn ontworpen voor het HOSTEN van WEB BOOEKEN, en werken mogelijk niet hetzelfde in lokale repo versies, of in ebook/PDF versies.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-19-website-maken/",
    "title": "Websites maken in R",
    "description": "Met R kun je ook website maken. Maar hoe doe je dat? Emily Zabor schreef hierover een leerzaam blog dat ik hier licht heb bewerkt en aangevuld.",
    "author": [
      {
        "name": "Emily C. Zabor, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-06-09",
    "categories": [],
    "contents": "\r\nDeze tutorial is van Emily C. Zabor die ik heb bewerkt, vooral ook om te zien of ik mijn eigen website kan maken. Deze tutorial laat je zien hoe je een website maakt met gebruik van R, RMarkdown en GitHub.\r\nDeze tutorial presenteerde zij voor het eerst op R Gebruikers Groep Bijeenkomst op 23 Januari 2018 op het Memorial Sloan Kettering Cancer Center Department van Epidemiologie and Biostatistiek.\r\nDeze versie ververste en presenteerde zij op de R Dames NYC Bijeenkomst op 15 Februarie 2018.\r\nTypen websites\r\nDe belangrijkste typen websites die je zou willen maken zijn:\r\nPersoonlijke websites\r\nWebsites om een pakket te presenteren\r\nProject websites\r\nBlogs\r\nDe basis van R Markdown website\r\nWat jij minimaal nodig hebt voor een R Markdown website zijn:\r\nindex.Rmd: bevat de inhoud van de homepage van de website\r\n_site.yml: bevat de metadata voor de website\r\nEen basis voorbeeld voor een _site.yml voor een website met twee pagina’s kan er zo uitzien:\r\n\r\n\r\n\r\nAnd a basic index.Rmd to create the Home page:\r\n\r\n\r\n\r\nHier vind je een overzicht van de basis van R Markdown website hier.\r\nGitHub\r\nDeze tutorial left de nadruk op het hosten van websites via GitHub. Hosten van websites via GitHub is gratis.\r\nAls je nog geen GitHub account hebt, teken dan op via https://github.com/join?source=header-home met username YOUR_GH_NAME. Ik zal naar deze username, YOUR_GH_NAME, als “jouw GitHub username” refereren in deze hele tutorial.\r\nEr zijn andere mogelijkheden om jouw website gratis te hosten. Een ander populaire keuze is Netlify.\r\nPersoonlijke websites\r\nEen voorbeeld van een homepage van Emily Zobore’s website is:\r\n\r\nEr zijn twee belangrijke stappen om een persoonlijke website te maken die op GitHub wordt gehost:\r\nGitHub setup\r\nLokale setup\r\nGitHub setup\r\nCreëer een GitHub repository (“repo”) genaamd YOUR_GH_NAME.github.io, waar YOUR_GH_NAME jouw GitHub username is.\r\nInitialiseer het met een README\r\nVoor hen die met GitHub weinig ervaring hebben: dit kan het proces van klonen van repository en daarmee het afstemmen met de ‘master branch’ vergemakkelijken.\r\n\r\nLokale setup\r\nKloon deze remote repository op een locale directory met dezelfde naam, YOUR_GH_NAME.github.io\r\nVoeg een R Project toe aan deze directory\r\nMaak een _site.yml en een index.Rmd file in jouw nieuwe directory\r\nWaarom heb ik een R Project nodig?\r\nHet R Project is gemakkelijk omdat RStudio jouw project als een website zal herkennen en zorgt voor de goede gereedschappen die je nodig hebt.\r\nOpgelet: Nadat je een R Project met de goede files hebt gemaakt, kan het zijn dat je het project moet sluiten en heropenen voordat R het herkent als een website en de goede gereedschappen toont.\r\nCreëer inhoud\r\nPas de _site.yml file aan door de metadata te veranderen en het thema van jouw website. Kijk maar eens naar de Jekyll thema’s hier en speel wat met de verschillende opties. Thema’s zijn makkelijk te veranderen, zelfs nadat je de inhoud hebt toegevoegd.\r\nBijvoorbeeld de _site.yml voor de persoonlijke website van Emily ziet er zo uit:\r\n\r\n\r\n\r\nPas aan en creëer .Rmd files met de inhoud van jouw website, die er html-pagina’s voor jouw website van maken als jij ze ‘knit’.\r\nDe index.Rmd file voor de homepage van Emily’s persoonlijke website ziet er zo uit:\r\n\r\n\r\n\r\nAls je een keer de inhoud hebt geschreven en de lay-out hebt opgezet, zoek dan de Build tab in RStudio op en selecteer “Build Website”:\r\n\r\nNu heeft jouw local directory alle files die nodig zijn om jouw website te maken:\r\n\r\nDe website uitzetten\r\nBasis benadering:\r\nSelecteer “Upload files” van de hoofpagina pagina van jouw GitHub repository:\r\n\r\nEn sleep eenvoudig of selecteer de files van jouw locale repository:\r\n\r\nGeavanceerde benadering (aangeraden):\r\ngebruik Git als cliënt of van binnenuit RStudio (een andere goede reden om een R Project! te gebruiken)\r\n\r\nMaar dit is geen Git/GitHub tutorial. Als je meer wilt leren over Git/GitHub, ik raad jou aan dit te doen, dan is dit een goede bron om mee te beginnen: http://happygitwithr.com/\r\nAangepaste domeinen\r\nHet standaardadres om jouw wite te hosten is http://YOUR_GH_NAME.github.io, maar je kunt jouw domeinnaam ook aanpassen. Dan zijn er twee stappen te zetten:\r\nIn jouw GitHub repository YOUR_GH_NAME.github.io, ga je naar Settings > GitHub pages. Typ jouw domeinnaam in de box onder Custom domain en sla het op (Save).\r\n\r\nVoeg een CNAME file toe aan jouw GitHub repository YOUR_GH_NAME.github.io.\r\nHet zal als volgt in jouw repository verschijnen:\r\n\r\nAnd inside the file you will simply have your domain name:\r\n\r\nPakket websites\r\nEen voorbeeld hiervan is deze website van Emily’s R-pakket ezfun:\r\n\r\nGebruik Hadley Wickham’s goede pakket pkgdown om makkelijk een website van jouw pakket te maken die op GitHub wordt gehost. Details over pkgdown kun je hier vinden de pkgdown website, die ook met inzet van pkgdown is gemaakt.\r\nDit veronderstelt wel dat je al een R-pakket met een locale directory hebt en een GitHub repository.\r\nFrom within your package directory run:\r\n\r\n\r\n\r\nDit zal een folder toevoegen met de naam docs binnen de locale directory voor jouw pakket\r\nUpload/push deze veranderingen in de GitHub repository voor jouw pakket\r\nIn the GitHub repository voor jouw pakket ga je naar Settings > GitHub pages. Selecteer “master branch/docs folder” als de bron en sla op (Save)\r\n\r\nDe persoonlijke pagina zal worden toegevoegd aan jouw persoonlijke website en aan YOUR_GH_NAME.github.io/repo_name\r\nDe homepage kun je via README file op jouw repository binnenhalen\r\nDe referentiepagina van de site omvat alle functies met hun beschrijving\r\nElke functie klikt door naar de hulppagina ervan,\r\nEn in bepaalde gevallen ook naar vignettes met goede informatie\r\nEn dan ben je klaar, zo makkelijk als dat.\r\nProject websites\r\nOok als je geen pakket maakt kun je nog wel een repository maken. Emily Zabore heeft bijvoorbeeld een pagina op haar website die linkt naar de repository waarin deze tutorial is opgeslagen.\r\n\r\nLokale setup\r\nVanuit de lokale directory van het project waar het jou om te doen is:\r\nCreëer een _site.yml en index.Rmd file in jouw nieuwe directory\r\nPas deze files met jouw inhoud en layout, net zoals bij persoonlijke websites\r\nGitHub setup\r\nUpload/push deze nieuwe files in de GitHub repository voor jouw project\r\nGa naar GitHub pagina’s voor de repository en ga naar Settings > GitHub Pages, waar je de “master branch” folder selecteert en je drukt op Save\r\n\r\nBlogs\r\nR Markdown websites zijn makkelijk te maken en uit te zetten, maar het wordt lastiger als je het voortdurend moet verversen of veranderingen moet aanbrengen, zoals dat het geval is bij een blog. Gelukkig, het R-pakket blogdown bestaat juist voor dit doel. blogdown is een R pakket dat jou in staat stelt statistische websites te makenthat allows you to create static websites, wat betekent dat de uitgezette versie van de website alleen bestaat uit JavaScript, HTML, CSS en plaatjes. Gelukkig is het blogdown pakket zo opgezet dat je over al die zaken niets af hoeft te weten om toch nog een mooie website te maken voor jouw blog, met de ondersteuning van Hugo.\r\nVoor de beste referentie van blogdown website, kijk naar dun blogdown boekje.\r\nEmily Zabore heeft geen persoonlijk blog, maar wel een website/blog gebouwd rond deze bijeenkomsten in New York R-Ladies NYC en dat is hier als voorbeeld toegevoegd.\r\n\r\nSetup\r\nDe eerste drie stappen zijn hetzelfde als het maken van een basis R Markdown website:\r\nCreëer een GitHub repository met de naam YOUR_GH_NAME.github.io, waar YOUR_GH_NAME jouw GitHub gebruikersnaam is, geïnistialiseerd met een README file\r\nKloon deze GitHub repo op een lokale directory met dezelfde naam\r\nVoeg een R Project aan jouw lokale directory toe\r\nDan beginnen we met blogdown.\r\nInstalleer blogdown en Hugo\r\n\r\n\r\n\r\nKies een thema en vind de link naar de thema’s van de GitHub repository. In dit geval zijn de thema’s niet zo makkelijk te wisselen als binnen de basis R Markdown website, dus kies het thema zorgvuldig.\r\nGenereer een nieuwe site binnen jouw project sessie. De optie theme_example = TRUE zal voor de files van een voorbeeldsite zorgen die je op basis van wat je nodig hebt kunt aanpassen.. “user/repo” refereert naar de GitHub gebruikers naam (user) en de GitHub repository (repo) voor jouw geselecteerde thema.\r\n\r\n\r\n\r\nDit zal alles van de filestructuur van jouw nieuwe blog genereren.\r\n\r\nNadat je dit hebt afgerond, moet je sluiten en dan het project weer heropenen. Als je heropent, zal RStudio het project als een website herkennen.\r\nHet aanpassen van het beeld\r\nVeranderingen voer je in de config.toml file door (hetzelfde als de _site.yml die we bij de R Markdown websites tegenkwamen); zo verander je de layout en het beeld van jouw website. De beschikbare kenmerken in de config.toml zullen verschillend zijn afhankelijk van jouw thema en de meeste themavoorbeelden hebben een eigen config.toml die je als template kunt gebruiken.\r\nAls je een keer de kenmerken van jouw website hebt aangepast, klik dan op RStudio’s “Serve Site” om de site lokaal al te bekijken.\r\n\r\nEen nieuwe blog post schrijven\r\nEr zijn verschillende manieren om een nieuwe blogpost op jouw site te schrijven, maar het is het makkelijkste om dat via “New Post” in RStudio te doen:\r\n\r\nDit opent een pop-up waar je de meta-data voor a nieuwe post kunt plaatsen:\r\n\r\nIn aanvulling op Titel, Auteur en Datum van de post, kun je aanvullend ook categorieën creëren, die jouw post in folders organiseren en kun je tags aan de posts toevoegen, waarmee ze te zoeken zijn binnen de inhoud van jouw website. Wees er wel van bewust dat het functioneren van deze kenmerken varieert per thema. Bepaalde blogs kunnen ook in toekomst worden geplaatst.\r\nZie onderaan dat je ook kunt kiezen tussen een regulier markdown (.md) of R markdown (.Rmd) file. .Rmd files moeten worden gerenderd voordat ze html pagina’s genereren. Dus het is het beste dit te gebruiken waar ook een R code in zit.\r\nEen file naam en een ‘slug’ worden automatisch genereerd gebaseerd op de andere metadata. De ‘slug’ is een URL-vriendelijke titel van jouw post.\r\n\r\nPresenteren\r\nEen blogdown site is een beetje lastig te bouwen en te presenteren via GitHub vergeleken met een reguliere R Markdown website en vergeleken met wat hierboven is beschreven.\r\nProbleem 1: Omdat het een statistische site betreft, de files genereren automatisch on line in een aparte subdirectory onder de naam public binnen jouw lokale directory. Echter, dit veroorzaakt problemen met het hosten (presenteren) van GitHub omdat de files dan in de lokale YOUR_GH_NAME.github.io directory moeten zitten.\r\nDe oplossing:\r\nHou aparte directories voor de bron files (deze directory bijvoorbeeld “source”, bron noemen) en voor de statische files (de directory YOUR_GH_NAME.github.io) die gegenereerd worden. De “source” folder is waar your R project en config.toml files zich bevinden.\r\n\r\nGebruik in jouw config.toml de optie publishDir = om blogdown te publiceren via de YOUR_GH_NAME.github.io folder. Dat is eenvoudiger dan de standaard manier op jouw lokale locatie.\r\n\r\nProbleem 2: GitHub gebruikt standaard Jekyll met website inhoud en dat moet ongedaan worden gemaakt omdat blogdown sites met Hugo worden gebouwd.\r\nOm dit op te lossne moet je een lege file toevoegen met de naam .nojekyll in jouw GitHub repo YOUR_GH_NAME.github.io, voordat je het publiseert.\r\n\r\nHet pakket Distill\r\nMijn eigen blog Harrie’s hoekje maak ik met het pakket Distill. Om een blog te maken installeer je Distill. Dan maak je een new blog en gebruik je Distill Blog.\r\nDan worden er een project gemaakt met een aantal documenten: _site.yml\r\nindex.rmd\r\nabout.rmd\r\n_posts/welcome/welcome.rmd\r\nVervolgens pas je de *_site.yml* aan op basis van hoe jouw blog eruit moet zien.\r\nals je een post wilt creëren open je het programma (library(distill)) en je tikt in `create_post(“Hier de naam van de post”). Hier Distill vind je veel meer informatie hierover.Hier [Tensorflow)(https://blogs.rstudio.com/tensorflow/) vind je een ander voorbeeld hoe de blog er dan uit kan zien.\r\nMet Distill kun je op dezelfde eenvoudige manier een website maken Distill-website. Nadat je het Distill hebt geinstalleerd en geopend (library(Distll)). Krijg je de volgende documenten _site.yml (om de website te configureren)\r\nindex.rmd (voor de homepage)\r\nabout.rmd (waar de website over gaat)\r\nVerder werkt dit hetzelfde.\r\nAanvullende bronnen\r\nHieronder vind je de aanvullende bronnen en linken waar in deze tutorial naar wordt verwezen:\r\nhttp://rmarkdown.rstudio.com/rmarkdown_websites.html: an overview of R Markdown website basics\r\nhttp://jekyllthemes.org/: Jekyll themes for use with your R Markdown website\r\nhttp://happygitwithr.com/: an introduction to Git/GitHub\r\nhttp://pkgdown.r-lib.org/: Hadley Wickham’s pkgdown website\r\nhttps://bookdown.org/yihui/blogdown/: Yihui Xie’s blogdown book\r\nhttps://themes.gohugo.io/: Hugo themes for use with your blogdown website\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-18-interactieve-grafiek/",
    "title": "Interactieve grafiek met plotly",
    "description": "In deze twee maanden wilde ik toch eens kijken naar interactieve mogelijkheden die het programma R/RStudio ons biedt. `Plotly` is zo'n mogelijkheid en daarover gaat dit blog. `Shiny` is de andere mogelijkheid en daar zal ik een volgende keer aandacht aan besteden. `Plotly` heeft een eigen website waar veel informatie over het programma is te vinden [hier adres website](https://plot.ly/). Er is ook een uitgebreide handleiding over `Plotly` geschreven [hier handleiding](https://plotly-r.com/the-plotly-cookbook.html). Onlangs stond er op de blog van RBloggers een goede introductie van Laura Ellis, die mij veel vertelde over het gebruik van `Plotly`. Haar bijdrage [zie hier](https://www.r-bloggers.com/create-interactive-ggplot2-graphs-with-plotly/) heb ik hier naar het Nederlands overgezet en hier en daar iets bewerkt.",
    "author": [
      {
        "name": "Laura Ellis op R-bloggers (13 maart 2019), bewerking Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-04-03",
    "categories": [],
    "contents": "\r\nInteractieve ggplot2 grafieken met plotly\r\nEen van de belangrijkste redenen dat ik verliefd werd op R, schrijft Laura Ellis in haar blog, kwam door ggplot2. Zoals Jennifer Thompson het zo welsprekend verwoordde:\r\n\r\n“Ik gebruikte R voor ggplot, maar tot dan toe hield ik er nooit van”.\r\n—Jennifer Thompson\r\n\r\nVoor iemand die zeer geïnteresseerd is in het vertellen van verhalen, wordt ggplot2 al snel het favoriete datavisualisatietool. Het is eigenlijk het Zwitserse zakmes voor datavisualisatie. Het kan een grafiek met allerlei afmetingen aan. Deze mogelijkheid is ongelofelijk handig tijdens data verkenningsfasen. Soms wil je echter wel eens naar trends kijken zonder dat het je al te veel moeite kost. Dan bekijk je dichte scatterplots met uitschieters. ggplot2 is hier geweldig in. Maar soms kun je niet alle mogelijke dimensies in de statische grafieken onderzoeken. Dan kun je plotly gebruiken.\r\nVoer plotly in. Het plotly-pakket en de ggploty-functie gaan goed samen, ze bewaren de hoge kwaliteit van ggplot2 grafieken en kunnen het ook nog interactief maken.\r\nIn deze tutorial onderzoeken we de mediaan van de gerapporteerde lonen van creatieve beroepen. We onderzoeken de gegevens van deze beroepsgroep van de stad Austin (Texas, USA) voor de jaren 2016 en 2017. Deze dataset is afkomstig van het open dataportaal Austin hier. Het is verbazendwekkend om te zien hoe het AustinGO2.0-team steeds nieuwe en spannende lokale datasets beschikbaar stelt om te verkennen!\r\nInstalleer en laden van pakketten\r\nVoordat we verder gaan, moeten we eerst de pakketten installeren. Als je dat een keer hebt gedaan moet je ze wel steeds voor gebruik laden.\r\n\r\n\r\n\r\nDataset laden\r\nOm het reproduceer te maken moet je de dataset van de Ausatin open data portal halen en uploaden en Laura Ellis heeft dat hier in haar github repo geplaatst githubrepo\r\n\r\n\r\n[1] 54  4\r\n\r\n   SOC.Code          Occupation        X2016.Median.hourly.earnings\r\n Length:54          Length:54          Min.   : 6.79               \r\n Class :character   Class :character   1st Qu.:14.29               \r\n Mode  :character   Mode  :character   Median :20.24               \r\n                                       Mean   :19.68               \r\n                                       3rd Qu.:24.25               \r\n                                       Max.   :34.37               \r\n X2017.Median.hourly.earnings\r\n Min.   : 6.88               \r\n 1st Qu.:15.29               \r\n Median :20.10               \r\n Mean   :20.12               \r\n 3rd Qu.:25.69               \r\n Max.   :34.37               \r\n\r\n'data.frame':   54 obs. of  4 variables:\r\n $ SOC.Code                    : chr  \"11-2011\" \"13-1011\" \"25-1099\" \"25-4011\" ...\r\n $ Occupation                  : chr  \"Advertising and promotions managers\" \"Agents and business managers of artists, performers, and athletes\" \"Postsecondary teachers\" \"Archivists\" ...\r\n $ X2016.Median.hourly.earnings: num  30.1 21.8 26.9 26.7 32.6 ...\r\n $ X2017.Median.hourly.earnings: num  29.7 22.3 26.3 27.3 30.5 ...\r\n\r\n  SOC.Code\r\n1  11-2011\r\n2  13-1011\r\n3  25-1099\r\n4  25-4011\r\n5  25-4012\r\n6  25-4013\r\n                                                         Occupation\r\n1                               Advertising and promotions managers\r\n2 Agents and business managers of artists, performers, and athletes\r\n3                                            Postsecondary teachers\r\n4                                                        Archivists\r\n5                                                          Curators\r\n6                               Museum technicians and conservators\r\n  X2016.Median.hourly.earnings X2017.Median.hourly.earnings\r\n1                        30.11                        29.73\r\n2                        21.83                        22.31\r\n3                        26.92                        26.31\r\n4                        26.66                        27.31\r\n5                        32.56                        30.46\r\n6                        24.33                        25.74\r\n\r\nTransformeer de data\r\nWe voeren wat eenvoudige transformaties uit om de data voor te bereiden voor een makkelijke scatterplot. Eerst geven we de kolommen een andere naam en daarna hebben we een nieuwe kolom gemaakt om de veranderingen in het jaar 2016 tot 2017 te berekenen dat iets zegt over veranderingen in het mediane loon per beroep.\r\n\r\n\r\n\r\nCreëer de scatterplot\r\nWe creëerden een eenvoudige ggplot2-scatterplot van de beroepen met de 2017 mediane loon vs percentage jaarverbetering. We voegden een eenvoudige horizontale lijn toe om de nul in de plot te markeren. Dit stelt ons in staat om de jaarveranderingen in het mediane loon gemakkelijker te verwerken. Vervolgens zetten we nog een laatste stap en voeren we onze ggplot2-scatterplot in de ggplotly-functie in. Zo ontstaat een interactieve grafiek!\r\n\r\n\r\n{\"x\":{\"data\":[{\"x\":[29.73,22.31,26.31,27.31,30.46,25.74,30.4,17.47,28.42,18.09,6.88,9.53,22.13,11.14,25.75,29.46,20.66,12.67,22.34,24.12,15.76,24.34,16,27.91,15.28,17.68,21.1,19.43,23.34,17.52,22.15,20.1,17.6,25.53,15.68,21.39,27.72,34.37,9.77,27.52,10.83,28.01,13.98,12.47,9.36,11.78,15.22,25.53,19.02,15.32,13.8,14.59,15.57,20.1],\"y\":[1.26,-2.2,2.27,-2.44,6.45,-5.8,-1.91,-19.17,-0.96,2.32,-1.33,3.64,-56.06,1.33,1.23,-3.08,-2.08,-9.04,-5.78,-1.43,4.19,-8.18,-8.55,-5.88,1.55,-1.09,-0.14,1.82,-4.43,-0.4,0.67,5.68,21.85,-5.28,0.06,4.38,-0.69,0,-1.45,-22.91,1.72,3.75,-0.72,-5.5,2.5,0.76,-6.88,-5.37,-9.37,-2.61,-6.56,-0.97,-12.1,0.69],\"text\":[\"Median_2017: 29.73<br />Percent_Improvement:   1.26\",\"Median_2017: 22.31<br />Percent_Improvement:  -2.20\",\"Median_2017: 26.31<br />Percent_Improvement:   2.27\",\"Median_2017: 27.31<br />Percent_Improvement:  -2.44\",\"Median_2017: 30.46<br />Percent_Improvement:   6.45\",\"Median_2017: 25.74<br />Percent_Improvement:  -5.80\",\"Median_2017: 30.40<br />Percent_Improvement:  -1.91\",\"Median_2017: 17.47<br />Percent_Improvement: -19.17\",\"Median_2017: 28.42<br />Percent_Improvement:  -0.96\",\"Median_2017: 18.09<br />Percent_Improvement:   2.32\",\"Median_2017:  6.88<br />Percent_Improvement:  -1.33\",\"Median_2017:  9.53<br />Percent_Improvement:   3.64\",\"Median_2017: 22.13<br />Percent_Improvement: -56.06\",\"Median_2017: 11.14<br />Percent_Improvement:   1.33\",\"Median_2017: 25.75<br />Percent_Improvement:   1.23\",\"Median_2017: 29.46<br />Percent_Improvement:  -3.08\",\"Median_2017: 20.66<br />Percent_Improvement:  -2.08\",\"Median_2017: 12.67<br />Percent_Improvement:  -9.04\",\"Median_2017: 22.34<br />Percent_Improvement:  -5.78\",\"Median_2017: 24.12<br />Percent_Improvement:  -1.43\",\"Median_2017: 15.76<br />Percent_Improvement:   4.19\",\"Median_2017: 24.34<br />Percent_Improvement:  -8.18\",\"Median_2017: 16.00<br />Percent_Improvement:  -8.55\",\"Median_2017: 27.91<br />Percent_Improvement:  -5.88\",\"Median_2017: 15.28<br />Percent_Improvement:   1.55\",\"Median_2017: 17.68<br />Percent_Improvement:  -1.09\",\"Median_2017: 21.10<br />Percent_Improvement:  -0.14\",\"Median_2017: 19.43<br />Percent_Improvement:   1.82\",\"Median_2017: 23.34<br />Percent_Improvement:  -4.43\",\"Median_2017: 17.52<br />Percent_Improvement:  -0.40\",\"Median_2017: 22.15<br />Percent_Improvement:   0.67\",\"Median_2017: 20.10<br />Percent_Improvement:   5.68\",\"Median_2017: 17.60<br />Percent_Improvement:  21.85\",\"Median_2017: 25.53<br />Percent_Improvement:  -5.28\",\"Median_2017: 15.68<br />Percent_Improvement:   0.06\",\"Median_2017: 21.39<br />Percent_Improvement:   4.38\",\"Median_2017: 27.72<br />Percent_Improvement:  -0.69\",\"Median_2017: 34.37<br />Percent_Improvement:   0.00\",\"Median_2017:  9.77<br />Percent_Improvement:  -1.45\",\"Median_2017: 27.52<br />Percent_Improvement: -22.91\",\"Median_2017: 10.83<br />Percent_Improvement:   1.72\",\"Median_2017: 28.01<br />Percent_Improvement:   3.75\",\"Median_2017: 13.98<br />Percent_Improvement:  -0.72\",\"Median_2017: 12.47<br />Percent_Improvement:  -5.50\",\"Median_2017:  9.36<br />Percent_Improvement:   2.50\",\"Median_2017: 11.78<br />Percent_Improvement:   0.76\",\"Median_2017: 15.22<br />Percent_Improvement:  -6.88\",\"Median_2017: 25.53<br />Percent_Improvement:  -5.37\",\"Median_2017: 19.02<br />Percent_Improvement:  -9.37\",\"Median_2017: 15.32<br />Percent_Improvement:  -2.61\",\"Median_2017: 13.80<br />Percent_Improvement:  -6.56\",\"Median_2017: 14.59<br />Percent_Improvement:  -0.97\",\"Median_2017: 15.57<br />Percent_Improvement: -12.10\",\"Median_2017: 20.10<br />Percent_Improvement:   0.69\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(81,160,213,1)\",\"opacity\":0.7,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(81,160,213,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5.5055,35.7445],\"y\":[0,0],\"text\":\"yintercept: 0\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(44,82,140,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":\"Austins Mediaan Creatief Uurloon\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[5.5055,35.7445],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Mediaan Beroepsuurloon in 2017\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-59.9555,25.7455],\"tickmode\":\"array\",\"ticktext\":[\"-40\",\"-20\",\"0\",\"20\"],\"tickvals\":[-40,-20,0,20],\"categoryorder\":\"array\",\"categoryarray\":[\"-40\",\"-20\",\"0\",\"20\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"% Toename Jaar na Jaar (2016 tot 2017)\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"c34f5c7e3d\":{\"x\":{},\"y\":{},\"type\":\"scatter\"},\"c34218a17bd\":{\"yintercept\":{}}},\"cur_data\":\"c34f5c7e3d\",\"visdat\":{\"c34f5c7e3d\":[\"function (y) \",\"x\"],\"c34218a17bd\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\r\nVoeg de labels toe\r\nDe bovenstaande grafiek is geweldig omdat we met succes gebruik hebben gemaakt van plotly om de ggplot2-scatterplot interactief te maken. De data hebben echter niet alles wat we willen en het is niet erg mooi geformatteerd. Laten we dit oplossen!\r\nCorinne Leopold, van het team van Laura Ellis, vond een veel efficiëntere manier om labels toe te wijzen in de plots dan de oplossing die Laura eerder vond eerdere oplossing. We voegen eenvoudigweg de details toe via de esthetische tekstuele eigenschap van de ggplot. Vervolgens wijzen we het toe aan de tooltip in de ggplotly-functie.\r\n\r\n\r\n{\"x\":{\"data\":[{\"x\":[29.73,22.31,26.31,27.31,30.46,25.74,30.4,17.47,28.42,18.09,6.88,9.53,22.13,11.14,25.75,29.46,20.66,12.67,22.34,24.12,15.76,24.34,16,27.91,15.28,17.68,21.1,19.43,23.34,17.52,22.15,20.1,17.6,25.53,15.68,21.39,27.72,34.37,9.77,27.52,10.83,28.01,13.98,12.47,9.36,11.78,15.22,25.53,19.02,15.32,13.8,14.59,15.57,20.1],\"y\":[1.26,-2.2,2.27,-2.44,6.45,-5.8,-1.91,-19.17,-0.96,2.32,-1.33,3.64,-56.06,1.33,1.23,-3.08,-2.08,-9.04,-5.78,-1.43,4.19,-8.18,-8.55,-5.88,1.55,-1.09,-0.14,1.82,-4.43,-0.4,0.67,5.68,21.85,-5.28,0.06,4.38,-0.69,0,-1.45,-22.91,1.72,3.75,-0.72,-5.5,2.5,0.76,-6.88,-5.37,-9.37,-2.61,-6.56,-0.97,-12.1,0.69],\"text\":[\"Beroep: Advertising and promotions managers<br />2017: 29.73<br />2016: 30.11<br />% Toename Jaar na Jaar: 1.26<br />\",\"Beroep: Agents and business managers of artists, performers, and athletes<br />2017: 22.31<br />2016: 21.83<br />% Toename Jaar na Jaar: -2.2<br />\",\"Beroep: Postsecondary teachers<br />2017: 26.31<br />2016: 26.92<br />% Toename Jaar na Jaar: 2.27<br />\",\"Beroep: Archivists<br />2017: 27.31<br />2016: 26.66<br />% Toename Jaar na Jaar: -2.44<br />\",\"Beroep: Curators<br />2017: 30.46<br />2016: 32.56<br />% Toename Jaar na Jaar: 6.45<br />\",\"Beroep: Museum technicians and conservators<br />2017: 25.74<br />2016: 24.33<br />% Toename Jaar na Jaar: -5.8<br />\",\"Beroep: Librarians<br />2017: 30.4<br />2016: 29.83<br />% Toename Jaar na Jaar: -1.91<br />\",\"Beroep: Library technicians<br />2017: 17.47<br />2016: 14.66<br />% Toename Jaar na Jaar: -19.17<br />\",\"Beroep: Audio-visual and multimedia collections specialists<br />2017: 28.42<br />2016: 28.15<br />% Toename Jaar na Jaar: -0.96<br />\",\"Beroep: Art directors<br />2017: 18.09<br />2016: 18.52<br />% Toename Jaar na Jaar: 2.32<br />\",\"Beroep: Craft artists<br />2017: 6.88<br />2016: 6.79<br />% Toename Jaar na Jaar: -1.33<br />\",\"Beroep: Fine artists, including painters, sculptors, and illustrators<br />2017: 9.53<br />2016: 9.89<br />% Toename Jaar na Jaar: 3.64<br />\",\"Beroep: Multimedia artists and animators<br />2017: 22.13<br />2016: 14.18<br />% Toename Jaar na Jaar: -56.06<br />\",\"Beroep: Artists and related workers, all other<br />2017: 11.14<br />2016: 11.29<br />% Toename Jaar na Jaar: 1.33<br />\",\"Beroep: Commercial and industrial designers<br />2017: 25.75<br />2016: 26.07<br />% Toename Jaar na Jaar: 1.23<br />\",\"Beroep: Fashion designers<br />2017: 29.46<br />2016: 28.58<br />% Toename Jaar na Jaar: -3.08<br />\",\"Beroep: Graphic designers<br />2017: 20.66<br />2016: 20.24<br />% Toename Jaar na Jaar: -2.08<br />\",\"Beroep: Merchandise displayers and window trimmers<br />2017: 12.67<br />2016: 11.62<br />% Toename Jaar na Jaar: -9.04<br />\",\"Beroep: Set and exhibit designers<br />2017: 22.34<br />2016: 21.12<br />% Toename Jaar na Jaar: -5.78<br />\",\"Beroep: Designers, all other<br />2017: 24.12<br />2016: 23.78<br />% Toename Jaar na Jaar: -1.43<br />\",\"Beroep: Actors<br />2017: 15.76<br />2016: 16.45<br />% Toename Jaar na Jaar: 4.19<br />\",\"Beroep: Producers and directors<br />2017: 24.34<br />2016: 22.5<br />% Toename Jaar na Jaar: -8.18<br />\",\"Beroep: Dancers<br />2017: 16<br />2016: 14.74<br />% Toename Jaar na Jaar: -8.55<br />\",\"Beroep: Choreographers<br />2017: 27.91<br />2016: 26.36<br />% Toename Jaar na Jaar: -5.88<br />\",\"Beroep: Music directors and composers<br />2017: 15.28<br />2016: 15.52<br />% Toename Jaar na Jaar: 1.55<br />\",\"Beroep: Musicians and singers<br />2017: 17.68<br />2016: 17.49<br />% Toename Jaar na Jaar: -1.09<br />\",\"Beroep: Entertainers and performers, sports, and related workers *<br />2017: 21.1<br />2016: 21.07<br />% Toename Jaar na Jaar: -0.14<br />\",\"Beroep: Radio and television announcers<br />2017: 19.43<br />2016: 19.79<br />% Toename Jaar na Jaar: 1.82<br />\",\"Beroep: Editors<br />2017: 23.34<br />2016: 22.35<br />% Toename Jaar na Jaar: -4.43<br />\",\"Beroep: Writers and authors<br />2017: 17.52<br />2016: 17.45<br />% Toename Jaar na Jaar: -0.4<br />\",\"Beroep: Media and communication workers, all other<br />2017: 22.15<br />2016: 22.3<br />% Toename Jaar na Jaar: 0.67<br />\",\"Beroep: Audio and video equipment technicians<br />2017: 20.1<br />2016: 21.31<br />% Toename Jaar na Jaar: 5.68<br />\",\"Beroep: Radio operators *<br />2017: 17.6<br />2016: 22.52<br />% Toename Jaar na Jaar: 21.85<br />\",\"Beroep: Sound engineering technicians<br />2017: 25.53<br />2016: 24.25<br />% Toename Jaar na Jaar: -5.28<br />\",\"Beroep: Photographers<br />2017: 15.68<br />2016: 15.69<br />% Toename Jaar na Jaar: 0.06<br />\",\"Beroep: Camera operators, television, video, and motion picture<br />2017: 21.39<br />2016: 22.37<br />% Toename Jaar na Jaar: 4.38<br />\",\"Beroep: Film and video editors<br />2017: 27.72<br />2016: 27.53<br />% Toename Jaar na Jaar: -0.69<br />\",\"Beroep: Media and communication equipment workers, all other<br />2017: 34.37<br />2016: 34.37<br />% Toename Jaar na Jaar: 0<br />\",\"Beroep: Ushers, lobby attendants, and ticket takers<br />2017: 9.77<br />2016: 9.63<br />% Toename Jaar na Jaar: -1.45<br />\",\"Beroep: Costume attendants<br />2017: 27.52<br />2016: 22.39<br />% Toename Jaar na Jaar: -22.91<br />\",\"Beroep: Entertainment attendants and related workers, all other<br />2017: 10.83<br />2016: 11.02<br />% Toename Jaar na Jaar: 1.72<br />\",\"Beroep: Makeup artists, theatrical and performance<br />2017: 28.01<br />2016: 29.1<br />% Toename Jaar na Jaar: 3.75<br />\",\"Beroep: Library assistants, clerical<br />2017: 13.98<br />2016: 13.88<br />% Toename Jaar na Jaar: -0.72<br />\",\"Beroep: Musical instrument repairers and tuners<br />2017: 12.47<br />2016: 11.82<br />% Toename Jaar na Jaar: -5.5<br />\",\"Beroep: Sewers, hand *<br />2017: 9.36<br />2016: 9.6<br />% Toename Jaar na Jaar: 2.5<br />\",\"Beroep: Tailors, dressmakers, and custom sewers *<br />2017: 11.78<br />2016: 11.87<br />% Toename Jaar na Jaar: 0.76<br />\",\"Beroep: Cabinetmakers and bench carpenters *<br />2017: 15.22<br />2016: 14.24<br />% Toename Jaar na Jaar: -6.88<br />\",\"Beroep: Model makers, wood *<br />2017: 25.53<br />2016: 24.23<br />% Toename Jaar na Jaar: -5.37<br />\",\"Beroep: Furnace, kiln, oven, drier, and kettle operators and tenders *<br />2017: 19.02<br />2016: 17.39<br />% Toename Jaar na Jaar: -9.37<br />\",\"Beroep: Jewelers and precious stone and metal workers<br />2017: 15.32<br />2016: 14.93<br />% Toename Jaar na Jaar: -2.61<br />\",\"Beroep: Photographic process workers and processing machine operators *<br />2017: 13.8<br />2016: 12.95<br />% Toename Jaar na Jaar: -6.56<br />\",\"Beroep: Etchers and engravers *<br />2017: 14.59<br />2016: 14.45<br />% Toename Jaar na Jaar: -0.97<br />\",\"Beroep: Molders, shapers, and casters (except metal and plastic) *<br />2017: 15.57<br />2016: 13.89<br />% Toename Jaar na Jaar: -12.1<br />\",\"Beroep: <br />2017: 20.1<br />2016: 20.24<br />% Toename Jaar na Jaar: 0.69<br />\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(81,160,213,1)\",\"opacity\":0.7,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(81,160,213,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5.5055,35.7445],\"y\":[0,0],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(44,82,140,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":\"Austins Mediaan Creatief Uurloon\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[5.5055,35.7445],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Mediaan Beroepsuurloon in 2017\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-59.9555,25.7455],\"tickmode\":\"array\",\"ticktext\":[\"-40\",\"-20\",\"0\",\"20\"],\"tickvals\":[-40,-20,0,20],\"categoryorder\":\"array\",\"categoryarray\":[\"-40\",\"-20\",\"0\",\"20\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"% Toename Jaar na Jaar (2016 tot 2017)\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"c344d5348c5\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"scatter\"},\"c3459c369c7\":{\"yintercept\":{}}},\"cur_data\":\"c344d5348c5\",\"visdat\":{\"c344d5348c5\":[\"function (y) \",\"x\"],\"c3459c369c7\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\r\nOver het opslaan en delen van de grafiek vermeldt Laure Ellis ook nog het een en ander. Maar daar kijken we hier verder niet naar. Mij ging het erom plotly zelf beter in de vingers te krijgen. De volgende keer maar eens kijken hoe het met Shiny gaat.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-20-bbc-en-data-journalisme/",
    "title": "BBC en data-journalisme",
    "description": "Een blog over hoe de BBC omgaat met visualisatie en data-journalisme",
    "author": [
      {
        "name": "R-bloggers, bewerking Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-02-20",
    "categories": [],
    "contents": "\r\nHoe kunnen we grafieken in BBC-stijl produceren\r\nBij het BBC datateam hebben ze een R-pakket en een R-kookboek ontwikkeld. Met dat pakket en kookboek wordt het proces van het maken van publicatieklare grafieken in hun huisstijl geautomatiseerd. Met behulp van R’s ggplot2-pakket wordt er een meer reproduceerbaar proces van gemaakt. Voor nieuwe R-gebruikers wordt het zo ook gemakkelijker gemaakt om deze grafieken te maken.\r\n\r\nHet kookboek, dat in februari 2019 hier verscheen, heb ik in het Nederlands vertaald om zelf goed door te krijgen hoe het allemaal werkt. Het kookboek en deze Nederlandse versie wil iedereen helpen om dit soort grafieken te maken:\r\nLaten we eens zien hoe we de verschillende elementen van deze grafieken in elkaar kunnen zetten. Maar eerst eens enkele administratieve zaken…\r\nLaad eerst alle pakketten die je nodig hebt in R\r\nEen paar van de stappen in dit kookboek - om grafieken in R in het algemeen te kunnen maken - vereisen dat bepaalde pakketten worden geïnstalleerd en geladen. Zo hoeft u ze niet één voor één te installeren en te laden. Door de p_load-functie in het pacman-pakket kunt u ze allemaal tegelijk laden met de volgende code.\r\n\r\n\r\n\r\nInstalleer in ieder geval het bbplot pakket\r\nbbplot staat niet op CRAN (het algemene platform voor R-pakketten). U moet het direct vanaf Github installeren met behulp van devtools.\r\nAls u het devtools-pakket niet heeft geïnstalleerd, moet u ook de eerste regel in de code hieronder uitvoeren.\r\n\r\n\r\n\r\nVoor meer informatie over bbplot-pakket, bekijk het volgende Github repo. Dat wat u moet weten over het gebruik van het pakket en de functies, is hieronder gedetailleerd te vinden.\r\nWanneer u het pakket hebt gedownload en met succes hebt geïnstalleerd, bent u in staat om grafieken te gaan maken.\r\nHoe werkt dat bbplot-pakket eigenlijk?\r\nHet pakket heeft twee functies, bbc_style() en finalise_plot().\r\nbbc_style(): heeft geen argumenten en wordt toegevoegd aan de ggplot ‘keten’ nadat je een plot hebt gemaakt. Wat het doet is over het algemeen tekstgrootte, lettertype en kleur, aslijnen, aslijnen, as-tekst, marges en vele andere standaardgrafiekonderdelen in BBC-stijl. Die zijn geformuleerd op basis van aanbevelingen en feedback van het ontwerpteam.\r\nMerk op dat kleuren voor lijnen in het geval van een lijndiagram bijvoorbeeld of balken voor een staafdiagram, niet uit het kader van de bbc_style() functie komen. Die moeten expliciet worden ingesteld in uw standaard ggplot-grafiekfuncties.\r\nDe code hieronder laat zien hoe de bbc_style() gebruikt moet worden binnen de standaard workflow om grafieken te maken. Dit is een voorbeeld van een zeer eenvoudige lijndiagram. De data waar gebruik van wordt gemaakt komen uit het gapminder-pakket.\r\n\r\n\r\n\r\n\r\n\r\n\r\nDit is wat de bbc_style()-functie eigenlijk onder de motorkap doet. Het wijzigt in wezen bepaalde argumenten in de thema functie van ggplot2.\r\nHet eerste argument is bijvoorbeeld het instellen van het lettertype, de grootte, het lettertype en de kleur van het titelelement van de grafiek.\r\n\r\n\r\nfunction () \r\n{\r\n    font <- \"Helvetica\"\r\n    ggplot2::theme(plot.title = ggplot2::element_text(family = font, \r\n        size = 28, face = \"bold\", color = \"#222222\"), plot.subtitle = ggplot2::element_text(family = font, \r\n        size = 22, margin = ggplot2::margin(9, 0, 9, 0)), plot.caption = ggplot2::element_blank(), \r\n        legend.position = \"top\", legend.text.align = 0, legend.background = ggplot2::element_blank(), \r\n        legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), \r\n        legend.text = ggplot2::element_text(family = font, size = 18, \r\n            color = \"#222222\"), axis.title = ggplot2::element_blank(), \r\n        axis.text = ggplot2::element_text(family = font, size = 18, \r\n            color = \"#222222\"), axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, \r\n            b = 10)), axis.ticks = ggplot2::element_blank(), \r\n        axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), \r\n        panel.grid.major.y = ggplot2::element_line(color = \"#cbcbcb\"), \r\n        panel.grid.major.x = ggplot2::element_blank(), panel.background = ggplot2::element_blank(), \r\n        strip.background = ggplot2::element_rect(fill = \"white\"), \r\n        strip.text = ggplot2::element_text(size = 22, hjust = 0))\r\n}\r\n<bytecode: 0x000000001ed90ee8>\r\n<environment: namespace:bbplot>\r\n\r\nU kunt deze instellingen voor uw grafiek wijzigen of extra thema-argumenten toevoegen. Dit kan door thema-functie aan te roepen met de argumenten die u wilt. Maar, let op: om het te laten werken, moet u de bbc_style functie hebben aangeroepen. Anders zal bbc_style() deze overrulen.\r\nDit voegt enkele rasterlijnen toe, door extra thema-argumenten toe te voegen aan wat er in de bbc_style() functie zit. Er zitten verschillende gelijkaardige voorbeelden in het kookboek.\r\n\r\n\r\n\r\nSla jouw afgeronde grafiek op\r\nNa het toevoegen van de bbc_style() aan uw grafiek is er nog een stap om uw grafiek klaar te maken voor publicatie. finalise_plot(), de tweede functie van het bbplot-pakket, zal de titel links uitlijnen, ondertitelen en de footer toevoegen met een bron en een afbeelding in de rechter benedenhoek van uw grafiek. Het zal het ook opslaan op de door u opgegeven locatie. De functie heeft vijf argumenten:\r\nHier zijn de argumenten van de functie: finalise_plot(plot_name, source, save_filepath, width_pixels = 640, height_pixels = 450).\r\nplot_name: de variabelenaam die u uw plot heeft genoemd, bijvoorbeeld voor het voorbeeld hierboven zou “lijn” de plot_name zijn\r\nsource: de brontekst die u linksonder in uw plot wilt weergeven. U moet het woord \"Source: \"Bron:\"voor het woord typen, dus bijvoorbeeld source = \"Bron\": Gapminder\" zou de juiste manier zijn om dat te doen.\r\nSave_filepath: type hier het precieze bestandspad waarin u uw afbeelding wilt opslaan, inclusief de .png extensie aan het einde. Dit is afhankelijk van uw werkmap en of u zich in een specifiek R-project bevindt. Een voorbeeld van een bestandspad zou zijn: Desktop/R_projecten/figuren/lijngrafiek.png.\r\nwidth_pixels: dit is standaard ingesteld op 640px, dus noem dit argument alleen als u wilt dat de grafiek een andere breedte heeft, en geef aan wat u wilt dat het is.\r\nhoogte_pixels: dit is standaard ingesteld op 450px, dus noem dit argument alleen als u wilt dat de grafiek een andere hoogte heeft en geef aan wat u wilt dat het is.\r\nlogo_image_path: dit argument specificeert het pad voor het beeld/logo in de rechter benedenhoek van het diagram. De standaardinstelling is voor een PNG-bestand met een achtergrond die overeenkomt met de achtergrondkleur van het diagram. Dus specificeer het argument niet als u wilt dat het zonder logo verschijnt. Als u uw eigen logo wilt toevoegen, hoeft u alleen het pad naar uw PNG-bestand te specificeren. Het pakket is opgezet met een brede en dunne afbeelding in gedachten.\r\nVoorbeeld van hoe de finalise_plot() wordt gebruikt in een standaard workflow. Deze functie wordt aangeroepen zodra u uw grafiekgegevens, titels en de bbc_style() eraan hebt toegevoegd:\r\n\r\n\r\n\r\nAls u eenmaal uw plot hebt gemaakt en er relatief tevreden mee bent, kunt u de finalise_plot() functie gebruiken om de laatste aanpassingen te maken en uw grafiek op te slaan zodat u er buiten RStudio naar kunt kijken.\r\nHet is belangrijk om te vermelden dat het een goed idee is om dit in een vroeg stadium te doen omdat de positie van de tekst en andere elementen niet nauwkeurig worden weergegeven in het RStudio Plots paneel. Hier is het afhankelijk van de grootte en de aspect ratio die u wilt dat uw plot verschijnt. Het opslaan en openen van de bestanden geeft u een nauwkeurige weergave van hoe de grafiek eruit ziet.\r\nDe finalise_plot()-functie doet meer dan alleen het opslaan van je grafiek. Het lijnt ook de titel en ondertiteling links uit zoals standaard is voor BBC-grafieken, voegt een footer toe met het logo aan de rechterkant en laat je de brontekst aan de linkerkant invoeren.\r\nDus hoe kunt u de bovenstaande voorbeeldplot opslaan?\r\n\r\n\r\n\r\nMaak een lijn figuur\r\n\r\n\r\n\r\n\r\n\r\n\r\nMake een grafiek met verschillende lijnen\r\n\r\n\r\n\r\n\r\n\r\n\r\nMaak een staafdiagram\r\n\r\n\r\n\r\n\r\n\r\n\r\nMaak een gestapelde staaf diagram\r\n\r\n\r\n\r\n\r\n\r\n\r\nDit voorbeeld toont proporties, maar u wilt misschien een gestapelde staafdiagram maken met nummerwaarden - dit is eenvoudig te veranderen!\r\nDe waarde die wordt doorgegeven aan het position-argument zal bepalen of uw gestapelde grafiek verhoudingen of werkelijke waarden toont.\r\npositie = \"fill\" zal uw stapels vertonen als proporties, en position = \"identity\" zal getalwaarden laten zien.\r\nMaak een gegroepeerde staafdiagram\r\nHet maken van een gegroepeerde staafdiagram lijkt erg op het maken van een staafdiagram.\r\nJe hoeft alleen maar position = \"identity\" te veranderen in position=\"dodge\" en de vulling wordt esthetisch.\r\n\r\n\r\n\r\n\r\n\r\n\r\nMaak een halter grafiek\r\nEen andere manier om het verschil te laten zien is een ‘dumbbell’-grafiek:\r\n\r\n\r\n\r\nMaak een histogram\r\n\r\n\r\n\r\nBreng veranderingen aan in de legenda\r\nVerwijder de legenda\r\nVerwijder de legende - het is beter om de gegevens direct te labelen met tekstannotaties.\r\nGebruik guides(colour=FALSE) om de legende te verwijderen voor een specifieke esthetiek (vervang kleur door de relevante esthetiek).\r\n\r\n\r\n\r\nU kunt ook alle legenden in één keer verwijderen met behulp van theme(legend.position = \"none\"):\r\n\r\n\r\n\r\nVerander de positie van de legenda\r\nDe standaardpositie van de legenda staat bovenaan uw grafiek. Verplaats de legenda naar links, rechts of onderaan buiten de plot met:\r\n\r\n\r\n\r\nOm echt precies te zijn over waar we onze legenda naartoe willen, in plaats van “rechts” of “boven” te specificeren om de algemene positie van waar de legende in onze grafiek verschijnt te veranderen, kunnen we het specifieke coördinaten geven.\r\nBijvoorbeeld legend.position=c(0.98,0.1) zal de legenda naar rechtsonder verplaatsen. Ter referentie, c(0,0) is linksonder, c(1,0) is rechtsonder, c(0,1) is linksboven en zo verder). Het vinden van de exacte positie kan wat vallen en opstaan met zich meebrengen.\r\nOm de exacte positie te controleren waar de legenda in uw definitieve plot verschijnt, moet u het bestand controleren dat is opgeslagen nadat u uw finalise_plot() functie hebt uitgevoerd, aangezien de positie relevant is voor de afmetingen van de grafiek.\r\n\r\n\r\n\r\nOm de legenda tegen de linkerkant van uw grafiek te laten aankomenen, kan het makkelijker zijn om een negatieve linkermarge voor de legenda in te stellen met behulp van legend.margin. De syntaxis is margin(top, right, bottom, left).\r\nU zult moeten experimenteren om het juiste getal te vinden om de marge voor uw grafiek in te stellen. Sla het op met finalise_plot() en bekijk hoe het eruit ziet.\r\n\r\n\r\n\r\nVerwijder de titel van de legenda\r\nVerwijder de titel van de legenda door uw thema()aan te passen. Vergeet niet dat eventuele wijzigingen aan het thema moeten worden toegevoegd nadat u bbc_style() hebt opgeroepen!\r\n\r\n\r\n\r\nGooi de volgorde van de legenda om\r\nSoms moet u de volgorde van uw legenda wijzigen, zodat deze overeenkomt met de volgorde van uw balken. Hiervoor heb je guides nodig:\r\n\r\n\r\n\r\nVerander de layout van jouw legenda\r\nAls u veel waarden in uw legenda hebt, moet u de lay-out misschien om esthetische redenen aanpassen.\r\nU kunt het aantal rijen opgeven dat u wilt dat u in uw legenda door als argument guides te gebruiken gebruiken. Het onderstaande codefragment, bijvoorbeeld, zal een legende met 4 rijen maken:\r\n\r\n\r\n\r\nHet kan nodig zijn om fill in de bovenstaande code te veranderen in een esthetiek die uw legenda beschrijft, zoals size, colour, enz.\r\nLaat de legendasymbolen er anders uit zien\r\nU kunt het standaardinstelling van de legendesymbolen overschrijven, zonder de manier waarop ze in de plot verschijnen, door het argument override.aes toe te voegen aan guides.\r\nHet onderstaande maakt bijvoorbeeld de grootte van de legendesymbolen groter:\r\n\r\n\r\n\r\nBreng wat ruimte aan tussen de legenda labels\r\nDe standaard ggplot-legenda heeft bijna geen ruimte tussen individuele legenda-items. Niet ideaal.\r\nU kunt ruimte toevoegen door de schaallabels handmatig te wijzigen.\r\nAls u bijvoorbeeld de kleur van uw geoms zo hebt ingesteld dat deze afhankelijk is van uw gegevens, krijgt u een legenda voor de kleur en kunt u de exacte labels aanpassen om wat extra ruimte te krijgen met behulp van het onderstaande fragment:\r\n\r\n\r\n\r\nAls uw legende iets anders laat zien, moet u de code dienovereenkomstig wijzigen. Bijvoorbeeld, voor het vullen heeft u scale_fill_manual() in plaats daarvan nodig.\r\nPas de assen aan\r\nGooi de coordinaten van een plot om\r\nVoeg coord_flip() toe om verticale staven horizontaal te maken:\r\n\r\n\r\n\r\n\r\n\r\n\r\nToevoegen/weghalen van gridlijnen\r\nHet standaard thema heeft alleen rasterlijnen voor de y-as. Voeg x rasterlijnen toe met panel.grid.major.x = element_line.\r\n(Verwijder op dezelfde manier de rasterlijnen op de y-as met panel.grid.major.y=element_blank())\r\n\r\n\r\n\r\n\r\n\r\n\r\nVerander de astekst met de hand\r\nU kunt de tekstlabels van de assen vrij wijzigen met scale_y_continuous of scale_x_continuous:\r\n\r\n\r\n\r\nDit specificeert ook de grenzen van uw grafiek en waar u as-tekens wilt hebben.\r\nVoeg duizend scheidingstekens toe aan u as-labels\r\nU kunt aangeven dat u wilt dat uw as-tekst duizend scheidingstekens heeft met een argument schale_y_continuous.\r\nEr zijn twee manieren om dit te doen, een in basis R die een beetje lastig is:\r\n\r\n\r\n\r\nDe tweede manier is gebaseerd op het scales- pakket, maar is veel beknopter:\r\n\r\n\r\n\r\nVoeg een percentagesymbool toe aan jouw aslabels\r\nDit is ook gemakkelijk met een argument toe te voegen aan scale_y_continuous:\r\n\r\n\r\n\r\nVerander de limieten van de plot\r\nDe lange manier om de grenzen van uw grafiek expliciet in te stellen is met scale_y_continuous zoals hierboven. Maar als u de pauzes of labels niet hoeft op te geven, dan is dat met xlim of ylim:\r\n\r\n\r\n\r\nVoeg astitels toe\r\nOns standaardthema heeft geen as-titels, maar u kunt ze misschien handmatig toevoegen. Dit doet u door theme() - merk op dat je dit moet doen na de aanroep naar bbc_style() anders worden je wijzigingen overruled:\r\n\r\n\r\n\r\nPas astitels aan\r\nAls u in de assen titels toevoegt, zijn dit standaard de kolomnamen van uw dataset. U kunt dit veranderen door labs()op te roepen in wat u doet.\r\nAls u bijvoorbeeld wilt dat de x-as de titel “Ik ben een as” krijgt en de y-as label leeg is, dan is dit het formaat:\r\n\r\n\r\n\r\nAdd axis ticks\r\nU kunt as-streepjes toevoegen door axis.ticks.x of axis.ticks.y toe te voegen aan uw theme:\r\n\r\n\r\n\r\nToevoegen van annotaties\r\nVoeg een annotatie toe\r\nDe eenvoudigste manier om een tekstannotatie toe te voegen aan uw grafiek is met behulp van geom_label:\r\n\r\n\r\n\r\nDe exacte positionering van de annotatie zal afhangen van de x en y argumenten (wat een beetje lastig is!) en de tekstuitlijning, met behulp van hjust en vjust - maar meer van dat hieronder.\r\nVoeg regelafbrekingen waar nodig in uw label toe met n\\ en stel de regelhoogte in met lineheight.\r\n\r\n\r\n\r\nLaten we onze directe labels erin krijgen!\r\n\r\n\r\n\r\n\r\n\r\n\r\nTekst links en rechts uitlijnen\r\nDe argumenten hjust en vjust dicteren horizontale en verticale tekstuitlijning. Ze kunnen een waarde tussen 0 en 1 hebben, waarbij 0 links-uitlijnend en 1 rechts-uitlijnend is (of onder- en boven-uitlijnend voor verticale tekstuitlijning).\r\nVoeg labels toe op basis van jouw data\r\nMet de bovenstaande methode voor het toevoegen van annotaties aan uw grafiek kunt u de x- en y-coördinaten precies aangeven. Dit is erg handig als we een tekstannotatie op een bepaalde plaats willen toevoegen, maar het zou erg vervelend zijn om te herhalen.\r\nGelukkig, als u labels wilt toevoegen aan al uw datapunten, kunt u in plaats daarvan eenvoudigweg de positie instellen op basis van uw gegevens.\r\nLaten we zeggen dat we gegevenslabels willen toevoegen aan onze staafdiagram:\r\n\r\n\r\n\r\nBovenstaande code voegt automatisch één tekstlabel toe voor elk continent zonder dat we vijf keer geom_label moeten toevoegen.\r\n(Als je in de war raakt over waarom we de x instellen als de continenten en y als levensverwachting, als de grafiek ze andersom lijkt te tekenen, dan is dat omdat we de coördinaten van de plot hebben omgedraaid met coord_flip(), wat je kunt doen lees meer over hier.)\r\nVoeg links uitgelijnde labels toe aan jouw staafdiagrammen\r\nAls u liever links uitgelijnde labels voor uw balken toevoegt, stelt u gewoon het x argument in op basis van uw gegevens, maar specificeer dan het y argument direct, met een numerieke waarde.\r\nDe exacte waarde van y hangt af van het bereik van uw gegevens.\r\n\r\n\r\n\r\nVoeg een lijn toe\r\nVoeg een lijn toe met geom_segment:\r\n\r\n\r\n\r\nHet size-argument specificeert de dikte van de lijn.\r\nVoeg een gecurfte lijn toe\r\nVoor een gekromde lijn gebruikt u geom_curve in plaats van geom_segment:\r\n\r\n\r\n\r\nHet curvature argument bepaalt de kromming van de curve: 0 is een rechte lijn, negatieve waarden geven een linkse curve en positieve waarden geven een rechtse curve.\r\nVoeg een pijl toe\r\nEen lijn in een pijl omzetten is vrij eenvoudig: voeg gewoon het arrow argument toe aan je geom_segment of geom_curve:\r\n\r\n\r\n\r\nHet eerste argument, unit, stelt de grootte van de pijlpunt in.\r\nVoeg een lijn over de hele figuur toe\r\nDe eenvoudigste manier om een lijn over het hele perceel toe te voegen is met geom_vline(), voor een verticale lijn, of geom_hline(), voor een horizontale lijn.\r\nOptionele extra argumenten stellen u in staat om de grootte, kleur en het type lijn te specificeren (de standaard optie is een effen lijn).\r\n\r\n\r\n\r\nDe lijn voegt natuurlijk niet veel toe in dit voorbeeld, maar dit is handig als je iets wilt benadrukken, bijvoorbeeld een drempelwaarde of een gemiddelde waarde.\r\nHet is ook vooral handig omdat onze ontwerpstijl - zoals u misschien al hebt gemerkt in de grafieken op deze pagina - bestaat uit het toevoegen van een verticale of horizontale basislijn aan onze grafieken. Dit is de code om te gebruiken:\r\n\r\n\r\n\r\nWerken met kleinere figuren\r\nKleine, meervoudige kaarten zijn eenvoudig te maken met ggplot: het heet facetteren.\r\nFacetteren\r\nAls u gegevens hebt die u wilt visualiseren opgesplitst naar een variabele, moet u facet_wrap of facet_grid gebruiken.\r\nVoeg de variabele die je wilt delen aan deze regel code toe: facet_wrap( ~ variabele) `.\r\nEen extra argument bij facet_wrap, ncol, stelt u in staat om het aantal kolommen te specificeren:\r\n\r\n\r\n\r\n\r\n\r\n\r\nVrije schalen\r\nHet is u wellicht opgevallen dat Oceanië, met zijn relatief kleine bevolking, volledig is verdwenen in de bovenstaande grafiek.\r\nStandaard wordt bij facetteren gebruik gemaakt van vaste asschalen over de kleine veelvouden. Het is altijd het beste om dezelfde y-as schaal over kleine veelvouden te gebruiken, om misleiding te voorkomen, maar soms moet je deze onafhankelijk van elkaar instellen voor elk veelvoud, wat we kunnen doen door het argument schales= \"free\".\r\nAls je alleen de schalen voor één as wilt vrijgeven, zet je het argument op free_x of free_y.\r\n\r\n\r\n\r\n\r\n\r\n\r\nDoe iets anders uiteindelijk\r\nToename en afname van marges\r\nU kunt de marge rond bijna elk element van uw plot - de titel, ondertitels, legenda - of de grafiek zelf wijzigen.\r\nNormaal gesproken hoeft u de standaardmarges van het thema niet te wijzigen, maar als u dat wel doet, is de syntaxis theme(ELEMENT=element_text(margin=margin(0, 5, 10, 0))).\r\nDe getallen specificeren respectievelijk de boven-, rechter-, onder-, en linkermarge - maar u kunt ook direct aangeven welke marge u wilt wijzigen. Laten we bijvoorbeeld proberen de ondertitel een extra grote ondermarge te geven: You can change the margin around almost any element of your plot - the title, subtitles, legend - or the plot itself.\r\n\r\n\r\n\r\nHm… misschien niet.\r\nExporteer jouw figuur en x-as marges\r\nU moet wel nadenken over uw x-as margematen wanneer u een figuur produceert dat buiten de standaardhoogte in bbplot ligt, dat is 450px. Dit kan bijvoorbeeld het geval zijn als u een staafdiagram maakt met veel balken en ervoor wilt zorgen dat er wat ademruimte is tussen elke staaf en labels. Als u de marges laat zoals ze zijn voor figuren met een grotere hoogte, dan kunt u een grotere afstand tussen de as en uw labels krijgen.\r\nHier is een handleiding waar we aan werken als het gaat om de marges en de hoogte van uw staafdiagram (met coord_flip erop toegepast):\r\n\r\nsize\r\nt\r\nb\r\n550px\r\n5\r\n10\r\n650px\r\n7\r\n10\r\n750px\r\n10\r\n10\r\n850px\r\n14\r\n10\r\n\r\nDus wat u zou moeten doen is deze code toevoegen aan uw grafiek als u bijvoorbeeld de hoogte van je plot 650px wilde hebben in plaats van 450px.\r\n\r\n\r\n\r\nHoewel het veel minder waarschijnlijk is, maar als u het equivalent wilt doen voor een lijndiagram en het wilt exporteren op een grotere hoogte dan de standaard hoogte, moet u hetzelfde doen. Maar ook uw waarden voor t veranderen in negatieve waarden op basis van de bovenstaande tabel.\r\nHeroderdenen van de staven op basis van de grootte\r\nStandaard zal R uw gegevens in alfabetische volgorde weergeven, maar in plaats daarvan herordenen op basis van grootte is eenvoudig: gewoon reorder() om de x of y variabele leggen die u wilt herschikken, en geef aan voor welke variabele u de gegevens wilt herschikken.\r\nBijvoorbeeld x = reorder (country, pop). Oplopende volgorde is de standaard, maar u kunt deze veranderen in aflopende volgorde door desc() rond de variabele te plaatsen waar u de volgorde van wilt wijzigen:\r\n\r\n\r\n\r\n\r\n\r\n\r\nStaven met de hand herordenen\r\nSoms moet u uw gegevens ordenen op een manier die niet alfabetisch of gerangschikt op grootte is.\r\nOm deze correct te ordenen moet u de factorniveaus van uw gegevens instellen voordat u het figuur maakt.\r\nSpecificeer de volgorde waarin u de categorieën in het levels argument wilt afdrukken:\r\n\r\n\r\n\r\nU kunt dit ook gebruiken om de delen van een gestapelde staafdiagram opnieuw te ordenen.\r\nKleur staven conditioneel\r\nU kunt esthetische waarden zoals vulling, alpha en grootte voorwaardelijk instellen met ifelse().\r\nDe syntaxis is fill=ifelse(logical_condition, fill_if_true, fill_if_false).\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2019-02-20-bbc-en-data-journalisme/bbc-en-data-journalisme_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-23-data-visualization-a-practical-introduction/",
    "title": "Data visualisatie. Een practische introductie",
    "description": "Naar aanleiding van het nieuwe boek van Kieran Healey. Data visualization/A Practical Introduction.",
    "author": [
      {
        "name": "Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-01-23",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn 2014 schreeft Kieran Healey samen met James Moody in de ‘Annual Review of Sociology’ (2014. 40: 105-28) een artikel over datavisualisatie in de sociologie. Visualiseren van data verdient meer aandacht in de sociale wetenschappen. Ondanks dat er in de beginjaren van de sociologie hier de nodige aandacht voor was, zijn de gereedschappen om dit goed te maken achtergebleven. Mensen als Du Bois, geïnspireerd door expressieve schilderkunst, deden dat op een geweldige manier. Vele jaren later waren er ook mensen als Cleveland en, vooral, Tufte die hier baanbrekend werk hebben verricht. Zij lieten vooral zien hoe de visualisatie eruit zou moeten zien en hadden weer minder aandacht voor hoe het zou doen en wat daarvoor nodig is. Methodes en gereedschappen ontbraken lange tijd. In deze tijd van delen van codes en data delen is er veel meer mogelijk. Onderzoekers en publicisten kunnen nu een stap voorwaarts zetten.\r\nSinds dat artikel heeft Kieran Healey niet stil gezeten. Vijf jaar lang hield hij zich bezig met datavisualisatie en publiceerde hier regelmatig over en hield de ene na de andere workshop. Ikzelf hield hem de afgelopen twee jaar nauwlettend in het oog omdat ik grote waardering voor zijn werk heb. Recent verscheen zijn nieuwe boek ‘Data visualization. A practical introduction’ (Princeton and Oxford: Princeton University Press, 2019). De tekst heb ik letterlijk op internet zien ontstaan in bookdown-vorm. Het boek heb ik deze maand ontvangen. Het is een ‘must’ voor mensen die willen leren hoe je data visualiseert maar ook voor mensen die willen leren hoe je op een moderne manier met data omgaat. Een prachtig boek, ik kan dat niet voldoende benadrukken.\r\nIn het eerste hoofdstuk (Look at Data) kijkt Kieran Healy met ons naar data. Visualisaties zijn, volgens hem, bedoeld om naar te kijken en daarom moet je ook weten wie er naar kijkt en waarom. Visualiseren is een goede manier om data nader te onderzoeken, te begrijpen en samenhang in de data te kunnen verklaren. Figuren kunnen slecht gemaakt worden, bijvoorbeeld vanwege een slechte smaak of omdat iets niet goed te lezen is. Aan een slecht figuur kunnen ook slechte data ten grondslag liggen. Tot slot kan de grafiek je ook misleiden en kan er een gat zitten tussen de data en de esthetica. Perceptie.\r\nVervolgens legt hij ons in het tweede hoofdstuk (Get Started) uit hoe we grafiek gaan maken. Healey werkt met R en het pakket ggplot2 waar hij in het hele boek mee werkt. Hij laat zien hoe je met R en zijn bedieningspaneel RStudio moet werken, hoe je met RMarkdown kunt werken en hoe je een project start. Vervolgens legt hij een aantal basiszaken van R uit die je ook kunt overslaan als je deze kennis al hebt. Maar Healey is heel scherp en duidelijk en een hele goede leermeester, volgens mij, voor mensen die er weinig van weten. Maar dat is hij ook voor mensen die al meer weten en vervolgens laat hij aan het einde van het hoofdstuk de eerste, zeer eenvoudige figuur zien van de samenhang tussen wat mensen verdienen en de levensduur in de wereld.\r\n\r\n\r\n# A tibble: 1,704 x 6\r\n   country     continent  year lifeExp      pop gdpPercap\r\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\r\n 1 Afghanistan Asia       1952    28.8  8425333      779.\r\n 2 Afghanistan Asia       1957    30.3  9240934      821.\r\n 3 Afghanistan Asia       1962    32.0 10267083      853.\r\n 4 Afghanistan Asia       1967    34.0 11537966      836.\r\n 5 Afghanistan Asia       1972    36.1 13079460      740.\r\n 6 Afghanistan Asia       1977    38.4 14880372      786.\r\n 7 Afghanistan Asia       1982    39.9 12881816      978.\r\n 8 Afghanistan Asia       1987    40.8 13867957      852.\r\n 9 Afghanistan Asia       1992    41.7 16317921      649.\r\n10 Afghanistan Asia       1997    41.8 22227415      635.\r\n# ... with 1,694 more rows\r\n\r\n\r\n\r\n\r\n(#fig:01-first_plot)Life expectancy plotted against GDP per capita for a large number of country-years.\r\n\r\n\r\n\r\nIn hoofdstuk 3 (Make a Plot) gaat hij veel uitgebreider in op hoe we een figuur maken. Belangrijk is steeds dat data heel netjes zijn opgebouwd. Als dat het geval is werkt het grafiekenprogramma ggplot2 goed. Niets voor niets is het onderdeel van het tidyverse-pakket. ggplot2 is laagsgewijs opgebouwd waarbij je steeds een aantal stappen achter elkaar moet zetten:\r\n1. Je vertelt ggplot()functie eerst over welke data we het hebben.\r\n2. Dan vertel je ggplot() welke relaties je wilt zien.\r\n3. Vervolgens vertel je ggplot() hoe je de relaties wilt zien.\r\n4. Voeg er dan nog een laag aan toe (geom) als dat nodig is en voeg die toe aan p, waar Healey de hele tijd mee werkt.\r\n5. Tot slot gebruik je, eventueel, nog enkele schalen, labels, titels en dergelijk. En die voeg je er aan het einde aan toe.\r\nJe krijgt dan zo’n commando met zo’n resultaat:\r\n\r\n\r\n\r\nFigure 1: A more polished plot of Life Expectancy vs GDP.\r\n\r\n\r\n\r\nIn het vierde hoofdstuk (Show the Right Numbers) gaat hij verder met het uitleggen van ggplot en allerlei andere zaken die er mee kunt doen. In dit hoofdstuk laat hij bijvoorbeeld zien hoe je een aantal figuren naast elkaar kunt zetten.\r\n\r\n\r\n\r\n\r\n\r\n\r\nEn Healey laat hele andere figuren zien, zoals hieronder, waar hij positieve en negeatieve verschillen laat zien tussen de USA en de OECD-landen.\r\n\r\n\r\n\r\nIn het vijfde hoofdstuk (Graph Tables, Add Labels, Make Notes) gaat hij verder met het maken van een figuur, maar hij laat dan veel meer mogelijkheden zien. Het ggplot2-pakket is, zoals gezegd, onderdeel van Wickhams tidyverse-pakket dat je in staat stelt de data op een eenvoudige en logische manier aan te passen, hier dus voordat je de grafiek maakt.\r\nEerst pas je de dataset aan:\r\n\r\n\r\n\r\n\r\n\r\n# A tibble: 4 x 2\r\n  bigregion total\r\n  <fct>     <dbl>\r\n1 Northeast   100\r\n2 Midwest     101\r\n3 South       100\r\n4 West        101\r\n\r\nVervolgens maak je de grafiek en zet je preferenties van religie naast elkaar.\r\n\r\n\r\n\r\nEn dan verdeel je het duidelijk per regio en trek je die regio’s uit elkaar.\r\n\r\n\r\n\r\nIn dit hoofdstuk laat hij ook nog enkele aanvullingen zien waarmee je kunt werken. Je kunt er een tekst inzetten.\r\n\r\n\r\n\r\nOf een tekst en een blok om iets extra’s aan te geven.\r\n\r\n\r\n\r\nHij geeft ook aan hoe je de labels eraan zet.\r\n\r\n\r\n\r\nWanneer je het maken van een grafiek en wat daarbij komt kijken onder de knie hebt, gaat hij in de twee volgende hoofdstukken in op specieke onderwerpen. Modelleren is een belangrijk onderdeel van omgaan met data. Ook modellen kun je visualiseren en dat werken met modellen is voor hem het onderwerp van het zesde hoofdstuk (Work with Models). Hieronder zie je bijvoorbeeld drie statistische modellen netjes op een rij gezet:\r\n\r\n\r\n[1] \"#E41A1C\" \"#377EB8\" \"#4DAF4A\"\r\n\r\n\r\nbroom is een R-pakket waar je op een goede en eenvoudige manier mee kunt modelleren. Hiermee krijg je schattingen en intervallen, maar die kun je ook weer visualiseren. Hoe je dat doet, zie je hieronder.\r\n\r\n\r\nClasses 'tbl_df', 'tbl' and 'data.frame':   1704 obs. of  6 variables:\r\n$ country : Factor w/ 142 levels \"Afghanistan\",..: 1 1 ...\r\n$ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 ...\r\n$ year : int 1952 1957 ...\r\n$ lifeExp : num 28.8 ...\r\n$ pop : int 8425333 9240934 ...\r\n$ gdpPercap: num 779 ...\r\n\r\n\r\n\r\n\r\n\r\n\r\nCall:\r\nlm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-49.161  -4.486   0.297   5.110  25.175 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       4.781e+01  3.395e-01 140.819  < 2e-16 ***\r\ngdpPercap         4.495e-04  2.346e-05  19.158  < 2e-16 ***\r\npop               6.570e-09  1.975e-09   3.326 0.000901 ***\r\ncontinentAmericas 1.348e+01  6.000e-01  22.458  < 2e-16 ***\r\ncontinentAsia     8.193e+00  5.712e-01  14.342  < 2e-16 ***\r\ncontinentEurope   1.747e+01  6.246e-01  27.973  < 2e-16 ***\r\ncontinentOceania  1.808e+01  1.782e+00  10.146  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 8.365 on 1697 degrees of freedom\r\nMultiple R-squared:  0.5821,    Adjusted R-squared:  0.5806 \r\nF-statistic: 393.9 on 6 and 1697 DF,  p-value: < 2.2e-16\r\n\r\n\r\n\r\n\r\n\r\n\r\n# A tibble: 7 x 5\r\n  term              estimate std.error statistic p.value\r\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)          47.8      0.34     141.         0\r\n2 gdpPercap             0        0         19.2        0\r\n3 pop                   0        0          3.33       0\r\n4 continentAmericas    13.5      0.6       22.5        0\r\n5 continentAsia         8.19     0.570     14.3        0\r\n6 continentEurope      17.5      0.62      28.0        0\r\n7 continentOceania     18.1      1.78      10.2        0\r\n\r\n\r\n\r\n\r\n\r\n\r\n# A tibble: 7 x 7\r\n  term         estimate std.error statistic p.value conf.low conf.high\r\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\r\n1 (Intercept)     47.8      0.34     141.         0    47.2      48.5 \r\n2 gdpPercap        0        0         19.2        0     0         0   \r\n3 pop              0        0          3.33       0     0         0   \r\n4 continentAm~    13.5      0.6       22.5        0    12.3      14.6 \r\n5 continentAs~     8.19     0.570     14.3        0     7.07      9.31\r\n6 continentEu~    17.5      0.62      28.0        0    16.2      18.7 \r\n7 continentOc~    18.1      1.78      10.2        0    14.6      21.6 \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nLanden van Europa, provincies van Nederland of steden van een provincie kun je goed visualiseren. Soms kun je gegevens van die landen, provincies of steden ook goed zichtbaar maken. Dat kan ook met ggplot2 en daarover schrijft Healey in hoofdstuk 7 (Draw maps). Hieronder zie je bijvoorbeeld het percentage ‘black Americans’ per countie afgebeeld.\r\n\r\n\r\n       long      lat group order  region subregion\r\n1 -87.46201 30.38968     1     1 alabama      <NA>\r\n2 -87.48493 30.37249     1     2 alabama      <NA>\r\n3 -87.52503 30.37249     1     3 alabama      <NA>\r\n4 -87.53076 30.33239     1     4 alabama      <NA>\r\n5 -87.57087 30.32665     1     5 alabama      <NA>\r\n6 -87.58806 30.32665     1     6 alabama      <NA>\r\n\r\n[1] 15537     6\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Mapping the results\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 3: Election 2016 by State\r\n\r\n\r\n\r\nOp het verfijnen van de figuren gaat Healey in het laatste en achtste hoofdstuk (Refine Your Plots) in. Dat komt natuurlijk vooral in de laatste fase van het maken van figuren aan de orde wanneer de figuren klaar moeten worden gemaakt om gepubliceerd te worden of als je een speciale aanpassing in je hoofd hebt. Hier bespreekt hij kleurengebruik en gaat hij in op het gebruik van een bepaald thema (zeg je wilt de figuur in de stijl van de Economist hebben) of wanneer je een van een slecht figuur een goed figuur wilt maken, zoals hieronder:\r\n\r\n\r\n\r\nFigure 4: Redrawing as a connected scatterplot.\r\n\r\n\r\n\r\nEn dan hier de betere figuur:\r\n\r\n\r\n\r\nFigure 5: Plotting the ratio of revenue to employees against time.\r\n\r\n\r\n\r\nHealey’s boek is een prachtig boek. Niet alleen omdat hij ons goed naar figuren laat kijken en ons leert hoe je die moet maken. Met Data Visualization leert hij ons hoe je op een moderne manier met data om kunt gaan: elegant, logisch en coherent. Naast zijn boek heeft hij een groot aantal codes geschreven en beschikbaar gesteld voor vrij gebruik (zie zijn website: https://kieranhealy.org/ of zijn codes op github: https://github.com/kjhealy). Hij laat je heel goed zien hoe hij alles heeft gemaakt en het klopt allemaal. Zijn boek zal ik blijven lezen en zijn werk zal ik blijven volgen. Mijn petje af Kieran, en heel hartelijke dank voor al die dingen die je gedaan hebt en met anderen deelt. Mij heb in in ieder geval geïnspireerd om er een cursus rondom op te bouwen. Binnenkort volgt hier de link van de cursus. Binnenkort staat hier ook de link naar de code van dit document (.rmd).\r\n\r\n\r\n",
    "preview": "posts/2019-01-23-data-visualization-a-practical-introduction/data-visualization-a-practical-introduction_files/figure-html5/01-first_plot-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-statistisch-omdenken/",
    "title": "Statistisch omdenken",
    "description": "Over 'Statistical rethinking' van Richard McElreath (2016).",
    "author": [
      {
        "name": "Harrie Jonkman met dan aan Solomon Kurz.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-12-23",
    "categories": [],
    "contents": "\r\nOver Bayesiaanse statistiek zijn ondertussen verschillende boeken geschreven die jou leren om hiermee te werken. Het boek van Nzoufras (Bayesian modeling using WinBUGS), Lunn et al. (The BUGS Book. A Practical Introduction to Bayesian Analysis), Kéry (Introduction to WinBUGS for ecologists), Broemeling (Bayesian Methods in Epidemiology) en Cowles (Applied Bayesian Statistics) introduceren niet alleen het concept maar laten ook zien hoe je hier als onderzoeker mee kunt werken. Deze wetenschappers gebruiken het programma WinBUGS of OpenBUGS. Jim Alberts (Bayesian Computation with R) en John Kruschke’s (Doing Bayesian Data Analysis) zijn ook wetenschappelijke boeken die deze vorm van onderzoek introduceren, maar zij beiden werken met R. In de serie cursusboeken Bayesiaanse statistiek is het boek van Richard McElreath (Statistical Rethinking. A Bayesian Course with Examples in R and Stan) het meest recent. Het is een echt cursusboek waar je onder begeleiding of zelf mee aan de slag kunt. Het boek zit vol met codes want McElreath is ervan overtuigd dat als je met deze analysetechniek wilt kunnen werken, je ook moet weten hoe je er mee kunt werken. Interessant zoals hij vaststelt dat tegenwoordig studenten computerwijs maar dat zij van computercodes zelf weer betrekkelijk weinig weten. Om daarmee verder te komen, is het nodig dat ze dat wel leren. McElreaths boek is een oefenboek en de lezer wordt expliciet uitgenodigd problemen met de computer uit te werken. Om ermee te werken moet je R en RStudio op de computer installeren, het pakket binnen halen en je moet rstan (een C++ compiler) installeren. McElreath levert er de codes bij en via internet kun je ook nog zijn interesssante colleges volgen. Allemaal geen eenvoudige kost maar degene die er de moeite voor neemt krijgt veel terug voor zijn of haar inspanningen.\r\nEen algemene introductie\r\nDe eerste drie hoofdstukken van Statistical rethinking zijn een algemene introductie op Bayesiaanse statistiek. Wetenschappers maken wetenschappelijke modellen die hij ziet als golems, Joodse kleibeeldjes die na de middeleeuwen voor de waarheid stonden en deden wat hen werd opgedragen, althans dat dachten mensen. Wetenschappelijke modellen zijn ook constructen die voor de waarheid staan en een duidelijk doel hebben. Ze berekenen zaken voor ons, voeren indrukwekkende calculaties uit en vinden patronen die anders voor ons onder de oppervlakte zouden blijven. Volgens McElreath is het huidige statistisch instrumentarium te beperkt om antwoorden te geven op complexere zaken waar we tegenwoordig mee worden geconfronteerd. Wetenschap kan deductieve falsificatie, waar wetenschappers na Popper steeds op gewezen hebben, nauwelijks waar maken omdat hypotheses toch iets anders zijn dan modellen die wel te onderzoeken zijn. Modellen kun je toetsen en hypothesen eigenlijk niet. Hoe je de modellen onderzoekt, doet er dan wel toe. In die nieuwe gereedschapskist die ons ter beschikking staat, zitten voor McElreath drie hele duidelijke gereedschappen:\r\n- Baysiaanse data analyse waarbij het gaat om waarschijnlijkheid en waarbij je het aantal mogelijkheden die volgens onze aannames kunnen gebeuren steeds moet tellen. Het is de meest logische en consistente manier om met informatie om te gaan.\r\n- Multilevel modellen waarbij clusters of groepen worden onderscheiden en waarbij steeds andere waarden kunnen gelden juist omdat ze zo bij elkaar horen.\r\n- Model vergelijkingen en informatiecriteria waarbij het gaat om criteria waarmee vergelijkingen worden gemaakt en accuratesse wordt ingeschat.\r\nMcElreath begint zijn boek heel eenvoudig. Stel, zo legt hij voor, dat we plastic aardbol hebben (een opblaasbol, zeg maar) en deze een aantal keren in de lucht gooien. We vangen het met een vinger op en kijken wat er onder die vinger zit (water of land). Stel dat we dat negen keer achter elkaar doen en dat deze activiteit dan de volgende gegevens oplevert.\r\n\r\n\r\n# A tibble: 9 x 1\r\n  toss \r\n  <chr>\r\n1 w    \r\n2 l    \r\n3 w    \r\n4 w    \r\n5 w    \r\n6 l    \r\n7 w    \r\n8 l    \r\n9 w    \r\n\r\nStel dat water het succes is en we negen keer gooien en laten we uitdrukken wat we gevonden hebben:\r\n\r\n\r\n# A tibble: 9 x 3\r\n  toss  n_trials n_success\r\n  <chr>    <int>     <int>\r\n1 w            1         1\r\n2 l            2         1\r\n3 w            3         2\r\n4 w            4         3\r\n5 w            5         4\r\n6 l            6         4\r\n7 w            7         5\r\n8 l            8         5\r\n9 w            9         6\r\n\r\nAls we dat dan vervolgens ook nog in tekeningen uitdrukken, zien die er achtereenvolgens zo een beetje uit:\r\n\r\n\r\n\r\nDe Bayesiaanse techniek bestaat steeds uit een vast aantal componenten. Allereerst is er de likelihood en dat zijn de data waar je mee te maken hebt. Hier boven is dat bijvoorbeeld het aantal keren dat je gegooid hebt en de keren dat de vinger water raakt. Dan heb je de parameters waarmee je werkt (bv. aantal keren water en de kans op water). Dan heb je de prior, de inschatting die je van te voren maakt. Als je niks weet kun je zeggen dat de kans op water=.5 (net zo groot als de kans op land). Maar als je weet dat er meer water is dan land kun je zeggen dat de kans op water tussen .5 en .9 ligt. Tot slot is er de posterior, de combinatie van alle drie (likelihood, parameters en prior) en veelal uitgedrukt wordt als:\r\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\]\r\nNu zijn er verschillende technieken om deze posterior uit te rekenen. Om met complexe modellen te kunnen werken,worden MCMC-technieken gebruikt en daar maakt Richard Elreath in zijn boek ruim gebruik van. Als het model dat is gekozen een posterior distributie maakt, is eigenlijk het werk gedaan en kunnen hiermee verschillende vragen worden beantwoord als:\r\n- wat is de kans dat een bepaalde waarde voorkomt?\r\n- wat is de kans dat de waarde tussen 50% en 80% in ligt?\r\n- welke waarde heeft de hoogste posterior waarschijnlijkheid?\r\netc. Met Bayesiaanse technieken maak je samples die je weer gebruikt om intervallen te definiëren, om puntschattingen te maken, om voorspellingen te doen, om de gegevens op een andere manier te simuleren of wat je ook maar wilt.\r\nEenvoudige Regressies\r\nNadat McElreath in het eerste deel van het boek de basis heeft uitgelegd, begint hij in het tweede deel verschillende modellen uit te leggen. Hij begint met betrekkelijk simpele lineaire modellen. Stel dat we een dataset nemen die in het pakket zit. De meest simpele vorm druk je zo uit:\r\n\\[\r\n\\begin{eqnarray}\r\n\\text{outcome}_i & \\sim & \\text{Normal}(\\mu_i, \\sigma) \\\\\r\n\\mu_i & = & \\beta \\times \\text{predictor}_i \\\\\r\n\\beta & \\sim & \\text{Normal}(0, 10) \\\\\r\n\\sigma & \\sim & \\text{HalfCauchy}(0, 1)\r\n\\end{eqnarray}\r\n\\] Stel dat we een dataset nemen die in het pakket zit, bv.;\r\n\r\n\r\n\r\nWe openen een ander pakket, waar ik in de conclusie op terug kom.\r\n\r\n\r\n\r\nWe kijken eens even hoe het data bestand eruit ziet:\r\n\r\n\r\n'data.frame':   544 obs. of  4 variables:\r\n $ height: num  152 140 137 157 145 ...\r\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\r\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\r\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\r\n\r\nDan halen we alleen de variabele lengte eruit:\r\n\r\n\r\n   height\r\n1 151.765\r\n2 139.700\r\n3 136.525\r\n4 156.845\r\n5 145.415\r\n6 163.830\r\n\r\nVervolgens gebruiken we alleen de data van de volwassenen:\r\n\r\n\r\n\r\nHieronder draaien we dan een analyse. En dit wordt steeds op dezelfde manier gedefinieerd. Je definieert het model, dan gebruik je brm en zegt welke data je gebruikt en welke statistische familie je gebruikt, vervolgens definieer je het statistische model, je definieert de parameters die gebruikt, je definieert het aantal iteraties en hoeveel je daarbij als warming up gebruikt, je definieert het aantallen kettingen en het aantal computerdelen.\r\n\r\n\r\n\r\nLaten we zien wat het grafische oplevert:\r\n\r\n\r\n\r\nLaten we de gegevens ook in een tabel samenvatten:\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: height ~ 1 \r\n   Data: d2 (Number of observations: 352) \r\nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n          Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\r\nIntercept   154.61      0.42   153.82   155.47       3481 1.00\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\r\nsigma     7.75      0.30     7.19     8.34       3570 1.00\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Eff.Sample \r\nis a crude measure of effective sample size, and Rhat is the potential \r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nAls we aan deze simpele regressie nog een andere, onafhankelijke variabele toevoegen (gewicht), krijgen we de volgende analyse:\r\n\r\n\r\n\r\nOok hier eerst het grafische:\r\n\r\n\r\n\r\nEn vervolgens de gegevens voor de tabel:\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: height ~ 1 + weight \r\n   Data: d2 (Number of observations: 352) \r\nSamples: 4 chains, each with iter = 41000; warmup = 40000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n          Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\r\nIntercept   113.87      1.95   109.96   117.68        770 1.00\r\nweight        0.91      0.04     0.82     0.99        757 1.00\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\r\nsigma     5.11      0.20     4.73     5.51       2485 1.00\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Eff.Sample \r\nis a crude measure of effective sample size, and Rhat is the potential \r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nNadat McElreath heeft laten zien hoe het werkt met eenvoudige regressies (inclusief interacties dat in het volgende hoofdstuk aan de orde komt), gaat hij in op het gebruik van informatie criteria. Dat zijn criteria waarmee je modellen met elkaar kunt vergelijken. De DIC en WAIC zijn informatiecriteria die in deze techniek het beste te gebruiken zijn.\r\nLaten we eens zien. Haal een dataset binnen, bv. de dataset milk die in het pakket rethinking zit. Daar gebruiken we alleen de complete gegevens van en we passen de variabele neocortex aan.\r\n\r\n\r\n\r\nHoeveel dimensies d zitten er in:\r\n\r\n\r\n[1] 17  9\r\n\r\nLaad dan weer het pakket brms.\r\n\r\n\r\n\r\nWe onderzoeken vier verschillende kcal.per.g modellen .\r\n\r\n\r\n\r\nVervolgens kun je deze vier modellen met elkaar vergelijken en de modellen met de laagste waic-score laat de beste balans zien tussen eenvoud en complexiteit.\r\n\r\n\r\n                WAIC   SE\r\nb6.11          -8.73 3.70\r\nb6.12          -7.45 3.19\r\nb6.13          -8.91 4.24\r\nb6.14         -16.94 5.20\r\nb6.11 - b6.12  -1.28 1.16\r\nb6.11 - b6.13   0.17 2.33\r\nb6.11 - b6.14   8.21 4.94\r\nb6.12 - b6.13   1.45 2.99\r\nb6.12 - b6.14   9.49 5.05\r\nb6.13 - b6.14   8.04 3.55\r\n\r\nAndere regressies\r\nVoor Bayesiaanse technieken worden tegenwoordig MCMC-technieken gebruikt die met ingewikkelde random berekeningen als het ware het complexe geheel kunnen opsplitsen in kleinere eenheden. Door dit heel vaak te draaien kunnen de posterior samples worden gemaakt. Met Gibbs en Metropolitan wordt al sinds de negentiger jaren gewerkt. Gelman en anderen hebben de laatste jaren de Hamiltonian Monte Carlo ontwikkelt die nog weer beter werkt in complexe modellen (Stan). Stan heeft weer een eigen taal. Maar met McElreaths MAP en met Brueckners brms pakket kan hier makkelijker mee worden gewerkt. Het betrekkelijke ingewikkelde HMC werkt in deze pakketten achter de coulissen. Laten we een voorbeeld nemen:\r\nWe laden eerst de rugged data in.\r\n\r\n\r\n\r\nDan gaan we over naar brms.\r\n\r\n\r\n\r\nWe doen wat data manipulatie.\r\n\r\n\r\n\r\nVervolgens werken we met HMC en we zien bij het brm-pakket ook weer dezelfde volgorde. Eerst het model definiëren, dan brm en vaststellen welke data je gebruikt en welke statistische familie. Dan schrijf je het model uit en vervolgens definieer je de priors.\r\n\r\n\r\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 0.141 seconds (Warm-up)\r\nChain 1:                0.105 seconds (Sampling)\r\nChain 1:                0.246 seconds (Total)\r\nChain 1: \r\n\r\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 2).\r\nChain 2: \r\nChain 2: Gradient evaluation took 0 seconds\r\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 2: Adjust your expectations accordingly!\r\nChain 2: \r\nChain 2: \r\nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 2: \r\nChain 2:  Elapsed Time: 0.134 seconds (Warm-up)\r\nChain 2:                0.107 seconds (Sampling)\r\nChain 2:                0.241 seconds (Total)\r\nChain 2: \r\n\r\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 3).\r\nChain 3: \r\nChain 3: Gradient evaluation took 0.001 seconds\r\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\r\nChain 3: Adjust your expectations accordingly!\r\nChain 3: \r\nChain 3: \r\nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 3: \r\nChain 3:  Elapsed Time: 0.112 seconds (Warm-up)\r\nChain 3:                0.12 seconds (Sampling)\r\nChain 3:                0.232 seconds (Total)\r\nChain 3: \r\n\r\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 4).\r\nChain 4: \r\nChain 4: Gradient evaluation took 0 seconds\r\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 4: Adjust your expectations accordingly!\r\nChain 4: \r\nChain 4: \r\nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 4: \r\nChain 4:  Elapsed Time: 0.109 seconds (Warm-up)\r\nChain 4:                0.113 seconds (Sampling)\r\nChain 4:                0.222 seconds (Total)\r\nChain 4: \r\n\r\nEn dan, in dit geval alleen, de resultaten voor de tabel.\r\n\r\n\r\n Family: gaussian \r\n  Links: mu = identity; sigma = identity \r\nFormula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa \r\n   Data: dd (Number of observations: 170) \r\nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample\r\nIntercept              9.22      0.14     8.95     9.49       2923\r\nrugged                -0.20      0.08    -0.35    -0.05       2603\r\ncont_africa           -1.94      0.23    -2.39    -1.50       2523\r\nrugged:cont_africa     0.39      0.13     0.13     0.65       2377\r\n                   Rhat\r\nIntercept          1.00\r\nrugged             1.00\r\ncont_africa        1.00\r\nrugged:cont_africa 1.00\r\n\r\nFamily Specific Parameters: \r\n      Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\r\nsigma     0.95      0.05     0.85     1.06       3900 1.00\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Eff.Sample \r\nis a crude measure of effective sample size, and Rhat is the potential \r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\nAls we met deze MCMC-techniek kunnen werken, kunnen we vervolgens ook andere statistische modellen (GLM-modellen) draaien. Denk aan exponentiële, gamma en poisson distributies en daarmee zijn andere regressies (andere families) mogelijk.\r\nLaten we een logistische regressie als voorbeeld nemen en de chimpanzees data laden.\r\n\r\n\r\n\r\nDan weer naar brms.\r\n\r\n\r\n\r\nEerst een logistische regressie met alleen een intercept.\r\n\\[\r\n\\begin{eqnarray}\r\n\\text{pulled_left}_i & \\sim & \\text{Binomial} (1, p_i) \\\\\r\n\\text{logit} (p_i) & = & \\alpha \\\\\r\n\\alpha & \\sim & \\text{Normal} (0, 10)\r\n\\end{eqnarray}\r\n\\] En dan weer het model draaien.\r\n\r\n\r\n\r\nLaten we alleen eens naar het intercept kijken.\r\n\r\n\r\n          Estimate Est.Error Q2.5 Q97.5\r\nIntercept     0.32      0.09 0.14   0.5\r\n\r\nDeze resultaten kunnen ook omgevormd worden tot de logistische functie.\r\n\r\n\r\n[1] 0.5448789 0.6130142\r\n\r\n           Estimate Est.Error      Q2.5     Q97.5\r\nIntercept 0.5786748 0.5228834 0.5348199 0.6226705\r\n\r\nVervolgens voegen we aan het logistisch model enkele predictoren aan toe en draaien nog eens twee modellen.\r\n\r\n\r\n\r\nEn dan vergelijken we de drie modellen die we tot dan toe hebben gemaakt met elkaar:\r\n\r\n\r\n\r\n\r\n\r\n                WAIC   SE\r\nb10.1         688.00 7.06\r\nb10.2         680.38 9.36\r\nb10.3         682.60 9.46\r\nb10.1 - b10.2   7.62 6.20\r\nb10.1 - b10.3   5.40 6.28\r\nb10.2 - b10.3  -2.22 0.81\r\n\r\nOp dezelfde manier werkt McElreath ook andere count-regressies en dergelijke uit, waar we hier verder niet op ingaan.\r\nMultilevel en andere zaken\r\nWanneer hij heeft uitgelegd hoe Bayesiaanse analyse werkt en je ook kunt werken met informatiecriteria, komt hij bij de de multilevel modellen uit (zijn derde uitgangspunt). Multilevel analyse is toch wel zo’n beetje de ‘state of art’ in regressie analyses omdat het enkele voordelen heeft, waaronder:\r\n- het maakt betere schattingen over herhaalde metingen;\r\n- het verbetert de schattingen als er verschillen zijn tussen subsamples en; - het kan variaties en gemiddelden over deze subsamples beter inschatten en het voorkomt zo versimpelen.\r\nOm dit duidelijk te maken onderzoeken we de overleving van kikkers in verschillende omgevingen en definiëren enkele modellen. We werken met de reedfrogs data van het rethinking pakket.\r\n\r\n\r\n\r\nWe laten rethinking los en laden brms.\r\n\r\n\r\n\r\nWat zit er in het reedfrogs-data bestand? Laten we dat eens onderzoeken we met tidyverse pakket.\r\n\r\n\r\nObservations: 48\r\nVariables: 5\r\n$ density  <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, ...\r\n$ pred     <fct> no, no, no, no, no, no, no, no, pred, pred, pred...\r\n$ size     <fct> big, big, big, big, small, small, small, small, ...\r\n$ surv     <int> 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, ...\r\n$ propsurv <dbl> 0.90, 1.00, 0.70, 1.00, 0.90, 0.90, 1.00, 0.90, ...\r\n\r\nWe maken de tank variabele (hogere orde variabele, de omgevingen van de kikker).\r\n\r\n\r\n\r\nHier is de formule voor het model zonder multilevel karakter nog.\r\n\\[\r\n\\begin{eqnarray}\r\n\\text{surv}_i & \\sim & \\text{Binomial} (n_i, p_i) \\\\\r\n\\text{logit} (p_i) & = & \\alpha_{\\text{tank}_i} \\\\\r\n\\alpha_{\\text{tank}} & \\sim & \\text{Normal} (0, 5)\r\n\\end{eqnarray}\r\n\\]\r\nHier is de code daarvan:\r\n\r\n\r\n\r\nVervolgens is hier het multilevel model\r\n\\[\r\n\\begin{eqnarray}\r\n\\text{surv}_i & \\sim & \\text{Binomial} (n_i, p_i) \\\\\r\n\\text{logit} (p_i) & = & \\alpha_{\\text{tank}_i} \\\\\r\n\\alpha_{\\text{tank}} & \\sim & \\text{Normal} (\\alpha, \\sigma) \\\\\r\n\\alpha & \\sim & \\text{Normal} (0, 1) \\\\\r\n\\sigma & \\sim & \\text{HalfCauchy} (0, 1)\r\n\\end{eqnarray}\r\n\\]\r\nEn dat multilevel model specificeer je weer zo (inclusief hyperparameter tank:\r\n\r\n\r\n\r\nLaten we de twee modellen (geen en wel multilvel) eens naast elkaar zetten:\r\n\r\n\r\n                WAIC   SE\r\nb12.1         201.06 9.23\r\nb12.2         200.53 7.21\r\nb12.1 - b12.2   0.53 4.45\r\n\r\nNaast dat intercept model laat je ook zien dat er verschillen in de slope kunnen zitten. Want qua drukte zijn café’s niet alleen verschillend van elkaar, ook is de drukte over de dag heen (drukte*tijdstip) verschillend. Ook dat kun je in jouw modellen op nemen.\r\nTot slot\r\nMet het OpenBugs programma weet ikzelf goed te werken en ook werken met het Bayesiaanse MLwiN-deel gaat mij goed af. De laatste jaren wordt er steeds weer met steeds beterem, nieuwe programma’s gewerkt (zoals nu met Stan). Voor mensen die dagelijks met deze programma’s werken is het geen probleem om zich dat eigen te maken. Ikzelf werk er soms mee maar niet dagelijks en dan is het wel een grote inspanning om bij te blijven. Ik heb het idee dat de ontwikkelaars daar niet altijd bij stil staan. In dit blog heb ik gebruik gemaakt van het nieuwe brms-programma van Bürkner. Salomon Kurz heeft dit heel goed voor het boek van McElreath verwerkt (https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/). Toch heb ik van het boek van Richard McElreath heel veel geleerd en ik raad het mensen aan het te lezen.\r\n\r\n\r\n",
    "preview": "posts/2018-12-23-statistisch-omdenken/statistisch-omdenken_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-26-communiceren-met-rmarkdown/",
    "title": "Communiceren met RMarkdown",
    "description": "RMarkdown is de nieuwe manier om diverse wetenschappelijke producten te delen met anderen. Het kan op verschillende manieren gereproduceerd worden en het kan de opbrengsten aantrekkelijk communiceren naar de buitenwereld. Hier een introductie op de werkwijze en enkele mogelijke producten.",
    "author": [
      {
        "name": "M. Schmidt en bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-11-26",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\nVan \\(\\Latex\\) naar RMarkdown\r\nEerder schreef ik een snelle introductie op enkele algemene kenmerken van LaTeX, een opensource software syteem om verschillende soorten documenten te zetten. Dit programmma wordt vooral gebruikt voor maken van wetenschappelijke documenten. Die introductie is hier  te vinden. Daarin laat ik zien hoe \\(\\Latex\\) werkt en welke verschillende soorten documenten je ermee kunt maken (waaronder artikel, boek, rapport, een poster, een proefschrift, een presentatie). Verder kun je hier \\(\\Latex\\)-tutorial ook informatie vinden en hier Snel overzicht.\r\n\\(\\Latex\\) werkt met veel verschillende commando’s en je moet de tijd nemen dit te leren. Op internet is overigens wel goede informatie te vinden en de meeste problemen kun je zelf oplossen. De laatste jaren vinden er binnen het programma R veel ontwikkelingen plaats die het maken van wetenschappelijke documenten vergemakkelijken. De ontwikkelingen vallen onder de term RMarkdown dat in 2015 met de introductie van het knitr-pakket werd geïntroduceerd. Hier wordt binne R gebruik gemaakt van de Markdown taal waarmee technische documenten betrekkelijk eenvoudig te maken zijn. Het pakket maakt het mogelijk om verschillende soorten documenten te maken (waaronder pdf, html en word). Tot slot maakt RMarkdown het mogelijk om tekst (inclusief bv. grafieken, tabellen en referentie) en analyses in een keer te draaien. Met RMarkdown, knitr en alles wat hier aan vastzit kun je onder anderen:\r\n- Een goed uitziend wetenschappelijk document maken in verschillende formats tegelijkertijd (pdf, html en word);\r\n- Kun je met notebooks werken waarin naast tekst analyses zijn opgenomen;\r\n- Kun je nieuwe vormen van presentaties voorbereiden;\r\n- Kun je dashboards maken waarin informatie overzichtelijk, aantrekkelijk, flexibel en interactief wordt gepresenteerd;\r\n- Kun je interactieve toepassingen maken bijvoorbeeld door de inzet van Shiny; - Kun je wetenschappelijke artikelen maken; - Kun je boeken zelf maken; - KUn je een blog en website maken.\r\nEen uitgebreide gids hierover vind je hier RMarkdown: The Definitive Guide. En een handleiding vind je hier Cheat sheet RMarkdown.\r\nInstallatie\r\nOm met onderstaande te kunnen werken moet je in ieder geval R installeren R alsmede RStudio RStudio. Om pdf te draaien moet je ook Latex op je computer installeren. Heb je dat niet kun je ook binnen R/RStudio het pakket tinytex. Als je R en RStudio hebt binnengehaald moet je de pakketten knitr en rmarkdown binnenhalen. Voor achtergrondinformatie over R, RStudio en RMarkdown vind je hier RRStudioRMarkdowninformatie\r\nEerder schreef ik al uitgebreid over Reproducable Research en de workshop die Schmidt op 11 Mei 2016 en er waren wat aanvullende materialen beschikbaar zie post Reproducable Research.Delen daarvan breng ik hier nog eens onder de aandacht en ik update de informatie.\r\nInstructies bij het installeren\r\nVoordat je aan het werk gaat, zorg ervoor dat je het volgende hebt gedaan:\r\nOpen RStudio.\r\nInstalleer en download het devtools R pakket door het volgende commando te runnen.\r\n\r\n\r\ninstall.packages(\"devtools\") # Nodig voor deze sessie\r\nlibrary(\"devtools\")          # Nodig voor deze sessie   \r\n\r\nCheck of je de goede versie hebt van R en RStudio door devtools::session_info() in de R console te draaien.\r\nHier geeft devtools:: aan om de session_info() functie in R te gebruiken ipv het devtools pakket en de sessionInfo() functie binnen het utils pakket. Het runnen van devtools::session_info() stelt ons in staat de versie van R en RStudio vast te stellen.\r\nHeb je de volgende versie van R en RStudio?\r\nR: Versie 3.3.0 (2016-05-03)\r\nRStudio: 0.99.1172\r\nZo ja dan kun je van start gaan!\r\nZo nee dan heb je nieuwe versies van R en RStudio nodig, volg dan Setup in dit document.\r\n\r\nInstalleer vervolgens enkele R pakketten die je nodig hebt.\r\n\r\n\r\n## Installeer de goede pakketten\r\ninstall.packages(\"rmarkdown\")  # Dit zorgt voor koele dynamische documenten\r\ninstall.packages(\"knitr\")      # Hier kun je R code Chunks mee runnen\r\ninstall.packages(\"ggplot2\")    # Voor het plotten van mooie figuren\r\ninstall.packages(\"DT\")         # Om interactieve HTML tabellen te maken\r\n\r\n\r\n\r\n## Deze pakketten ook laden om er zeker van te zijn dat je de goede pakketten hebt\r\nlibrary(\"rmarkdown\")           # Dit zorgt voor koele dynamische documenten\r\nlibrary(\"knitr\")               # Hier kun je R code Chunks mee runnen\r\nlibrary(\"ggplot2\")             # Voor het plotten van mooie figuren\r\nlibrary(\"DT\")                  # Om interactieve HTML tabellen te maken\r\n\r\nAls je de pakketten zonder fouten hebt geladen, kun je beginnen!\r\nGoede bronnen\r\nDeze tutorial kon niet samengesteld worden zonder onderstaande goede bronnen:\r\nDe RMarkdown website van RStudio.\r\nDr. Yuhui Xie’s boek: Dynamic Documents with R and Knitr 2nd Edition [@Xie2015] en zijn Knitr website.\r\nHEEL VEEL DANK aan Dr. Xie voor het schrijven van het knitr pakket!!\r\n\r\nDr. Karl Broman’s “Knitr in a Knutshell”.\r\nCheatsheets released by RStudio.\r\nSinds 2016 zijn er meer interessante documenten uitgekomen: 1. Het standaardboek over RMarkdown RMarkdown: The Definitive Guide 2. Christopher Gandrud, Reproducable Research with R and RStudio.\r\nDynamische documenten\r\nLiterate programming, zoals dat in het Engels wordt genoemd, is het basisidee achter dynamische documenten en is geintroduceerd door Donald Knuth in 1984. Oorspronkelijk om de broncode en de bijbehorende software documentatie samen te brengen. Tegenwoordig creeren we dynamische documenten waarin het programma of de analyse code samen draaien om tot ‘outputs’ te komen (bv. tabellen, plots, modellen, etc) die worden uitgelegd via narratief schrijven.\r\nTraditioneel gebruikten mensen commentaren om het verhaal in de code file kwijt te kunnen raken (voor R zou dat een .R file zijn). Deze file zou het volgende in kunnen houden:\r\n\r\n\r\n# Titel:  Relatie tussen Autogewicht en Gasefficientie/of-verbruik\r\n# Door :  Harrie Jonkman  \r\n# Datum:  11 Mei 2016\r\n\r\n# Ik verspel dat er een relatie is tussen het gewicht van de auto en de afstand die met de brandstof afgelegd kan worden.  \r\n# Dat test ik met een lineaire analyse van de ''mtcarsdataset' als onderdeel van de R datasets\r\n\r\n# Hoe zien de data eruit?\r\n#datatable(mtcars) # Interactieve tabel \r\n\r\n# Is er een relatie tussen het gewicht en de afstand per, in dit geval, gallon?\r\nlm_mpg <- lm(mpg ~ wt, data = mtcars) # Run het lineaire model dat mpg voorspelt op basis van wt\r\ncoef_lm_mpg <- coef(summary(lm_mpg))  # Haal de coefficienten eruit voor de tabel die komt \r\nkable(coef_lm_mpg)                    # Maak een niet-interactieve tabel - een functie in knitr\r\n\r\n# Plot de relatie tussen gewicht en afstand in mijl per gallon   \r\nplot <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # \r\n  geom_smooth(method = \"lm\") + theme_bw() +           # Maak een lineair model en maak het zwart en wit\r\n  xlab(\"Weight (1000lbs)\") + ylab(\"Miles per Gallon\") # Voeg tekst aan de assen toe\r\n\r\n\r\n# Het lijkt erop dat met een toename van 1000 pounds er een afname is van brandstof gebruik met 5.34 mijl per gallon\r\n# Het eind\r\n\r\nDe gebruiker zal de commentaren lezen en de codes zelf runnen.\r\nEchter, ‘literate programming’ stelt ons in staat de code te runnen en de resultaten te beschrijven, allemaal in een document dat we kunnen delen. We zouden bijvoorbeeld het volgende kunnen doen:\r\n``` Relatie tussen gewicht van de auto en het brandstofverbruikDoor: Harrie JonkmanDatum: 27 november 2018\r\nIk voorspel dat er een relatie is tussen het gewicht en de afstand die met de brandstof kan worden afgelegd. Ik test dat met een lineair model op een dataset in R en zet dit als volgt in het programma.\r\n\r\n\r\n# Hoe zien de data eruit?\r\ndatatable(mtcars) # Interactieve tabel \r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\n# Is er een relatie tussen het gewicht van de auto en de afstand die het kan afleggen met de brandstof?\r\nlm_mpg <- lm(mpg ~ wt, data = mtcars) # Run het lineair model dat mpg voorspelt op basis van wt\r\ncoef_lm_mpg <- coef(summary(lm_mpg))  # Haal de co?ffici?nten voor de tabel eruit\r\nkable(coef_lm_mpg)                    # Maak een niet-interactieve tabel - functie in knitr\r\n\r\nEstimate\r\nStd. Error\r\nt value\r\nPr(>|t|)\r\n(Intercept)\r\n37.285126\r\n1.877627\r\n19.857575\r\n0\r\nwt\r\n-5.344472\r\n0.559101\r\n-9.559044\r\n0\r\n\r\n# Plot de relatie tussen gewicht en mijl per gallon  \r\nplot <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # \r\n  geom_smooth(method = \"lm\") + theme_bw() +           # voeg lineair model toe en maak het zwart en wit \r\n  xlab(\"Weight (1000lbs)\") + ylab(\"Miles per Gallon\") # Voeg tekst aan x en y-as toe\r\n\r\nHet lijkt erop dat met elke 1000 pond er een afname is in brandstof gebruik met 5.3444716 mijl per gallon\r\nHet einde\r\n```\r\nReproduceerbaar onderzoek\r\nGoede uitvoering van reproduceerbaar onderzoek houdt in ieder geval in:\r\nHet hele project in een directory plaatsen die wordt ondersteund door de ‘version control’.\r\nCode en data vrijlaten.\r\nAlles documenteren en de code als documentatie gebruiken!\r\nFiguren, tabellen en de statistiek zijn het resultaat van scripts en codes die in de tekst staan.\r\nSchrijf in de codes de paden die worden gebruikt.\r\nStel ‘seed’ in zodat een volgende persoon dezelfde resultaten krijgt.\r\nLaat ook informatie zien waarmee de codefile wordt uitgevoerd. Je kunt bijvoorbeeld de devtools::session_info() gebruiken.\r\nWat het belang van reproduceerbaar onderzoek betreft, zie ook het document van de Koninklijke Nederlandse Academie voor Wetenschappen\r\nMarkdown\r\nOm RMarkdown helemaal te begrijpen moeten we het eerst iets over Markdown weten. Dat is een systeem om een simpele, leesbare tekst te maken die eenvoudig kan worden omgezet naar HTML. Markdown is essentieel voor twee dingen:\r\nEen kale tekst die de syntax vormt.\r\nEen software gereedschap dat in Perl is geschreven.\r\nHet zet de kale tekst om in HTML.\r\n\r\nBelangrijkste doel van Markdown:\r\nMaakt de syntax van het orginele (pre-HTML) document zo leesbaar mogelijk.\r\n\r\nZou je deze code liever in HTML lezen?\r\n\r\n<body>\r\n  <section>\r\n    <h1>Paklijst voor bergklimmen<\/h1>\r\n    <ul>\r\n      <li>Bergschoenen<\/li>\r\n      <li>Klimgordel<\/li>\r\n      <li>Rugzak<\/li>\r\n      <li>Touw<\/li>\r\n      <li>Zelfzekering<\/li>\r\n    <\/ul>\r\n  <\/section>\r\n<\/body>\r\nOf deze code in Markdown?\r\n\r\n# Paklijst voor bergklimmen\r\n\r\n* Bergschoenen\r\n* Klimgordel\r\n* Rugzak  \r\n* Touw\r\n* Zelfzekering\r\nMarkdown is makkelijker om te lezen!\r\nWe zullen meer over de syntax van Markdown praten nadat we RMarkdown hebben geintroduceerd maar laten we ons allereerst beseffen hoeveel makkelijker ons leven is/zal zijn omdat Markdown bestaat! Dank je John Gruber en Aaron Swartz (RIP) voor het ontwikkelen van Markdown in 2004!\r\nRMarkdown\r\nRMarkdown is een variant van Markdown dat het makkelijker maakt om met RStudio dynamische documenten, presentaties en rapporten te maken. Het omvat ‘R code chunks’ (ik laat hier even het Engels staan) om met knitr te gebruiken waarmee makkelijker reproduceerbare (web-based) rapporten gemaakt kunnen worden die automatisch aangepast worden wanneer de onderliggende code is veranderd.\r\nRMarkdown laat jou Markdown combineren met plaatjes, linken, tabellen, \\(\\LaTeX\\) en de code zelf.\r\nRStudio zorgt ervoor dat het maken van documenten met RMarkdown makkelijk wordt.\r\nRStudio is (net als R) vrij te gebruiken en draait op elk systeem.\r\nRMarkdown geeft verschillende typen files waaronder onder anderen:\r\nHTML\r\nPDF\r\nMarkdown\r\nMicrosoft Word\r\nPresentaties:\r\nOpvallende HTML5 presentaties:\r\nioslides\r\nSlidy\r\nSlidify\r\n\r\nPDF presentaties:\r\nBeamer\r\n\r\nHandouts:\r\nTufte Handouts\r\n\r\n\r\nHTML R Package Vignettes\r\nEven Entire Websites!\r\nTerwijl er heel veel verschillende documenten kunnen worden geleverd met RMarkdown, ligt hier de nadruk in de eerste plaats op HTML output files omdat die voor mijn onderzoek misschien het meest bruikbaar en flexibel zijn.\r\nWaarom R Markdown?\r\nEen aantrekkelijk gereedschap voor reproduceerbare en dynamische rapporten!\r\nTerwijl het was gemaakt voor R, accepteert het veel programmeertalen. Om het eenvoudig te houden, werken we vandaag alleen met R.\r\nEen code kan op een aantal manieren worden uitgevoerd:\r\nInline Code: Een korte code die in de geschreven tekst van het document wordt uitgevoerd.\r\nCode Chunks: Delen van het document omvatten verschillende zinnen analyse code. Dat kan een plot of een tabel zijn, maar ook berekeningen van de samenvattende statistiek, pakketten laden, etc.\r\n\r\nHet is makkelijk om:\r\nPlaatjes op te nemen.\r\nDe Markdown syntax te leren.\r\n\\(\\LaTeX\\) elementen op te nemen.\r\nInteractieve tabellen op te nemen.\r\nGebruik de versie via Git.\r\nDan is het makkelijker om te delen en samen te werken in analyses, projecten en publicaties!\r\n\r\nExterne linken toe te voegen - Rmarkdown begrijpt zelfs enige html codes!\r\nOm mooie documenten te maken.\r\n\r\nJe hoeft je geen zorgen te maken over pagina breuken of het plaatsen van de figuren.\r\nConsolideer jouw code en plaats het in een file:\r\nPowerpoint, PDFs, html documenten en word files\r\n\r\nEenvoudige werkwijze\r\nIn het kort, om een rapport te maken:\r\nOpen een .Rmd file.\r\nMaak een YAML kop (meer hierover zo dadelijk!)\r\n\r\nSchrijf de inhoud met RMarkdown syntax.\r\nNeem mee de R code in code chunks of met een inline code.\r\nDraai de document output.\r\nOverzicht van de stappen die RMarkdown maakt om een ‘gerenderd’ document te krijgen:\r\nMaak een .Rmd rapport met ‘R code chunks’ en markdown verhalen (zoals hierboven in stappen beschreven).\r\nGeef de .Rmd-file aan knitr om de ‘R code chunks’ uit te voeren en een nieuwe .md file te maken.\r\nKnitr is een pakket binnen R die jou in staat stelt de code binnen RMarkdown documenten uit te voeren zoals HTML, latex, pdf, word en andere document types.\r\n\r\nGeef de .md file aan pandoc, die er een definitief document van maakt (b.v. html, Microsoft word, pdf, etc.).\r\nPandoc is een universeel gereedschap om documenten te converteren en zet het ene document type (in dit geval: .Rmd) om in een ander (in dit geval: HTML)\r\n\r\nHoewel dit mogelijk wat ingewikkeld lijkt, kunnen we op de Knit knop drukken boven aan de pagina\r\nof we kunnen de volgende code runnen:\r\n\r\nrmarkdown::render(\"RMarkdown_LesNederlandsBeperkt.Rmd\", \"html_document\")\r\n\r\nMaak een .Rmd file\r\nLaten we eens met een RMarkdown gaan werken!\r\nIn de menu bar, klik je op File -> New File -> RMarkdown\r\nDan krijg je het volgende te zien\r\nHierbinnen kies je het type output dat je wilt hebben. Opgelet: deze output kan later heel makkelijk worden aangepast!\r\nKlik OK\r\nYAML koppen\r\nYAML staat voor “YAML Ain’t Markup Language” en is eigenlijk de structuur voor de metadata van het document. Het staat tussen twee regels van drie streepjes --- en wordt automatisch omgezet door RStudio. Een eenvoudig voorbeeld:\r\n\r\n---\r\ntitle:  \"Analyse Rapport\"  \r\nAuthor:  \"Harrie Jonkman\"  \r\ndate: \"1 Maart 2017\"  \r\noutput:  html_document\r\n---\r\nHet voorbeeld boven zal een HTML document maken. Echter, de volgende opties zijn ook beschikbaar.\r\nhtml_document\r\npdf_document\r\nword_document\r\nbeamer_presentation (pdf powerpoint)\r\nioslides_presentation (HTML powerpoint)\r\nen nog meer …\r\nHier ligt de nadruk op HTML files. Echter voel je vrij als je hier wat mee wilt spelen door bv. word en pdf documenten te maken. Presentatie-documenten kennen een wat andere syntax (bv. om aan te geven wanneer de ene dia eindigt en de andere begint) en dan is er nog wat markdown syntax specifiek voor presentaties maar die gaat voorbij het doel van deze workshop.\r\nMarkdown Basis\r\nKijk hiernaar RMarkdown Reference Guide\r\nHaal hier ook informatie vandaan RMarkdown Cheatsheet:\r\nMarkdown Basis van RStudio’s RMarkdown CheatsheetHandige tips:\r\nEindig elke regel met drie spaties om een nieuwe regel te beginnen.\r\nWoorden binnen een code moeten aan beide kanten zo’n kommateken kennen: `\r\nOm iets tot superscript te maken moet je een ^ aan beide zijden plaatsen. Superscript werd gevormd door Super^script^ te typen.\r\nVergelijkingen kunnen in een inline code worden geplaatst met $ en als blok gecentreerd binnen het document door $$. Bijvoorbeeld \\(E = mc^2\\) staat tussen de regels terwijl het volgende geblokt wordt opgenomen: \\[E = mc^2\\]\r\nAnder wiskundig materiaal:\r\n- Vierkantswortel: $\\sqrt{b}$ zal \\(\\sqrt{b}\\) maken - Breuken: $\\frac{1}{2}$ = \\(\\frac{1}{2}\\)\r\n- - Vergelijkingen met breuken: $f(x)=\\frac{P(x)}{Q(x)}$ = \\(f(x)=\\frac{P(x)}{Q(x)}\\)\r\n- Binomiale Coefficienten: $\\binom{k}{n}$ = \\(\\binom{k}{n}\\)\r\n- Integralen: $$\\int_{a}^{b} x^2 dx$$ = \\[\\int_{a}^{b} x^2 dx\\]\r\nShareLaTeX is een prachtige bron voor LaTeX-codes.\r\n\r\nNog wat wiskundig materiaal:\r\nBeschrijving\r\nCode\r\nVoorbeelden\r\nGriekse letters\r\n$\\alpha$ $\\beta$ $\\gamma$ $\\rho$ $\\sigma$ $\\delta$ $\\epsilon$ $mu$\r\n\\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\rho\\) \\(\\sigma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\mu\\)\r\nBinaire handelingen\r\n$\\times$ $\\otimes$ $\\oplus$ $\\cup$ $\\cap$\r\n\\(\\times\\) \\(\\otimes\\) \\(\\oplus\\) \\(\\cup\\) \\(\\cap\\) \\(\\times\\)\r\nRelationele handelingen\r\n$< >$ $\\subset$ $\\supset$ $\\subseteq$ $\\supseteq$\r\n\\(< >\\) \\(\\subset\\) \\(\\supset\\) \\(\\subseteq\\) \\(\\supseteq\\)\r\nVerder\r\n$\\int$ $\\oint$ $\\sum$ $\\prod$\r\n\\(\\int\\) \\(\\oint\\) \\(\\sum\\) \\(\\prod\\)\r\n\r\nUitdaging: Probeer eens de volgende output te maken:\r\n\r\nVandaag voel ik mij vet omdat ik RMarkdown leer.\r\nhoning is heel zoet.\r\nYAS!!!!!!\r\nR2 waarden zijn informatief!\r\n\\(R^{2}\\) beschrijft de variantie verklaard door het model.\r\nIk kende geen RMarkdown Vandaag heb ik RMarkdown geleerd\r\nRStudio link\r\nOutput van het volgende:\r\n\r\n# RMarkdown   \r\n## R   \r\n### Knitr   \r\n#### Pandoc  \r\n##### HTML  \r\n\\(\\sqrt{b^2 - 4ac}\\)\r\n\\[\\sqrt{b^2 - 4ac}\\]\r\n\\(X_{i,j}\\)\r\n\r\nVandaag maak ik een dynamisch document!\r\n\r\nHet volgende lijstje:\r\nChocolade Chips Kook Recept\r\nboter\r\nsuiker\r\nEen mengsel van bruine en witte suiker maakt het lekkerder\r\nmix dat met boter voordat je de eieren eraan toevoegt\r\n\r\n\r\neieren\r\nvanille\r\nMix wat droge ingredienten:\r\nmeel, zout, bak soda\r\n\r\nchocolade chips\r\nEen Code in het document\r\nEr zijn twee manieren om een code in een RMarkdown document op te nemen.\r\nCode in het document: Korte code als een onderdeel van het geschreven document.\r\nCode Chunks: Delen van het document die verschillende programmeer of analyse codes omvatten. Daarmee kan een figuur of tabel worden gemaakt, statistieken worden berekend, pakketten worden geladen, etc.\r\nR Code in het document\r\nEen R code kan in het document wordt gemaakt door een komma hoog achterwaarts (`) en de letter r gevolgd door nog zo’n komma.\r\nBijvoorbeeld: 211 is 2048.\r\nStel dat je een p-waarde rapporteert en je wilt niet terug om de statistische test steeds weer uit te voeren. De p-waarde was eerder 0.0045.\r\nDit is echt handig als de resultaten op papier moeten worden gezet. Bijvoorbeeld, je hebt een aantal statistieken uitgevoerd voor jouw wetenschappelijke vragen is dit een manier waarop R die waarde in a variabele naam bewaart. Bijvoorbeeld: Wijkt het brandstofverbruik van de automaat significant af de auto met handtransmissie significant af binnen de mtcars data set?\r\n\r\n\r\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\r\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\r\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\r\n\r\nOm de p-waarde vast te stellen kunnen we transmission_ttest$p.value als R code in het document gebruiken.\r\nDe p-waarde is dan 0.0013736.\r\nR Code Chunks\r\nR code chunks (nogmaals ik gebruik maar de Engelse benaming hier, sorry)kunnen worden gebruikt om de R output in het document te krijgen of om de code als illustratie zichtbaar te maken.\r\nDe anatomie van een code chunk:\r\nOm een R code chunk te plaatsen, kun je met de hand typen door ```{r} gevolgd door ``` op een volgende regel. Je kunt ook de Insert a new code chunk knop gebruiken of de ‘shortcut key’. Dat geeft dan de volgende code chunk:\r\nEen code chunk invoeren\r\n```{r}\r\nn <- 10\r\nseq(n)\r\n```\r\nGeef de code chunk een betekenisvolle naam die samenhangt met wat het doet. Hieronder heb ik code chunk 10-random-numbers genoemd:\r\n\r\n```{r 10-random-numbers}\r\nn <- 10\r\nseq(n)\r\n```\r\nDe code chunk input en output zien er dan als volgt uit:\r\n\r\n\r\nn = 10\r\nseq(n)\r\n\r\n [1]  1  2  3  4  5  6  7  8  9 10\r\n\r\nKnitr\r\nKnitr is een R-pakket dat werkt met\r\nIdentificeren van de code zowel van de chunks als in de tekst zelf\r\nEvalueren van de hele code en geeft de resultaten terug\r\nTeruggeven van de geformuleerde resultaten en combineert met de orginele file.\r\nKnitr draait de code zoals die in de R console zou draaien.\r\nKnitr werkt vooral met code chunks.\r\nEen code chunk ziet er als volgt uit:\r\n\r\n<div class=\"layout-chunk\" data-layout=\"l-body\">\r\n\r\n```r\r\nx <- rnorm(100)  \r\ny <- 2*x + rnorm(100)\r\n```\r\n\r\n<\/div>\r\nGoede praktijken met betrekking tot code chunks:\r\nBenoem/label jouw code chunks!\r\nIn plaats van de chunk opties te specificeren in iedere chunk, kun je de algemene chunk opties aan het begin van het document vastzetten.\r\nChunk Labels\r\nChunk labels krijgen unieke IDs in een document en zijn goed voor:\r\nOm externe files te genereren zoals plaatjes en ‘cached’ documenten.\r\nChunk labels zijn vaak output als fouten omhoog komen(vaker voor codes in het document).\r\nAls je de code chunk een naam geef, gebruik dan - of _ tussen woorden voor code chunks labels in plaats van ruimtes. Dat helpt jou en andere gebruikers bij het navigeren in het document.\r\nChunk labels moeten uniek zijn in het document - anders zal er een fout optreden!\r\nChunk Opties\r\nDruk tab als tussen de haakjes code chunk opties omhoog komen.\r\nresults = \"asis\" staat voor “as is” en geeft de output van een niet geformateerde versie.\r\ncollapse is een andere chunk optie die handig kan zijn, zeker als een code chunk veel korte R uitdrukking heeft met wat output.\r\nEr zijn teveel chunk opties om hier te behandelen. Kijk na deze workshop nog eens wat rond voor deze opties.\r\nEen mooie website om dat op te doen is Knitr Chunk Options.\r\n\r\nUitdaging\r\nDraai de code chunk hieronder en speel wat met de volgende knitr code chunk opties:\r\n\r\n\r\neval = TRUE/FALSE\r\necho = TRUE/FALSE\r\ncollapse = TRUE/FALSE\r\nresults = \"asis\",\"markup en \"hide\r\n\r\n\r\nSla je resultaten op in markdown.Opgelet: Wees er zeker van dat je jouw chunks een naam geeft!\r\n\r\n\r\n\r\n1+1\r\n2*5\r\nseq(1, 21, by = 3)\r\nhead(mtcars)\r\n\r\nEnkele voorbeelden voortbouwend op de chunk hierboven\r\nResultaten van results=\"markup\", collapse = TRUE}:\r\n\r\n\r\n1+1\r\n[1] 2\r\n2*5\r\n[1] 10\r\nseq(1, 21, by = 3)\r\n[1]  1  4  7 10 13 16 19\r\nhead(mtcars)\r\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\r\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\r\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\r\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\r\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\r\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\r\n\r\nResultaten van results=\"asis\", collapse = TRUE}:\r\n\r\n\r\n1+1\r\n[1] 2\r\n\r\n2*5\r\n[1] 10\r\n\r\nseq(1, 21, by = 3)\r\n[1] 1 4 7 10 13 16 19\r\n\r\nhead(mtcars)\r\n\r\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\nMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1\r\n\r\nGlobale opties\r\nHet kan zijn dat je dezelfde chunk settings wilt handhaven voor het gehele document. Het kan daarom handig zijn om de opties in een keer te typen in plaats van het iedere keer weer voor een chunk te moeten doen. Om dat te doen kun je de globale chunk opties bovenaan het document vaststellen.\r\n\r\nknitr::opts_chunk$set(echo = FALSE, \r\n                      eval = TRUE, \r\n                      message = FALSE,\r\n                      warning = FALSE, \r\n                      fig.path = \"Figures/\",\r\n                      fig.width = 12, \r\n                      fig.height = 8)\r\nAls je bijvoorbeeld met iemand samenwerkt die de code niet wil zien, kun je schrijven eval = TRUE en echo = FALSE gebruiken zodat de code wel gedraaid wordt maar niet getoond. In aanvulling wil je misschien message = FALSE en warning = FALSE gebruiken zodat jouw samenwerkingspartner geen enkele boodschap of waarschuwing van R ziet.\r\nAls je figuren wilt opslaan en bewaren in een subdirectory binnen het project, gebruik dan fig.path = \"Figures/\". Hier verwijst de \"Figures/\" naar een folder Figures binnen de huidige directory waar de figuur die gemaakt wordt in het document wordt opgeslagen.Opgelet: de figuren worden niet standaard opgeslagen.\r\nGlobale chunk opties zullen voor de rest van het documenten worden vastgezet. Als je wilt dat een bepaalde chunk afwijkt van de globale opties, maak dat aan het begin van die bepaalde chunk duidelijk.\r\nFiguren\r\nKnitr maakt vrij eenvoudig figuren. Als een analyse code binnen een chunk een bepaald figuur moet produceren, dan zal hij dat in het document afdrukken.\r\nEnkele knitr chunk opties gerelateerd aan figuren:\r\nfig.width en fig.height\r\nStandaard: fig.width = 7, fig.height = 7\r\n\r\nfig.align: Hoe het figuur uit te lijnen\r\nOpties omvatten: \"left\", \"right\" en \"center\"\r\n\r\nfig.path: Een file pad naar de directory waar knitr de grafische output moet opslaan die er met de chunk wordt gemaakt.\r\nStandaard: 'figure/'\r\n\r\nEr is zelfs een fig.retina(alleen voor HTML output) voor hogere figuur resoluties met retina afdrukken.\r\n\r\n\r\n\r\nEen enkelvoudig figuur maken:\r\nMet fig.align = \"center\"\r\n\r\n\r\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\r\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\r\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \r\n\r\n\r\nMet fig.align = \"right\"\r\n\r\n\r\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\r\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\r\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \r\n\r\n\r\nMet fig.align = \"left\"\r\n\r\n\r\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\r\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\r\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \r\n\r\n\r\nMet fig.width = 2, fig.height = 2\r\n\r\n\r\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\r\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\r\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \r\n\r\n\r\nMet fig.width = 10, fig.height = 10\r\n\r\n\r\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\r\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\r\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \r\n\r\n\r\n\r\n\r\nmyplots <- list()  # new empty list\r\nfor(i in 1:ncol(mtcars)){\r\n  col <- names(mtcars)[i]\r\n  ggp <- ggplot(mtcars, aes_string(x = col)) +\r\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\r\n    geom_vline(xintercept = mean(mtcars[[col]]), col = \"red\") \r\n  myplots[[i]] <- ggp  # add each plot into plot list\r\n}\r\nmultiplot(plotlist = myplots, cols = 4) # must load in multiplot function from the Rcookbook see http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/\r\n\r\n\r\nTabellen\r\nTabellen kunnen in Markdown voor nogal wat hoofdpijn kosten. We gaan er hier verder niet op in. Als je meer wilt leren over Markdown-tabellen kijk naar documentation on tables op de RMarkdown website.\r\nEr zijn enkele tabeltypen die handig kunnen zijn. Hier zullen we ons vorig voorbeeld gebruiken van de mtcars data\r\nIn zijn Knitr in a Knutshell introduceert Dr. Karl Broman: kable, panderen xtable en vooral die eerste twee deden mij plezier:\r\nkable: Binnen het knitr pakket - niet veel opties maar het ziet er goed uit.\r\npander: Binnen het pander pakket - heeft veel opties en handigheden. Makkelijk voor het vetmaken van waarden (bv. waarden onder een bepaalde waarde).\r\nkable en pander tabellen zijn mooi en handig bij het maken van niet-interactieve tabellen:\r\n\r\n\r\nkable(head(mtcars, n = 4)) # kable table with 4 rows\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\nMazda RX4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\n0\r\n1\r\n4\r\n4\r\nMazda RX4 Wag\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\n0\r\n1\r\n4\r\n4\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\n1\r\n1\r\n4\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n3\r\n1\r\n\r\n# Pander table\r\n# install.packages(\"pander\") # install pander first\r\nlibrary(pander)\r\npander(head(mtcars, n = 4))\r\nTable continues below\r\n \r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\nMazda RX4\r\n21\r\n6\r\n160\r\n110\r\n3.9\r\n2.62\r\n16.46\r\n0\r\n1\r\nMazda RX4 Wag\r\n21\r\n6\r\n160\r\n110\r\n3.9\r\n2.875\r\n17.02\r\n0\r\n1\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.32\r\n18.61\r\n1\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n \r\ngear\r\ncarb\r\nMazda RX4\r\n4\r\n4\r\nMazda RX4 Wag\r\n4\r\n4\r\nDatsun 710\r\n4\r\n1\r\nHornet 4 Drive\r\n3\r\n1\r\n\r\nZie ook hoe je mooie tabellen kunt maken hierKable\r\nHTML Widgets\r\nMet de uitgave van de nieuwe RMarkdown v2 is het makkelijker dan ooit tevoren om HTML Widgets te gebruiken. Volg de link om uit te zoeken in welke widgets jij ge?nteresseerd bent!\r\nOnlangs ontdekte ik bijvoorbeeld het DT pakket waarmee tabellen interactief kunnen worden gemaakt in de HTML output. Daarbij levert Plotly for R echt mooie interactieve grafieken op, welke gebaseerd zijn op Plotly.\r\nCool, of niet?\r\n\r\n\r\n# DT table = interactive\r\n# install.packages(\"DT\") # install DT first\r\nlibrary(DT)\r\ndatatable(head(mtcars, n = nrow(mtcars)), options = list(pageLength = 5)) \r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}\r\n# plotly\r\n# install.packages(\"plotly\")\r\nlibrary(plotly)\r\nmtcars$car <- row.names(mtcars)\r\nplot_ly(mtcars, x = wt, y = mpg, \r\n        text = paste(\"Car: \", car),\r\n        mode = \"markers\", color = wt, size = wt)\r\n\r\nError in plot_ly(mtcars, x = wt, y = mpg, text = paste(\"Car: \", car), : object 'wt' not found\r\n\r\nSpelling controleren\r\nIn de spelling kunnen natuurlijk altijd fouten zitten en daarom kan het nodig zijn dat we onze spelling in het document willen controleren. Er zijn twee manieren om de spelling te controleren:\r\nDruk op de “ABC check mark”  links van de vergrootglasknop in RStudio.\r\nGebruik de aspell() functie van het utils pakket. Je kunt dan echter beter de code chunks overslaan. De aspell() functie kan een filter functie overnemen om bepaalde regels in de files over te slaan en kan worden gebruikt met de knit_filter() die ontworpen is om de code chunks in een file over te slaan.\r\nKnitr Thema’s\r\nHet knitr-syntax-thema kan worden aangepast of helemaal naar de hand worden gezet. Als je de standaardthema’s niet wilt, gebruik dan het knit_theme om het te veranderen. Er zijn 80 thema’s opgenomen binnen knitr en we kunnen de namen ervan zien door knit_theme$get().\r\nWat zijn de eerste 30 knitr thema’s?\r\n\r\n\r\nhead(knit_theme$get(), 30)\r\n\r\n [1] \"acid\"          \"aiseered\"      \"andes\"         \"anotherdark\"  \r\n [5] \"autumn\"        \"baycomb\"       \"bclear\"        \"biogoo\"       \r\n [9] \"bipolar\"       \"blacknblue\"    \"bluegreen\"     \"breeze\"       \r\n[13] \"bright\"        \"camo\"          \"candy\"         \"clarity\"      \r\n[17] \"dante\"         \"darkblue\"      \"darkbone\"      \"darkness\"     \r\n[21] \"darkslategray\" \"darkspectrum\"  \"default\"       \"denim\"        \r\n[25] \"dusk\"          \"earendel\"      \"easter\"        \"edit-anjuta\"  \r\n[29] \"edit-eclipse\"  \"edit-emacs\"   \r\n\r\nWij kunnen knit_theme$set() gebruiken om het thema vast te zetten. Om het thema op fruit vast te zetten, kunnen we de bijvoorbeeld de volgende code gebruiken:\r\n\r\n\r\nknit_theme$set(\"fruit\")\r\n\r\nHier is de link naar jouw favoriete thema 80 knitr highlight themes.\r\nInhoudsopgave\r\nEen inhoudsopgave kan aan het gerenderd document worden toegevoegd door de toc optie in de YAML kop te gebruiken.\r\nOpties hierbij:\r\ntoc: of de inhoudsopgave moeten worden meegenomen:\r\ntoc: true: hier wordt de inhoudsopgave meegenomen\r\nDefault:toc: false: Hier wordt de inhoudsopgave niet meegenomen\r\n\r\ntoc_depth:: Hoeveel niveau’s moeten in de inhoudsopgave worden worden meegenomen?\r\nDefault: doc_depth: 3 zal koppen tot en met ### meenemen.\r\n\r\nnumber_sections: Voegt sectienummers toe aan de koppen. Bijvoorbeeld, dit document heeft number_sections: true\r\nDefault: number_sections: false\r\nOpgelet: Met elk # zal er een decimaal punt worden toegevoegd aan alle koppen.\r\n\r\ntoc_float:\r\n2 andere mogelijke parameters binnen toc_float:\r\ncollapsed: Controleert of de inhoudsopgave alleen aan het begin verschijnt. Het zal met de cursor erover verschijnen.\r\nDefault: collapsed: TRUE\r\n\r\nsmooth_scroll: Controleert of de pagina scrolls werken wanneer op de onderdelen van de inhoudsopgave wordt geklikt.\r\nDefault: smooth_scroll: true\r\n\r\n\r\n\r\nBijvoorbeeld:\r\n\r\noutput:\r\n  html_document:\r\n    toc: true\r\n    toc_depth: 2\r\n---\r\n\r\nUitdaging: Maak de YAML kop voor een HTML document die het volgende inhoudt:\r\n\r\n\r\nInhoudsopgave\r\nLaat de inhoudsopgave vloeien\r\nSectie koppen met twee hashtags (##)\r\nGenummerde secties\r\nGeen makkelijke scrolling\r\n\r\nThema’s\r\nRMarkdown heeft verschillende opties voor de HTML documenten. Enkele mogelijkheden waaruit kan worden gekozen, hier met de Engelse termen:\r\ntheme\r\nhighlight\r\nsmart\r\nDe HTML output thema’s komen van Bootswatch library. Valide HTML themes omvatten de volgende:\r\ncerulean, cosmo,flatly, journal, readable,spacelab en united.\r\nBijvoorbeeld, het thema van de pagina is readable.\r\n\r\nZet het op nul voor geen thema (in dit geval kun je de css parameter gebruiken om jouw eigen stijl te gebruiken).\r\nHighlight specificeert de wijze waarop de syntax stijl oplicht. Stijlen die mogelijk zijn omvatten de volgende:\r\ndefault, espresso, haddock, kate, monochrome, pygments, tango, textmate en zenburn.\r\nOok hier, plaats nul om syntax oplichting te voorkomen.\r\nSmart indiceert of de typografisch correcte output wordt weergegeven, zet rechte aanhalingstekens om in gekru, — rechte aanhalingstekens, – om in gekrulde aanhalingstekens en … in ellipsen. Smart is standaard ingesteld.\r\nBijvoorbeeld:\r\n\r\n---\r\noutput:\r\n  html_document:\r\n    theme: slate\r\n    highlight: tango\r\n---\r\nAls je wilt kun je ook jouw eigen stijl-thema produceren en gebruiken. Als je dat zou doen, zou de output sectie van jouw YAML kop er z’on beetje zo uitzien:\r\n\r\noutput:\r\n  html_document:\r\n    css: styles.css\r\nAls je nog wat verder wilt gaan en jouw eigen thema wilt schrijven in aanvulling op het oplichten, zou de YAML kop er beetje zo uitzien:\r\n\r\n---\r\noutput:\r\n  html_document:\r\n    theme: null\r\n    highlight: null\r\n    css: styles.css\r\n---\r\nHier is een link naar Pr?sance en Stijl in de HTML output.\r\nBibliografie\r\nHet is ook mogelijk om een bibliografie file in de YAML kop mee te nemen. Bibliografie formats die door Pandoc gelezen kunnen worden zijn:\r\nFormat\r\nFile extension\r\nMODS\r\n.mods\r\nBibLaTeX\r\n.bib\r\nBibTeX\r\n.bibtex\r\nRIS\r\n.ris\r\nEndNote\r\n.enl\r\nEndNote XML\r\n.xml\r\nISI\r\n.wos\r\nMEDLINE\r\n.medline\r\nCopac\r\n.copac\r\nJSON citeproc\r\n.json\r\nOm een bibliografie in RMarkdown te maken, zijn er twee files nodig:\r\nEen bibliografie file met informatie over elke referentie.\r\nEen citaat stijl taal (CSL) om het format de referentie te bepalen.\r\nEen voorbeeld YAML kop met een bibliografie en een citaat stijl taal (CSL) file is:\r\n\r\noutput: html_document\r\nbibliography: bibliography.bib\r\ncsl: nature.csl\r\nBekijk de erg behulpzame webpagina van het R Core team op bibliographies and citations.\r\nAls je R pakketten wilt citeren, heeft knitr zelfs een functie die write_bib() heet en die .bib overzicht van R pakketten kan leveren. Het wordt zelfs in een file geschreven!\r\n\r\n\r\nwrite_bib(file = \"r-packages.bib\") # will write all packages  \r\nwrite_bib(c(\"knitr\", \"ggplot2\"), file = \"r-packages2.bib\") # Only writes knitr and ggplot2 packages\r\n\r\nPlaatsen\r\nDe bibliografie wordt automatisch aan het einde van het document geplaatst. Daarom moet je jouw .Rmd document met # Referenties eindigen zodat de bibliografie naar de kop voor bibliografie komt.\r\nStylen\r\nCitation Sylte Language (CSL) is een op XML-gebaseerde taal die het format van citaten en bibliografie?n vaststelt. Referentie management programma’s zoals Zotero, Mendeley en Papers gebruiken allemaal CSL.\r\nZoek jouw favoriete tijdschrift en CSL in de Zotero Style Repository, waar nu meer dan 8,152 CSLs inzitten. Is er een stijl waar je naar zoekt en die er niet in zit?\r\n\r\noutput: html_document\r\nbibliography: bibliography.bib\r\ncsl: nature.csl\r\nIn de github repo voor deze workshop heb ik de nature.csl en the-isme-journal.csl toegevoegd om mee te spelen. Download anders een stijl van de Zotero Style Repository!\r\nCitaten\r\nCitaten gaan tussen vierkante haakjes [ ] en worden afgescheiden door punt-komma’s’ ;. Elk citaat moet een sleutel hebben, samen de @ + de citaat identificatie van de database vormen en die optioneel a prefix, a locator en a suffix hebben. Om te controleren wat de citaatsleutel is van een referentie, werp dan een blik op de .bib file. Hier in die file, kun je de sleutel voor elke referentie veranderen. Echter, wees er wel van bewust dat elke ID uniek is!\r\nHier zijn wat voorbeelden met bijpassende code in het Engels:\r\nMicrobes control Earth’s biogeochemical cycles [@Falkowski2008].\r\nCode: Microbes contorl Earth's biogeochemical cycles  [@Falkowski2008].\r\n\r\nI love making beautiful plots with ggplot2 [@R-ggplot2]\r\nCode: I love making beautiful plots with ggplot2 [@R-ggplot2]\r\n\r\nDr. Yuhui Xie’s book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\r\nCode: Dr. Yuhui Xie's book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\r\n\r\nA great article in Science regarding biogeography of microbes asks readers to imagine their Alice in Wonderland to shrink down to understand the microbial world [@Green2008].\r\nCode: A great article in *Science* regarding biogeography of microbes asks readers to imagine they are Alice in Wonderland to and shrink down to understand the microbial world [@Green2008].\r\n\r\nHet is cool dat de enige refenties die aan het document worden toegevoegd degene zijn die jijzelf citeert!\r\nPubliceren via RPubs\r\nAls je een keer een mooi dynamisch document hebt gemaakt wil je dat mogelijk delen met anderen. Een mogelijkheid om het te delen met de wereld is om het te hosten op RPubs. Met RStudio kan dit heel makkelijk! Doe het volgende:\r\nMaak een aansprekend .Rmd document.\r\nKlik op de knop om jouw gerenderd HTML document te puliceren.\r\nIn de rechter bovenhoek van het previewscherm klik je op de publiceerknop en volg je de aanwijzingen.\r\nOpgelet: Je moet een RPubs profiel hebben aangemaakt.\r\n\r\nAls je het profiel hebt, let dan op het volgende:\r\nDe titel van het document.\r\nEen beschrijving van het document.\r\nDe URL waar de website wordt gehost.\r\nOpgelet: Het begin van de URL zal zijn: www.rpubs.com/your_username/name_of_your_choice\r\n\r\n\r\nRPubs vernieuwen\r\nAls je veranderingen in het document wilt aanbrengen is het makkelijk om de webpagina te vernieuwen. Als je een keer jouw aangepaste documentg hebt gerenderd klik je op de  knop rechtsboven in de hoek van de preview scherm. Het aangepaste document zal dezelfde URL hebben als het orginele document.\r\nEnkele RMarkdown-documenten\r\nNet als \\(\\Latex\\) leent RMarkdown zich voor het opmaken van hele verschillende wetenschappelijke producten. Van een aantal belangrijke artikele heb ik wat voorbeelden gemaakt. Als basis heb ik een klein rapport van twee collega’s van mij genomen. Voor mensen die hiermee willen werken, is er een syntax. Met behulp daarvan kun je het product makkelijk en eenvoudig maken.\r\nArtikel\r\nIedereen die wel eens een wetenschappelijk artikel maakt, weet dat hij of zij soms tegen ingewikkelde zaken aanloopt. De consistente opbouw bijvoorbeeld, het toevoegen van een tabel of een figuur, de referenties goed plaatsen. Als je in Word werkt is het soms lastig als je van volgorde verandert. Bij RMarkdown kun je het pakket rticlesbinnenhalen. Dan krijg je enkele templates waaruit je een keuze kunt maken. Hier JSS-voorbeeld het artikel voor Journal of Statistical Software.\r\nBoek\r\nEen boek bestaat uit verschillende onderdelen, bijvoorbeeld uit een een inhoudsopgave, de hoofdstukken en de literatuur. Hier wordt een stramien voor de opbouw van een boek aangeboden dat eenvoudig is aan te passen. Van het rapprt heb ik een klein boekje gemaakt, dat vind je hier DenHaagboekhtml\r\nHet is dan ook makkelijk om van hetzelfde een eboek te maken. Dat eboek vind je hier EboekDenHaag\r\nRapport\r\nVan hetzelfde onderzoek heb ik ook een fraai vormgegeven rapport gemaakt in de Tufte-stijl. Dat staat hier DenHaagRapport\r\nPresentatie\r\nDe presentaties kunnen er ook net wat strakker uitzien. Hier geef ik twee voorbeelden Beamer en ook in een andere stijl kun je het zelf draaien ioslides\r\nEen blog en website\r\nDit blog is met het pakket Radix gemaakt. Van Den Haag heb ik ook een blog gemaakt en dat vind je hier.Blog\r\nTutorial\r\nOver RMarkdown heb ik eerder deze tutorial van Schmidt bewerkt, zie Dynamische documenten\r\nEen dashboard\r\nEen dashboard is een fraaie manier om kort maar krachtig enkele resultaten te presenteren. Zie hier Dashboard\r\nTot slot\r\nHartelijke dank Marian Schmidt voor het opzetten van de workshop RMarkdown waar ik heel veel van heb geleerd en in dit document gebruik van heb gemaakt. Hartelijk dank natuurlijk ook naar Yihui Xie die zoveel hieromtrent heeft ontwikkeld en dit in alle openbaarheid met anderen deelt.\r\n\r\n\r\n",
    "preview": "posts/2018-11-26-communiceren-met-rmarkdown/Figures/single-fig-center-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-simple-features/",
    "title": "Bewerking geografische data in R: Nieuwe ontwikkelingen",
    "description": "Het nieuwe R sf-pakket, dat sp vervangt om met geografische objecten om te gaan, is  ontworpen om makkelijk met Tidyverse om te gaan. Hier laat ik zien hoe sf-objecten als data-frames worden opgeslagen en jou in staat stelt om met  ggplot2, dplyr en tidyr te werken. Ook het R-pakket tmap biedt veel nieuwe mogelijkheden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-11-14",
    "categories": [],
    "contents": "\r\nOorspronkelijk was sp het standaardpakket om in R met geografische data om te gaan. Dit pakket(samen met andere pakketten zoals raster) maakt van R een krachtig GIS-gereedschap. Echter, spis vandaag de dag wat gedateerd. Ik heb wat gelezen en gewerkt met R package sf dat bedoeld is om sp op te volgen. Dit pakket is onderdeel van R Simple Features, kan files makkelijk inlezen, topologische handelingen uitvoeren en files schrijven.\r\nIk ben erg onder de indruk van watsfkan; het lijkt alles te kunnen wat sp, rgdalen rgeos kunnen, maar op een meer moderne intuïtieve manier. Maar wat vooral aantrekkelijk is vansfis dat de ontwikkelaars van dit pakket aansluiten bij de ontwerp principes van Hadley Wickhams Tidyverse. Het zijn de volgende zaken die opvallen:\r\nGeografische objecten worden opgeslagen als data frames, waarbij de geometrische kenmerken in één list worden opgeslagen;\r\nAlle functies beginnen met st_ om het eenvoudig te maken;\r\nFuncties zijn ‘pipe-vriendelijk’;\r\ndplyr en tidyr werken met de sf objecten;\r\nggplot2 is binnenkort in staat om sf objecten direct te plotten.\r\nMet deze kenmerken pastsf veel beter bij moderne data analyse-opzet dan sp. Je kunt nu direct metdplyr functies als mutate() of select() werken.\r\nPakketten die je voor onderstaande nodig hebt\r\nNatuurlijk moeten we sf en tidyverse (waarin ggplot2, dplyr en tidyr zitten) openen. Daarnaast openen we ook viridis (voor palette-kleuren) en rvestpakket (om html-data van het web te halen).\r\n\r\n\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\nlibrary(viridis)\r\nlibrary(rvest)\r\n\r\nSimple Features as data frames\r\nSimple Features is een open source standaard voor de weergave van objecten (d.w.z. functies). Het eerste vignet voor het sf pakket beschrijft in detail de verschillende soorten functies die kunnen worden weergegeven (bijvoorbeeld POINT, LINESTRING, POLYGON, etc.) en hoe ermee te werken met de functies in sf.sf maakt gebruik van het bekende data frame om functies op te slaan. Het allereerste materiaal hieronder komt van van het eerste vignet.\r\nIn dit pakket worden functies opgeslagen als data frames van de sf klasse. Elke rij bestaat uit een functie/eenheid en elke kolom uit een attribuut/kenmerk. Het verschil met een normaal dataframe is dat er een extra ‘list’-kolom van de klasse sfc is toegevoegd waarin de geometrische kenmerken zijn opgeslagen.\r\nLaten we beginnen met het laden van wat voorbeeldgegevens die in het pakket zitten. Dit is een vormbestand van provincies in North Carolina. Ik zal dit ook omzetten naar een sp object ter vergelijking.\r\n\r\n\r\n\r\nHet resulterende sf object is in wezen slechts een data.frame met een extra kolom voor de geografische informatie.\r\n\r\n\r\n[1] \"sf\"         \"data.frame\"\r\n\r\nObservations: 2\r\nVariables: 15\r\n$ AREA      <dbl> 0.114, 0.061\r\n$ PERIMETER <dbl> 1.442, 1.231\r\n$ CNTY_     <dbl> 1825, 1827\r\n$ CNTY_ID   <dbl> 1825, 1827\r\n$ NAME      <fct> Ashe, Alleghany\r\n$ FIPS      <fct> 37009, 37005\r\n$ FIPSNO    <dbl> 37009, 37005\r\n$ CRESS_ID  <int> 5, 3\r\n$ BIR74     <dbl> 1091, 487\r\n$ SID74     <dbl> 1, 0\r\n$ NWBIR74   <dbl> 10, 10\r\n$ BIR79     <dbl> 1364, 542\r\n$ SID79     <dbl> 0, 3\r\n$ NWBIR79   <dbl> 19, 12\r\n$ geometry  <MULTIPOLYGON [Â°]> MULTIPOLYGON (((-81.47276 3..., M...\r\n\r\n# A tibble: 2 x 15\r\n   AREA PERIMETER CNTY_ CNTY_ID NAME  FIPS  FIPSNO CRESS_ID BIR74\r\n  <dbl>     <dbl> <dbl>   <dbl> <fct> <fct>  <dbl>    <int> <dbl>\r\n1 0.114      1.44  1825    1825 Ashe  37009  37009        5  1091\r\n2 0.061      1.23  1827    1827 Alle~ 37005  37005        3   487\r\n# ... with 6 more variables: SID74 <dbl>, NWBIR74 <dbl>, BIR79 <dbl>,\r\n#   SID79 <dbl>, NWBIR79 <dbl>, geometry <MULTIPOLYGON [Â°]>\r\n\r\nHet mooie hiervan is dat iedereen weet hoe te werken met data-frames in R. Dus deze sf objecten zijn eenvoudig te inspecteren en mee te spelen. Bovendien houdt dit de geometrie en attribuutgegevens bij elkaar op één plaats, d.w.z. ze staan in dezelfde rij van het gegevensframe. Vergelijk dat maar met sp, dat deze gegevens deze gegevens heel anders opslaat:\r\n\r\n\r\n[1] \"SpatialPolygonsDataFrame\"\r\nattr(,\"package\")\r\n[1] \"sp\"\r\n\r\nFormal class 'SpatialPolygonsDataFrame' [package \"sp\"] with 5 slots\r\n  ..@ data       :'data.frame': 2 obs. of  14 variables:\r\n  .. ..$ AREA     : num [1:2] 0.114 0.061\r\n  .. ..$ PERIMETER: num [1:2] 1.44 1.23\r\n  .. ..$ CNTY_    : num [1:2] 1825 1827\r\n  .. ..$ CNTY_ID  : num [1:2] 1825 1827\r\n  .. ..$ NAME     : Factor w/ 100 levels \"Alamance\",\"Alexander\",..: 5 3\r\n  .. ..$ FIPS     : Factor w/ 100 levels \"37001\",\"37003\",..: 5 3\r\n  .. ..$ FIPSNO   : num [1:2] 37009 37005\r\n  .. ..$ CRESS_ID : int [1:2] 5 3\r\n  .. ..$ BIR74    : num [1:2] 1091 487\r\n  .. ..$ SID74    : num [1:2] 1 0\r\n  .. ..$ NWBIR74  : num [1:2] 10 10\r\n  .. ..$ BIR79    : num [1:2] 1364 542\r\n  .. ..$ SID79    : num [1:2] 0 3\r\n  .. ..$ NWBIR79  : num [1:2] 19 12\r\n  ..@ polygons   :List of 2\r\n  .. ..$ :Formal class 'Polygons' [package \"sp\"] with 5 slots\r\n  .. .. .. ..@ Polygons :List of 1\r\n  .. .. .. .. ..$ :Formal class 'Polygon' [package \"sp\"] with 5 slots\r\n  .. .. .. .. .. .. ..@ labpt  : num [1:2] -81.5 36.4\r\n  .. .. .. .. .. .. ..@ area   : num 0.114\r\n  .. .. .. .. .. .. ..@ hole   : logi FALSE\r\n  .. .. .. .. .. .. ..@ ringDir: int 1\r\n  .. .. .. .. .. .. ..@ coords : num [1:27, 1:2] -81.5 -81.5 -81.6 -81.6 -81.7 ...\r\n  .. .. .. ..@ plotOrder: int 1\r\n  .. .. .. ..@ labpt    : num [1:2] -81.5 36.4\r\n  .. .. .. ..@ ID       : chr \"1\"\r\n  .. .. .. ..@ area     : num 0.114\r\n  .. ..$ :Formal class 'Polygons' [package \"sp\"] with 5 slots\r\n  .. .. .. ..@ Polygons :List of 1\r\n  .. .. .. .. ..$ :Formal class 'Polygon' [package \"sp\"] with 5 slots\r\n  .. .. .. .. .. .. ..@ labpt  : num [1:2] -81.1 36.5\r\n  .. .. .. .. .. .. ..@ area   : num 0.0614\r\n  .. .. .. .. .. .. ..@ hole   : logi FALSE\r\n  .. .. .. .. .. .. ..@ ringDir: int 1\r\n  .. .. .. .. .. .. ..@ coords : num [1:26, 1:2] -81.2 -81.2 -81.3 -81.3 -81.3 ...\r\n  .. .. .. ..@ plotOrder: int 1\r\n  .. .. .. ..@ labpt    : num [1:2] -81.1 36.5\r\n  .. .. .. ..@ ID       : chr \"2\"\r\n  .. .. .. ..@ area     : num 0.0614\r\n  ..@ plotOrder  : int [1:2] 1 2\r\n  ..@ bbox       : num [1:2, 1:2] -81.7 36.2 -80.9 36.6\r\n  .. ..- attr(*, \"dimnames\")=List of 2\r\n  .. .. ..$ : chr [1:2] \"x\" \"y\"\r\n  .. .. ..$ : chr [1:2] \"min\" \"max\"\r\n  ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\r\n  .. .. ..@ projargs: chr \"+proj=longlat +datum=NAD27 +no_defs +ellps=clrk66 +nadgrids=@conus,@alaska,@ntv2_0.gsb,@ntv1_can.dat\"\r\n\r\nMerk hier op dat de attribuutgegevens worden opgeslagen als een data.frame in het data slot en dat de functies afzonderlijk worden opgeslagen. Dit kan heel verwarrend zijn om direct mee te werken.\r\nDe geometrie lijst-kolom van een sf object is een object van klasse sfc en een extra klasse die overeenkomt met het geometrietype, in dit geval sfc_MULTIPOLYGON. Het is toegankelijk met st_geometrie(). Aanvullende informatie over de kenmerken, zoals het coördinatensysteem, wordt als attributen opgeslagen:\r\n\r\n\r\nGeometry set for 2 features \r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: -81.74107 ymin: 36.23436 xmax: -80.90344 ymax: 36.58965\r\nepsg (SRID):    4267\r\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\r\n\r\n[1] \"sfc_MULTIPOLYGON\" \"sfc\"             \r\n\r\n$class\r\n[1] \"sfc_MULTIPOLYGON\" \"sfc\"             \r\n\r\n$precision\r\n[1] 0\r\n\r\n$bbox\r\n     xmin      ymin      xmax      ymax \r\n-81.74107  36.23436 -80.90344  36.58965 \r\n\r\n$crs\r\nCoordinate Reference System:\r\n  EPSG: 4267 \r\n  proj4string: \"+proj=longlat +datum=NAD27 +no_defs\"\r\n\r\n$n_empty\r\n[1] 0\r\n\r\nTot slot zijn er individuele eenvoudige kenmerken sfg objecten met extra klassen die overeenkomen met het specifieke type eigenschap. De klassen XY en MULTIPOLYGON geven aan dat dit een tweedimensionale MULTIPOLYGON geometrie is.\r\n\r\n\r\n[1] \"XY\"           \"MULTIPOLYGON\" \"sfg\"         \r\n\r\nIntern zijn deze sfg objecten vectoren voor punten, matrices voor LINESTRING objecten, en lijsten voor al het andere. Meer details zijn beschikbaar in de vignetten van het pakket.\r\nOndersteuning van Tidyverse\r\nWat we vooral van de vorige sectie hebben geleerd is dat sf-objecten data frames zijn! Aangezien data frames de kern vormen van de Tidyverse-pakketten, mag je veronderstellen dat de functies van Tidyverse pakketten van toepassing zouden moeten zijn op de geografische objecten van sf. Zeker, de makers van sf hebben methoden geleverd voor alle standaard dplyr en tidyr handelingen die we kennen en liefhebben. Verder ondersteunt de ontwikkelingsversie van ggpplot2 het plotten van sf objecten.\r\nggplot\r\nMet sp moesten geografische objecten eerst worden geconverteerd naar dataframes (bijv. met fortify())) voordat ze met ggplot2 werden geplot (dat was betrekkelijk ingewikkeld allemaal). Maar omdatsf-objecten al dataframes zijn, kunnen ze met behulp van de nieuwe geom_sf() direct worden geplot.\r\n\r\n\r\n\r\nDaarnaast kan de nieuwe coord_sf() gebruikt worden om deze kenmerken in een andere projectie te plotten, bijvoorbeeld een Albers equal area projectie (in de geografie worden verschillende systemen gebruikt).\r\n\r\n\r\n\r\ndplyr\r\ndplyr is de gouden standaard voor datamanipulatie en biedt een verscheidenheid aan voordelen ten opzichte van basis R-functies. Het is speciaal ontworpen voor het werken met data.frame-achtige objecten zoals die uit het sf pakket. De volgende werkwoorden werken alleen op de attribuutgegevens en laten de geometrieën onaangeroerd:\r\nselect() behoudt de gespecificeerde variabelen, eventueel onder een andere naam\r\nrename() een variabele een andere naam geven en alle andere ongewijzigd laten\r\nfilter() returns the rows that match the given conditions\r\nmutate() voegt nieuwe variabelen toe op basis van bestaande variabelen\r\ntransmute() creëert nieuwe variabelen en laat bestaande variabelen vallen\r\narrange() sorteert op basis van de gegeven variabelen\r\nslice() selecteert rijen op basis van rijnummer\r\nsample_n() trekt steekproeven met n kenmerken willekeurig\r\nHieronder zien we enkele voorbeelden:\r\n\r\n\r\nSimple feature collection with 3 features and 2 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: -79.45597 ymin: 33.94867 xmax: -78.11374 ymax: 35.31512\r\nepsg (SRID):    4267\r\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\r\n      name area_km2                       geometry\r\n1  Sampson     2410 MULTIPOLYGON (((-78.11377 3...\r\n2  Robeson     2400 MULTIPOLYGON (((-78.86451 3...\r\n3 Columbus     2400 MULTIPOLYGON (((-78.65572 3...\r\n\r\nMerk op dat de geometrie steeds ongewijzigd blijft.\r\n\r\n\r\n[1] \"area_km2\" \"geom\"    \r\n\r\nWe kunnen een willekeurige steekproef van functies uit de set nemen met behulp van sample_n():\r\n\r\n\r\n# A tibble: 4 x 2\r\n   AREA                                                       geometry\r\n  <dbl>                                            <MULTIPOLYGON [Â°]>\r\n1 0.111 (((-79.24619 35.86815, -79.23799 35.83725, -79.54099 35.83699~\r\n2 0.219 (((-78.92107 35.57886, -78.99881 35.60132, -78.93889 35.76144~\r\n3 0.203 (((-77.10377 35.55019, -77.11939 35.5855, -77.14835 35.598, -~\r\n4 0.121 (((-79.68596 34.80526, -79.91995 34.80792, -79.90142 34.85241~\r\n\r\nHet is ook mogelijk om functies uit sf te gebruiken die inwerken op de geometrie-kolom binnen een mutatie-instructie. Als er bijvoorbeeld nog geen gebiedskolom bestaat, kan men een gebiedskolom maken met behulp van st_area():\r\n\r\n\r\n# A tibble: 6 x 4\r\n  name     area_m2     area                                   geometry\r\n  <fct>    <S3: unit> <dbl>                        <MULTIPOLYGON [Â°]>\r\n1 Ashe     113738860~ 0.114 (((-81.47276 36.23436, -81.54084 36.27251~\r\n2 Allegha~  61107726~ 0.061 (((-81.23989 36.36536, -81.24069 36.37942~\r\n3 Surry    142348991~ 0.143 (((-80.45634 36.24256, -80.47639 36.25473~\r\n4 Curritu~  69454629~ 0.07  (((-76.00897 36.3196, -76.01735 36.33773,~\r\n5 Northam~ 152074053~ 0.153 (((-77.21767 36.24098, -77.23461 36.2146,~\r\n6 Hertford  96772795~ 0.097 (((-76.74506 36.23392, -76.98069 36.23024~\r\n\r\nGegroepeerde handelingen\r\ndplyr staat ook toe om in groepen te werken op sf objecten. group_by()groepeert een gegevensframe op basis van variabelen in de tabel. Vervolgens wordt summarise() gebruikt om groepssamenvattingen van de gegevens uit te voeren. Laten we beginnen met het toevoegen van een willekeurige groeperingsvariabele en vervolgens het gemiddelde van de gebieden over deze variabele berekenen.\r\n\r\n\r\n\r\nMerk op dat naast de attribuutgegevens die worden geaggregeerd, ook de geometrieën zijn geaggregeerd. Alle geometrieën in elke groep zijn samengevoegd en de grenzen tussen aangrenzende geometrieën zijn opgelost. Intern wordt de functie st_union() gebruikt om dit te bereiken.\r\nNet als bij een normaal gegevensframe kunnen gegroepeerde filtering en mutatie worden uitgevoerd op sf objecten. Bijvoorbeeld, om de proportionele verdeling van geboorten tussen provincies binnen elke groep te berekenen, gebruikt u een gegroepeerde mutate():\r\n\r\n\r\n# A tibble: 100 x 4\r\n   group  AREA area_prop                                      geometry\r\n   <chr> <dbl>     <dbl>                           <MULTIPOLYGON [Â°]>\r\n 1 A     0.114      12.6 (((-81.47276 36.23436, -81.54084 36.27251, -~\r\n 2 C     0.061      12.6 (((-81.23989 36.36536, -81.24069 36.37942, -~\r\n 3 C     0.143      12.6 (((-80.45634 36.24256, -80.47639 36.25473, -~\r\n 4 B     0.07       12.6 (((-76.00897 36.3196, -76.01735 36.33773, -7~\r\n 5 B     0.153      12.6 (((-77.21767 36.24098, -77.23461 36.2146, -7~\r\n 6 A     0.097      12.6 (((-76.74506 36.23392, -76.98069 36.23024, -~\r\n 7 A     0.062      12.6 (((-76.00897 36.3196, -75.95718 36.19377, -7~\r\n 8 A     0.091      12.6 (((-76.56251 36.34057, -76.60424 36.31498, -~\r\n 9 C     0.118      12.6 (((-78.30876 36.26004, -78.28293 36.29188, -~\r\n10 B     0.124      12.6 (((-80.02567 36.25023, -80.45301 36.25709, -~\r\n# ... with 90 more rows\r\n\r\nOm alleen landen te behouden binnen groepen die een groter gebied hebben dan een bepaalde drempel, kan een gegroepeerde filter() worden gebruikt:\r\n\r\n\r\n# A tibble: 38 x 3\r\n   group  AREA                                                geometry\r\n   <chr> <dbl>                                     <MULTIPOLYGON [Â°]>\r\n 1 B     0.07  (((-76.00897 36.3196, -76.01735 36.33773, -76.03288 36~\r\n 2 B     0.153 (((-77.21767 36.24098, -77.23461 36.2146, -77.29861 36~\r\n 3 B     0.124 (((-80.02567 36.25023, -80.45301 36.25709, -80.43531 3~\r\n 4 B     0.153 (((-79.53051 36.24614, -79.53058 36.23616, -80.02567 3~\r\n 5 B     0.072 (((-78.49252 36.17359, -78.51472 36.17522, -78.51709 3~\r\n 6 B     0.081 (((-81.80622 36.10456, -81.81715 36.10939, -81.82231 3~\r\n 7 B     0.064 (((-81.94135 35.95498, -81.9614 35.93922, -81.94495 35~\r\n 8 B     0.128 (((-78.25455 35.81553, -78.26685 35.84838, -78.30841 3~\r\n 9 B     0.17  (((-79.53782 35.89097, -80.0426 35.91681, -80.0381 36.~\r\n10 B     0.111 (((-79.24619 35.86815, -79.23799 35.83725, -79.54099 3~\r\n# ... with 28 more rows\r\n\r\nSamenvoegen\r\ndplyr heeft een reeks functies voor het samenvoegen van gegevensframes op basis van gedeelde kolommen. Deze functies zijn allemaal geïmplementeerd in sf en zijn een geweldige manier om extra attribuutgegevens uit andere bronnen aan uw ruimtelijke gegevens toe te voegen. Het is echter alleen mogelijk om een sf object te verbinden met een gewoon data.frame. Je kunt niet twee sf objecten met elkaar verbinden.\r\nLaten we beginnen met enkele county-level populatiegegevens van Wikipedia af te halen.\r\n\r\n\r\n\r\nNu voegen we deze populatiegegevens samen met onze ruimtelijke gegevens en plotten ze.\r\n\r\n\r\n\r\nAlle andere verbindingsfuncties (bijv. left_join(), anti_join(), etc.) werken op dezelfde manier. Als het tweede argument van een van deze functies een sf object is, en geen normaal gegevensframe, zal er een fout optreden. Vermoedelijk komt dit omdat het onduidelijk is hoe de twee verschillende geometrieën gecombineerd moeten worden, hoewel er wel wat discussie lijkt te zijn over hoe je verbindingen met twee sets van geometrieën kunt implementeren:\r\n\r\n\r\nError: y should be a data.frame; for spatial joins, use st_join\r\n\r\nDeze dplyr functies zijn allemaal voor het verbinden op basis van attribuutgegevens. Als je op zoek bent naar een ruimtelijke verbinding (bijv. twee sf objecten op basis van een snijpunt van geometrieën) dan moet je de functie st_join()gebruiken.\r\ntidyr handelingen\r\nDe tidyr werkwoorden gather() en spread() worden gebruikt om de data frames te transformeren van breed naar lang formaat of vice versa. Bijvoorbeeld, zeg dat u gegevens wilt opslaan over het BBP voor alle landen en een set van jaren. Dit kan worden opgeslagen in een lang formaat (met kolommen land, jaar en gdp), wat als een “tidy” formaat wordt beschouwd, of in een breed formaat (met kolommen land, gdp2000, gdp2001, ….), wat beter is voor weergavedoeleinden. tidyr kan overstappen naar een ander format en nu kan dit ook worden gedaan met sf objecten.\r\nAls we de North Carolina dataset als voorbeeld nemen, zijn BIR74 en BIR79 het aantal geboorten in de provincie in respectievelijk 1974 en 1979. Met gather() kunnen we dit gemakkelijk omzetten in een lang formaat:\r\n\r\n\r\nSimple feature collection with 6 features and 3 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: -81.74107 ymin: 36.23388 xmax: -80.43531 ymax: 36.58965\r\nepsg (SRID):    4267\r\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\r\n     county  year births                       geometry\r\n1      Ashe BIR74   1091 MULTIPOLYGON (((-81.47276 3...\r\n2 Alleghany BIR74    487 MULTIPOLYGON (((-81.23989 3...\r\n3     Surry BIR74   3188 MULTIPOLYGON (((-80.45634 3...\r\n4      Ashe BIR79   1364 MULTIPOLYGON (((-81.47276 3...\r\n5 Alleghany BIR79    542 MULTIPOLYGON (((-81.23989 3...\r\n6     Surry BIR79   3616 MULTIPOLYGON (((-80.45634 3...\r\n\r\nMerk op dat de attribuutgegevens mooi getransponeerd zijn. Het resultaat hiervan is dat elke functie twee rijen heeft en dat de functiegeometrieën gedupliceerd zijn. Voor mij lijkt dit vreemd om dezelfde geometrie op meerdere plaatsen op te slaan, dus ik ben niet zeker van wat deze gather() functie op sf objecten oplevert.\r\nWe kunnen dit terugzetten naar het originele brede formaat met spread():\r\n\r\n\r\nSimple feature collection with 3 features and 3 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: -81.74107 ymin: 36.23388 xmax: -80.43531 ymax: 36.58965\r\nepsg (SRID):    4267\r\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\r\n     county BIR74 BIR79                       geometry\r\n1 Alleghany   487   542 MULTIPOLYGON (((-81.23989 3...\r\n2      Ashe  1091  1364 MULTIPOLYGON (((-81.47276 3...\r\n3     Surry  3188  3616 MULTIPOLYGON (((-80.45634 3...\r\n\r\nDe blog (hier verwijzingHJ) biedt een aanvullende introductie op het sf-pakket. Het bevat een tutorial die jou door een aantal basisprincipes van het pakket leidt, waaronder lezen en schrijven van/naar shapefiles, herprojecten, afdrukken met ggplot, filteren en andere datavormen vinden met dplyr. Het maakt gebruik van een dataset van FiveThirtyEight die bijhoudt hoe vaak elk lid van het Amerikaanse Congres heeft gestemd in lijn met President Trumps. We zullen sf gebruiken om deze gegevens samen te voegen met een ander bestand om ze ruimtelijk te verkennen. Voordat u begint, moet u de benodigde datasets downloaden en in uw werkmap plaatsen. Laad vervolgens de benodigde pakketten voor deze tutorial.\r\n\r\n\r\n\r\nHet lezen van gegevens in R met sf is een relatief eenvoudige taak. Het ondersteunt het direct importeren van eenvoudige functies uit een PostGIS database met verschillende R database tools. In dit geval lezen we gewoon uit een shapefile in onze werkmap.\r\n\r\n\r\nReading layer `congressional_districts' from data source `H:\\MapsinR\\congressional-trump-scores-master\\congressional_districts.shp' using driver `ESRI Shapefile'\r\nSimple feature collection with 433 features and 2 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: -124.7631 ymin: 24.5231 xmax: -66.9499 ymax: 49.38436\r\nepsg (SRID):    4269\r\nproj4string:    +proj=longlat +datum=NAD83 +no_defs\r\n\r\nSimple feature collection with 6 features and 2 fields\r\ngeometry type:  MULTIPOLYGON\r\ndimension:      XY\r\nbbox:           xmin: -124.4096 ymin: 33.87981 xmax: -114.0428 ymax: 42.00105\r\nepsg (SRID):    4269\r\nproj4string:    +proj=longlat +datum=NAD83 +no_defs\r\n  district state                       geometry\r\n1       40    CA MULTIPOLYGON (((-118.281 34...\r\n2        2    CA MULTIPOLYGON (((-122.4463 3...\r\n3       31    CA MULTIPOLYGON (((-117.7044 3...\r\n4       10    CA MULTIPOLYGON (((-121.557 37...\r\n5        9    CA MULTIPOLYGON (((-121.8513 3...\r\n6        4    NV MULTIPOLYGON (((-119.4398 3...\r\n\r\nOm de attributen geassocieerd met deze polygonen te bekijken of te manipuleren, kunnen we het object eenvoudigweg behandelen als een conventioneel R data frame. De basisplotfunctie geeft u een snel en handig overzicht van elk attribuut dat in kaart is gebracht over alle polygonen. Vervolgens moeten we de stemgegevens van elk lid van het Huis van Afgevaardigden laden. Deze gegevens zitten in een ander databestand.\r\n\r\n\r\n  X.1 X             name chamber party state district trump_score\r\n1   1 1 David G. Valadao   House     R    CA       21       1.000\r\n2   2 2   Carlos Curbelo   House     R    FL       26       0.935\r\n3   3 3     Erik Paulsen   House     R    MN        3       1.000\r\n4   4 4 Barbara Comstock   House     R    VA       10       0.970\r\n5   5 5  Darrell E. Issa   House     R    CA       49       1.000\r\n6   6 6  Edward R. Royce   House     R    CA       39       0.970\r\n  trump_margin predicted_score trump_plus_minus\r\n1       -0.155           0.296            0.704\r\n2       -0.161           0.284            0.652\r\n3       -0.094           0.428            0.572\r\n4       -0.100           0.413            0.556\r\n5       -0.075           0.478            0.522\r\n6       -0.086           0.449            0.521\r\n\r\nDe mensen van ‘FiveThirtyEight’ hebben een reeks samenvattende statistieken berekend die bij elke Vertegenwoordiger hoort en gekoppeld aan de stemrealiteit van Trump’s standpunt over die maatregel. Zij hebben zo een Trump Score (Trump_score) berekend die ons vertelt welk deel van de tijd elke wetgever in lijn met de president heeft gestemd. We zijn vooral geinteresseerd in het visualiseren van deze variabele. Om deze variabele in kaart te brengen zullen we de tabelvormige stemgegevens moeten samenvoegen met onze districtspolygonen. Omdat we sf objecten kunnen behandelen als data frames, is het eenvoudig om dit te doen met behulp van de join-functies van dplyr.\r\n\r\n\r\n\r\nNu we beide datasets hebben gecombineerd tot een sf-object, kunnen we ze in kaart brengen met ggplot. Wanneer je ggplot een sf-object geeft, weet het commando geom_sf de punten, lijnen of veelhoeken te tekenen op basis van het bekende tekstgeometrieveld in het object. Het andere sf-specifieke ggplot commando in dit voorbeeld is coord_sf, waarmee je een alternatieve projectie voor je kaart kunt specificeren. Je kunt een co?rdinatensysteem selecteren aan de hand van zijn epsg-code, die te vinden is op spatialreference.org.\r\n\r\n\r\n\r\nDe kaart versterkt ons begrip van het sterk partijdige karakter van het Congres. De meeste troefscores liggen aan de uitersten, omdat maar weinig wetgevers zich bereid hebben getoond om met hun partijlijnen te breken.\r\nOnze volgende stap zal zijn om in te zoomen op een bepaald deel van het land. We zullen dplyr gebruiken om slechts drie staten in het hoger gelegen Midwesten (Minnesota, Wisconsin en Iowa) in kaart te brengen. Onderweg zullen we ook een sf object opnieuw projecteren, polygon centroiden extraheren, en die centroiden gebruiken om elk district van een label te voorzien.\r\n\r\n\r\n\r\nDit voorbeeld laat zien hoe je sf-functies in magrittr pijpleidingen kunt nestelen met behulp van de %>% operator waarmee een gebruiker van een ander pakket waarschijnlijk bekend is. Vervolgens zul je zien hoe je dplyr kunt gebruiken om polygonen op te lossen. Laten we eens kijken naar de originele kaart van de hele Verenigde Staten, maar deze keer voegen we de Trump Scores samen tot het niveau van de Staat.\r\n\r\n\r\nSimple feature collection with 6 features and 3 fields\r\ngeometry type:  GEOMETRY\r\ndimension:      XY\r\nbbox:           xmin: -124.4096 ymin: 30.22333 xmax: -71.78699 ymax: 42.05059\r\nepsg (SRID):    4269\r\nproj4string:    +proj=longlat +datum=NAD83 +no_defs\r\n# A tibble: 6 x 4\r\n  state avg_trump_score districts                             geometry\r\n  <chr>           <dbl>     <int>                      <GEOMETRY [Â°]>\r\n1 AL              0.861         7 MULTIPOLYGON (((-88.05338 30.50699,~\r\n2 AR              0.992         4 POLYGON ((-94.48558 33.65331, -94.4~\r\n3 AZ              0.650         9 POLYGON ((-109.0476 32.42638, -109.~\r\n4 CA              0.377        53 MULTIPOLYGON (((-119.5773 33.27858,~\r\n5 CO              0.615         7 POLYGON ((-104.9434 40.99808, -104.~\r\n6 CT              0.147         5 MULTIPOLYGON (((-73.63057 40.98071,~\r\n\r\nWe kunnen de bekende dplyr-syntax gebruiken om de districten te groeperen op staat en samenvattende statistieken te berekenen. Omdat sf-objecten zo goed integreren met dplyr, groepeert de functie automatisch de ruimtelijke gegevens samen met de tabel. Het eindresultaat is vergelijkbaar met dat van de geoprocessing dissolve tool in een traditioneel desktop GIS.\r\n\r\n\r\n\r\ntmap\r\nNaast sfis tmapeen ander R-pakket dat jou kan ondersteunen bij het maken van geografische kaarten. Met het tmap-pakket kunnen thematische kaarten met grote flexibiliteit worden gegenereerd. De syntaxis voor het maken van plots is ook hier vergelijkbaar met die van ggplot2, maar dan op maat gemaakt voor kaarten. Het pakket tmapbiedt een instructief vignet dat is bedoeld voor degenen die binnen een paar minuten aan de slag willen met tmap. Een meer gedetailleerde beschrijving van tmap is te vinden in een artikel gepubliceerd in het Journal of Statistical Software (JSS), dat tmap versie 1.11-2 beschrijft. De wijzigingen in versie 2.0 worden beschreven in vignette(\"tmap-changes-v2\").\r\nEen goede plek om te beginnen is om een kaart van de wereld te maken. Na installeren tmap, moeten we met de volgende coderegels de onderstaande kaart maken:\r\n\r\n\r\n\r\nHet object Wereld is een ruimtelijk object van klasse sf uit het [sf-pakket] (https://CRAN.R-project.org/package=sf); het is een data.frame met een speciale kolom die een geometrie voor elke rij bevat, in dit geval polygonen. Om het in tmap te tekenen, moet je het eerst specificeren met tm_shape. Plot-lagen kunnen worden toegevoegd met de + operator, in dit geval tm_polygonen. Er zijn veel laagfuncties in tmap, die gemakkelijk te vinden zijn in de documentatie door hun tm_ prefix. Zie ook `?‘tmap-element’``.\r\nMeerdere vormen en lagen\r\nEen vorm is een ruimtelijk object (met een klasse van sf, sp of raster). Meerdere vormen en ook meerdere lagen per vorm kunnen worden uitgezet:\r\n\r\n\r\n\r\nFacetten\r\nFacetten kunnen op drie manieren worden gemaakt:\r\nDoor meerdere variabele namen toe te kennen aan één esthetiek:\r\n\r\n\r\n\r\nDoor de ruimtelijke gegevens te splitsen met het by argument van tm_facets:\r\n\r\n\r\n\r\nDoor gebruik te maken van de tmap_arrange functie:\r\n\r\n\r\n\r\nSnelle thematische kaart\r\nKaarten kunnen snell worden gemaakt met slechts één functie op te roepen. Deze functie is qtm:\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-11-14-simple-features/simple-features_files/figure-html5/ggplot-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-visualisatiegapminder/",
    "title": "Visualisatie met inzet van Gapminder",
    "description": "Een voorbeeld van datavisualisatie: Trends op het gebied van de wereldgezondheid\nen de economie",
    "author": [
      {
        "name": "Rafael A. Irizarry, bewerkt H. Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-08-14",
    "categories": [],
    "contents": "\r\nEen praktijkvoorbeeld: Trends op het gebied van de wereldgezondheid en de economie\r\nIn dit deel laten we zien hoe relatief eenvoudig de ggplot -code inzichtelijke en esthetisch aangename plots kan maken die ons helpen trends in de wereldgezondheid en economie beter te begrijpen. Later breiden we de code iets uit om de plots te perfectioneren en beschrijven we enkele algemene principes als leidraad voor datavisualisatie.\r\nVoorbeeld 1: Levensverwachting en vruchtbaarheidcijfers\r\nHans Rosling was medeoprichter van de Gapminder Foundation, een organisatie die het publiek wil stimuleren om gegevens te gebruiken om veelvoorkomende mythes over de zogenaamde ontwikkelingswereld te verdrijven. De organisatie gebruikt gegevens om aan te tonen hoe de daadwerkelijke tendensen in gezondheid en economie de verhalen tegenspreken die van de media komen en sensationeel berichten over catastrofes, tragedies en andere ongelukkige gebeurtenissen, zoals die op de website van de Gapminder Foundation staan.\r\n\r\n\r\n\r\nJournalisten en lobbyisten vertellen dramatische verhalen. Dat moeten ze want dat is hun taak. Ze vertellen verhalen over bijzondere gebeurtenissen en ongewone mensen. De stapels dramatische verhalen die een al te dramatisch wereldbeeld vormen en zorgen voor sterke negatieve stressgevoelens: “De wereld wordt erger”, “Wij vs. zij! , ,,Andere mensen zijn vreemd”, “De bevolking blijft maar groeien” en “Het maakt niemand wat uit”!\r\n\r\n\r\n\r\nHans Rosling bracht actuele data-gebaseerde trends op een eigen dramatische manier over aan de hand van effectieve datavisualisatie. Dit hoofdstuk is gebaseerd op twee gesprekken die deze benadering van onderwijs illustreren: Nieuwe inzichten in armoede en De beste Statistiek die je gezien hebt.\r\nIn deze paragraaf willen we aan de hand van gegevens een antwoord geven op de volgende twee vragen:\r\nIs het een eerlijke typering van de wereld van vandaag om te zeggen dat ze verdeeld is in westerse rijke naties en de ontwikkelingslanden in Afrika, Azi? en Latijns-Amerika?\r\nIs de inkomensongelijkheid tussen landen de afgelopen 40 jaar toegenomen?\r\nOm deze vraag te beantwoorden zullen we gebruik maken van de gapminder dataset in dslabs. Deze dataset is gemaakt met behulp van een aantal spreadsheets die beschikbaar zijn bij de Stichting [Gapminder] (http://www.gapminder.org/). U kunt op deze manier toegang krijgen tot de tafel:\r\n\r\n\r\nlibrary(dslabs)\r\ndata(gapminder)\r\nhead(gapminder)\r\n\r\n              country year infant_mortality life_expectancy fertility\r\n1             Albania 1960           115.40           62.87      6.19\r\n2             Algeria 1960           148.20           47.50      7.65\r\n3              Angola 1960           208.00           35.98      7.32\r\n4 Antigua and Barbuda 1960               NA           62.97      4.43\r\n5           Argentina 1960            59.87           65.39      3.11\r\n6             Armenia 1960               NA           66.86      4.55\r\n  population          gdp continent          region\r\n1    1636054           NA    Europe Southern Europe\r\n2   11124892  13828152297    Africa Northern Africa\r\n3    5270844           NA    Africa   Middle Africa\r\n4      54681           NA  Americas       Caribbean\r\n5   20619075 108322326649  Americas   South America\r\n6    1867396           NA      Asia    Western Asia\r\n\r\nDe quiz van Hans Rosling\r\nZoals gedaan in de New Insights on Poverty video, beginnen we met het testen van onze kennis over verschillen in kindersterfte tussen verschillende landen.\r\nVoor elk van de zes onderstaande paren landenparen, willen we weten welk land volgens u de hoogste kindersterfte in 2015 had? Welke paren lijken volgens jou het meest op elkaar?\r\nSri Lanka of Turkije\r\nPolen of Zuid-Korea\r\nMaleisi? of Rusland\r\nPakistan of Vietnam\r\nThailand of Zuid-Afrika\r\nWanneer deze vragen zonder gegevens worden beantwoord, worden de niet-Europese landen doorgaans gekozen als landen met hogere sterftecijfers: Sri Lanka boven Turkije, Zuid-Korea boven Polen en Maleisi? boven Rusland. Ook in landen die als ontwikkelingslanden worden beschouwd, Pakistan, Vietnam, Thailand en Zuid-Afrika, is het sterftecijfer even hoog.\r\nOm deze vragen __ met data__ te beantwoorden kunnen we het R-pakket dplyr gebruiken. Voor de eerste vergelijking zien we bijvoorbeeld dat\r\n\r\n\r\n    country infant_mortality\r\n1 Sri Lanka              8.4\r\n2    Turkey             11.6\r\n\r\nTurkije heeft een hogere score.\r\nWe kunnen deze code op alle vergelijkingen plakken en dan zien we het volgende:\r\n\r\ncountry\r\ninfant_mortality\r\ncountry1\r\ninfant_mortality1\r\nSri Lanka\r\n8.4\r\nTurkey\r\n11.6\r\nPoland\r\n4.5\r\nSouth Korea\r\n2.9\r\nMalaysia\r\n6.0\r\nRussia\r\n8.2\r\nPakistan\r\n65.8\r\nVietnam\r\n17.3\r\nThailand\r\n10.5\r\nSouth Africa\r\n33.6\r\n\r\nWe zien dat de Europese landen hogere cijfers hebben: Polen heeft een hoger percentage dan Zuid-Korea en Rusland een hoger percentage dan Maleisi?. We zien ook dat Pakistan een veel hoger percentage heeft dan Vietnam en Zuid-Afrika een veel hoger percentage dan Thailand. Het blijkt dat de meeste mensen het slechter doen dan wanneer ze zouden raden. We lijken wel onwetend zijn en we zijn verkeerd ge?nformeerd.\r\nLevensverwachting en vruchtbaarheidscijfers\r\nDe reden hiervoor is het idee dat de wereld in twee groepen te verdelen is: de Westerse wereld (West-Europa en Noord-Amerika), met z’n lange levensduur en kleine gezinnen, tegenover de ontwikkelingslanden (Afrika, Azi? en Latijns-Amerika), gekenmerkt door een korte levensduur en grote gezinnen. Maar rechtvaardigen de gegevens deze dichotomie van twee groepen wel?\r\nDe nodige gegevens om deze vraag te beantwoorden zitten ook in onze gapminder tabel. Met behulp van onze nieuw aangeleerde vaardigheden om data te visualiseren, zullen we in staat zijn om deze vraag te beantwoorden.\r\nDe eerste plot die we maken om te zien wat de gegevens zeggen over dit wereldbeeld is een spreidingplot van levensverwachting versus vruchtbaarheidscijfers (gemiddeld aantal kinderen per vrouw). We kijken eerst naar gegevens van zo’n vijftig jaar geleden, toen dit standpunt misschien nog te rechtvaardigen was.\r\n\r\n\r\n\r\nDe meeste punten vallen in twee verschillende categorie?n uiteen:\r\nLevensverwachting rond 70 jaar en 3 of minder kinderen per gezin\r\nLevensverwachting lager dan 65 jaar en met meer dan 5 kinderen per gezin.\r\nOm te bevestigen dat de landen inderdaad afkomstig zijn uit de regio’s die wij verwachten, kunnen wij kleur gebruiken om het continent te vertegenwoordigen.\r\n\r\n\r\n\r\nDus de visie in 1962, “het westen versus de ontwikkelingslanden”, was gebaseerd op een of andere realiteit. Maar is dat 50 jaar later nog steeds het geval?\r\nFacetteren\r\nWe konden de gegevens voor 2012 gemakkelijk in kaart brengen op dezelfde manier als voor 1962. Maar om de gegevens te vergelijken, kunnen we de inzichten het beste naast elkaar zetten. In ggplot kunnen we dit doen met’ faceting variabelen’: we stratificeren de gegevens met een of andere variabele en maken dezelfde plot voor elke groep.\r\nOm te facetteren (ik weet niet of het een Nederlands woord is, maar goed) voegen we een laag toe met de functie facet_grid, die automatisch de groepen scheidt. Met deze functie kunt u maximaal twee variabelen facetteren met behulp van kolommen om de ene variabele weer te geven en rijen om de andere weer te geven. De functie verwacht de rij- en kolomvariabelen die door een ~ van elkaar zijn gescheiden. Hier is een voorbeeld van een verstrooiingsplot met een facet_grid als laatste laag toegevoegd:\r\n\r\n\r\n\r\nWe zien een plot voor elk continent/jaarpaar. Maar dit is slechts een voorbeeld en is al meer dan wat we willen, want dat is gewoon om 1962 en 2012 te vergelijken. In dit geval is er maar ??n variabele en gebruiken we . om de facet te laten weten dat we geen van de variabelen gebruiken:\r\n\r\n\r\n\r\nDeze plot laat duidelijk zien dat de meerderheid van de landen zich heeft ontwikkeld van ontwikkelingslandcluster naar het ontwikkeldlandcluster. In 2012 heeft het oude perspectief geen zin meer. Dit wordt vooral duidelijk bij een vergelijking van Europa en Azi?, want vooral binnen dat laatste continent zijn er verschillende landen waarbinnen grote verbeteringen hebben doorgevoerd.\r\nfacet_wrap\r\nOm te onderzoeken hoe deze transformatie door de jaren heen is gegaan, kunnen we het perceel voor meerdere jaren maken. We kunnen bijvoorbeeld 1970, 1980, 1990, 2000 toevoegen. Als we dit doen, willen we niet dat alle percelen op dezelfde rij staan, het standaard gedrag van facet_grid, omdat ze te dun worden om de gegevens weer te geven. In plaats daarvan zullen we meerdere rijen en kolommen gebruiken. Dat kan met de functie facet_wrap , waarmee automatisch een reeks percelen onstaat met ded juiste afmetingen:\r\n\r\n\r\n\r\nDit plot toont duidelijk aan hoe de meeste Aziatische landen zich veel sneller hebben verbeterd dan de Europese.\r\nVaste schalen voor betere vergelijkingen\r\nMerk op dat de standaard keuze van het bereik van de assen een belangrijke is. Wanneer geen facet' wordt gebruikt, wordt het bereik bepaald door de gegevens die in de grafiek worden getoond. Bij gebruik vanfacet’ wordt dit bereik bepaald door de gegevens die op alle percelen worden weergegeven en wordt het dus voor alle percelen vastgehouden. Dit maakt vergelijkingen tussen percelen veel gemakkelijker. In bovenstaand perceel zien we bijvoorbeeld dat de levensverwachting in de meeste landen is gestegen en de vruchtbaarheid is afgenomen. We zien dit omdat de puntenwolk beweegt. Dit is niet het geval als we de schalen aanpassen:\r\n\r\n\r\n\r\nIn de plot hierboven moeten we speciale aandacht besteden aan het bereik om op te merken dat de rechter plot aan de rechter kant een grotere levensverwachting vertoont.\r\nAnimatie\r\nMet het gganimate pakket kunt u eenvoudig facetten omzetten in een animatie:\r\n\r\n\r\n\r\nTijdreeksfiguren\r\nBovenstaande visualisaties laten duidelijk zien dat data het oude beeld van het westen tegenover ontwikkelingslanden niet meer ondersteunen. Als we deze figuren eenmaal hebben gezien, rijzen er nieuwe vragen. Welke landen verbeteren bijvoorbeeld meer en welke minder? Was de verbetering de afgelopen 50 jaar constant of was er in bepaalde perioden sprake van een versnelling? Om deze vraag beter te kunnen beantwoorden, gaan we dieper in op de tijdreeksfiguren.\r\nIn tijdreeksfiguren staat tijd op de x-as en een uitkomst of meting die van belang is op de y-as. Hier is bijvoorbeeld een trendfiguur voor de vruchtbaarheidscijfers van Nederland:\r\n\r\n\r\n\r\nWe zien dat de trend helemaal niet lineair is. In plaats daarvan zien we een scherpe daling tijdens de jaren ’60 en ’70 naar onder de 2. Dan komt de trend terug op 2 en stabiliseert zich tijdens de jaren ’90.\r\nWanneer de punten regelmatig en dicht op elkaar liggen, zoals hier, maken we krommen door de punten als lijnen met elkaar te verbinden om aan te geven dat deze gegevens uit ??n land afkomstig zijn. Hiervoor gebruiken we de functie geom_line in plaats van geom_point.\r\n\r\n\r\n\r\nDit is met name nuttig wanneer we naar twee landen kijken. Als we de gegevens zo onderverdelen in twee landen, ??n uit Europa en ??n uit Azi?, dan kopieer je de gegevens naar de bovenstaande code:\r\n\r\n\r\n\r\nMerk op dat dit niet de figuur is die we willen. In plaats van een lijn voor elk land, worden de punten voor beide landen samengevoegd. Dit wordt eigenlijk verwacht omdat we ggplot niets hebben verteld over het willen hebben van twee aparte lijnen. Om ggplot te laten weten dat we twee afzonderlijke curven willen hebben, wijzen we elk punt toe aan een ‘groep’, ??n voor elk land:\r\n\r\n\r\n\r\nMaar welke lijn gaat over welk land? We kunnen kleuren toewijzen om dat onderscheid te maken. Een nuttig neveneffect van het gebruik van het ‘kleur’-argument om verschillende kleuren toe te wijzen aan de verschillende landen, is dat de gegevens automatisch worden gegroepeerd:\r\n\r\n\r\n\r\nUit het perceel blijkt duidelijk dat het vruchtbaarheidscijfer van Zuid-Korea in de jaren ’60 en ’70 drastisch is gedaald en in 1990 even hoog was als in Duitsland.\r\nDe voorkeur van labels boven legenda’s\r\nVoor trendplots raden we aan om de lijnen te labelen in plaats van legenda’s te gebruiken omdat de kijker snel kan zien welke lijn welk land is. Deze suggestie is eigenlijk van toepassing op de meeste figuren: labeling heeft meestal de voorkeur boven legenda’s.\r\nAan de hand van de gegevens over de levensverwachting laten we zien hoe we dit kunnen doen. We defini?ren een datatabel met de labellocaties en gebruiken dan een tweede mapping alleen voor deze labels:\r\n\r\n\r\n\r\nDe figuur toont duidelijk aan hoe een verbetering van levensverwachting de dalingen in vruchtbaarheidscijfers volgde. Terwijl de Duitsers in 1960 meer dan 15 jaar meer Zuid-Koreanen woonden, is de kloof in 2010 volledig gedicht. Het is een voorbeeld van de verbetering die veel niet-westerse landen in de afgelopen veertig jaar hebben bereikt.\r\nVoorbeeld 2: Inkomensverdeling\r\nEen andere veelgehoorde opmerking is dat de verdeling van de welvaart over de wereld de laatste decennia is verslechterd. Wanneer het algemene publiek wordt gevraagd of arme landen armer zijn geworden en rijke landen rijker, antwoordt de meerderheid ja. Door gebruik te maken van stratificatie, histogrammen, vloeiende verdeling en boxplots zullen we in staat zijn om te begrijpen of dit inderdaad het geval is. We leren dan ook hoe transformaties soms kunnen helpen om meer informatieve samenvattingen en figuren aan te bieden.\r\nTransformaties\r\nDe `gapminder’-gegevenstabel bevat een kolom met het bruto binnenlands product (BBP) van de landen. Het BBP meet de marktwaarde van de goederen en diensten die een land in een jaar produceert. Het BBP per persoon wordt vaak gebruikt als een ruwe samenvatting van hoe rijk een land is. Hier delen we deze hoeveelheid door 365 om de meer interpreteerbare maat dollars per dag te krijgen. Wanneer we de huidige US-dollar als eenheid gebruiken, wordt een persoon die met een inkomen van minder dan $ 2 per dag overleeft, gedefinieerd als een persoon die in absloute armoede leeft. Deze variabele voegen we toe aan de datatabel:\r\n\r\n\r\n\r\nMerk op dat de BBP-waarden zijn gecorrigeerd voor inflatie en staan voor de huidige US-dollar. Dus deze waarden zijn bedoeld om over de jaren heen vergelijkbaar te zijn. Merk ook op dat het hier om landsgemiddelden gaat en dat er binnen elk land veel variatie is. Alle hieronder beschreven grafieken en inzichten hebben betrekking op landsgemiddelden en staan dus niet individuele personen.\r\nVerdeling van het landinkomen\r\nHier is een histogram van de inkomens per dag uit 1970:\r\n\r\n\r\n\r\nWe gebruiken het color = \"black\" om een grens te trekken en de bins (‘bakjes’) duidelijk van elkaar te onderscheiden.\r\nIn dit diagram zien we dat voor de meeste landen gemiddelden zijn onder $10 per dag. Het grootste deel van de x-as is echter gewijd aan de 35 landen met gemiddelden boven $10. De grafiek is dus niet erg informatief over landen met waarden onder $10 per dag.\r\nHet is misschien informatiever om snel te kunnen zien hoeveel landen een gemiddeld daginkomen hebben van ongeveer $1 (extreem arm), $2 (zeer arm), $4 (arm), $8 (midden), $16 (welgesteld), $32 (rijk), $64 (zeer rijk) per dag. Deze veranderingen zijn vermenigvuldigend en logtransformaties.\r\nHier is de verdeling als we een log2 transformatie toepassen:\r\n\r\n\r\n\r\nIn zekere zin geeft dit een close up van de landen met een gemiddeld tot lager inkomen.\r\nWelke basis?\r\nIn het bovenstaande geval hebben we basis 2 gebruikt in de log-transformaties. Andere veelvoorkomende keuzes zijn basis \\(e\\) (de natuurlijke log) en basis 10.\r\nOver het algemeen raden wij het gebruik van het natuurlijke logboek voor het verkennen en visualiseren van gegevens aan.Dit is omdat \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^1, 10^2, \\dots\\) makkelijk zijn te berekenen in ons hoofd. Hetzelfde geldt niet voor \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\).\r\nIn het voorbeeld dollars per dag gebruikten we basis 2 in plaats van basis 10 omdat het resulterende bereik gemakkelijker te interpreteren is. Het bereik van de waarden die worden uitgezet is 0.3269426, 48.8852142.\r\nIn basis 10 verandert dit in een bereik dat zeer weinig gehele getallen omvat: slechts 0 en 1. Met basis twee omvat ons assortiment -2, -1, 0, 1, 2, 3, 4 en 5. Het is gemakkelijker om \\(2^x\\) en \\(10^x\\) te berekenen wanneer \\(x\\) een geheel getal is en tussen -10 en 10 ligt. Dus geven we de voorkeur aan meer kleine gehele getallen in de schaal. Een ander gevolg van een beperkt bereik is dat het kiezen van de ‘bin’-breedte een grotere uitdaging is. Met log base 2 weten we dat een ‘bin’-breedte van 1 zal vertalen naar een bin met bereik van \\(x\\) tot \\(2x\\).\r\nAls voorbeeld waarbij basis 10 zinvoller is, overweeg dan de populatiegrootte. Een logbasis 10 is zinvoller omdat het bereik hiervoor ongeveer 1000 tot 10 miljard is. Hier is het histogram van de getransformeerde waarden:\r\n\r\n\r\n\r\nHier zien we al snel dat de bevolking van een land varieert tussen de tienduizend en tien miljard.\r\nTransform the values or the scale?\r\nEr zijn twee manieren waarop we log-transformaties in grafieken kunnen gebruiken. We kunnen de waarden loggen voordat we ze plotten of gebruik maken van logschalen in de assen. Beide benaderingen zijn nuttig en hebben verschillende sterke punten. Als we de gegevens loggen, kunnen we gemakkelijker tussenliggende waarden interpreteren in de schaal. Bijvoorbeeld als we zien\r\n\r\n\r\n—-1—-x—-2——–3—-\r\n\r\n\r\nvoor log getransformeerde gegevens weten we dat de waarde van \\(x\\) is ongeveer 1,5. Als de weegschalen gelogd zijn\r\n\r\n\r\n—-1—-x—-10——100—\r\n\r\n\r\nom x te bepalen, moeten we \\(10^{1.5}\\) berekenen. Dat is niet gemakkelijk te doen in onze hoofden. Het voordeel van het tonen van gelogde schalen is echter dat de originele waarden worden weergegeven in de plot, die gemakkelijker te interpreteren zijn. Bijvoorbeeld, we zouden “32 dollar per dag” zien in plaats van “5 log basis 2 dollar per dag”.\r\nZoals we eerder leerden, als we de as willen schalen met logs kunnen we de functie ‘schaal_x_ccontinue’ gebruiken. Dus in plaats van eerst de waarden te loggen, passen we deze laag toe:\r\n\r\n\r\n\r\nMerk op dat de log base 10 transformatie zijn eigen functie heeft: scale_x_log10(), maar momenteel base 2 niet. Hoewel we dit zelf gemakkelijk konden defini?ren.\r\nMerk op dat er andere transformatie beschikbaar zijn via het trans argument. Zoals we later leren, is bijvoorbeeld de vierkantsworteltransformatie (sqrt) nuttig bij het tellen. De logistieke transformatie (logit) is nuttig bij het plotten van proporties tussen 0 en 1. De omgekeerde transformatie is nuttig als we willen dat kleinere waarden rechts of bovenop staan.\r\nModus\r\nIn de statistieken wordt deze hobbel ook wel modus genoemd. De modus van een verdeling is de waarde met de hoogste frequentie. De modus van de normale verdeling is het gemiddelde. Wanneer een distributie, zoals de bovenstaande, niet eentonig afneemt van de modus, noemen we de locaties waar het op en neer gaat weer lokale modi en zeggen dat de distributie meerdere modi heeft.\r\nHet histogram hierboven suggereert dat de inkomensverdeling van het land in 1970 twee modi kent: ??n met ongeveer 2 dollar per dag (1 in de log 2 schaal) en ??n met ongeveer 32 dollar per dag (5 in de log 2 schaal). Deze bimodaliteit is consistent met een dichotome wereld die bestaat uit landen met een gemiddeld inkomen van minder dan $8 (3 in de log 2 schaal) per dag en landen daarboven.\r\nStratificeren en boxplot\r\nHet histogram liet zien dat de inkomensverdelingswaarden een tweedeling vertonen. Het histogram laat echter niet zien of de twee groepen landen west tegenover de ontwikkelings wereld zijn.\r\nOm de verdeling naar geografische regio te zien, stratificeren we eerst de gegevens naar regio’s en onderzoeken we vervolgens de verdeling voor elke regio.\r\n\r\n\r\n[1] 22\r\n\r\nVanwege het aantal regio’s zijn histogrammen of gladde dichtheden voor elk niet nuttig. In plaats daarvan kunnen we boxplots naast elkaar stapelen:\r\n\r\n\r\n\r\nMerk op dat we de regionamen niet kunnen lezen omdat het standaard ggplot-gedrag is om de labels horizontaal te schrijven en hier lopen we dan de ruimte uit. We kunnen dit eenvoudig repareren door de etiketten te draaien. In de sheetuitleg vinden we dat we de namen kunnen roteren door het thema te veranderen via element_text. Het just=1 zorgt ervoor dat deze zich naast de as bevindt.\r\n\r\n\r\n\r\nWe zien nu dat er inderdaad een tweedeling is tussen het westen en de rest.\r\nOrden niet alfabetisch\r\nEr zijn nog een paar aanpassingen die we kunnen maken om in de grafiek deze realiteit beter bloot te leggen. Ten eerste helpt het om de regio’s in de boxplots te ordenen van arm naar rijk in plaats van alfabetisch. Dit kan worden gedaan met behulp van de reorder functie. Deze functie laat ons de orde van de niveaus van een factorvariabele op basis van een samenvatting veranderen die op een numerieke vector wordt berekend. Een karaktervector wordt in een factor gedwongen:\r\nHieronder staat een voorbeeld. Merk op hoe de volgorde van de niveaus verandert:\r\n\r\n\r\n[1] \"Asia\" \"West\"\r\n\r\n[1] \"West\" \"Asia\"\r\n\r\nTen tweede kunnen we kleur gebruiken om de verschillende continenten te onderscheiden, een visuele kleurschakering die helpt om specifieke regio’s te vinden. Hier is de code:\r\n\r\n\r\n\r\nDit figuur toont twee duidelijke groepen, met de rijke groep bestaande uit Noord-Amerika, Noord- en West-Europa, Nieuw-Zeeland en Australi?. Net als met het histogram, als we de plot herschikken met behulp van een logschaal zijn we in staat om verschillen binnen de deconcentratiewereld beter te zien.\r\n\r\n\r\n\r\nDe data tonen\r\nIn veel gevallen tonen we de gegevens niet omdat het rommel aan het figuur toevoegt en het bericht vertroebelt. In bovenstaand voorbeeld hebben we niet zoveel punten. Dan kunnen deze laag toevoegen met behulp van geom_point() en door punten toe te voegen kunnen we eigenlijk alle gegevens zien\r\n\r\n\r\n\r\nVerdelingen vergelijken\r\nDe bovenstaande verkennende gegevensanalyse heeft twee kenmerken van de gemiddelde inkomensverdeling in 1970 aan het licht gebracht. Aan de hand van een histogram vonden we een bimodale verdeling met de modi voor arme en rijke landen. Door in het onderzoek stratificatie per regio toe te passen zagen we met boxplots dat de rijke landen meestal in Europa, Noord-Amerika, Australi? en Nieuw-Zeelandm lagen. Met deze regio’s defini?ren we een vector:\r\n\r\n\r\n\r\nNu willen we ons richten op het vergelijken van de verschillen in verdelingen in de tijd.\r\nWe bevestigen eerst de bimodaliteit die we in 1970 waarnamen en dat deze wordt verklaard door de westelijk tegenover de ontwikkelingswereld- dichotomie. Dit doen we door histogrammen te maken voor de groepen die we hebben ge?dentificeerd. Merk op dat we de twee groepen maken met ifelse binnen een mutaat en dat we facet_grid gebruiken om een histogram te maken voor elke groep:\r\n\r\n\r\n\r\nNu kunnen we kijken of de scheiding vandaag de dag slechter is dan veertig jaar geleden. Dit doen we door zowel per regio als per jaar te facetteren:\r\n\r\n\r\n\r\nVoordat we de bevindingen van deze plot interpreteren, stellen we vast dat er meer landen vertegenwoordigd zijn in de histogrammen van 2010 dan in 1970: het aantal tellingen is groter. Een van de redenen hiervoor is dat verschillende landen na 1970 zijn opgericht. De Sovjet-Unie is in de jaren negentig is veranderd in verschillende landen, waaronder Rusland en Oekra?ne. Een andere reden is dat in 2010 voor meer landen gegevens beschikbaar zijn.\r\nWe hebben de figuren opnieuw gemaakt met behulp van alleen landen met gegevens die beschikbaar zijn voor beide jaren. In het hoofdstuk over datawisselingen leren we tidyverse tools waarmee we hiervoor effici?nte codes kunnen schrijven, maar hier een eenvoudige code met behulp van de kruispunt functie:\r\n\r\n\r\n\r\nThese 108 account for 86 % of the world population, so this subset should be representative.\r\nLaten we de plot opnieuw maken, maar alleen voor deze subset door simpelweg land % in% country_list aan de filterfunctie toe te voegen:\r\n\r\n\r\n\r\nWe zien nu dat terwijl de rijke landen procentueel wat rijker zijn geworden, de arme landen meer lijken te zijn verbeterd. We zien vooral dat het aandeel van ontwikkelingslanden waar mensen meer dan $16 per dag verdienen aanzienlijk toeneemt.\r\nOm te zien welke specifieke regio’s het meest verbeterden, kunnen we de boxplots die we hierboven hebben gemaakt met nu 2010 toegevoegd, opnieuw maken\r\n\r\n\r\n\r\nen dan met behulp van facet om de twee jaar te vergelijken:\r\n\r\n\r\n\r\nHier pauzeren we om nog een krachtige ggplot2-functie te introduceren. Omdat we elke regio voor en na willen vergelijken, zou het handig zijn om de 1970 boxplot naast de 2010 boxplot voor elke regio te hebben. Over het algemeen zijn vergelijkingen gemakkelijker wanneer gegevens naast elkaar worden uitgezet.\r\nDus in plaats van facetteren houden we de gegevens van elk jaar bij elkaar, maar vragen we ggplot om ze afhankelijk van het jaar te kleuren (of te vullen). Ggplot scheidt ze automatisch en plaatst de twee boxplots naast elkaar. Omdat jaar een getal is, maken we er een factor van omdat ggplot automatisch een kleur toewijst aan elke categorie van een factor:\r\n\r\n\r\n\r\nTot slot wijzen we erop dat als wat we het meest ge?nteresseerd zijn in het vergelijken van voor en na waarden, kan het zinvoller zijn om de verhoudingen, of verschil in de log schaal plot te zetten. We zijn nog steeds niet klaar om te leren om dit te leren coderen, maar hier hoe het figuur eruit zou zien:\r\n\r\n\r\n\r\nSubtiele verdelingplots\r\nMet behulp van dataverkenning hebben we ontdekt dat de inkomenskloof tussen rijke en arme landen de afgelopen veertig jaar aanzienlijk is gedicht. We gebruikten een reeks histogrammen en boxplots om dit te laten zien. Hier suggereren we een beknopte manier om deze boodschap over te brengen met slechts een plot. We zullen hiervoor subtiele verdelingplots gebruiken.\r\nLaten we beginnen met op te merken dat verdelingplots voor inkomensverdeling in 1970 en 2010 de boodschap afgeven dat de kloof aan het dichten is:\r\n\r\n\r\n\r\nIn de 1970 plot zien we twee heldere modi, arme en rijke landen. In 2010 lijkt het erop dat sommige arme landen naar rechts zijn verschoven, dat aangeeft dat de kloof is gedicht.\r\nDe volgende boodschap die we moeten uitdragen is dat de reden voor deze verandering in de verdeling is dat arme landen rijker werden in plaats van dat sommige rijke landen armer werden. Om dit te doen hoeven we alleen een kleur toe te wijzen aan de groepen die we hebben ge?dentificeerd tijdens de dataverkenning.\r\nVoordat we dit echter kunnen doen, moeten we leren hoe we deze vlotte dichtheden zo kunnen maken dat de informatie over het aantal landen in elke groep behouden blijft. Om te begrijpen waarom we dit nodig hebben, let dan op de discrepantie in de grootte van elke groep:\r\n\r\ngroup\r\nn\r\nDeveloping\r\n87\r\nWest\r\n21\r\n\r\nmaar als twee verdelingen overlappen, is de standaardinstelling dat het gebied dat door elke distributie wordt weergegeven tot 1 wordt opgeteld, ongeacht de grootte van elke groep:\r\n\r\n\r\n\r\nwaardoor het lijkt alsof er in elke groep evenveel landen zitten. Om dit te veranderen, zullen we moeten leren om berekende variabelen met de geom_density functie te benaderen.\r\nToegang tot computervariabelen\r\nOm de oppervlakten van deze verdelingen evenredig te laten zijn met de grootte van de groepen, kunnen we de y-aswaarden eenvoudig vermenigvuldigen met de grootte van de groep. Vanuit de geom_density help-file zien we dat de functie, die een variabele genaamd count berekenen, precies dit doet. We willen dat deze variabele op de y-as komt te staan.\r\nIn ggplot krijgen we toegang tot deze variabelen door ze te omringen met de naam ... Daarom zullen we de volgende mapping gebruiken:\r\n\r\n\r\n\r\nNu kunnen we de gewenste plot maken door simpelweg de mapping in het vorige codebrok te veranderen:\r\n\r\n\r\n\r\nAls we willen dat de verdelingen wordt subtieler worden, gebruiken we het ‘bw’-argument. We probeerden er een paar en kozen voor 0,75:\r\n\r\n\r\n\r\nDee plot laat nu heel duidelijk zien wat er aan de hand is. De verdeling in de ontwikkelingslanden is aan het veranderen. Een derde modus lijkt te bestaan en bestaat uit de landen die de kloof het meest hebben gedicht.\r\ncase_when\r\nWe kunnen dit cijfer zelfs iets informatiever maken. Uit de verkennende gegevensanalyse merkten we dat veel van de landen die het meest verbeterden, afkomstig waren uit Azi?. We kunnen de plot gemakkelijk wijzigen door belangrijke regio’s afzonderlijk weer te geven.\r\nWe introduceren de case_when functie die handig is voor het defini?ren van groepen. Het heeft momenteel geen data-argument (dit kan veranderen), dus we moeten de onderdelen van onze gegevenstabel benaderen met behulp van de stip plaatsbewerker:\r\n\r\n\r\n\r\nWe maken van deze ‘groep’-variabele een factor om de volgorde van de niveaus te bepalen:\r\n\r\n\r\n\r\nWe kiezen deze specifieke volgorde vanwege een reden die later duidelijk wordt.\r\nNu kunnen we eenvoudig de verdeling voor elk in kaart brengen. We gebruiken kleur' engrootte’ om de toppen duidelijk te laten zien:\r\n\r\n\r\n\r\nDe plot is rommelig en wat moeilijk te lezen. Soms krijg je een duidelijker beeld door de dichtheden op elkaar te stapelen:\r\n\r\n\r\n\r\nHier zien we duidelijk dat de verdelingen voor Oost-Azi?, Latijns-Amerika e.a. duidelijk naar rechts verschuiven. Terwijl Afrika bezuiden de Sahara blijft stagneren.\r\nMerk op dat we de niveaus van de groep zo ordenen dat de West verdeling eerst wordt uitgezet, dan Sub-Sahara Afrika. Als we eerst de twee uitersten in kaart brengen, zien we de resterende bimodaliteit beter.\r\nGewogen verdelingen\r\nTot slot merken we op dat deze uitkeringen in alle landen hetzelfde wegen. Dus als het grootste deel van de bevolking zich verbetert, maar in een heel groot land woont, zoals China, zullen we dit misschien niet op prijs stellen. We kunnen de vloeiende verdelingen eigenlijk wegen met behulp van het gewicht in kaart brengen argument. Het plot ziet er dan als volgt uit:\r\n\r\n\r\n\r\nDeze specifieke figuur laat heel duidelijk zien hoe de inkomenskloof wordt gedicht, waarbij de meeste armen in Afrika bezuiden de Sahara blijven.\r\nEcologische denkfout\r\nIn deze sectie hebben we regio’s van de wereld met elkaar vergeleken. We hebben gezien dat sommige regio’s het gemiddeld beter doen dan andere. Hier richten we ons op het beschrijven van het belang van verschillen binnen de groepen.\r\nHier richten we ons op de relatie tussen de overlevingskans van kinderen in een land en het gemiddelde inkomen. We beginnen met het vergelijken van deze hoeveelheden tussen regio’s. We defini?ren nog een paar regio’s:\r\n\r\n\r\n\r\nVervolgens berekenen we deze hoeveelheden per regio.\r\n\r\n\r\n# A tibble: 7 x 3\r\n  group              income infant_survival_rate\r\n  <chr>               <dbl>                <dbl>\r\n1 Sub-Saharan Africa   1.76                0.936\r\n2 Southern Asia        2.07                0.952\r\n3 Pacific Islands      2.70                0.956\r\n4 Northern Africa      4.94                0.970\r\n5 Latin America       13.2                 0.983\r\n6 East Asia           13.4                 0.985\r\n7 The West            77.1                 0.995\r\n\r\nDit laat een dramatisch verschil zien. Terwijl in het westen minder dan 0,5 procent van de kinderen sterft, is dat in Afrika bezuiden de Sahara meer dan 6 procent! De relatie tussen deze twee variabelen is bijna perfect lineair\r\n\r\n\r\n\r\nIn deze plot introduceren we het gebruik van het limit argument dat laat het bereik van de assen veranderen. We maken het bereik groter dan de gegevensbehoeften omdat we dit plot later zullen vergelijken met een plot met meer variabiliteit; we willen dat de bereiken hetzelfde zijn. We introduceren ook het `breaks’ argument, waarmee we de locatie van de aslabels kunnen instellen. Eindelijk introduceren we een nieuwe transformatie, de logistieke transformatie.\r\nLogistische transformatie\r\nDe logistische of logistieke transformatie voor een deel of een koers van \\(p\\) wordt gedefinieerd als\r\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\r\nWanneer \\(p\\) een proportie of waarschijnlijkheid is, wordt de hoeveelheid die wordt gelogd, \\(p/(1-p)\\) de odds genoemd. In het geval \\(p\\) is het aandeel van een kind dat overleefde. De kansen vertellen ons hoeveel meer kinderen worden uitgedreven om te overleven dan om te sterven. De logtransformatie maakt dit symmetrisch. Als de snelheden gelijk zijn, dan is de log odds 0. Bij toe- of afname verandert het in positieve en negatieve stappen.\r\nDeze schaal is handig als we verschillen in de buurt van 0 of 1 willen markeren. Voor overlevingskansen is dit van belang omdat een overlevingsgraad van 90% onaanvaardbaar is, terwijl een overlevingsgraad van 99% relatief goed is. We zouden veel liever een overlevingskans hebben die dichter bij 99,9 procent ligt. We willen dat onze schaal dit verschil benadrukt en de logit doet dit. Merk op dat 99.9/0.1 ongeveer 10 keer groter is dan 99/1, wat ongeveer 10 keer groter is dan 90/10. En door gebruik te maken van de log worden deze wissels steeds groter.\r\nToon de gegevens\r\nNu, terug naar onze plot. Kan nu geconcludeerd worden dat op basis van het bovenstaande een land met een laag inkomen een lage overlevingskans zal hebben? Kunnen we concluderen dat alle overlevingskansen in Afrika bezuiden de Sahara lager zijn dan in Zuid-Azi?, dat op zijn beurt lager is dan op de eilanden in de Stille Oceaan, enzovoort?\r\nDe conclusie gebaseerd op een plot dat weer wordt gebaseerd op gemiddelden wordt ecologische denkfout genoemd. De bijna perfecte relatie tussen overlevingskansen en inkomen zien we alleen bij de gemiddelden op regionaal niveau. Als we alle gegevens eenmaal hebben getoond, krijgen we een wat ingewikkelder verhaal:\r\n\r\n\r\n\r\nWe zien we een grote mate van variabiliteit. We zien dat landen uit dezelfde regio’s heel verschillend kunnen zijn en dat landen met hetzelfde inkomen verschillende overlevingskansen kunnen hebben. Terwijl Afrika bezuiden de Sahara bijvoorbeeld gemiddeld de slechtste gezondheids- en economische resultaten had, is er binnen die groep sprake van grote variabiliteit. Merk bijvoorbeeld op dat Mauritius en Botswana het beter doen dan Angola en Sierra Leone en Mauritius zelfs vergelijkbaar is met Westerse landen.\r\n\r\n\r\n",
    "preview": "posts/2018-11-14-visualisatiegapminder/visualisatiegapminder_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-sem/",
    "title": "SEM",
    "description": "In de sociale wetenschappen kunnen sommige constructen, zoals intelligentie, vertrouwen, motivatie, vervreemding of conservatisme, niet direct worden geobserveerd. Het zijn in essentie constructen of concepten waarvoor geen methode bestaat om ze direct te meten. Onderzoekers gebruiken hiervoor geobserveerde maten die indicatoren zijn voor een latente variabele. Structural equation modeling is een onderzoeks-raamwerk dat rekening kan houden met de meetfouten in de geobserveerde variabelen die in het model zitten. SEM is een flexibel en krachtige methode om tegelijkertijd op een goede manier de kwaliteit van het meten in de gaten te houden als om causale relaties tussen de constructen vast te stellen. In de map vind je een korte presentatie over SEM",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-07-07",
    "categories": [],
    "contents": "\r\nDe presentatie vind je hier\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-rrstudiormarkdown/",
    "title": "RRStudioRmarkdown",
    "description": "Hier een klein boekje om jou te laten wennen aan reproduceerbaar onderzoek. Het introduceert het programma R, de RStudio-schil en de programmeertaal RMarkdown.",
    "author": [
      {
        "name": "Chester Ismay, bewerkt Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-06-05",
    "categories": [],
    "contents": "\r\nDat boekje vind je hier. http://www.harriejonkman.nl/wp-content/uploads/2018/01/rbasics.pdf\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-bayes/",
    "title": "Bayes",
    "description": "Over de geschiedenis van de Bayesiaanse statistiek",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-05-14",
    "categories": [],
    "contents": "\r\n“Als de feiten veranderen, verander ik mijn mening. Wat doe jij dan, meneer?” (Keynes)\r\nThomas Bayes was een Engels wiskundige en presbyteriaans predikant die zichzelf in de tweede helft van de 18e eeuw enkele belangrijke vragen stelde, zoals hoe moeten we onze overtuigingen aanpassen als we nieuwe informatie krijgen en hoe houden we vast aan oude veronderstellingen, ook als ze onhoudbaar zijn geworden. Of laten we die overtuigingen heel makkelijk los, ook als we een klein beetje gaan twijfelen?\r\nBayes maakt ons ervan bewust dat we onze opvattingen steeds geleidelijk aan passen aan de werkelijkheid. Zijn stelling, of theorema, is een belangrijk instrument geworden voor allerlei wetenschappers en de Bayesiaanse regel staat niet zelden heel stoer op een t-shirt afgebeeld:\r\n\\[P(A|B) = \\frac {P(A) \\times P(B|A)}{P(B)}\\]\r\nBayes draaide dingen om en wilde iets zeggen over een hypothese gegeven het bewijsmateriaal dat we hebben. Hoe moet je een veronderstelling begrijpen in het licht van empirisch materiaal? Bayes’ theorie kent vervolgens een lange, ingewikkelde en verrassende geschiedenis die tot op de dag van vandaag doorgaat. Die geschiedenis heeft Sharon Bertsch McGrayne, een Amerikaanse wetenschapsjournalist, heel mooi in beeld gebracht. Maar het was niet Bayes zelf, maar zijn vriend Richard Price, een amateur wiskundige, die deze ideeën openbaar maakte na Bayes’ dood. Zonder Price had die theorie helemaal niet bestaan. Price stuurde Bayes wetenschappelijke werk in en zorgde er zo voor dat er een publicatie van kwam. Naar die stelling werd vervolgens helemaal niet meer omgekeken tot de beroemde Franse wiskundige Pierre Simon Laplace in het begin van de 19e eeuw deze ideeën uitbreidde en op slimme manieren toepaste. Daarna was het maatschappelijk aan en uit met die theorie, werd het gebruikt en raakte het ook weer uit de mode. De ideeën werden op het ene na het andere gebied toegepast, om vervolgens weer te worden veroordeeld omdat het zou werken met vage, subjectieve of onwetenschappelijke uitgangspunten. Rivaliserende kampen (Frequentisten en Baysianen) voerden hier lange tijd grote strijd over.\r\nDe theorie zelf kan eenvoudig worden uitgelegd. Je hebt een bepaalde hypothese, bijvoorbeeld over een munt. Je denkt dat de kans op kop of munt hetzelde is. Daar ga je van te voren van uit. Als je vijf keer gooit en je gooit steeds kop dan denk je nog dat het toeval is en je past jouw veronderstelling nog niet aan. Anders wordt het als dit 100 keer gebeurt, dan ga je toch echt twijfelen over de munt en ga je denken dat het misschien wel alleen maar kop kent. Bij waarschijnlijkheid gaat het om uitsluitende mogelijkheden die je toekent. Je hebt van tevoren een idee over iets (prior), vervolgens heb je de data (likelihood) en dat zorgt vervolgens voor jouw geupdated kennis (posterior). Bayes’ theorie is een manier om de waarschijnlijkheid steeds op een consistent en logische manier te herberekenen. Dat herberekenen van de hypothese (of kennis die we hebben) vindt dus steeds plaats in het licht van nieuwe bewijzen. Deze geupdated of aangepaste waarschijnlijkheid wordt de posterior probability of gewoon de posterior genoemd.\r\nOm het iets ingewikkelder te zeggen, de stelling van Bayes’s houdt nu in dat de posterior waarschijnlijkheid van een hypothese gelijk is aan het product van de voorafgaande waarschijnlijkheid van de hypothese (dus wat je weet al, de prior) en de waarschijnlijkheid van het bewijs gegeven de hypothese (de data, de likelihood). Dit deel je vervolgens door de waarschijnlijkheid van alle bewijzen. Dat laatste gebeurt omdat je zo steeds een waarde tussen 0 en 1 te krijgen, en hoe dichter bij 1 hoe groter de kans. Dat is precies wat op dat stoere t-shirt staat.\r\nHeel veel kennis is er opgebouwd met munten, kaarten en dergelijke. Moeilijker wordt het wanneer je de stelling van Bayes toepast op het echte leven. Maar ook hier gebeurt het op een zelfde manier. Stel dat je naar buitenloopt en ziet dat jouw tuinpad nat is. Dan denk je misschien dat het geregend heeft. Als je verder loopt en ziet dat de straten droog zijn, ga je toch denken dat jouw vrouw misschien de tuin heeft gesproeid. Wanneer je het weerbericht hebt gehoord (waarin regen wordt voorspeld) voordat je naar buitenloopt, denk je nog eerder dat het geregend heeft als je over het natte tuinpad loopt. Met dat weersbericht en dat natte tuinpad word je minder snel van jouw regengedachte afgehaald als je de droge straat ziet. In het dagelijks leven worden gedachten en veronderstellingen voortdurend geüpdated. Kennis wordt gebruikt en nieuw bewijsmateriaal wordt voortdurend daarmee gefilterd en verwerkt in jouw Bayesiaanse hoofd. Steeds meer doen we kennis op over hoe kansen zijn toe te wijzen en bewijs kan worden geëvalueerd in situaties die veel ingewikkelder zijn dan het gooien van munten of het inschatten van regen.\r\nMcGrayne besteedt veel aandacht aan allerlei bijdragen van individuele wetenschappers aan die boeiende geschiedenis van de Bayesiaanse theorie. Zo bespreekt ze uitgebreid hoe het in oorlogsvoering is gebruikt, in het proces van kolonel Dreyfus, hoe Alan Turing met deze theorie Duitse codes kraakte en er met dat kleine groepje slimme mensen voor zorgde dat de Tweede Wereldoorlog minder lang duurde. Maar veel meer voorbeelden komen aan de orde.\r\nBayesiaanse theorie wordt tegenwoordig op allerlei gebieden van wetenschap toegepast. Ook in onze dagelijks leven hebben we ermee te maken via vertaalmachines, spamfilters en stuurloze auto’s bijvoorbeeld. We zijn er onszelf nauwelijks van bewust. McGrayne heeft niet alleen oog voor statistici die zich succesvol wijdden aan Bayesiaanse statistiek. Ze laat ook zien hoe deze door andere statici (zoals Fischer, als vertegenwoordiger van de zogenaamde Frequentisten) heftig worden tegengewerkt. De kern van de verschillen tussen deze twee groepen is dat volgens Bayesianen de prior een subjectieve uitdrukking kan zijn van de mate van geloof in een hypothese. Je moet de kennis gebruiken die je al hebt, het is vreemd om steeds maar opnieuw en van voren af aan te beginnen. Frequentisten erkennen dit subjectieve element niet. Voor hen moet wetenschap een objectieve basis hebben, liefste in relatieve frequentie van gebeurtenissen in herhaalbare, welomschreven experimenten.Dat subjectieve element staat daar ver van.\r\nMcGrayne’s boek is een prachtig wetenschapshistorisch boek Zie ook haar presentatie hier. Ze laat zien dat de Bayesiaanse theorie weer helemaal terug is en dat komt natuurlijk vooral ook door de ontwikkeling van de computer en moderne algoritmen waarmee de theorie niet enkel theorie blijft maar ook praktisch kan worden toegepast. David Spiegelhalter en zijn biostatistische onderzoeksgroep hebben daar op een ongekende manier aan bijgedragen. De Bayesiaanse methode wordt in allerlei onderzoek gebruikt en in verschillende omstandigheden heeft het grote voordelen zien ten opzichte van de frequentistische statistiek. Er wordt vandaag de dag veel pragmatischer mee hiermee omgegaan. De tegenwoordige wetenschapper loopt steeds vaker met twee gereedschapskisten rond en op basis van het probleem besluit hij of zij tot een onderzoekswijze.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-reproducable-research/",
    "title": "Dynamische documenten maken met RMarkdown en Knitr",
    "description": "RMarkdown en Knitr zijn pakketten die je in staat stellen om reproduceerbare en dynamische documenten te maken. In deze blog wordt uitgelegd hoe je hiermee kunt werken.",
    "author": [
      {
        "name": "Marian L. Schmidt, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-03-15",
    "categories": [],
    "contents": "\r\n\r\nWelkom!\r\nDeze workshop is eerder gegeven door Schmidt op 11 Mei 2016 en er waren wat aanvullende materialen beschikbaar:\r\nKlas notities die voor de workshop konden worden gebruikt etherpad.\r\nDe workshop is ook live opgenomen en beschikbaar op YouTube. Kijk naar dit college op YouTube! Zeer leerzaam.\r\nDeze tutorial is gemaakt als onderdeel van een programma van Dr. C Titus Brown’s Data Intensive Biology (DIB) trainingsprogramma op de Universiteit van Californie, Davis. Dit DIB trainingsprogramma omvat lokale + remote workshops die onderwerpen en gereedschappen behandelen in de bio-informatica en data analyse.\r\nHet college werd gegeven vanuit Ann Arbor Michigan en drie klassen van elders deden tegelijkertijd mee:\r\nUniversiteit van Californi?, Davis\r\nSimon Fraser Universiteit\r\nOntario Instituut for Kanker Onderzoek\r\nDe Github repository, die ik binnen heb gehaald en hier gebruik voor deze les, kan worden gevonden op hier.\r\nOpgelet: Dit soort werk introduceert wel een groot aantal Engelse woorden in onze Nederlandse taal. Enkele daarvan heb ik laten staan, niet omdat ik ze mooi vind maar omdat ze al overal gebruikt worden. Gewone mensen praten al over bijvoorbeeld renderen, cachen, files, widgets en output. Met excuses hiervoor, bij voorbaat.\r\nInstructies bij het installeren\r\nVoordat je gaat werken met de workshopmaterialen, zorg ervoor dat je het volgende hebt gedaan:\r\nOpen RStudio.\r\nInstalleer en download het devtools R pakket door het volgende commando te runnen.\r\n\r\n\r\n\r\nCheck of je de goede versie hebt van R en RStudio door devtools::session_info() in de R console te draaien.\r\nHier geeft devtools:: aan om de session_info() functie in R te gebruiken ipv het devtools pakket en de sessionInfo() functie binnen het utils pakket. Het runnen van devtools::session_info() stelt ons in staat de versie van R en RStudio vast te stellen.\r\nHeb je de volgende versie van R en RStudio?\r\nR: Versie 3.3.0 (2016-05-03)\r\nRStudio: 0.99.1172\r\nZo ja dan kun je van start gaan!\r\nZo nee dan heb je nieuwe versies van R en RStudio nodig, volg dan Setup in dit document.\r\n\r\nInstalleer andere R pakketten die nodig zijn voor deze workshop.\r\n\r\n\r\n\r\n\r\n\r\n\r\nAls je de pakketten zonder fouten hebt geladen, ben je klaar voor deze workshop!\r\nZijn er nog problemen, meld het ajb!\r\nGoede bronnen\r\nDeze tutorial kon niet samengesteld worden zonder onderstaande goede bronnen:\r\nDe RMarkdown website van RStudio.\r\nDr. Yuhui Xie’s boek: Dynamic Documents with R and Knitr 2nd Edition [@Xie2015] en zijn Knitr website.\r\nHEEL VEEL DANK aan Dr. Xie voor het schrijven van het knitr pakket!!\r\n\r\nDr. Karl Broman’s “Knitr in a Knutshell”.\r\nCheatsheets released by RStudio.\r\nDynamische documenten\r\nLiterate programming, zoals dat in het Engels wordt genoemd, is het basisidee achter dynamische documenten en is ge?ntroduceerd door Donald Knuth in 1984. Oorspronkelijk om de broncode en de bijbehorende software documentatie samen te brengen. Tegenwoordig cre?ren we dynamische documenten waarin het programma of de analyse code samen draaien om tot ‘outputs’ te komen (bv. tabellen, plots, modellen, etc) die worden uitgelegd via narratief schrijven.\r\nDe drie stappen van Literate Programming:\r\nParse het bron document en haal de code en het verhaal uit elkaar.\r\nExecute bron code en laat de resultaten zien.\r\nMix resultaten van de broncode met het orginele verhaal.\r\nEr blijven 2 stappen voor ons voor wat betreft het schrijven:\r\nAnalyse code\r\nEen verhaal om de resultaten van de analyse code toe te lichten.\r\nTraditioneel gebruikten mensen commentaren om het verhaal in de code file kwijt te kunnen raken (voor R zou dat een .R file zijn). Deze file zou het volgende in kunnen houden:\r\n\r\n\r\n# Titel:  Relatie tussen Autogewicht en Gasefficientie/of-verbruik\r\n# Door :  Marian Schmidt  \r\n# Datum:  11 Mei 2016\r\n\r\n# Ik verspel dat er een relatie is tussen het gewicht van de auto en de afstand die met de brandstof afgelegd kan worden.  \r\n# Dat test ik met een lineaire analyse van de ''mtcarsdataset' als onderdeel van de R datasets\r\n\r\n# Hoe zien de data eruit?\r\n#datatable(mtcars) # Interactieve tabel \r\n\r\n# Is er een relatie tussen het gewicht en de afstand per, in dit geval, gallon?\r\nlm_mpg <- lm(mpg ~ wt, data = mtcars) # Run het lineaire model dat mpg voorspelt op basis van wt\r\ncoef_lm_mpg <- coef(summary(lm_mpg))  # Haal de coefficienten eruit voor de tabel die komt \r\nkable(coef_lm_mpg)                    # Maak een niet-interactieve tabel - een functie in knitr\r\n\r\n# Plot de relatie tussen gewicht en afstand in mijl per gallon   \r\nplot <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # \r\n  geom_smooth(method = \"lm\") + theme_bw() +           # Maak een lineair model en maak het zwart en wit\r\n  xlab(\"Weight (1000lbs)\") + ylab(\"Miles per Gallon\") # Voeg tekst aan de assen toe\r\nggplotly(plot)                                        # Maak de plot interactief  \r\n\r\n# Het lijkt erop dat met een toename van 1000 pounds er een afname is van brandstof gebruik met 5.34 mijl per gallon\r\n# Het eind\r\n\r\nDe gebruiker zal de commentaren lezen en de codes zelf runnen.\r\nEchter, ‘literate programming’ stelt ons in staat de code te runnen en de resultaten te beschrijven, allemaal in een document dat we kunnen delen. We zouden bijvoorbeeld het volgende kunnen doen:\r\nRelatie tussen gewicht van de auto en het brandstofverbruikDoor: Marian SchmidtDatum: 11 Mei 2016\r\nIk voorspel dat er een relatie is tussen het gewicht en de afstand die met de brandstof kan worden afgelegd.\r\nIk test dat met een lineair model op een dataset in R.\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nEstimate\r\nStd. Error\r\nt value\r\nPr(>|t|)\r\n(Intercept)\r\n37.285126\r\n1.877627\r\n19.857575\r\n0\r\nwt\r\n-5.344472\r\n0.559101\r\n-9.559044\r\n0\r\n\r\n{\"x\":{\"data\":[{\"x\":[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],\"y\":[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],\"text\":[\"wt: 2.620<br />mpg: 21.0\",\"wt: 2.875<br />mpg: 21.0\",\"wt: 2.320<br />mpg: 22.8\",\"wt: 3.215<br />mpg: 21.4\",\"wt: 3.440<br />mpg: 18.7\",\"wt: 3.460<br />mpg: 18.1\",\"wt: 3.570<br />mpg: 14.3\",\"wt: 3.190<br />mpg: 24.4\",\"wt: 3.150<br />mpg: 22.8\",\"wt: 3.440<br />mpg: 19.2\",\"wt: 3.440<br />mpg: 17.8\",\"wt: 4.070<br />mpg: 16.4\",\"wt: 3.730<br />mpg: 17.3\",\"wt: 3.780<br />mpg: 15.2\",\"wt: 5.250<br />mpg: 10.4\",\"wt: 5.424<br />mpg: 10.4\",\"wt: 5.345<br />mpg: 14.7\",\"wt: 2.200<br />mpg: 32.4\",\"wt: 1.615<br />mpg: 30.4\",\"wt: 1.835<br />mpg: 33.9\",\"wt: 2.465<br />mpg: 21.5\",\"wt: 3.520<br />mpg: 15.5\",\"wt: 3.435<br />mpg: 15.2\",\"wt: 3.840<br />mpg: 13.3\",\"wt: 3.845<br />mpg: 19.2\",\"wt: 1.935<br />mpg: 27.3\",\"wt: 2.140<br />mpg: 26.0\",\"wt: 1.513<br />mpg: 30.4\",\"wt: 3.170<br />mpg: 15.8\",\"wt: 2.770<br />mpg: 19.7\",\"wt: 3.570<br />mpg: 15.0\",\"wt: 2.780<br />mpg: 21.4\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1.513,1.56250632911392,1.61201265822785,1.66151898734177,1.7110253164557,1.76053164556962,1.81003797468354,1.85954430379747,1.90905063291139,1.95855696202532,2.00806329113924,2.05756962025316,2.10707594936709,2.15658227848101,2.20608860759494,2.25559493670886,2.30510126582278,2.35460759493671,2.40411392405063,2.45362025316456,2.50312658227848,2.55263291139241,2.60213924050633,2.65164556962025,2.70115189873418,2.7506582278481,2.80016455696203,2.84967088607595,2.89917721518987,2.9486835443038,2.99818987341772,3.04769620253165,3.09720253164557,3.14670886075949,3.19621518987342,3.24572151898734,3.29522784810127,3.34473417721519,3.39424050632911,3.44374683544304,3.49325316455696,3.54275949367089,3.59226582278481,3.64177215189873,3.69127848101266,3.74078481012658,3.79029113924051,3.83979746835443,3.88930379746835,3.93881012658228,3.9883164556962,4.03782278481013,4.08732911392405,4.13683544303798,4.1863417721519,4.23584810126582,4.28535443037975,4.33486075949367,4.3843670886076,4.43387341772152,4.48337974683544,4.53288607594937,4.58239240506329,4.63189873417722,4.68140506329114,4.73091139240506,4.78041772151899,4.82992405063291,4.87943037974684,4.92893670886076,4.97844303797468,5.02794936708861,5.07745569620253,5.12696202531646,5.17646835443038,5.2259746835443,5.27548101265823,5.32498734177215,5.37449367088608,5.424],\"y\":[29.1989406778126,28.9343555091934,28.6697703405742,28.4051851719549,28.1406000033357,27.8760148347165,27.6114296660973,27.3468444974781,27.0822593288588,26.8176741602396,26.5530889916204,26.2885038230012,26.023918654382,25.7593334857627,25.4947483171435,25.2301631485243,24.9655779799051,24.7009928112859,24.4364076426666,24.1718224740474,23.9072373054282,23.642652136809,23.3780669681898,23.1134817995705,22.8488966309513,22.5843114623321,22.3197262937129,22.0551411250937,21.7905559564744,21.5259707878552,21.261385619236,20.9968004506168,20.7322152819976,20.4676301133783,20.2030449447591,19.9384597761399,19.6738746075207,19.4092894389015,19.1447042702822,18.880119101663,18.6155339330438,18.3509487644246,18.0863635958054,17.8217784271861,17.5571932585669,17.2926080899477,17.0280229213285,16.7634377527093,16.49885258409,16.2342674154708,15.9696822468516,15.7050970782324,15.4405119096132,15.1759267409939,14.9113415723747,14.6467564037555,14.3821712351363,14.1175860665171,13.8530008978978,13.5884157292786,13.3238305606594,13.0592453920402,12.794660223421,12.5300750548017,12.2654898861825,12.0009047175633,11.7363195489441,11.4717343803249,11.2071492117056,10.9425640430864,10.6779788744672,10.413393705848,10.1488085372288,9.88422336860955,9.61963819999033,9.35505303137111,9.09046786275189,8.82588269413267,8.56129752551345,8.29671235689423],\"text\":[\"wt: 1.513000<br />mpg: 29.198941\",\"wt: 1.562506<br />mpg: 28.934356\",\"wt: 1.612013<br />mpg: 28.669770\",\"wt: 1.661519<br />mpg: 28.405185\",\"wt: 1.711025<br />mpg: 28.140600\",\"wt: 1.760532<br />mpg: 27.876015\",\"wt: 1.810038<br />mpg: 27.611430\",\"wt: 1.859544<br />mpg: 27.346844\",\"wt: 1.909051<br />mpg: 27.082259\",\"wt: 1.958557<br />mpg: 26.817674\",\"wt: 2.008063<br />mpg: 26.553089\",\"wt: 2.057570<br />mpg: 26.288504\",\"wt: 2.107076<br />mpg: 26.023919\",\"wt: 2.156582<br />mpg: 25.759333\",\"wt: 2.206089<br />mpg: 25.494748\",\"wt: 2.255595<br />mpg: 25.230163\",\"wt: 2.305101<br />mpg: 24.965578\",\"wt: 2.354608<br />mpg: 24.700993\",\"wt: 2.404114<br />mpg: 24.436408\",\"wt: 2.453620<br />mpg: 24.171822\",\"wt: 2.503127<br />mpg: 23.907237\",\"wt: 2.552633<br />mpg: 23.642652\",\"wt: 2.602139<br />mpg: 23.378067\",\"wt: 2.651646<br />mpg: 23.113482\",\"wt: 2.701152<br />mpg: 22.848897\",\"wt: 2.750658<br />mpg: 22.584311\",\"wt: 2.800165<br />mpg: 22.319726\",\"wt: 2.849671<br />mpg: 22.055141\",\"wt: 2.899177<br />mpg: 21.790556\",\"wt: 2.948684<br />mpg: 21.525971\",\"wt: 2.998190<br />mpg: 21.261386\",\"wt: 3.047696<br />mpg: 20.996800\",\"wt: 3.097203<br />mpg: 20.732215\",\"wt: 3.146709<br />mpg: 20.467630\",\"wt: 3.196215<br />mpg: 20.203045\",\"wt: 3.245722<br />mpg: 19.938460\",\"wt: 3.295228<br />mpg: 19.673875\",\"wt: 3.344734<br />mpg: 19.409289\",\"wt: 3.394241<br />mpg: 19.144704\",\"wt: 3.443747<br />mpg: 18.880119\",\"wt: 3.493253<br />mpg: 18.615534\",\"wt: 3.542759<br />mpg: 18.350949\",\"wt: 3.592266<br />mpg: 18.086364\",\"wt: 3.641772<br />mpg: 17.821778\",\"wt: 3.691278<br />mpg: 17.557193\",\"wt: 3.740785<br />mpg: 17.292608\",\"wt: 3.790291<br />mpg: 17.028023\",\"wt: 3.839797<br />mpg: 16.763438\",\"wt: 3.889304<br />mpg: 16.498853\",\"wt: 3.938810<br />mpg: 16.234267\",\"wt: 3.988316<br />mpg: 15.969682\",\"wt: 4.037823<br />mpg: 15.705097\",\"wt: 4.087329<br />mpg: 15.440512\",\"wt: 4.136835<br />mpg: 15.175927\",\"wt: 4.186342<br />mpg: 14.911342\",\"wt: 4.235848<br />mpg: 14.646756\",\"wt: 4.285354<br />mpg: 14.382171\",\"wt: 4.334861<br />mpg: 14.117586\",\"wt: 4.384367<br />mpg: 13.853001\",\"wt: 4.433873<br />mpg: 13.588416\",\"wt: 4.483380<br />mpg: 13.323831\",\"wt: 4.532886<br />mpg: 13.059245\",\"wt: 4.582392<br />mpg: 12.794660\",\"wt: 4.631899<br />mpg: 12.530075\",\"wt: 4.681405<br />mpg: 12.265490\",\"wt: 4.730911<br />mpg: 12.000905\",\"wt: 4.780418<br />mpg: 11.736320\",\"wt: 4.829924<br />mpg: 11.471734\",\"wt: 4.879430<br />mpg: 11.207149\",\"wt: 4.928937<br />mpg: 10.942564\",\"wt: 4.978443<br />mpg: 10.677979\",\"wt: 5.027949<br />mpg: 10.413394\",\"wt: 5.077456<br />mpg: 10.148809\",\"wt: 5.126962<br />mpg:  9.884223\",\"wt: 5.176468<br />mpg:  9.619638\",\"wt: 5.225975<br />mpg:  9.355053\",\"wt: 5.275481<br />mpg:  9.090468\",\"wt: 5.324987<br />mpg:  8.825883\",\"wt: 5.374494<br />mpg:  8.561298\",\"wt: 5.424000<br />mpg:  8.296712\"],\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":3.77952755905512,\"color\":\"rgba(51,102,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1.513,1.56250632911392,1.61201265822785,1.66151898734177,1.7110253164557,1.76053164556962,1.81003797468354,1.85954430379747,1.90905063291139,1.95855696202532,2.00806329113924,2.05756962025316,2.10707594936709,2.15658227848101,2.20608860759494,2.25559493670886,2.30510126582278,2.35460759493671,2.40411392405063,2.45362025316456,2.50312658227848,2.55263291139241,2.60213924050633,2.65164556962025,2.70115189873418,2.7506582278481,2.80016455696203,2.84967088607595,2.89917721518987,2.9486835443038,2.99818987341772,3.04769620253165,3.09720253164557,3.14670886075949,3.19621518987342,3.24572151898734,3.29522784810127,3.34473417721519,3.39424050632911,3.44374683544304,3.49325316455696,3.54275949367089,3.59226582278481,3.64177215189873,3.69127848101266,3.74078481012658,3.79029113924051,3.83979746835443,3.88930379746835,3.93881012658228,3.9883164556962,4.03782278481013,4.08732911392405,4.13683544303798,4.1863417721519,4.23584810126582,4.28535443037975,4.33486075949367,4.3843670886076,4.43387341772152,4.48337974683544,4.53288607594937,4.58239240506329,4.63189873417722,4.68140506329114,4.73091139240506,4.78041772151899,4.82992405063291,4.87943037974684,4.92893670886076,4.97844303797468,5.02794936708861,5.07745569620253,5.12696202531646,5.17646835443038,5.2259746835443,5.27548101265823,5.32498734177215,5.37449367088608,5.424,5.424,5.424,5.37449367088608,5.32498734177215,5.27548101265823,5.2259746835443,5.17646835443038,5.12696202531646,5.07745569620253,5.02794936708861,4.97844303797468,4.92893670886076,4.87943037974684,4.82992405063291,4.78041772151899,4.73091139240506,4.68140506329114,4.63189873417722,4.58239240506329,4.53288607594937,4.48337974683544,4.43387341772152,4.3843670886076,4.33486075949367,4.28535443037975,4.23584810126582,4.1863417721519,4.13683544303798,4.08732911392405,4.03782278481013,3.9883164556962,3.93881012658228,3.88930379746835,3.83979746835443,3.79029113924051,3.74078481012658,3.69127848101266,3.64177215189873,3.59226582278481,3.54275949367089,3.49325316455696,3.44374683544304,3.39424050632911,3.34473417721519,3.29522784810127,3.24572151898734,3.19621518987342,3.14670886075949,3.09720253164557,3.04769620253165,2.99818987341772,2.9486835443038,2.89917721518987,2.84967088607595,2.80016455696203,2.7506582278481,2.70115189873418,2.65164556962025,2.60213924050633,2.55263291139241,2.50312658227848,2.45362025316456,2.40411392405063,2.35460759493671,2.30510126582278,2.25559493670886,2.20608860759494,2.15658227848101,2.10707594936709,2.05756962025316,2.00806329113924,1.95855696202532,1.90905063291139,1.85954430379747,1.81003797468354,1.76053164556962,1.7110253164557,1.66151898734177,1.61201265822785,1.56250632911392,1.513,1.513],\"y\":[26.9637596243046,26.748211630978,26.532293643118,26.3159798039527,26.0992420315869,25.8820498075489,25.6643699464367,25.4461663458981,25.2273997165231,25.0080272917409,24.7880025185342,24.5672747307859,24.3457888084175,24.1234848272503,23.9002977068049,23.6761568661453,23.4509859014399,23.2247023032092,22.9972172362447,22.7684354108148,22.5382550797684,22.3065682020549,22.073260818275,21.8382136871272,21.6013032316477,21.3624028393291,21.1213845488153,20.8781211364057,20.6324885872813,20.3843688997083,20.1336531278785,19.8802445251398,19.624061610802,19.3650409583153,19.1031394977534,18.8383361460623,18.5706326249216,18.3000533934652,18.0266447019082,17.7504728499618,17.4716217986802,17.190190327058,16.9062889412588,16.6200367360143,16.3315583798572,16.0409813560006,15.748433546582,15.4540412060478,15.157927333949,14.8602104304317,14.5610035995316,14.2604139550846,13.9585422800956,13.6554828910183,13.3513236620105,13.0461461695019,12.740025923372,12.4330326570007,12.1252306540028,11.8166790943701,11.5074324069286,11.1975406184823,10.8870496928064,10.5760018548558,10.2644358972571,9.95238746743981,9.6398893347169,9.32697163731397,9.01366210982825,8.69998629192387,8.38596771927446,8.07162809788066,7.75698746294225,7.44206432347029,7.1268757937988,6.81143771310854,6.49576475401476,6.17987052120298,5.86376764102542,5.5474678428992,5.5474678428992,11.0459568708893,11.2588274100015,11.4718948670624,11.685170971489,11.8986683496337,12.1124006061819,12.3263824137488,12.5406296115153,12.7551593138153,12.96999002966,13.185141794249,13.400636313583,13.6164971233358,13.8327497631713,14.0494219676868,14.266543875108,14.4841482547477,14.7022707540356,14.9209501655981,15.1402287143902,15.3601523641871,15.5807711417929,15.8021394760335,16.0243165469005,16.2473666380091,16.4713594827389,16.6963705909696,16.9224815391307,17.1497802013801,17.3783608941716,17.6083244005099,17.8397778342311,18.0728342993708,18.307612296075,18.5442348238948,18.7828281372767,19.023520118358,19.266438250352,19.5117072017912,19.7594460674074,20.0097653533643,20.2627638386563,20.5185254843377,20.7771165901197,21.0385834062175,21.3029503917649,21.5702192684414,21.8403689531932,22.1133563760938,22.3891181105935,22.6675726760022,22.9486233256676,23.2321611137817,23.5180680386105,23.8062200853351,24.096490030255,24.3887499120139,24.6828731181045,24.9787360715631,25.276219531088,25.5752095372801,25.8755980490886,26.1772833193625,26.4801700583702,26.7841694309033,27.0891989274822,27.3951821442752,27.7020485003464,28.0097329152165,28.3181754647066,28.6273210287383,28.9371189411946,29.2475226490581,29.5584893857579,29.8699798618842,30.1819579750846,30.4943905399572,30.8072470380303,31.1204993874088,31.4341217313206,26.9637596243046],\"text\":[\"wt: 1.513000<br />mpg: 29.198941\",\"wt: 1.562506<br />mpg: 28.934356\",\"wt: 1.612013<br />mpg: 28.669770\",\"wt: 1.661519<br />mpg: 28.405185\",\"wt: 1.711025<br />mpg: 28.140600\",\"wt: 1.760532<br />mpg: 27.876015\",\"wt: 1.810038<br />mpg: 27.611430\",\"wt: 1.859544<br />mpg: 27.346844\",\"wt: 1.909051<br />mpg: 27.082259\",\"wt: 1.958557<br />mpg: 26.817674\",\"wt: 2.008063<br />mpg: 26.553089\",\"wt: 2.057570<br />mpg: 26.288504\",\"wt: 2.107076<br />mpg: 26.023919\",\"wt: 2.156582<br />mpg: 25.759333\",\"wt: 2.206089<br />mpg: 25.494748\",\"wt: 2.255595<br />mpg: 25.230163\",\"wt: 2.305101<br />mpg: 24.965578\",\"wt: 2.354608<br />mpg: 24.700993\",\"wt: 2.404114<br />mpg: 24.436408\",\"wt: 2.453620<br />mpg: 24.171822\",\"wt: 2.503127<br />mpg: 23.907237\",\"wt: 2.552633<br />mpg: 23.642652\",\"wt: 2.602139<br />mpg: 23.378067\",\"wt: 2.651646<br />mpg: 23.113482\",\"wt: 2.701152<br />mpg: 22.848897\",\"wt: 2.750658<br />mpg: 22.584311\",\"wt: 2.800165<br />mpg: 22.319726\",\"wt: 2.849671<br />mpg: 22.055141\",\"wt: 2.899177<br />mpg: 21.790556\",\"wt: 2.948684<br />mpg: 21.525971\",\"wt: 2.998190<br />mpg: 21.261386\",\"wt: 3.047696<br />mpg: 20.996800\",\"wt: 3.097203<br />mpg: 20.732215\",\"wt: 3.146709<br />mpg: 20.467630\",\"wt: 3.196215<br />mpg: 20.203045\",\"wt: 3.245722<br />mpg: 19.938460\",\"wt: 3.295228<br />mpg: 19.673875\",\"wt: 3.344734<br />mpg: 19.409289\",\"wt: 3.394241<br />mpg: 19.144704\",\"wt: 3.443747<br />mpg: 18.880119\",\"wt: 3.493253<br />mpg: 18.615534\",\"wt: 3.542759<br />mpg: 18.350949\",\"wt: 3.592266<br />mpg: 18.086364\",\"wt: 3.641772<br />mpg: 17.821778\",\"wt: 3.691278<br />mpg: 17.557193\",\"wt: 3.740785<br />mpg: 17.292608\",\"wt: 3.790291<br />mpg: 17.028023\",\"wt: 3.839797<br />mpg: 16.763438\",\"wt: 3.889304<br />mpg: 16.498853\",\"wt: 3.938810<br />mpg: 16.234267\",\"wt: 3.988316<br />mpg: 15.969682\",\"wt: 4.037823<br />mpg: 15.705097\",\"wt: 4.087329<br />mpg: 15.440512\",\"wt: 4.136835<br />mpg: 15.175927\",\"wt: 4.186342<br />mpg: 14.911342\",\"wt: 4.235848<br />mpg: 14.646756\",\"wt: 4.285354<br />mpg: 14.382171\",\"wt: 4.334861<br />mpg: 14.117586\",\"wt: 4.384367<br />mpg: 13.853001\",\"wt: 4.433873<br />mpg: 13.588416\",\"wt: 4.483380<br />mpg: 13.323831\",\"wt: 4.532886<br />mpg: 13.059245\",\"wt: 4.582392<br />mpg: 12.794660\",\"wt: 4.631899<br />mpg: 12.530075\",\"wt: 4.681405<br />mpg: 12.265490\",\"wt: 4.730911<br />mpg: 12.000905\",\"wt: 4.780418<br />mpg: 11.736320\",\"wt: 4.829924<br />mpg: 11.471734\",\"wt: 4.879430<br />mpg: 11.207149\",\"wt: 4.928937<br />mpg: 10.942564\",\"wt: 4.978443<br />mpg: 10.677979\",\"wt: 5.027949<br />mpg: 10.413394\",\"wt: 5.077456<br />mpg: 10.148809\",\"wt: 5.126962<br />mpg:  9.884223\",\"wt: 5.176468<br />mpg:  9.619638\",\"wt: 5.225975<br />mpg:  9.355053\",\"wt: 5.275481<br />mpg:  9.090468\",\"wt: 5.324987<br />mpg:  8.825883\",\"wt: 5.374494<br />mpg:  8.561298\",\"wt: 5.424000<br />mpg:  8.296712\",\"wt: 5.424000<br />mpg:  8.296712\",\"wt: 5.424000<br />mpg:  8.296712\",\"wt: 5.374494<br />mpg:  8.561298\",\"wt: 5.324987<br />mpg:  8.825883\",\"wt: 5.275481<br />mpg:  9.090468\",\"wt: 5.225975<br />mpg:  9.355053\",\"wt: 5.176468<br />mpg:  9.619638\",\"wt: 5.126962<br />mpg:  9.884223\",\"wt: 5.077456<br />mpg: 10.148809\",\"wt: 5.027949<br />mpg: 10.413394\",\"wt: 4.978443<br />mpg: 10.677979\",\"wt: 4.928937<br />mpg: 10.942564\",\"wt: 4.879430<br />mpg: 11.207149\",\"wt: 4.829924<br />mpg: 11.471734\",\"wt: 4.780418<br />mpg: 11.736320\",\"wt: 4.730911<br />mpg: 12.000905\",\"wt: 4.681405<br />mpg: 12.265490\",\"wt: 4.631899<br />mpg: 12.530075\",\"wt: 4.582392<br />mpg: 12.794660\",\"wt: 4.532886<br />mpg: 13.059245\",\"wt: 4.483380<br />mpg: 13.323831\",\"wt: 4.433873<br />mpg: 13.588416\",\"wt: 4.384367<br />mpg: 13.853001\",\"wt: 4.334861<br />mpg: 14.117586\",\"wt: 4.285354<br />mpg: 14.382171\",\"wt: 4.235848<br />mpg: 14.646756\",\"wt: 4.186342<br />mpg: 14.911342\",\"wt: 4.136835<br />mpg: 15.175927\",\"wt: 4.087329<br />mpg: 15.440512\",\"wt: 4.037823<br />mpg: 15.705097\",\"wt: 3.988316<br />mpg: 15.969682\",\"wt: 3.938810<br />mpg: 16.234267\",\"wt: 3.889304<br />mpg: 16.498853\",\"wt: 3.839797<br />mpg: 16.763438\",\"wt: 3.790291<br />mpg: 17.028023\",\"wt: 3.740785<br />mpg: 17.292608\",\"wt: 3.691278<br />mpg: 17.557193\",\"wt: 3.641772<br />mpg: 17.821778\",\"wt: 3.592266<br />mpg: 18.086364\",\"wt: 3.542759<br />mpg: 18.350949\",\"wt: 3.493253<br />mpg: 18.615534\",\"wt: 3.443747<br />mpg: 18.880119\",\"wt: 3.394241<br />mpg: 19.144704\",\"wt: 3.344734<br />mpg: 19.409289\",\"wt: 3.295228<br />mpg: 19.673875\",\"wt: 3.245722<br />mpg: 19.938460\",\"wt: 3.196215<br />mpg: 20.203045\",\"wt: 3.146709<br />mpg: 20.467630\",\"wt: 3.097203<br />mpg: 20.732215\",\"wt: 3.047696<br />mpg: 20.996800\",\"wt: 2.998190<br />mpg: 21.261386\",\"wt: 2.948684<br />mpg: 21.525971\",\"wt: 2.899177<br />mpg: 21.790556\",\"wt: 2.849671<br />mpg: 22.055141\",\"wt: 2.800165<br />mpg: 22.319726\",\"wt: 2.750658<br />mpg: 22.584311\",\"wt: 2.701152<br />mpg: 22.848897\",\"wt: 2.651646<br />mpg: 23.113482\",\"wt: 2.602139<br />mpg: 23.378067\",\"wt: 2.552633<br />mpg: 23.642652\",\"wt: 2.503127<br />mpg: 23.907237\",\"wt: 2.453620<br />mpg: 24.171822\",\"wt: 2.404114<br />mpg: 24.436408\",\"wt: 2.354608<br />mpg: 24.700993\",\"wt: 2.305101<br />mpg: 24.965578\",\"wt: 2.255595<br />mpg: 25.230163\",\"wt: 2.206089<br />mpg: 25.494748\",\"wt: 2.156582<br />mpg: 25.759333\",\"wt: 2.107076<br />mpg: 26.023919\",\"wt: 2.057570<br />mpg: 26.288504\",\"wt: 2.008063<br />mpg: 26.553089\",\"wt: 1.958557<br />mpg: 26.817674\",\"wt: 1.909051<br />mpg: 27.082259\",\"wt: 1.859544<br />mpg: 27.346844\",\"wt: 1.810038<br />mpg: 27.611430\",\"wt: 1.760532<br />mpg: 27.876015\",\"wt: 1.711025<br />mpg: 28.140600\",\"wt: 1.661519<br />mpg: 28.405185\",\"wt: 1.612013<br />mpg: 28.669770\",\"wt: 1.562506<br />mpg: 28.934356\",\"wt: 1.513000<br />mpg: 29.198941\",\"wt: 1.513000<br />mpg: 29.198941\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":3.77952755905512,\"color\":\"transparent\",\"dash\":\"solid\"},\"fill\":\"toself\",\"fillcolor\":\"rgba(153,153,153,0.4)\",\"hoveron\":\"points\",\"hoverinfo\":\"x+y\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":23.3059360730594,\"r\":7.30593607305936,\"b\":37.2602739726027,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1.31745,5.61955],\"tickmode\":\"array\",\"ticktext\":[\"2\",\"3\",\"4\",\"5\"],\"tickvals\":[2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"2\",\"3\",\"4\",\"5\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Weight (1000lbs)\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[4.12984123504415,35.317626607855],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Miles per Gallon\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"2bac7832611b\":{\"x\":{},\"y\":{},\"type\":\"scatter\"},\"2bacfc06143\":{\"x\":{},\"y\":{}}},\"cur_data\":\"2bac7832611b\",\"visdat\":{\"2bac7832611b\":[\"function (y) \",\"x\"],\"2bacfc06143\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\r\nHet lijkt erop dat met elke 1000 pond er een afname is in brandstof gebruik met 5.3444716 mijl per gallon\r\nHet einde\r\nAls we zo programmeren kunnen we ook:\r\nTangle: De broncode uit het document halen.\r\nWeave : Deze code pakken en de resultaten rechtstreeks laten zien.\r\nDat is wat we gaan doen.\r\nReproduceerbaar onderzoek\r\nReproduceerbaar onderzoek is een mogelijk product van dynamische documenten, echter, goed resultaat is niet gegarandeerd!\r\nGoede uitvoering van reproduceerbaar onderzoek houdt in ieder geval in:\r\nHet hele project in een directory plaatsen die wordt ondersteund door de ‘version control’.\r\nCode en data vrijlaten.\r\nAlles documenteren en de code als documentatie gebruiken!\r\nFiguren, tabellen en de statistiek zijn het resultaat van scripts en codes die in de tekst staan.\r\nSchrijf in de codes de paden die worden gebruikt.\r\nStel ‘seed’ in zodat een volgende persoon dezelfde resultaten krijgt.\r\nLaat ook informatie zien waarmee de code file wordt uitgevoerd. Je kunt bijvoorbeeld de devtools::session_info() gebruiken.\r\nOm meer over reproduceerbaarheid en datamanagement te lezen, kun je Vince Buffalo’s Boek erop naslaan[@Buffalo2015].\r\nMarkdown\r\nOm RMarkdown helemaal te begrijpen moeten we het eerst hebben over Markdown, wat een systeem is om een simpele, leesbare tekst te maken die eenvoudig kan worden omgezet naar HTML. Markdown is essentieel voor twee dingen:\r\nEen kale tekst die de syntax vormt\r\nEen software gereedschap dat in Perl is geschreven.\r\nZet de kale tekst om in HTML.\r\n\r\n\r\nBelangrijkste doel van Markdown:\r\nMaakt de syntax van het orginele (pre-HTML) document zo leesbaar als mogelijk.\r\n\r\nZou je deze code liever in HTML lezen?\r\n\r\n<body>\r\n  <section>\r\n    <h1>Paklijst voor bergklimmen<\/h1>\r\n    <ul>\r\n      <li>Bergschoenen<\/li>\r\n      <li>Klimgordel<\/li>\r\n      <li>Rugzak<\/li>\r\n      <li>Touw<\/li>\r\n      <li>Zelfzekering<\/li>\r\n    <\/ul>\r\n  <\/section>\r\n<\/body>\r\nOf deze code in Markdown?\r\n\r\n# Paklijst voor bergklimmen\r\n\r\n* Bergschoenen\r\n* Klimgordel\r\n* Rugzak  \r\n* Touw\r\n* Zelfzekering\r\nEen beetje een normaal mens vindt de Markdown code zeker makkelijker om te lezen!\r\nWe zullen meer over de syntax van Markdown praten nadat we RMarkdown hebben ge?ntroduceerd maar laten we ons allereerst beseffen hoeveel makkelijker ons leven is/zal zijn omdat Markdown bestaat! Dank je John Gruber en Aaron Swartz (RIP) voor het ontwikkelen van Markdown in 2004!\r\nRMarkdown\r\nRMarkdown is een variant van Markdown dat het makkelijker maakt om met RStudio dynamische documenten, presentaties en rapporten te maken. Het omvat ‘R code chunks’ (ik laat hier even het Engels staan) om met knitr te gebruiken waarmee makkelijker reproduceerbare (web-based) rapporten gemaakt kunnen worden die automatisch aangepast worden wanneer de onderliggende code is veranderd.\r\nRMarkdown laat jou Markdown combineren met plaatjes, linken, tabellen, LaTeX en de code zelf.\r\nRStudio zorgt ervoor dat het maken van documenten met RMarkdown makkelijk wordt\r\nRStudio is (net als R) vrij te gebruiken en draait op elk systeem.\r\nRMarkdown geeft verschillende typen files waaronder:\r\nHTML\r\nPDF\r\nMarkdown\r\nMicrosoft Word\r\nPresentaties:\r\nOpvallende HTML5 presentaties:\r\nioslides\r\nSlidy\r\nSlidify\r\n\r\nPDF presentaties:\r\nBeamer\r\n\r\nHandouts:\r\nTufte Handouts\r\n\r\n\r\nHTML R Package Vignettes\r\nEven Entire Websites!\r\n\r\nTerwijl er heel veel verschillende documenten kunnen worden geleverd met RMarkdown, leggen we vandaag de nadruk in de eerste plaats op HTML output files omdat die voor mijn onderzoek misschien het meest bruikbaar en flexibel zijn.\r\nWaarom R Markdown?\r\nEen aantrekkelijk gereedschap voor reproduceerbare en dynamische rapporten!\r\nTerwijl het was gemaakt voor R, accepteert het veel programmeertalen. Om het eenvoudig te houden, werken we vandaag alleen met R.\r\nEen code kan op een aantal manieren worden uitgevoerd:\r\nInline Code: Een korte code die in de geschreven tekst van het document wordt uitgevoerd.\r\nCode Chunks: Delen van het document omvatten verschillende zinnen voor programmeer of analyse code. Dat kan een plot of een tabel zijn, maar ook berekeningen van de samenvattende statistiek, pakketten laden, etc.\r\n\r\nHet is makkelijk om:\r\nPlaatjes op te nemen.\r\nDe Markdown syntax te leren.\r\nLaTeX vergelijkingen op te nemen.\r\nInteractieve tabellen op te nemen.\r\nGebruik de versie via Git.\r\nDan is het makkelijker om te delen en samen te werken in analyses, projecten en publicaties!\r\n\r\nExterne linken toe te voegen - Rmarkdown begrijpt zelfs enige html codes!\r\nOm mooie documenten te maken.\r\n\r\nJe hoeft je geen zorgen te maken over pagina breuken of het plaatsen van de figuren.\r\nConsolideer jouw code en plaats het in een file:\r\nPowerpoint, PDFs, html documenten en word files\r\n\r\nEenvoudige werkwijze\r\nIn het kort, om een rapport te maken:\r\nOpen een .Rmd file.\r\nMaak een YAML kop (meer hierover zo dadelijk!)\r\n\r\nSchrijf de inhoud met RMarkdown syntax.\r\nNeem mee de R code in code chunks of met een inline code.\r\nDraai de document output.\r\nWerkwijze om een rapport te makenOverzicht van de stappen die RMarkdown maakt om een ‘gerenderd’ document te krijgen:\r\nMaak een .Rmd rapport met ‘R code chunks’ en markdown verhalen (zoals hierboven in stappen beschreven).\r\nGeef de .Rmd-file aan knitr om de ‘R code chunks’ uit te voeren en een nieuwe .md file te maken.\r\nKnitr is een pakket binnen R die jou in staat stelt de code binnen RMarkdown documenten uit te voeren zoals HTML, latex, pdf, word en andere document types.\r\n\r\nGeef de .md file aan pandoc, die er een definitief document van maakt (b.v. html, Microsoft word, pdf, etc.).\r\nPandoc is een universeel gereedschap om documenten te converteren en zet het ene document type (in dit geval: .Rmd) om in een ander (in dit geval: HTML)\r\n\r\nHoe een Rmd document wordt omgezetHoewel dit mogelijk wat ingewikkeld lijkt, kunnen we op de “Knit” knop drukken boven aan de pagina die er zo uitziet:\r\nof we kunnen de volgende code runnen:\r\n\r\n\r\nMaak een .Rmd file\r\nHet wordt tijd! Laten we met RMarkdown gaan werken!\r\nIn de menu bar, klik je op File -> New File -> RMarkdown\r\nOf je klikt eenvoudig op het groene plus teken links boven in de hoek van RStudio.\r\n\r\n\r\nHet volgende zal omhoog komen.\r\nHierbinnen kies je het type output dat je wilt hebben. Opgelet: deze output kan later heel makkelijk worden aangepast!\r\n\r\nKlik OK\r\nYAML koppen\r\nYAML staat voor “YAML Ain’t Markup Language” en is eigenlijk een soort geklusterde structuur voor de metadata van het document. Het staat tussen twee regels van drie streepjes --- en wordt automatisch omgezet door RStudio. Een eenvoudig voorbeeld:\r\n\r\n---\r\ntitle:  \"Analyse Rapport\"  \r\nAuthor:  \"Harrie Jonkman\"  \r\ndate: \"1 Maart 2017\"  \r\noutput:  html_document\r\n---\r\nHet voorbeeld boven zal een HTML document maken. Echter, de volgende opties zijn ook beschikbaar.\r\nhtml_document\r\npdf_document\r\nword_document\r\nbeamer_presentation (pdf powerpoint)\r\nioslides_presentation (HTML powerpoint)\r\nen nog meer …\r\nVandaag leggen we de nadruk op HTML files. Echter voel je vrij als je hier wat mee wilt spelen door bv. word en pdf documenten te maken. Presentatie documenten kennen een wat andere syntax (bv. om aan te geven wanneer de ene dia eindigt en de andere begint) en dan is er nog wat markdown syntax specifiek voor presentaties maar die gaat voorbij het doel van deze workshop.\r\nIn deze workshops bouwen we verder voort op de details van YAML koppen.\r\nMarkdown Basis\r\nKijk hiernaar RMarkdown Reference Guide\r\nHaal hier ook informatie vandaan RMarkdown Cheatsheet:\r\nMarkdown Basis van RStudio’s RMarkdown CheatsheetHandige tips:\r\nEindig elke regel met twee spaties om een nieuwe paragraaf te beginnen.\r\nWoorden binnen een code moeten aan beide kanten zo’n kommateken kennen: `\r\nOm iets tot superscript te maken moet je een ^ aan beide zijden plaatsen. Superscript werd gevormd door Super^script^ te typen.\r\nVergelijkingen kunnen in een inline code worden geplaatst met $ en als blok gecentreerd binnen het document door $$. Bijvoorbeeld \\(E = mc^2\\) staat tussen de regels terwijl het volgende geblokt wordt opgenomen: \\[E = mc^2\\]\r\nOpgelet: Om met $ en $$ een superscript ^ te maken, is het nodig om voor elk aLFAnumeriEK dat superscript te gebruiken.\r\nAnder wiskundig materiaal:\r\nVierkantswortel: $\\sqrt{b}$ zal \\(\\sqrt{b}\\) maken\r\nBreuken: $\\frac{1}{2}$ = \\(\\frac{1}{2}\\)\r\nVergelijkingen met breuken: $f(x)=\\frac{P(x)}{Q(x)}$ = \\(f(x)=\\frac{P(x)}{Q(x)}\\)\r\n\r\n\r\nBinomiale Coefficienten: $\\binom{k}{n}$ = \\(\\binom{k}{n}\\)\r\nIntegralen: $$\\int_{a}^{b} x^2 dx$$ = \\[\\int_{a}^{b} x^2 dx\\]\r\n\r\nShareLaTeX is een prachtige bron voor LaTeX-codes.\r\n\r\nNog wat wiskundig materiaal:\r\nBeschrijving\r\nCode\r\nVoorbeelden\r\nGriekse letters\r\n$\\alpha$ $\\beta$ $\\gamma$ $\\rho$ $\\sigma$ $\\delta$ $\\epsilon$ $mu$\r\n\\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\rho\\) \\(\\sigma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\mu\\)\r\nBinaire handelingen\r\n$\\times$ $\\otimes$ $\\oplus$ $\\cup$ $\\cap$\r\n\\(\\times\\) \\(\\otimes\\) \\(\\oplus\\) \\(\\cup\\) \\(\\cap\\) \\(\\times\\)\r\nRelationele handelingen\r\n$< >$ $\\subset$ $\\supset$ $\\subseteq$ $\\supseteq$\r\n\\(< >\\) \\(\\subset\\) \\(\\supset\\) \\(\\subseteq\\) \\(\\supseteq\\)\r\nVerder\r\n$\\int$ $\\oint$ $\\sum$ $\\prod$\r\n\\(\\int\\) \\(\\oint\\) \\(\\sum\\) \\(\\prod\\)\r\n\r\nUitdaging: Probeer de volgende output te maken:\r\n\r\nVandaag voel ik mij vet omdat ik RMarkdown leer.\r\nhoning is heel zoet.\r\nYAS!!!!!!\r\nR2 waarden zijn informatief!\r\n\\(R^{2}\\) beschrijft de variantie verklaard door het model.\r\nIk kende geen RMarkdown Vandaag heb ik RMarkdown geleerd\r\nRStudio link\r\nOutput van het volgende:\r\n\r\n# RMarkdown   \r\n## R   \r\n### Knitr   \r\n#### Pandoc  \r\n##### HTML  \r\n\\(\\sqrt{b^2 - 4ac}\\)\r\n\\[\\sqrt{b^2 - 4ac}\\]\r\n\\(X_{i,j}\\)\r\n\r\nVandaag maak ik een dynamisch document!\r\n\r\nHet volgende lijstje:\r\nChocolade Chips Kook Recept\r\nboter\r\nsuiker\r\nEen mengsel van bruine en witte suiker maakt het lekkerder\r\nmix dat met boter voordat je de eieren eraan toevoegt\r\n\r\n\r\neieren\r\nvanille\r\nMix wat droge ingredienten:\r\nmeel, zout, bak soda\r\n\r\nchocolade chips\r\nFijn feitje! De inhoudsopgave van deze website is gemaakt met koppen met 1-3 pond symbolen! (Daarover dadelijk meer)\r\nEen Code in het document\r\nEr zijn twee manieren om een code in een RMarkdown document op te nemen.\r\nCode in het document: Korte code als een onderdeel van het geschreven document.\r\nCode Chunks: Delen van het document die verschillende programmeer of analyse codes omvatten. Daarmee kan een figuur of tabel worden gemaakt, statistieken worden berekend, pakketten worden geladen, etc.\r\nR Code in het document\r\nEen R code kan in het document wordt gemaakt door een komma hoog achterwaarts (`) en de letter r gevolgd door nog zo’n komma.\r\nBijvoorbeeld: 211 is 2048.\r\nStel dat je een p-waarde rapporteert en je wilt niet terug om de statistische test steeds weer uit te voeren. De p-waarde was eerder 0.0045.\r\nDit is echt handig als de resultaten op papier moeten worden gezet. Bijvoorbeeld, je hebt een aantal statistieken uitgevoerd voor jouw wetenschappelijke vragen is dit een manier waarop R die waarde in a variabele naam bewaart. Bijvoorbeeld: Wijkt het brandstofverbruik van de automaat significant af de auto met handtransmissie significant af binnen de mtcars data set?\r\n\r\n\r\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\r\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\r\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\r\n\r\nOm de p-waarde vast te stellen kunnen we transmission_ttest$p.value als R code in het document gebruiken.\r\nDe p-waarde is dan 0.0013736.\r\nR Code Chunks\r\nR code chunks (nogmaals ik gebruik maar de Engelse benaming hier, sorry)kunnen worden gebruikt om de R output in het document te krijgen of om de code als illustratie zichtbaar te maken.\r\nDe anatomie van een code chunk:\r\nOm een R code chunk te plaatsen, kun je met de hand typen door ```{r} gevolgd door ``` op een volgende regel. Je kunt ook de Insert a new code chunk knop gebruiken of de ‘shortcut key’. Dat geeft dan de volgende code chunk:\r\nEen code chunk invoeren\r\n```{r}\r\nn <- 10\r\nseq(n)\r\n```\r\nGeef de code chunk een betekenisvolle naam die samenhangt met wat het doet. Hieronder heb ik code chunk 10-random-numbers genoemd:\r\n\r\n```{r 10-random-numbers}\r\nn <- 10\r\nseq(n)\r\n```\r\nDe code chunk input en output zien er dan als volgt uit:\r\n\r\n\r\n [1]  1  2  3  4  5  6  7  8  9 10\r\n\r\nKnitr\r\nKnitr is een R-pakket dat werkt met\r\nIdentificeren van de code zowel van de chunks als in de tekst zelf\r\nEvalueren van de hele code en geeft de resultaten terug\r\nTeruggeven van de geformuleerde resultaten en combineert met de orginele file.\r\nKnitr draait de code zoals die in de R console zou draaien.\r\nKnitr werkt vooral met code chunks.\r\nEen code chunk ziet er als volgt uit:\r\n\r\n<div class=\"layout-chunk\" data-layout=\"l-body\">\r\n\r\n\r\n<\/div>\r\nGoede praktijken met betrekking tot code chunks:\r\nBenoem/label jouw code chunks!\r\nIn plaats van de chunk opties te specificeren in iedere chunk, kun je de algemene chunk opties aan het begin van het document vastzetten. Hierover meer in een minuut!\r\nChunk Labels\r\nChunk labels krijgen unieke IDs in een document en zijn goed voor:\r\nOm externe files te genereren zoals plaatjes en ‘cached’ documenten.\r\nChunk labels zijn vaak output als fouten omhoog komen(vaker voor codes in het document).\r\nNavigeren door lange .Rmd documenten.\r\nEen methode om door lange .Rmd files te navigerenAls je de code chunk een naam geef, gebruik dan - of _ tussen woorden voor code chunks labels in plaats van ruimtes. Dat helpt jou en andere gebruikers bij het navigeren in het document.\r\nChunk labels moeten uniek zijn in het document - anders zal er een fout optreden!\r\nChunk Opties\r\nDruk tab als tussen de haakjes code chunk opties omhoog komen.\r\nEnkele Knitr Chunk Optiesresults = \"asis\" staat voor “as is” en geeft de output van een niet geformateerde versie.\r\ncollapse is een andere chunk optie die handig kan zijn, zeker als een code chunk veel korte R uitdrukking heeft met wat output.\r\nEr zijn teveel chunk opties om hier te behandelen. Kijk na deze workshop nog eens wat rond voor deze opties.\r\nEen mooie website om dat op te doen is Knitr Chunk Options.\r\n\r\nUitdaging\r\nDraai de code chunk hieronder en speel wat met de volgende knitr code chunk opties:\r\n\r\n\r\neval = TRUE/FALSE\r\necho = TRUE/FALSE\r\ncollapse = TRUE/FALSE\r\nresults = \"asis\",\"markup en \"hide\r\n\r\n\r\nSla je resultaten op in markdown.Opgelet: Wees er zeker van dat je jouw chunks een naam geeft!\r\n\r\n\r\n\r\n1+1\r\n2*5\r\nseq(1, 21, by = 3)\r\nhead(mtcars)\r\n\r\nEnkele voorbeelden voortbouwend op de chunk hierboven\r\nResultaten van results=\"markup\", collapse = TRUE}:\r\n\r\n\r\n[1] 2\r\n[1] 10\r\n[1]  1  4  7 10 13 16 19\r\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\r\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\r\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\r\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\r\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\r\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\r\n\r\nResultaten van results=\"asis\", collapse = TRUE}:\r\n\r\n[1] 2 [1] 10 [1] 1 4 7 10 13 16 19 mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1\r\n\r\nGlobale opties\r\nHet kan zijn dat je dezelfde chunk settings wilt handhaven voor het gehele document. Het kan daarom handig zijn om de opties in een keer te typen in plaats van het iedere keer weer voor een chunk te moeten doen. Om dat te doen kun je de globale chunk opties bovenaan het document vaststellen.\r\n\r\nknitr::opts_chunk$set(echo = FALSE, \r\n                      eval = TRUE, \r\n                      message = FALSE,\r\n                      warning = FALSE, \r\n                      fig.path = \"Figures/\",\r\n                      fig.width = 12, \r\n                      fig.height = 8)\r\nAls je bijvoorbeeld met iemand samenwerkt die de code niet wil zien, kun je schrijven eval = TRUE en echo = FALSE gebruiken zodat de code wel gedraaid wordt maar niet getoond. In aanvulling wil je misschien message = FALSE en warning = FALSE gebruiken zodat jouw samenwerkingspartner geen enkele boodschap of waarschuwing van R ziet.\r\nAls je figuren wilt opslaan en bewaren in een subdirectory binnen het project, gebruik dan fig.path = \"Figures/\". Hier verwijst de \"Figures/\" naar een folder Figures binnen de huidige directory waar de figuur die gemaakt wordt in het document wordt opgeslagen.Opgelet: de figuren worden niet standaard opgeslagen.\r\nGlobale chunk opties zullen voor de rest van het documenten worden vastgezet. Als je wilt dat een bepaalde chunk afwijkt van de globale opties, maak dat aan het begin van die bepaalde chunk duidelijk.\r\nFiguren\r\nKnitr maakt vrij eenvoudig figuren. Als een analyse code binnen een chunk een bepaald figuur moet produceren, dan zal hij dat in het document afdrukken.\r\nEnkele knitr chunk opties gerelateerd aan figuren:\r\nfig.width en fig.height\r\nStandaard: fig.width = 7, fig.height = 7\r\n\r\nfig.align: Hoe het figuur uit te lijnen\r\nOpties omvatten: \"left\", \"right\" en \"center\"\r\n\r\nfig.path: Een file pad naar de directory waar knitr de grafische output moet opslaan die er met de chunk wordt gemaakt.\r\nStandaard: 'figure/'\r\n\r\nEr is zelfs een fig.retina(alleen voor HTML output) voor hogere figuur resoluties met retina afdrukken.\r\n\r\n\r\n\r\nEen enkelvoudig figuur maken:\r\nMet fig.align = \"center\"\r\n\r\n\r\n\r\nMet fig.align = \"right\"\r\n\r\n\r\n\r\nMet fig.align = \"left\"\r\n\r\n\r\n\r\nMet fig.width = 2, fig.height = 2\r\n\r\n\r\n\r\nMet fig.width = 10, fig.height = 10\r\n\r\n\r\n\r\n\r\n\r\n\r\nTabellen\r\nTabellen kunnen in Markdown voor nogal wat hoofdpijn kosten. We gaan er hier verder niet op in. Als je meer wilt leren over Markdown-tabellen kijk naar documentation on tables op de RMarkdown website.\r\nEr zijn enkele tabeltypen die handig kunnen zijn. Hier zullen we ons vorig voorbeeld gebruiken van de mtcars data\r\nIn zijn Knitr in a Knutshell introduceert Dr. Karl Broman: kable, panderen xtable en vooral die eerste twee deden mij plezier:\r\nkable: Binnen het knitr pakket - niet veel opties maar het ziet er goed uit.\r\npander: Binnen het pander pakket - heeft veel opties en handigheden. Makkelijk voor het vetmaken van waarden (bv. waarden onder een bepaalde waarde).\r\nkable en pander tabellen zijn mooi en handig bij het maken van niet-interactieve tabellen:\r\n\r\n\r\nkable(head(mtcars, n = 4)) # kable table with 4 rows\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\nMazda RX4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\n0\r\n1\r\n4\r\n4\r\nMazda RX4 Wag\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\n0\r\n1\r\n4\r\n4\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\n1\r\n1\r\n4\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n3\r\n1\r\n\r\n# Pander table\r\n# install.packages(\"pander\") # install pander first\r\nlibrary(pander)\r\npander(head(mtcars, n = 4))\r\nTable continues below\r\n \r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\nMazda RX4\r\n21\r\n6\r\n160\r\n110\r\n3.9\r\n2.62\r\n16.46\r\n0\r\n1\r\nMazda RX4 Wag\r\n21\r\n6\r\n160\r\n110\r\n3.9\r\n2.875\r\n17.02\r\n0\r\n1\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.32\r\n18.61\r\n1\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n \r\ngear\r\ncarb\r\nMazda RX4\r\n4\r\n4\r\nMazda RX4 Wag\r\n4\r\n4\r\nDatsun 710\r\n4\r\n1\r\nHornet 4 Drive\r\n3\r\n1\r\n\r\nHTML Widgets\r\nMet de uitgave van de nieuwe RMarkdown v2 is het makkelijker dan ooit tevoren om HTML Widgets te gebruiken. Volg de link om uit te zoeken in welke widgets jij ge?nteresseerd bent!\r\nOnlangs ontdekte ik bijvoorbeeld het DT pakket waarmee tabellen interactief kunnen worden gemaakt in de HTML output. Daarbij levert Plotly for R echt mooie interactieve grafieken op, welke gebaseerd zijn op Plotly.\r\nCool, of niet?\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}\r\nSpelling controleren\r\nIn de spelling kunnen natuurlijk altijd fouten zitten en daarom kan het nodig zijn dat we onze spelling in het document willen controleren. Er zijn twee manieren om de spelling te controleren:\r\nDruk op de “ABC check mark”  links van de vergrootglasknop in RStudio.\r\nGebruik de aspell() functie van het utils pakket. Je kunt dan echter beter de code chunks overslaan. De aspell() functie kan een filter functie overnemen om bepaalde regels in de files over te slaan en kan worden gebruikt met de knit_filter() die ontworpen is om de code chunks in een file over te slaan.\r\nKnitr Thema’s\r\nHet knitr-syntax-thema kan worden aangepast of helemaal naar de hand worden gezet. Als je de standaardthema’s niet wilt, gebruik dan het knit_theme om het te veranderen. Er zijn 80 thema’s opgenomen binnen knitr en we kunnen de namen ervan zien door knit_theme$get().\r\nWat zijn de eerste 30 knitr thema’s?\r\n\r\n\r\n [1] \"acid\"          \"aiseered\"      \"andes\"         \"anotherdark\"  \r\n [5] \"autumn\"        \"baycomb\"       \"bclear\"        \"biogoo\"       \r\n [9] \"bipolar\"       \"blacknblue\"    \"bluegreen\"     \"breeze\"       \r\n[13] \"bright\"        \"camo\"          \"candy\"         \"clarity\"      \r\n[17] \"dante\"         \"darkblue\"      \"darkbone\"      \"darkness\"     \r\n[21] \"darkslategray\" \"darkspectrum\"  \"default\"       \"denim\"        \r\n[25] \"dusk\"          \"earendel\"      \"easter\"        \"edit-anjuta\"  \r\n[29] \"edit-eclipse\"  \"edit-emacs\"   \r\n\r\nWij kunnen knit_theme$set() gebruiken om het thema vast te zetten. Om het thema op fruit vast te zetten, kunnen we de bijvoorbeeld de volgende code gebruiken:\r\n\r\n\r\n\r\nHier is de link naar jouw favoriete thema 80 knitr highlight themes.\r\nAndere programmeer talen\r\nTerwijl knitr binnen een R omgeving moet draaien, ondersteunt het ook andere programmeertalen waaronder:\r\nPython\r\nRuby\r\nHaskell\r\nawk/gawk\r\nsed\r\nshell scripts\r\nPerl\r\nSAS\r\nTikZ\r\nGraphviz\r\nC++\r\nEn andere talen…\r\nWe moeten echter het corresponderende software pakket installeren om een taal te gebruiken.\r\nGebruik de engine functie in Knitr. Deze functie laat de gebruiker de taal van een chunk specificeren.\r\nengine = \"bash\" zal de boel in bash laden and stelt de gebruiker in staat de scripts zo binnen de code chunk te schrijven .\r\nPr?sance\r\nEr zijn verschillende opties die we kunnen controleren binnen ons .Rmd document. Dit onderdeel helpt bij het introduceren en verkennen van sommige van deze opties waarmee je HTML documenten kunt aanpassen.\r\nCode Folding\r\nZoals het je misschien is opgevallen heeft elk van de code chunks in dit document een interactieve  knop. Deze wordt gecontroleerd in de YAML kop and is nieuw in RMarkdown v2.\r\nWanneer de knitr code chunk optie echo = TRUE is gespecificeerd (default = TRUE) zal de R code in het output document verschijnen. Echter, er zijn momenten waarin de gebruiker de code helemaal niet wil laten zien (echo = FALSE).\r\n-code_folding:\r\n- code_folding: hide: Kan de R code meenemen maar deze is standaard verborgen.\r\n- code_folding: show: Laat de R code zien. De lezers kunnen dan op de  knop drukken om de chunk te verstoppen als ze dat willen.\r\n\r\noutput: html_document\r\n    code_folding: show\r\nInhoudsopgave\r\nEen inhoudsopgave kan aan het gerenderd document worden toegevoegd door de toc optie in de YAML kop te gebruiken.\r\nOpties hierbij:\r\ntoc: of de inhoudsopgave moeten worden meegenomen:\r\ntoc: true: hier wordt de inhoudsopgave meegenomen\r\nDefault:toc: false: Hier wordt de inhoudsopgave niet meegenomen\r\n\r\ntoc_depth:: Hoeveel niveau’s moeten in de inhoudsopgave worden worden meegenomen?\r\nDefault: doc_depth: 3 zal koppen tot en met ### meenemen.\r\n\r\nnumber_sections: Voegt sectienummers toe aan de koppen. Bijvoorbeeld, dit document heeft number_sections: true\r\nDefault: number_sections: false\r\nOpgelet: Met elk # zal er een decimaal punt worden toegevoegd aan alle koppen.\r\n\r\ntoc_float:\r\n2 andere mogelijke parameters binnen toc_float:\r\ncollapsed: Controleert of de inhoudsopgave alleen aan het begin verschijnt. Het zal met de cursor erover verschijnen.\r\nDefault: collapsed: TRUE\r\n\r\nsmooth_scroll: Controleert of de pagina scrolls werken wanneer op de onderdelen van de inhoudsopgave wordt geklikt.\r\nDefault: smooth_scroll: true\r\n\r\n\r\n\r\nBijvoorbeeld:\r\n\r\noutput:\r\n  html_document:\r\n    toc: true\r\n    toc_depth: 2\r\n---\r\n\r\nUitdaging: Maak de YAML kop voor een HTML document die het volgende inhoudt:\r\n\r\n\r\nInhoudsopgave\r\nLaat de inhoudsopgave vloeien\r\nSectie koppen met twee hashtags (##)\r\nGenummerde secties\r\nGeen makkelijke scrolling\r\n\r\nThema’s\r\nRMarkdown heeft verschillende opties die de pr?sance van de HTML documenten controleren. Enkele mogelijkheden waaruit kan worden gekozen, hier met de Engelse termen:\r\ntheme\r\nhighlight\r\nsmart\r\nDe HTML output thema’s komen van Bootswatch library. Valide HTML themes omvatten de volgende:\r\ncerulean, cosmo,flatly, journal, readable,spacelab en united.\r\nBijvoorbeeld, het thema van de pagina is readable.\r\n\r\nZet het op nul voor geen thema (in dit geval kun je de css parameter gebruiken om jouw eigen stijl te gebruiken).\r\nHighlight specificeert de wijze waarop de syntax stijl oplicht. Stijlen die mogelijk zijn omvatten de volgende:\r\ndefault, espresso, haddock, kate, monochrome, pygments, tango, textmate en zenburn.\r\nOok hier, plaats nul om syntax oplichting te voorkomen.\r\nSmart indiceert of de typografisch correcte output wordt weergegeven, zet rechte aanhalingstekens om in gekru, — rechte aanhalingstekens, – om in gekrulde aanhalingstekens en … in ellipsen. Smart is standaard ingesteld.\r\nBijvoorbeeld:\r\n\r\n---\r\noutput:\r\n  html_document:\r\n    theme: slate\r\n    highlight: tango\r\n---\r\nAls je wilt kun je ook jouw eigen stijl-thema produceren en gebruiken. Als je dat zou doen, zou de output sectie van jouw YAML kop er z’on beetje zo uitzien:\r\n\r\noutput:\r\n  html_document:\r\n    css: styles.css\r\nAls je nog wat verder wilt gaan en jouw eigen thema wilt schrijven in aanvulling op het oplichten, zou de YAML kop er beetje zo uitzien:\r\n\r\n---\r\noutput:\r\n  html_document:\r\n    theme: null\r\n    highlight: null\r\n    css: styles.css\r\n---\r\nHier is een link naar Pr?sance en Stijl in de HTML output.\r\nVerbergen\r\nProbleem: Sommige code chunks nemen veel tijd in beslag en hoeven niet vaak te worden ververst.\r\nOplossing: Verbergen (Caching)! Als een code chunk niet is aangepast sinds de laatste keer dat het document is gerenderd, zullen de oude resultaten worden gebruik inplaats van de chunk nogmaals te draaien.\r\nEen hele simpele oplossing: Stop ‘knitting’ voortijdig -> als de rest van het document niet hoeft te worden gedraaid, zet je eenvoudig een knit_exit() vast en de rest van het document wordt genegeerd. Daarvoor in de plaats komen de resultaten en de code chunk uit de vorige tekst terug.\r\nsloom-laden wordt over gesproken als een object niet in het geheugen wordt geladen totdat het wordt gebruikt. Daarvoor in de plaats wordt er een “belofte” gemaakt, die makkelijk is voor de computer makkelijk is (om daar meer van te leren, tik ?promise).\r\nAls het document wordt gerenderd, worden verborgen chunks overgeslagen en de output die eerder gecre?erd met de chunks die sloom laden, worden geladen van de cache folder. Echter als er een kleine verandering in de chunk wordt aangebracht (zelfs een witte ruimte telt!) zal knitr deze verandering opmerken en zal ze de chunk draaien als het document wordt gerenderd.\r\nDe gebruiker kan ook het pad instellen voor waar de verborgen files zijn door cache.path te gebruiken.\r\nStandaard: cache.path = \"file_name_cache/\"\r\nBijvoorbeeld: Probeer de github repo voor de vorige les waar de verborgen chunks in de RMarkdown_Lesson_cache directory staan.\r\n\r\nEnkele zaken met betrekking tot verbergen:\r\nR wordt om de paar maanden ververst. R.version.string\r\nAan externe files van knitr kunnen deze veranderingen voorbij gaan, het kan zijn dat ze ververst moeten worden en dat de resultaten opnieuw gedraaid moeten worden.\r\nSoms kan een verborgen chunk vertrouwen op objecten van een ander verborgen chunk. Dit kan een serieus probleem zijn - dus wees voorzichtig! We moeten de chunk afhankelijkheden dus dekken!\r\nChunk afhankelijkheden\r\nMet de hand\r\nWe kunnen met de hand specificeren of chunks van elkaar afhankelijk zijn.\r\ndependson specificeert van welke chunk de huidige chunk afhankelijk is.\r\nVoorbeelden hiervan:\r\ndepesndson = 1: Chunk vertrouwt op de eerste Chunk\r\ndependson = c(6,8): Chunk vertrouwt op de 6de en 8ste chunks\r\ndependson = -1: Chunk vertrouwt op de vorige chunk.\r\ndependson = c(-1, -2): Chunk vertrouwt op de twee vorige chunks.\r\nOpgelet: Als dependson een bepaalde waarde aanneemt, kan het niet afhankelijk zijn van een latere chunk - alleen van vorige chunks. Daarom is het makkelijk om chunks namen te geven.\r\n\r\nVoorbeelden:\r\ndependson = c(\"Chunk-1\", \"Chunk-2\", \"Chunk-3\")\r\ndependson = c(\"data-generation\", \"data-transforamtion\")\r\n\r\n\r\nAls resultaat, elke keer als de verborgen chunks \"Chunk-1\", \"Chunk-2\" en \"Chunk-3\" opnieuw worden opgebouwd, zal de huidige chunk zijn verborgenheid verliezen en zal het opnieuw worden gedraaid!\r\nAutomatisch\r\nVoeg in: autodep chunk optie en de functie dep_auto()\r\nautodep en dep_auto() staan er voor dat de objecten in de huidige chunk door vorige chunks zijn gemaakt. Dus de huidige chunk hangt af van de vorige chunk.\r\nVoor een meer conservatieve benadering voeg dep_prev() in.\r\ndep_prev staat er voor dat een gevouwen chunk afhangt van al zijn vorige chunks. Dus als vorige chunks zijn ververst, zullen ook alle latere chunks worden ververst.\r\nKnitr indentificeert alleen veranderingen in de opgevouwen chunks, niet in de niet opgevouwen chunks! Gelukkig geeft knitr een waarschuwing wanneer het een afhankelijkheid ziet met een niet opgevouwen chunk.\r\nCache met de hand laden\r\nStel dat je aan het eind van een document een z berekent, maar je wilt de z in een eerdere chunk gebruiken. Dit is onmogelijk omdat knitr het document op een liniarie manier samenvoegt en het geen objecten kan gebruiken die in de toekomst worden gemaakt.\r\nVoeg in: load_cache, die de chunk label zoekt in de ‘cache’ database\r\n\r\nload_cache(label, object, notfound = \"NOT AVAILABLE\", \r\n  path = opts_chunk$get(\"cache.path\"), lazy = TRUE)\r\nAls jij dan een z in een ‘’inline R expressie’ gebruikt, geeft het NOT AVAILABLE terug en omdat je hebt gespecificeerd notfound = \"NOT AVAILALBE\" zal het naar het einde teruggaan en de waarde z verplaatsen.\r\nZo handig!\r\nZijeffecten\r\nEen zijeffect refereert naar een statusverandering die optreedt buiten een functie die de teruggeven waarde niet representeert.\r\npar() en options() zijn zijeffecten in de betekenis dat ze niet opgevouwen zijn.\r\nStel globale opties van de eerste chunk in en vouw deze chunk nooit op.\r\nWe moeten voorzichtig zijn met de chunk opties om er zeker van te zijn dat de resultaten van de opgevouwen chunks worden ververst.\r\nWe kunnen ook het sloom-laden afzetten met cache.lazy = FALSE.\r\nBibliografie\r\nHet is ook mogelijk om een bibliografie file in de YAML kop mee te nemen. Bibliografie formats die door Pandoc gelezen kunnen worden zijn:\r\nFormat\r\nFile extension\r\nMODS\r\n.mods\r\nBibLaTeX\r\n.bib\r\nBibTeX\r\n.bibtex\r\nRIS\r\n.ris\r\nEndNote\r\n.enl\r\nEndNote XML\r\n.xml\r\nISI\r\n.wos\r\nMEDLINE\r\n.medline\r\nCopac\r\n.copac\r\nJSON citeproc\r\n.json\r\nOm een bibliografie in RMarkdown te maken, zijn er twee files nodig:\r\nEen bibliografie file met informatie over elke referentie.\r\nEen citaat stijl taal (CSL) om het format de referentie te bepalen.\r\nEen voorbeeld YAML kop met een bibliografie en een citaat stijl taal (CSL) file is:\r\n\r\noutput: html_document\r\nbibliography: bibliography.bib\r\ncsl: nature.csl\r\nBekijk de erg behulpzame webpagina van het R Core team op bibliographies and citations.\r\nAls je R pakketten wilt citeren, heeft knitr zelfs een functie die write_bib() heet en die .bib overzicht van R pakketten kan leveren. Het wordt zelfs in een file geschreven!\r\n\r\n\r\n\r\nPlaatsen\r\nDe bibliografie wordt automatisch aan het einde van het document geplaatst. Daarom moet je jouw .Rmd document met # Referenties eindigen zodat de bibliografie naar de kop voor bibliografie komt.\r\n\r\nlaatste woorden...\r\n\r\n# Referenties\r\nStylen van citeren\r\nCitation Sylte Language (CSL) is een op XML-gebaseerde taal die het format van citaten en bibliografie?n vaststelt. Referentie management programma’s zoals Zotero, Mendeley en Papers gebruiken allemaal CSL.\r\nZoek jouw favcoriete tijdschrift en CSL in de Zotero Style Repository, waar nu meer dan 8,152 CSLs inzitten. Is er een stijl waar je naar zoekt en die er niet in zit?\r\n\r\noutput: html_document\r\nbibliography: bibliography.bib\r\ncsl: nature.csl\r\nIn de github repo voor deze workshop heb ik de nature.csl en the-isme-journal.csl toegevoegd om mee te spelen. Download anders een stijl van de Zotero Style Repository!\r\nCitaten\r\nCitaten gaan tussen vierkante haakjes [ ] en worden afgescheiden door punt-komma’s’ ;. Elk citaat moet een sleutel hebben, samen de @ + de citaat identificatie van de database vormen en die optioneel a prefix, a locator en a suffix hebben. Om te controleren wat de citaatsleutel is van een referentie, werp dan een blik op de .bib file. Hier in die file, kun je de sleutel voor elke referentie veranderen. Echter, wees er wel van bewust dat elke ID uniek is!\r\nHier zijn wat voorbeelden met bijpassende code in het Engels:\r\nMicrobes control Earth’s biogeochemical cycles [@Falkowski2008].\r\nCode: Microbes contorl Earth's biogeochemical cycles  [@Falkowski2008].\r\n\r\nI love making beautiful plots with ggplot2 [@R-ggplot2]\r\nCode: I love making beautiful plots with ggplot2 [@R-ggplot2]\r\n\r\nDr. Yuhui Xie’s book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\r\nCode: Dr. Yuhui Xie's book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\r\n\r\nA great article in Science regarding biogeography of microbes asks readers to imagine their Alice in Wonderland to shrink down to understand the microbial world [@Green2008].\r\nCode: A great article in *Science* regarding biogeography of microbes asks readers to imagine they are Alice in Wonderland to and shrink down to understand the microbial world [@Green2008].\r\n\r\nHet is cool dat de enige refenties die aan het document worden toegevoegd degene zijn die jijzelf citeert!\r\nPubliceren via RPubs\r\nAls je een keer een mooi dynamisch document hebt gemaakt wil je dat mogelijk delen met anderen. Een mogelijkheid om het te delen met de wereld is om het te hosten op RPubs. Met RStudio kan dit heel makkelijk! Doe het volgende:\r\nMaak een aansprekend .Rmd document.\r\nKlik op de  knop om jouw gerenderd HTML document te puliceren.\r\nIn de rechter bovenhoek van het previewscherm klik je op de publiceer  knop en volgt de aanwijzingen.\r\nOpgelet: Je moet een RPubs profiel hebben aangemaakt.\r\n\r\nAls je het profiel hebt let dan op het volgende:\r\nDe titel van het document.\r\nEen beschrijving van het document.\r\nDe URL waar de website wordt gehost.\r\nOpgelet: Het begin van de URL zal zijn: www.rpubs.com/your_username/name_of_your_choice\r\n\r\n\r\nRPubs vernieuwen\r\nAls je veranderingen in het document wilt aanbrengen is het makkelijk om de webpagina te vernieuwen. Als je een keer jouw aangepaste documentg hebt gerenderd klik je op de  knop rechtsboven in de hoek van de preview scherm. Het aangepaste document zal dezelfde URL hebben als het orginele document.\r\nDank\r\nBedankt dat je aan deze tutorial hebt meegedaan.\r\nAls je updates van deze les wilt maken, stuur dan me een ‘pull request’. Wat je ook kunt doen is een e-mail sturen naar:\r\nMarian SchmidtE-mail: marschmi at umich.eduTwitter @micro_marian\r\nVeel succes met de voortgang van dynamische documenten!\r\nInformatie over de sessie\r\n\r\n\r\n- Session info -----------------------------------------------------\r\n setting  value                       \r\n version  R version 3.5.1 (2018-07-02)\r\n os       Windows 10 x64              \r\n system   x86_64, mingw32             \r\n ui       RTerm                       \r\n language (EN)                        \r\n collate  Dutch_Netherlands.1252      \r\n ctype    Dutch_Netherlands.1252      \r\n tz       Europe/Berlin               \r\n date     2019-03-04                  \r\n\r\n- Packages ---------------------------------------------------------\r\n package     * version    date       lib\r\n assertthat    0.2.0      2017-04-11 [1]\r\n backports     1.1.3      2018-12-14 [1]\r\n callr         3.1.1      2018-12-21 [1]\r\n cli           1.0.1      2018-09-25 [1]\r\n colorspace    1.4-0      2018-10-06 [1]\r\n crayon        1.3.4      2017-09-16 [1]\r\n crosstalk     1.0.0      2016-12-21 [1]\r\n data.table    1.12.0     2019-01-13 [1]\r\n desc          1.2.0      2018-05-01 [1]\r\n devtools      2.0.1      2018-10-26 [1]\r\n digest        0.6.18     2018-10-10 [1]\r\n distill       0.6.0.9000 2019-03-04 [1]\r\n dplyr       * 0.8.0.1    2019-02-15 [1]\r\n DT          * 0.5        2018-11-05 [1]\r\n evaluate      0.13       2019-02-12 [1]\r\n fs            1.2.6      2018-08-23 [1]\r\n ggplot2     * 3.1.0      2018-10-25 [1]\r\n glue          1.3.0      2018-07-17 [1]\r\n gtable        0.2.0      2016-02-26 [1]\r\n highr         0.7        2018-06-09 [1]\r\n htmltools     0.3.6      2017-04-28 [1]\r\n htmlwidgets   1.3        2018-09-30 [1]\r\n httpuv        1.4.5.1    2018-12-18 [1]\r\n httr          1.4.0      2018-12-11 [1]\r\n jsonlite      1.6        2018-12-07 [1]\r\n knitr       * 1.21       2018-12-10 [1]\r\n labeling      0.3        2014-08-23 [1]\r\n later         0.8.0      2019-02-11 [1]\r\n lazyeval      0.2.1      2017-10-29 [1]\r\n magrittr      1.5        2014-11-22 [1]\r\n memoise       1.1.0      2017-04-21 [1]\r\n mime          0.6        2018-10-05 [1]\r\n munsell       0.5.0      2018-06-12 [1]\r\n pander      * 0.6.3      2018-11-06 [1]\r\n pillar        1.3.1      2018-12-15 [1]\r\n pkgbuild      1.0.2      2018-10-16 [1]\r\n pkgconfig     2.0.2      2018-08-16 [1]\r\n pkgload       1.0.2      2018-10-29 [1]\r\n plotly      * 4.8.0      2018-07-20 [1]\r\n plyr          1.8.4      2016-06-08 [1]\r\n prettyunits   1.0.2      2015-07-13 [1]\r\n processx      3.2.1      2018-12-05 [1]\r\n promises      1.0.1      2018-04-13 [1]\r\n ps            1.3.0      2018-12-21 [1]\r\n purrr         0.3.0      2019-01-27 [1]\r\n R6            2.4.0      2019-02-14 [1]\r\n Rcpp          1.0.0      2018-11-07 [1]\r\n remotes       2.0.2      2018-10-30 [1]\r\n rlang         0.3.1      2019-01-08 [1]\r\n rmarkdown   * 1.11       2018-12-08 [1]\r\n rprojroot     1.3-2      2018-01-03 [1]\r\n scales        1.0.0      2018-08-09 [1]\r\n sessioninfo   1.1.1      2018-11-05 [1]\r\n shiny         1.2.0      2018-11-02 [1]\r\n stringi       1.3.1      2019-02-13 [1]\r\n stringr       1.4.0      2019-02-10 [1]\r\n testthat      2.0.1      2018-10-13 [1]\r\n tibble        2.0.1      2019-01-12 [1]\r\n tidyr         0.8.2      2018-10-28 [1]\r\n tidyselect    0.2.5      2018-10-11 [1]\r\n usethis       1.4.0      2018-08-14 [1]\r\n viridisLite   0.3.0      2018-02-01 [1]\r\n withr         2.1.2      2018-03-15 [1]\r\n xfun          0.5        2019-02-20 [1]\r\n xtable        1.8-3      2018-08-29 [1]\r\n yaml          2.2.0      2018-07-25 [1]\r\n source                          \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n R-Forge (R 3.5.1)               \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n Github (rstudio/distill@6aaffa3)\r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.0)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.1)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.2)                  \r\n CRAN (R 3.5.1)                  \r\n\r\n[1] C:/Users/HarrieJonkman/Documents/R/win-library/3.5\r\n[2] C:/Program Files/R/R-3.5.1/library\r\n\r\nReferentie\r\n\r\n\r\n",
    "preview": "posts/2018-11-14-reproducable-research/reproducable-research_files/figure-html5/single-fig-center-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-exploratie/",
    "title": "Data exploratie",
    "description": "Een introductie op data exploratie aan de hand van een boek van Chester Ismay en Albert Y. Kim.",
    "author": [
      {
        "name": "Chester Ismay en Albert Y. Kim, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-10-15",
    "categories": [],
    "contents": "\r\nDe tekst vind je hier: http://www.harriejonkman.nl/wp-content/uploads/2018/01/MDtotaal.pdf\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-psm/",
    "title": "PSM",
    "description": "Om precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma's te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma's vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma's de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma's werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht^[Khandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). *Handbook on Impactevaluation. Quantative Methods and Practices*. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). *Impact Evaluation in Practice*. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). *Methods Matter. Improving Causal Inference in Educational and Social Science Research*. New York: Oxford University]. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo'n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "contents": "\r\nImpactevaluatie\r\nOm precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma’s te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma’s vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma’s de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma’s werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht2. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo’n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden.\r\nPropensity Score Matching als optie\r\nPropensity Score Matching is een statische techniek waarin een individu (of andere eenheid) die behandeld wordt of die ergens aan meedoet, wordt gematcht met een of meer respondenten uit de controlegroep op basis van de propensity score. Deze matchingstechniek versterkt causale argumenten in quasi-experimentele en observationele studies omdat ze selectie-bias reduceert. In dit artikel concentreren we ons op hoe we propensity score matching uitvoeren via een voorbeeld uit het onderwijsveld. Het doel van dit artikel is om informatie te geven zodat het gebruikt kan worden door onderzoekers en mensen die evaluatiestudies uitvoeren3 4.\r\nStap voor stap\r\nPropensity score matching is een statistische techniek waarin een geval uit de behandelgroep wordt gematcht met een of meer gevallen uit de controlegroep gebaseerd op ieders propensity score. Elders zijn er veel uitleg en onderbouwingen te vinden voor propensity score (Adelson, 20135; Holland, 1986 6; Rubin, 2005 7; Rudner & Peyton, 20068; Shadish, Cook, & Campbell, 20029; Stone & Tang, 201310 ). In dit artikel concentreren we ons op de vraag hoe we propensity score matching moeten uitvoeren door een voorbeeld uit het onderwijsveld te geven. In dit document willen we vooral een stap-voor-stap voorbeeld geven van de uitvoering van propensity score matching in R. We gebruiken hierbij het MatchIt-pakket met ‘nearest-neighbor 1-to-1 matching’. Terwijl er andere software is dan R voor het uitvoeren van propensity score matching, hebben we voor R gekozen omdat het open-source software is en omdat het breed gebruikt wordt door data-wetenschappers binnen verschillende disciplines. Het doel van dit artikel is om informatie te verschaffen zodat propensity score matching binnen het bereik komt van onderzoek en evaluatie.\r\nInformatie over de gebruikte dataset\r\nData van een observationele studie door Falbe (2014) worden hier gebruikt om te illustreren hoe propensity score matching werkt. In die studie gebruikt Falbe algemeen toegankelijke schooldata van verschillende staten om te onderzoeken of een bepaalde interventie een voorspeller was van succes in de resultaten op lezen of rekenen/wiskunde wanneer er voor de schoolgrootte worden gecontrolleerd (tot), percentage studenten van minderheden (min) en het percentage studenten dat een vrije of in prijs gereduceerde lunch ontvangt (dis)11. Voor het voorbeeld hier gebruiken we alleen Falbe’s schooldata van de staat New York. In de New York data set, zijn er 25 stw scholen en 560 niet-stw scholen (stw refereert hier naar een bepaalde interventie). Als matchingsvariabelen koos Falbe schoolgrootte, percentage minderheids studenten en het percentage studenten dat een vrije of een in prijs gereduceerde lunch ontvangt. De reden dat ze voor deze matchings variabelen koos, was dat vorig onderzoek had aangetoond dat deze variabelen samenhingen met academische resultaten. Door op deze variabelen te matchen, kon ze, dat was haar doel, selectie-bias reduceren tussen experimentele (“treated”, bijvoorbeeld stw) en controle groep (“control”, bijvoorbeeld niet-stw-scholen). Let op dat ondanks Falbe in haar studie naar correlaties zocht en niet-experimenteel van opzet was, we hier de termen behandeling (treated) en controle (control) gebruiken omdat deze termen worden gebruikt in het programma en de output van het MatchIt pakket.\r\nDe stappen in het uitvoeren van Propensity Score Matching in R\r\nStep 1 Installeer R.\r\nR is een gratis toegankelijk statistisch pakket dat via URL van het ‘R Core Team’ (2014) kan worden binnengehaald. Zie de referentie12. Let op de specifieke informatie voor het binnenhalen en het openen van de software. R is beschikbaar voor Windows, Mac OS X en Linux systemen. Dit artikel is op de R versie 3.3.2.\r\nStep 2 Installeer en laad het MatchIt pakket.\r\nMatchIt is een R-pakket dat R-gebruikers goed in staat stelt om met propensity score matching te werken; specifieke informatie over het MatchIt pakket kan worden gevonden in de artikelen van Ho, Kosoke, King, and Stuart (2007a13, 2007b14, 201115, 201316). Om het MatchIt pakket te gebruiken, moet je dat pakket eerst installeren en laden. Zoals met alle R-pakketten hoef je dat installeren alleen de eerste keer te doen. Bij MatchIt dus ook; echter je moet het pakket wel steeds laden als je de R-software gebruikt. Om een pakket te installeren, open je R en selecteer je het ‘Packages’ -menu. Kies dan: ‘Install Package(s)’. Een pop-up beeld van CRAN Mirror verschijnt. Kies het pakket dat je wilt uit een lijst en klik OK. Een ander beeld (“Packages” genaamd) verschijnt. Ga naar beneden en selecteer MatchIt en klik weer op OK. Het pakket wordt meteen binnengehaald. Als je vervolgens het MatchIt pakket wilt laden, kies dan voor ‘Load Package’ van het dropdown menu onder het ‘Package’-menu in R. Een beeld verschijnt met beschikbare pakketten waar jij jouw pakket uit moet selecteren. Kies uit de lijst voor MatchIt en klik op OK. Het MatchIt pakket wordt nu geladen.\r\n\r\n\r\n\r\nStep 3 Klaarmaken en laden van de data.\r\nOm propensity score matching uit te voeren heb je een dataset nodig die bestaat uit gevallen die in rijen staan (bv. individuen) en variabelen die in kolommen staan. Je hebt een groepvariabele nodig en een of meer matchingsvariabelen. De groepvariabele is de variabele die specificeert tot welke groep een geval (een individu of een school) behoort (bv. experimenteel of controle). De matchingsvariabelen zijn de variabelen waar je de groepen op wilt gelijk zetten. In de dataset van Falbe (2014) is bijvoorbeeld de stw-variabele de groepvariabele. Deze variabele geeft aan of een bepaald geval (een school hier) de interventie heeft ontvangen (1) of niet (0). De andere variabelen tot (schoolgrootte), min (percentage studenten met een andere etnische achtergrond in de school) en dis (percentage van de studenten dat een vrije of in prijs gereduceerde lunch ontvangt) zijn de matchingsvariabelen. De dataset die we hiervoor gebruiken kan worden binnengehaald via Randolph (2014a)17. In jouw eigen datasets moet je er wel zeker van zijn dat er geen missende data zijn want dan kan R de analyse niet uitvoeren. Ofschoon er verschillende functies zijn om Excel, SPSS of andere dataformaten in R te importeren, vinden wij het het makkelijkste om de data op te slaan als een .csv-file voordat je de data in R binnehaalt. Als je de file in Excel hebt opgeslagen, heb je de optie om het als een .csv-file op te slaan. Als je de file opslaat, let dan wel op waar deze wordt opgeslagen. Nu moet je de lokatie van jouw file die tussen haakjes staat in de eerste regel van de R-code (zie hieronder) aan jouw lokatie aanpassen. Let op dat in de eerste regel voorwaartste schuine streepjes staan eerder dan terugwaartse om jouw file lokatie te specificeren. Het voorbeeld hieronder is een filelokatie in Windows, waar de data staan in de file die newyork.csv heet in de folder R/PSM genaamd op de C-schijf. De eerste regel leest de data van jouw computer en hernoemt het als mydata. De tweede regel maakt deze data beschikbaar voor deze R sessie. De derde regel van de code hieronder print de variabelenamen en de eerste gevallen in de dataset. We doen dat alleen om de data te controleren en te begrijpen waar elke kolom voorstaat.\r\n\r\n\r\n                               school  tot  min  dis stw\r\n1           SKANEATELES MIDDLE SCHOOL  380 0.03 0.00   0\r\n2        MARCUS WHITMAN MIDDLE SCHOOL  276 0.04 0.00   0\r\n3       BLIND BROOK-RYE MIDDLE SCHOOL  376 0.09 0.00   0\r\n4            BRONXVILLE MIDDLE SCHOOL  404 0.11 0.00   0\r\n5            BRIARCLIFF MIDDLE SCHOOL  374 0.12 0.00   0\r\n6                   RYE MIDDLE SCHOOL  754 0.17 0.00   0\r\n7           EASTCHESTER MIDDLE SCHOOL  704 0.26 0.00   0\r\n8             SCARSDALE MIDDLE SCHOOL 1172 0.27 0.00   0\r\n9  EDGEMONT JUNIOR-SENIOR HIGH SCHOOL  920 0.42 0.00   0\r\n10        SEVEN BRIDGES MIDDLE SCHOOL  619 0.17 0.01   0\r\n\r\nNa de code zie je hierboven de resultaten staan van wat de R-code oplevert. Het laat de eerste tien gevallen in de dataset zien en wat de kolommen inhouden. Het laat zien dat de kolommen van links naar rechts de gevalnummer zijn (een uniek id-nummer voor elke school), school (de naam van elke school), tot (het totale aantal studenten in op die school), min (het percentage van minderheidsstudenten op de school), dis (precentage studenten dat een vrije of in prijs gereduceerde lunch) en stw (of de school de interventie ontvangt met een (1) Scholen die deze wel of (0) niet ontvangen).\r\nStep 4. Matching uitvoeren en de resultaten evalueren.\r\nDe volgende stap is nodig om te laten zien hoe de matching wordt uitgevoerd en de resultaten vervolgens kunnen worden geevalueerd. De eerste regel in de code die hieronder te zien is, voert de matching uit en de groepsvariabele is stw en de variabelen waarop wordt gematcht zijn tot, min en dis. Je moet deze variabelen in de code eventueel later vervangen door de variabelenamen die in jouw eigen dataset voorkomen. In de methode hieronder wordt de ‘nearest neighbor-methode’ gebruikt. Het ratio-commanda geeft aan dat er een ??n op ??n matching wordt toegepast en dat betekent dat elk geval van de behandeling wordt gematcht aan een geval uit de controlegroep. Het getal waarin gematcht wordt kan toenemen; meestal wordt hier een getal tussen 1 en 5 gebruikt.\r\nHieronder zie je de code om de PSM uit te voeren en de resultaten ervan.\r\n\r\n\r\nCall:\r\nmatchit(formula = stw ~ tot + min + dis, data = mydata, method = \"nearest\", \r\n    ratio = 1)\r\n\r\nSummary of balance for all data:\r\n         Means Treated Means Control SD Control Mean Diff  eQQ Med\r\ndistance        0.0943        0.0405     0.0503    0.0537   0.0559\r\ntot           832.6400      568.8998   333.6746  263.7402 300.0000\r\nmin             0.1664        0.2767     0.3011   -0.1103   0.0200\r\ndis             0.1840        0.4079     0.2500   -0.2239   0.2500\r\n         eQQ Mean   eQQ Max\r\ndistance   0.0599    0.1875\r\ntot      310.9600 1124.0000\r\nmin        0.1276    0.6300\r\ndis        0.2276    0.4900\r\n\r\n\r\nSummary of balance for matched data:\r\n         Means Treated Means Control SD Control Mean Diff eQQ Med\r\ndistance        0.0943        0.0942     0.0513    0.0001  0.0004\r\ntot           832.6400      830.6400   315.6859    2.0000 99.0000\r\nmin             0.1664        0.1772     0.1330   -0.0108  0.0200\r\ndis             0.1840        0.1808     0.1361    0.0032  0.0100\r\n         eQQ Mean  eQQ Max\r\ndistance   0.0005   0.0024\r\ntot      115.8400 247.0000\r\nmin        0.0260   0.1500\r\ndis        0.0256   0.0900\r\n\r\nPercent Balance Improvement:\r\n         Mean Diff. eQQ Med eQQ Mean eQQ Max\r\ndistance    99.8180 99.3674  99.1609 98.7414\r\ntot         99.2417 67.0000  62.7476 78.0249\r\nmin         90.2061  0.0000  79.6238 76.1905\r\ndis         98.5707 96.0000  88.7522 81.6327\r\n\r\nSample sizes:\r\n          Control Treated\r\nAll           559      25\r\nMatched        25      25\r\nUnmatched     534       0\r\nDiscarded       0       0\r\n\r\n\r\n[1] \"To identify the units, use first mouse button; to stop, use second.\"\r\n\r\ninteger(0)\r\n\r\n\r\nEr kunnen ook andere methodes gebruikt worden; een korte beschrijving hiervan vind je in de lijst hieronder. We moedigen MatchIt-gebruikers aan om verschillende methodes te gebruiken en te zien welke methode het beste werkt voor een bepaalde dataset. In dit geval probeerden we alle matchingmethodes die er tegenwoordig gebruikt kunnen worden in MatchIt en kozen voor de ’nearest neighbor-methode’omdat dit leidde tot de laagste gemiddelde verschillen tussen groepen. Sommmige andere methodes zijn\r\n- Exact Matching - Elke eenheid uit de treated groep heeft precies dezelfde waarden aan die uit de controle groep op elke covariaat. Als er veel covariaten zijn en/of covariaten die een groot aantal waarden aan kunnen nemen, dan kan het zijn dat Exact Matching niet mogelijk is (method = “exact”).\r\n- Subclassificatie - Deze techniek breekt de data als het ware in enkele subklassen zodat de verdelingen van de covariaten hetzelfde zijn binnen iedere subklasse (method = “subclass”).\r\n- Nearest Neighbor - Deze techniek matcht een treated-eenheid aan die van de controlegroep in termen van afstand die in een logit-waarde wordt uitgedrukt (method = “nearest”).\r\n- Optimal Matching - Deze techniek richt zich op het minimaliseren van de gemiddelde absolute afstand over alle gematchte paren (method = “optimal”). Deze methode van matching vraagt een bepaald pakket.\r\n- Genetic Matching - dit gebruikt een intensief genetisch zoekalgoritme om de treatment aan de controle eenheid te koppelen (method = “genetic”). - Coarsened Exact Matching techniek matcht op een covariaat terwijl de balans op de andere covariaten wordt vastgehouden. Het werkt “goed voor multicategoriale behandelingen, wanneer er met blokken in experimentele designs wordt gewerkt en bij het evalueren van extreme counterfactuals” (Ho, Kosuke, King, & Stuart, 2011, p.1218) (method = “cem”).\r\nZie documentatie over MatchIt voor verdere detailt over de matchingsprocedure zoals hierboven is uitgelegd. (Ho, Kosuke, King, & Stewart, 2007a19, 2007b20, 201121, 201322).\r\nDe resultaten van de matching worden opgeslagen in een variabele die heet m.out. De tweede regel geeft een samenvatting van de matching. De derde en vierde regel produceren plots en histogrammen. De resultaten laten zien dat de matching bijzonder goed werkt voor deze dataset. In de samenvatting lezen we dat voor de matching het gemiddel aantal studenten (tot) in de treatment groep van scholen 263.74 studenten minder was dan in de controle (de niet-stw) scholen. De treated-scholen had 11% minder minderheidsstudenten (min) en 22% procent minder studenten in armoede (dis) dan in de controlescholen. Na de matching echter zijn deze verschillen dramatisch teruggebracht zoals we in de samenvatting kunnen lezen. Het gemiddelde verschil in aantallen studenten tussen de treated en de controlescholen is tot 2 teruggebracht; het was 263 voor de matching. Het percentage verschil in minderheidsstudenten tussen de treated en controle scholen is nog slechts 1%; het was 11% voor de matching. Tenslotte, het gemiddelde verschil tussen treated en controle scholen in termen van percentage achterstandsstudenten is tot 3/10 procent teruggebracht; het was 22% voor de matching.\r\nSamengevat, de treated en controle scholen zijn na de matching vrijwel hetzelfde geworden wat betreft het aantal studenten, het percentage minderheidsstudenten en het percentage studenten dat een vrij en in prijs verlaagde lunch ontvangt. Voor de matching waren de treated scholen gemiddeld groter, hadden ze minder minderheidsstudenten en minder achterstandsstudenten dan de controle scholen. De kolommen rechts in de samenvatting laten de mediaan, het gemiddelde en het maximum quartiel zien tussen de treated en de controledata; kleinere QQ-waarden laten een betere matching zien. Het valt op dat de QQ-waarden na matching kleiner zijn dan voor de matching.\r\nMet de derde en vierde regel van de code worden jitterplots en histogrammen gemaakt om de kwaliteit van de matching aan te tonen. In een jitterplot representeert elke cirkel een propensityscore van een geval. Bovenaan zie je dat er geen niet-gematchte treatment eenheden zijn. De middelste grafieken laten de sterke match zien tussen de gematchte treatment en controleeenheden. Onderaan zien je de niet gematchte controleeenheden die in de analyses verder niet gebruikt zullen worden.\r\nDaaronder zie je de histogrammen voor en na de matching. De histogrammen voor de matching, links, verschillen in sterke mate. De histogrammen na de matching, rechts, zijn voor een groot deel hetzelfde. Samengevat kunnen we stellen dat zowel de numerieke als de visuele data laten zien dat de matching succesvol was.\r\nStep 5. Exporteren van een datafile om vervolgens de analyses te kunnen uitvoeren.\r\nAls de matching een keer is afgerond, wil je een dataset hebben die bestaat uit enkel gematchte gevallen waarmee je de statistische analyses kunt doen die je voor ogen hebt. Hieronder zie je de code staan waarmee je in de eerste regel een R-dataset maakt met gematchte behandel en controlegevallen (bijvoorbeeld hier zitten de meer dan 500 controlgevallen niet meer in die niet zijn gematcht.) De tweede regel zorgt er voor dat de gematchte data in een .csv-file worden omgezet die verder in R kunnen worden geanalyseerd of eenvoudig kunnen worden omgezet naar een ander statistisch software-pakket; in dit geval is de output dataset opgeslagen als een file die newyork_nearest1 wordt genoemd in een specifieke folder (R/PSM in dit geval) op de C-schijf. (De dataset voor de New York data kan worden gedownload van Randolph (2014b))23.\r\n\r\n\r\n\r\nWat vervolgens de analyse betreft was Falbe (2014) ge?nteresseerd in de vraag of stw-scholen het beter doen dan niet-stw schools ten aanzien van academische resultaten24. Om die analyse uit te voeren, voegde ze de lees-en rekenscores toe aan de gematchte dataset en voorspelde zo de academische resultaten op basis van de volgende variabelen: of een school een stw-school was of niet, de schoolgrootte, het percentage minderheidstudenten in een school en het percentage studenten dat een vrije of in prijs verlaagde lunche ontvangt. Het blijkt dat matching in dit geval belangrijk was. Zonder matching hadden stw-scholen statistisch gezien betere resultaten dan wanneer ze niet gematcht waren. Fable vond geen statistisch gezien significant verschil tussen stw niet stw-scholen op de academische resultaten. Zonder propensity score matching zou Falbe een hele andere conclusie hebben getrokken ten aanzien van de effectiviteit van de interventie, die dan eigenlijk het resultaat is van selectie-bias. Het is duidelijk dat propensity score matching een bruikbaar gereedschap is om selectie-bias te reduceren en causale conclusies te trekken. We hopen dat door deze stap-voor-stap gids een brede groep onderzoekers en evaluatiemedewerkers propensity score matching aan hun repertoire van data analysetechnieken kan toevoegen en deze techniek in hun werk kan gebruiken.\r\nReferenties\r\nAdelson, J. L. (2013). Educational research with real data: Reducing selection bias with propensity score analysis. Practical Assessment Research & Evaluation, 18(15). Retrieved from http://pareonline.net/getvn.asp?v=18&n=15\r\nFalbe, K. (2014). The relationship between Schools to Watch ? designation and ac study of Colorado, New York, Ohio, and Virginia (Doctoral dissertation). Available from Proquest Dissertations and Theses database. (UMI No. 3581272)\r\nGertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). Impact Evaluation in Practice. Washington D.C.: The World Bank\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007a). MatchIt: Nonparametric preprocessing for parametric causal inference. Political Analysis, 15(3), 199\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007b). Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference. Journal of Statistical Software. Retrieved from http://gking.harvard.edu/matchit/\r\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2013). MatchIt: Nonparametric preprocessing for parametric causal inference [software]. Retrived from http://gking.harvard.edu/matchit\r\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945-960.\r\nKhandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). Handbook on Impactevaluation. Quantative Methods and Practices. Washington D.C: The World Bank.\r\nMurnane, R.J. & Willet, J.B.(2011). Methods Matter. Improving Causal Inference in Educational and Social Science Research. New York: Oxford University.\r\nRandolph, J. J. (2014a). New York educational data set example before matching. Retrieved from http://justusrandolph.net/psm/newyork.csv\r\nRandolph, J. J. (2014b). New York educational data set example after matching. Retrieved from http://justusrandolph.net/psm/newyork_nearest100.c sv\r\nR Core Team (2014). R: A language and environment for statistical computing. (3.0.3 ) [Computer software]. Vienna, Austria: Foundation for Statistical Computing. Retrieved from http://www.R-project.org/.\r\nRubin D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(496), 322-331.\r\nRudner, L. M., & Peyton, J. (2006). Consider propensity scores to compare treatments. Practical Assessment Research & Evaluation, 11(9). Retrieved from: http://pareonline.net/getvn.asp?v=11&n=9\r\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Boston, MA: Houghton Mifflin.\r\nStone, C. A. & Tang, Y. (2013). Comparing propensity score methods in balancing covariates and recovering impact in small sample educational program evaluations.\r\nNote: A previous version of this paper was delivered at the 2014 annual meeting of Mercer University Atlanta Research Conference, Atlanta, GA.\r\nCitation:\r\nRandolph, Justus J., Falbe, Kristina, Manuel, Austin Kureethara, & Balloun, Joseph L. (2014). A Step-byStep Guide to Propensity Score Matching in R. Practical Assessment, Research & Evaluation, 19(18). Available online: http://pareonline.net/getvn.asp?v=19&n=18\r\nCorresponding Author:\r\nJustus J. Randolph Tift College of Education Mercer University 3001 Mercer University Dr. Atlanta, GA 30341 randolph_jj@mercer.edu\r\nKhandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). Handbook on Impactevaluation. Quantative Methods and Practices. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). Impact Evaluation in Practice. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). Methods Matter. Improving Causal Inference in Educational and Social Science Research. New York: Oxford University↩\r\nKhandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). Handbook on Impactevaluation. Quantative Methods and Practices. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). Impact Evaluation in Practice. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). Methods Matter. Improving Causal Inference in Educational and Social Science Research. New York: Oxford University↩\r\nAlles gebaseerd op: Randolph, Justus J., Falbe, Kristina, Manuel, Austin Kureethara, & Balloun, Joseph L. (2014). A Step-byStep Guide to Propensity Score Matching in R. Practical Assessment, Research & Evaluation, 19(18). Available online: http://pareonline.net/getvn.asp?v=19&n=18 ↩\r\nHet correspondentieadres van hem is: Justus J. Randolph Tift College of Education Mercer University 3001 Mercer University Dr. Atlanta, GA 30341 randolph_jj@mercer.edu↩\r\nEducational research with real data: Reducing selection bias with propensity score analysis. Practical Assessment Research & Evaluation, 18(15). Retrieved from http://pareonline.net/getvn.asp?v=18&n=15 ↩\r\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945-960. ↩\r\nRubin D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(496), 322-331. ↩\r\nRudner, L. M., & Peyton, J. (2006). Consider propensity scores to compare treatments. Practical Assessment Research & Evaluation, 11(9). Retrieved from: http://pareonline.net/getvn.asp?v=11&n=9 ↩\r\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Boston, MA: Houghton Mifflin. ↩\r\nStone, C. A. & Tang, Y. (2013). Comparing propensity score methods in balancing covariates and recovering impact in small sample educational program evaluations. ↩\r\nFalbe, K. (2014). The relationship between Schools to Watch ? designation and ac study of Colorado, New York, Ohio, and Virginia (Doctoral dissertation). Available from Proquest Dissertations and Theses database. (UMI No. 3581272)↩\r\nR Core Team (2014). R: A language and environment for statistical computing. (3.0.3 ) [Computer software]. Vienna, Austria: Foundation for Statistical Computing. Retrieved from http://www.R-project.org/. ↩\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007a). MatchIt: Nonparametric preprocessing for parametric causal inference. Political Analysis, 15(3) , 199 ↩\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007b). Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference. Journal of Statistical Software. Retrieved from http://gking.harvard.edu/matchit/ ↩\r\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit↩\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2013). MatchIt: Nonparametric preprocessing for parametric causal inference [software]. Retrived from http://gking.harvard.edu/matchit ↩\r\nRandolph, J. J. (2014a). New York educational data set example before matching. Retrieved from http://justusrandolph.net/psm/newyork.csv ↩\r\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit↩\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007a). MatchIt: Nonparametric preprocessing for parametric causal inference. Political Analysis, 15(3) , 199 ↩\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007b). Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference. Journal of Statistical Software. Retrieved from http://gking.harvard.edu/matchit/ ↩\r\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit↩\r\nHo, D., Kosuke, I., King, G., & Stuart, E. (2013). MatchIt: Nonparametric preprocessing for parametric causal inference [software]. Retrived from http://gking.harvard.edu/matchit ↩\r\nRandolph, J. J. (2014b). New York educational data set example after matching. Retrieved from http://justusrandolph.net/psm/newyork_nearest100.csv↩\r\nFalbe, K. (2014). The relationship between Schools to Watch ? designation and ac study of Colorado, New York, Ohio, and Virginia (Doctoral dissertation). Available from Proquest Dissertations and Theses database. (UMI No. 3581272). ↩\r\n",
    "preview": "posts/2018-11-14-psm/psm_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-tufte/",
    "title": "Tufte",
    "description": "De Tufte-stijl is een stijl die Edward Tufte gebruikt in zijn boeken en handouts. Tufte's stijl is bekend vanwege zijn veelvuldig gebruik van opmerkingen aan de zijkant (sidenotes), strakke integratie van zijn grafieken met tekst en zijn duidelijk gezette typografie. Deze stijl is geimplementeerd in repectievelijk LaTeX en HTML/CSS^[Zie Github repositories [tufte-latex](https://github.com/tufte-latex/tufte-latex) en [tufte-css](https://github.com/edwardtufte/tufte-css)], respectively. Beide implementaties zitten nu ook in het [**tufte** pakket](https://github.com/rstudio/tufte). Als je een LaTeX/PDF output wilt, gebruik dan `tufte_handout` format voor handouts en `tufte_book` voor boeken. Voor HTML output, gebruik je `tufte_html`. Deze formatten kunnen worden gespecificeerd in de YAML metadata aan het begin van een R Markdown-document (zie het voorbeeld hieronder), of overgebracht via de `rmarkdown::render()` functie. Zie @R-rmarkdown voor meer informatie over **rmarkdown**.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "contents": "\r\nHier vind je een document over het werken in de Tufte stijl\r\nZie Github repositories tufte-latex en tufte-css↩\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-op-weg-naar-infografieken/",
    "title": "Op weg naar infografieken",
    "description": "Hier gaat het om een korte handleiding voor R_gebruikers die omwille van de leesbaarheid en esthetiek hun figuren in het populaire grafische design programma Illustrator willen 'oppoetsen'. Als het op visualisatie aankomt blijven de meeste R-gebruikers binnen dit programma werken. Dat is natuurlijk prima als het gaat om figuren die de analyse moeten ondersteunen en jij degene bent die er alleen naar moet kijken. Dan hoef je ook niets over de context te vermelden, niets verder uit te leggen of ervoor te zorgen dat het er allemaal mooi uitziet. Het doel dan is vooral snel figuren maken zodat je gevoel bij jouw data krijgt. R biedt je ook heel veel mogelijkheden, ook voor goede visualisatie. Echter, als het gaat om het maken van figuren die voor een breder publiek toegankelijk en leesbaar zijn en die zelf een verhaal moeten vertellen, kan het wel eens bruikbaarder en efficiënter zijn om dit R-figuur als PDF op te slaan en aanpassingen door te voeren in een vector georienteerd programma zoals Adobe Illustrator (https://www.adobe.com/nl/) of zijn open-source alternatief Inkscape (https://www.inkscape.org/nl/). Inkscape is vrij toegankelijk maar hier besteden wij enkel aandacht aan het bewerken in Adobe Illustrator.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-07-14",
    "categories": [],
    "contents": "\r\nHier vind je een goed document\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-latex/",
    "title": "Latex",
    "description": "Introductie op Latex.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://www.harriejonkman.nl"
      }
    ],
    "date": "2017-04-14",
    "categories": [],
    "contents": "\r\nHier vind je een snelle introductie op de werking van Latex\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-visualisatie/",
    "title": "Visualisatie",
    "description": "Hoe kun je goed werken aan datavisualisatie met ggplot2 binnen R/RStudio",
    "author": [
      {
        "name": "Zev Ross, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-02-11",
    "categories": [],
    "contents": "\r\nInleiding\r\nggplot is een R-pakket om data te onderzoeken en vooral om mooie en duidelijke figuren te maken. Deze figuren zien er grafisch fantastisch uit en kunnen op verschillende manieren worden aangepast. Er zijn verschillende tutorials en boeken beschikbaar die je uitleggen hoe het werkt. Hier volgen we de uitleg van Zev Ross.\r\nSnelle eerste blik\r\nBekijk de dataset eerst goed. In dit geval gaat het om een data uit Chicago over luchtvervuiling over een aantal jaren. Het jaartal zit erin, het aantal doden per dag, de temperatuur, dan twee uitkomstmaten, het tijdstip en de seizoenen. Het zijn deze data die we zichtbaar gaan maken met het pakket ggplot van R.\r\n\r\n\r\nlogical(0)\r\n\r\n     city       date death temp dewpoint      pm10        o3 time\r\n3654 chic 1997-01-01   137 36.0    37.50 13.052268  5.659256 3654\r\n3655 chic 1997-01-02   123 45.0    47.25 41.948600  5.525417 3655\r\n3656 chic 1997-01-03   127 40.0    38.00 27.041751  6.288548 3656\r\n3657 chic 1997-01-04   146 51.5    45.50 25.072573  7.537758 3657\r\n3658 chic 1997-01-05   102 27.0    11.25 15.343121 20.760798 3658\r\n3659 chic 1997-01-06   127 17.0     5.75  9.364655 14.940874 3659\r\n     season\r\n3654 winter\r\n3655 winter\r\n3656 winter\r\n3657 winter\r\n3658 winter\r\n3659 winter\r\n\r\nHet default-figuur in ggplot2\r\nHet standaardfiguur is de volgende waarbij je aangeeft hoe de dataset in elkaar zit, naar welke variabelen je kijkt en wat je wilt zien. Let goed op, je ziet dat ggplot met duidelijke lagen werkt waar steeds passende commando’s bij horen.\r\n\r\n\r\n\r\nWerken met de titel\r\nJe kunt er ook een titel aan toevoegen.\r\n\r\n\r\n\r\nVet en meer ruimte\r\nDe titel kun je ook vet maken en meer ruimte geven.\r\n\r\n\r\n\r\nBij een lange titel\r\nDe titel kun je ook meer ruimte geven via commando (lineheight).\r\n\r\n\r\n\r\nOp de assen werken\r\nJe kunt ook tekst aan de x en y as toevoegen o.a. door (labs() of bijvoorbeel xlab())\r\n\r\n\r\n\r\nVerwijderen van gegevens\r\nJe kunt ook gegevens op de y-as weghalen door labels (theme(), axis.ticks.y)\r\n\r\n\r\n\r\nGegevens verder aanpassen\r\nJe kunt de gegevens op de assen aanpassen, bv. op de x en roteren (axis.text.x)\r\n\r\n\r\n\r\nEen kleurtje geven\r\nDe tekst op x en y-as kun je ook nog een kleurtje geven door(theme(), axis.title.x)\r\n\r\n\r\n\r\nBeperken\r\nJe kunt de gegevens die je wilt laten zien, ook weer in bereik beperken (ylim).\r\n\r\n\r\n\r\nGestandardiseerd\r\nAls de assen hetzelfde moeten zijn, kan ook (coord_equal())\r\n\r\n\r\n\r\nLabels veranderen\r\nJe kunt ook de labels aanpassen (label=function(x){}) en de gegevens over de maanden toevoegen.\r\n\r\n\r\n\r\nLegenda\r\nDe legenda kan worden aangepast nu.\r\n\r\n\r\n\r\nTitel legenda\r\nJe kunt de titel van de legenda ‘uitzetten’ (legend.title)\r\n\r\n\r\n\r\nAanpassen stijl van de titel\r\nOok de stijl van de legenda titel kun je aanpassen (legend.title)\r\n\r\n\r\n\r\nTitel naam veranderen\r\nJe kunt ook de titelzelf veranderen (name)\r\n\r\n\r\n\r\nAchtergrondkleur aanpassen\r\nJe kunt ook de achtergrondkleur van de legenda aanpassen (legend.key)\r\n\r\n\r\n\r\nOf het symbool\r\nOf alleen het symbool in de legenda(guides(), guide_legend)\r\n\r\n\r\n\r\nDe temperatuur ipv punt\r\nJe kunt ook de temperatuur ipv een punt afdrukken (show_guide)\r\n\r\n\r\n\r\nAndere mogelijke aanpassingen\r\nEr zijn nog meer aanpassingen van de legenda mogelijk via (guides(), override.aes)\r\n\r\n\r\n\r\nAanpassen van de achtergrondkleur\r\nDe achtergrond is aan te passen (panel.background)\r\n\r\n\r\n\r\nOok de grid lijnen zijn aan te passen (panel.grid.major)\r\n\r\n\r\n\r\nBerperkte kleuraanpassingen\r\nNiet in het panel maar verder wel kleur aanpassen (plot.background)\r\n\r\n\r\n\r\nWerken met margins\r\nDe plot margin aanpassen(plot.margin)\r\nthe default\r\n\r\n\r\n\r\nWerken met thema’s\r\nJe kunt ook met thema’s werken, mooi en consistent, bijvoorbeeld hier eentje uit het blad ‘The Economist’.\r\nUse a new theme (theme_XX())\r\n\r\n\r\n\r\nElementen\r\nJe kunt ook de omvang van de elementen aanpassen via text elements (theme_set(), base_size)\r\n\r\n\r\n\r\nJe kunt ook een thema-stijl zelf ontwikkelen\r\n\r\n\r\nfunction (base_size = 11, base_family = \"\", base_line_size = base_size/22, \r\n    base_rect_size = base_size/22) \r\n{\r\n    half_line <- base_size/2\r\n    theme(line = element_line(colour = \"black\", size = base_line_size, \r\n        linetype = 1, lineend = \"butt\"), rect = element_rect(fill = \"white\", \r\n        colour = \"black\", size = base_rect_size, linetype = 1), \r\n        text = element_text(family = base_family, face = \"plain\", \r\n            colour = \"black\", size = base_size, lineheight = 0.9, \r\n            hjust = 0.5, vjust = 0.5, angle = 0, margin = margin(), \r\n            debug = FALSE), axis.line = element_blank(), axis.line.x = NULL, \r\n        axis.line.y = NULL, axis.text = element_text(size = rel(0.8), \r\n            colour = \"grey30\"), axis.text.x = element_text(margin = margin(t = 0.8 * \r\n            half_line/2), vjust = 1), axis.text.x.top = element_text(margin = margin(b = 0.8 * \r\n            half_line/2), vjust = 0), axis.text.y = element_text(margin = margin(r = 0.8 * \r\n            half_line/2), hjust = 1), axis.text.y.right = element_text(margin = margin(l = 0.8 * \r\n            half_line/2), hjust = 0), axis.ticks = element_line(colour = \"grey20\"), \r\n        axis.ticks.length = unit(half_line/2, \"pt\"), axis.title.x = element_text(margin = margin(t = half_line/2), \r\n            vjust = 1), axis.title.x.top = element_text(margin = margin(b = half_line/2), \r\n            vjust = 0), axis.title.y = element_text(angle = 90, \r\n            margin = margin(r = half_line/2), vjust = 1), axis.title.y.right = element_text(angle = -90, \r\n            margin = margin(l = half_line/2), vjust = 0), legend.background = element_rect(colour = NA), \r\n        legend.spacing = unit(2 * half_line, \"pt\"), legend.spacing.x = NULL, \r\n        legend.spacing.y = NULL, legend.margin = margin(half_line, \r\n            half_line, half_line, half_line), legend.key = element_rect(fill = \"grey95\", \r\n            colour = \"white\"), legend.key.size = unit(1.2, \"lines\"), \r\n        legend.key.height = NULL, legend.key.width = NULL, legend.text = element_text(size = rel(0.8)), \r\n        legend.text.align = NULL, legend.title = element_text(hjust = 0), \r\n        legend.title.align = NULL, legend.position = \"right\", \r\n        legend.direction = NULL, legend.justification = \"center\", \r\n        legend.box = NULL, legend.box.margin = margin(0, 0, 0, \r\n            0, \"cm\"), legend.box.background = element_blank(), \r\n        legend.box.spacing = unit(2 * half_line, \"pt\"), panel.background = element_rect(fill = \"grey92\", \r\n            colour = NA), panel.border = element_blank(), panel.grid = element_line(colour = \"white\"), \r\n        panel.grid.minor = element_line(size = rel(0.5)), panel.spacing = unit(half_line, \r\n            \"pt\"), panel.spacing.x = NULL, panel.spacing.y = NULL, \r\n        panel.ontop = FALSE, strip.background = element_rect(fill = \"grey85\", \r\n            colour = NA), strip.text = element_text(colour = \"grey10\", \r\n            size = rel(0.8), margin = margin(0.8 * half_line, \r\n                0.8 * half_line, 0.8 * half_line, 0.8 * half_line)), \r\n        strip.text.x = NULL, strip.text.y = element_text(angle = -90), \r\n        strip.placement = \"inside\", strip.placement.x = NULL, \r\n        strip.placement.y = NULL, strip.switch.pad.grid = unit(half_line/2, \r\n            \"pt\"), strip.switch.pad.wrap = unit(half_line/2, \r\n            \"pt\"), plot.background = element_rect(colour = \"white\"), \r\n        plot.title = element_text(size = rel(1.2), hjust = 0, \r\n            vjust = 1, margin = margin(b = half_line)), plot.subtitle = element_text(hjust = 0, \r\n            vjust = 1, margin = margin(b = half_line)), plot.caption = element_text(size = rel(0.8), \r\n            hjust = 1, vjust = 1, margin = margin(t = half_line)), \r\n        plot.tag = element_text(size = rel(1.2), hjust = 0.5, \r\n            vjust = 0.5), plot.tag.position = \"topleft\", plot.margin = margin(half_line, \r\n            half_line, half_line, half_line), complete = TRUE)\r\n}\r\n<bytecode: 0x000000001dc91dd0>\r\n<environment: namespace:ggplot2>\r\n\r\nfunction (base_size = 12, base_family = \"\") \r\n{\r\n  theme(\r\n    line = element_line(colour = \"black\", size = 0.5, linetype = 1, lineend = \"butt\"), \r\n    rect = element_rect(fill = \"white\", colour = \"black\", size = 0.5, linetype = 1), \r\n    text = element_text(family = base_family, face = \"plain\", colour = \"black\", size = base_size, hjust = 0.5, vjust = 0.5, angle = 0, lineheight = 0.9), \r\n    \r\n    axis.text = element_text(size = rel(0.8), colour = \"grey50\"), \r\n    strip.text = element_text(size = rel(0.8)), \r\n    axis.line = element_blank(), \r\n    axis.text.x = element_text(vjust = 1), \r\n    axis.text.y = element_text(hjust = 1), \r\n    axis.ticks = element_line(colour = \"grey50\"), \r\n    axis.title.x = element_text(), \r\n    axis.title.y = element_text(angle = 90), \r\n    axis.ticks.length = unit(0.15, \"cm\"), \r\n    axis.ticks.margin = unit(0.1, \"cm\"), \r\n    \r\n    legend.background = element_rect(colour = NA), \r\n    legend.margin = unit(0.2, \"cm\"), \r\n    legend.key = element_rect(fill = \"grey95\", colour = \"white\"), \r\n    legend.key.size = unit(1.2, \"lines\"), \r\n    legend.key.height = NULL, \r\n    legend.key.width = NULL, \r\n    legend.text = element_text(size = rel(0.8)), \r\n    legend.text.align = NULL, \r\n    legend.title = element_text(size = rel(0.8), face = \"bold\", hjust = 0), \r\n    legend.title.align = NULL, \r\n    legend.position = \"right\", \r\n    legend.direction = NULL, \r\n    legend.justification = \"center\", \r\n    legend.box = NULL, \r\n\r\n    panel.background = element_rect(fill = \"grey90\", colour = NA), \r\n    panel.border = element_blank(), \r\n    panel.grid.major = element_line(colour = \"white\"), \r\n    panel.grid.minor = element_line(colour = \"grey95\", size = 0.25), \r\n    panel.margin = unit(0.25, \"lines\"), \r\n    panel.margin.x = NULL, \r\n    panel.margin.y = NULL, \r\n\r\n    strip.background = element_rect(fill = \"grey80\", colour = NA), \r\n    strip.text.x = element_text(), \r\n    strip.text.y = element_text(angle = -90), \r\n    \r\n    plot.background = element_rect(colour = \"white\"), \r\n    plot.title = element_text(size = rel(1.2)), \r\n    plot.margin = unit(c(1, 1, 0.5, 0.5), \"lines\"), complete = TRUE)\r\n}\r\n\r\nWerken met kleur\r\nBij categoriale variabelen kun je de kleur zelf aanpassen (scale_color_manual)\r\n\r\n\r\n\r\nPalet\r\nJe kunt voor categoriale variabelen ook een ‘’inbouw-palette’ gebruiken (gebaseerd op colorbrewer2.org) (scale_color_brewer):\r\n\r\n\r\n\r\nKleur keus bij continue variabelen\r\nBij continue variabelen kun je ook een kleurkeus gebruiken met een schaal (scale_color_gradient(), scale_color_gradient2())\r\n\r\n\r\n\r\nEenzelfde resultaat\r\n\r\n\r\n\r\nWerken met annotatie\r\nDe annotatie-tekst kun je overal in het figuur kwijt top-rechts, top-links etc. (annotation_custom() and textGrob())\r\n\r\n\r\n\r\nWerken met coordinaten\r\nDe figuur omgedraaid (coord_flip())\r\n\r\n\r\n\r\nWerken met verschillende typen plots\r\nAlternatieven voor de box plot (geom_jitter() and geom_violin()) Box plots kunnen goed zijn, maar ook vervelen. Hier wat alternatieven, eerst - een box plot en dan de rest:\r\n\r\n\r\n\r\nEen lint creeren\r\nEerst een lint maken (geom_ribbon())\r\n\r\n\r\n\r\nEen filter\r\nDan een filter eroverheen.\r\n\r\n\r\n\r\nEven wat anders\r\nHier een correlation plot (geom_tile()) Opgepast, de namen zijn gesorteerd zodat de ordening in de uiteindelijk plot goed is.\r\n\r\n\r\n         death dewpoint    o3 pm10  temp\r\ndeath        1    -0.47 -0.24 0.00 -0.49\r\ndewpoint    NA     1.00  0.45 0.33  0.96\r\no3          NA       NA  1.00 0.21  0.53\r\npm10        NA       NA    NA 1.00  0.37\r\ntemp        NA       NA    NA   NA  1.00\r\n\r\n       Var1     Var2 value\r\n1     death    death  1.00\r\n6     death dewpoint -0.47\r\n7  dewpoint dewpoint  1.00\r\n11    death       o3 -0.24\r\n12 dewpoint       o3  0.45\r\n13       o3       o3  1.00\r\n\r\n\r\nWerken met vegen (‘smooths’)\r\nHier de standaard (stat_smooth())\r\n\r\n\r\n\r\nFormule aanpassen\r\nJe kunt ook de formule aanpassen(stat_smooth(formula=))\r\n\r\n\r\n\r\nLiniaire lijn\r\nJe kunt ook een liniaire lijn trekken (stat_smooth(method=“lm”))\r\n\r\n\r\n\r\nReferenties\r\nZev Ross. Beautiful plotting in R: a ggplopt2 cheatsheet Zev Ross\r\nHarvard University. Introduction to R Graphics wiht ggplot2 Harvard\r\nDawn Koffman. Introduction to ggplot2 Office of Population Research, Princeton University. Princeton\r\nRStudio. Data visualisation with ggplot2. Cheatsheet. RStudio\r\nWickham, H. (2010). ggplot2: Elegant graphics for data analysis(Use R!). Houston: Rice University.\r\nChang, W. (2013).RGraphics Cookbook. a practical recipes for visualizing data Sebastopol: O’Reilly Media.\r\n",
    "preview": "posts/2018-11-14-visualisatie/visualisatie_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-05-05T14:08:58+02:00",
    "input_file": {}
  }
]
